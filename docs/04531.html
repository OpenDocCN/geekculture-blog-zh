<html>
<head>
<title>Scene Text Recognition Using ResNet and Transformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于ResNet和Transformer的场景文本识别</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/scene-text-recognition-using-resnet-and-transformer-c1f2dd0e69ae?source=collection_archive---------2-----------------------#2021-06-29">https://medium.com/geekculture/scene-text-recognition-using-resnet-and-transformer-c1f2dd0e69ae?source=collection_archive---------2-----------------------#2021-06-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/436381f07504f4b0facdd0fedb371259.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1WT_pPwjJLfyJ_bxR362tA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">source by Dipak</figcaption></figure><p id="5a4a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们遇到过许多不规则的裁剪过的图像，其中包含文本表示。已经引入了许多复杂的想法来从图像中提取文本。正如我所说的，光学字符识别(OCR)，基于RNN的seq2seq注意力方法被发现是从结构图像中提取序列信息的传统方法，但是许多研究人员发现，与不规则图像一起工作是非常困难的，并且训练时间使它们更加昂贵。基于RNN的seq2seq注意方法需要输入的序列表示，该序列表示随着输入的不同而变化，因此很难在数百万幅图像上进行训练。由于我们处理的是自然场景图像，因此大多数时候模型无法预测文本或字符。</p><p id="02da" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">基本上，如果我们选择任何一个模型，我们会发现它们都有一个共同点，那就是自我关注。它使模型能够通过位置对计算得出序列中不同位置之间的依赖关系。但是自我注意方法在单词序列中是有效的，在这种情况下，注意机制可以观察句子中的所有单词序列。在将图像翻译成文本的情况下，很难理解特征图，也很难创建依赖关系。简而言之，我将解释两个模型，这两个模型使用强大而复杂的方法来解决图像文本识别问题，以将二维CNN特征直接连接到由整体表示引导的基于注意力的序列编码器和解码器，并使用ResNet和Transformer的概念。</p><h1 id="f050" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">目录</strong></h1><blockquote class="kq kr ks"><p id="098f" class="iu iv kt iw b ix iy iz ja jb jc jd je ku jg jh ji kv jk jl jm kw jo jp jq jr hb bi translated"><em class="hi"> 1。商务问题<br/> 2。绩效指标<br/> 3。数据来源<br/> 4。探索性数据分析:<br/> 5。ResNet架构简介<br/> 6。变压器架构简介<br/> 7。型号:一个<br/> 8。型号:两个<br/> 9。未来工作<br/> 10。参考文献</em></p></blockquote><h1 id="79fd" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak"> <em class="kx"> 1。商业问题</em> </strong></h1><p id="7c25" class="pw-post-body-paragraph iu iv hi iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr hb bi translated">在现实世界中，大多数时候我们都会遇到不同形式的图像。它可以是规则的、不规则的图像以及其中的文本格式。从它们中提取字符串是一项具有挑战性的任务。因此，我们已经获得了5000个不规则和自然场景图像的数据集，业务问题是使用最先进的深度学习概念从这些图像中成功预测字符串。</p><h1 id="72fc" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">2.绩效指标:</h1><p id="ff17" class="pw-post-body-paragraph iu iv hi iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr hb bi translated">我们使用了一个定制的<strong class="iw hj"> <em class="kt">准确性度量</em> </strong>，它是给定预测字符串和真实字符串匹配的字符序列总数除以真实字符串中字符总数的比值。</p><h1 id="f7f7" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">3.数据源:</h1><ol class=""><li id="d0d1" class="ld le hi iw b ix ky jb kz jf lf jj lg jn lh jr li lj lk ll bi translated">IC Dar _ 2017 _ table _ dataset:<a class="ae lm" href="http://cvit.iiit.ac.in/research/projects/cvit-projects/the-iiit-5k-word-dataset" rel="noopener ugc nofollow" target="_blank">http://cvit . iiit . AC . in/research/projects/cvit-projects/the-iiit-5k-word-dataset</a></li></ol><p id="433a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们将该数据集用于研究目的。</p><p id="66f1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">数据集引用的详细信息:</p><pre class="ln lo lp lq fd lr ls lt lu aw lv bi"><span id="472e" class="lw jt hi ls b fi lx ly l lz ma">@InProceedings{MishraBMVC12,<br/>  author    = "Mishra, A. and Alahari, K. and Jawahar, C.~V.",<br/>  title     = "Scene Text Recognition using Higher Order Language Priors",<br/>  booktitle = "BMVC",<br/>  year      = "2012",<br/>}</span></pre><h1 id="fdd3" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">4.探索性数据分析</h1><p id="ce48" class="pw-post-body-paragraph iu iv hi iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr hb bi translated">我们使用的是IIIT 5000字的数据集，该数据集包含总共5000个文本图像及其相应的。mat格式。我们必须提取图像及其字符串。</p><p id="919e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下面显示了一些随机图像及其地面真实字符串字符</p><pre class="ln lo lp lq fd lr ls lt lu aw lv bi"><span id="d861" class="lw jt hi ls b fi lx ly l lz ma"><strong class="ls hj">#Displaying iamge with groundtruth string charcters<br/>for</strong> (batch, (inp, tar)) <strong class="ls hj">in</strong> enumerate(train_batches):<br/>  <strong class="ls hj">if</strong> batch == 3:<br/>    <strong class="ls hj">break</strong><br/>  plt.figure(figsize=(3, 3))        <br/>  plt.title('Image' )<br/>  plt.imshow(tf.keras.preprocessing.image.array_to_img(inp[0][0]))<br/>  print(str(tar))<br/>  plt.axis('off')<br/>  plt.show()</span></pre><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/a38a73433893a55f5f1cb4baba43d8ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*xN-litm8J8kNnJvTivVCgw.png"/></div></figure><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/fc909643c4e23791a2d7fdfcbcec22d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*B_sW0cms4kno_59jm_P1aQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Scouce-IIIT 5K</figcaption></figure><h1 id="2e9b" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak"> 5。ResNet架构简介</strong></h1><p id="d15a" class="pw-post-body-paragraph iu iv hi iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr hb bi translated">我们知道深度学习模型处理训练相当大量的隐藏层。最近的证据表明，更深的网络是非常重要的，并在ImageNet数据集上产生突出的结果。训练时间与我们使用的隐藏层数和激活类型成正比。所以训练更深层次的神经网络更加困难。在大型神经网络中，我们在反向传播时经常会遇到梯度消失的问题。</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/8d19c184d0e8b6b88945b2eb17d35838.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0wu3X_FUqR24jTrzqRVRVw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">comparison between large and small neural network — <a class="ae lm" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">(source link)</a></figcaption></figure><p id="9a1b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">正如我们所看到的，简单地堆叠层并不能减少训练误差，也不会导致模型过度拟合的问题。但是为了解决这个问题，我们可以在隐藏层之间添加一个中间归一化层，以解决收敛问题以及反向传播时的过拟合问题。</p><p id="5b74" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">那么问题就来了，如果我们可以用一个中间规范化层来解决梯度问题，为什么我们还需要ResNet概念？</p><p id="1fe3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">随着隐藏层的增加，训练错误会暴露出来，从而降低模型的性能。研究人员发现，退化与过度拟合无关，而只是由于添加了更多层，使得模型难以优化。因此，为了解决这个问题，ResNet在堆叠层的顶部引入了身份映射，这为梯度提供了清晰的网络，从而可以轻松地进行反向传播。</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es me"><img src="../Images/eed09ae1c43d83a76a653302f58350de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*jtTI285kPlsaA173nQoN0g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Bypassing identity mapping and adding with the residual network — <a class="ae lm" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">(source link)</a></figcaption></figure><p id="e73f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">F(x)定义堆叠层的输出，其大小可以是2层或更多层。然后快捷连接加上<strong class="iw hj"> <em class="kt"> relu激活</em> </strong>前的剩余输出。<a class="ae lm" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">该操作既不增加额外参数也不增加计算复杂性，且可以容易地帮助使用随机梯度下降(SGD)的反向传播</a>。通过这种机制，我们可以在不影响训练精度的情况下训练更深层次的神经网络。通过这种方式，加上“n”个堆叠在一起的层以及身份映射，创建了ResNet架构。</p><p id="e1e8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">剩余网络的身份映射的数学方程如下所示</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/2b5be9c668b609a90f20a9452ff63373.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*4GK4xAmbYhfQz12qpm-aQw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx"><a class="ae lm" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">source-link</a></figcaption></figure><p id="6461" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在上述函数<strong class="iw hj"> <em class="kt"> F(x，{Wi}) </em> </strong>中，表示要在整个堆叠层中学习的残差映射，而<strong class="iw hj"> <em class="kt"> x </em> </strong>是要添加残差的快捷连接<strong class="iw hj"> </strong>，条件是两者应该具有相同的维数。</p><p id="2896" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">还有另一种方式来解释这个概念，那就是“高速公路网”。这种机制有点类似于LSTM网络。在高速公路网络中，我们可以控制是否将信息量添加到下一层。它具有数据依赖性，并且具有在ResNet架构的情况下不具有的参数。但是性能方面的Resnet被发现更具适应性，并且可以解决退化问题。</p><p id="307d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">研究人员对普通网络和带有身份映射神经网络的残差网络进行了实验，发现即使增加额外的层数，ResNet模型的性能也更好。我们可以比较普通网络和残差网络，残差网络具有相同的参数数量、深度、宽度和计算成本，但发现其给出的结果优于ResNet。</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/57806324aba62f565cb25356caeb3094.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qtNLQrxIbB2sNshhhz4_HA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">comparison between plain network and ResNet of different types — <a class="ae lm" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">(source -link)</a></figcaption></figure><p id="e39c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">有不同类型的ResNet。一些例子是ResNet 32、ResNet 50、ResNet 101等。它们之间的共同区别是堆叠层内的层数和彼此相加的堆叠层数。</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/b29ee560468ae109a1a609d2f48effa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fnb1n2iya-3VaJoDy4Xrdw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Types of ResNet architecture — <a class="ae lm" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">(source -link)</a></figcaption></figure><p id="49e7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">现在的问题是为什么我们需要ResNet架构而不是VGG预训练来进行特征提取？</strong></p><p id="2780" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">由于我们知道深度学习网络非常深，这可能需要高计算能力，并且随着网络越来越深，模型经历过拟合和增加训练误差的机会更高。在图像到文本的任务中，我们需要一个网络，它可以更深入，但计算成本不高，并提供更好的精度增益。ResNet战胜了ImageNet检测、ImageNet本地化、COCO检测和COCO分段挑战。</p><p id="b857" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">ResNet的独特之处在于，即使增加了层数，它的复杂度仍低于VGG-16/19。</p><h1 id="9286" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">6.变压器架构简介</h1><p id="aa23" class="pw-post-body-paragraph iu iv hi iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr hb bi translated">在变形金刚进化之前，序列模式是用一种叫做RNN网络的概念来训练的。但是RNN在记忆长单词序列的过去信息方面非常失败，因此无法预测接下来的连续单词。为了解决这个问题，引入了具有内部遗忘门和加法门的长短期记忆(LSTM)。遗忘门仅允许来自先前时间步骤的部分信息传递到下一个时间步骤，添加门允许来自当前时间步骤的信息量与先前信息的部分相加。这个概念可以通过额外添加注意力概念来解决单词之间的长期依赖性。它仍然不能捕捉到大句子的依存关系，比如说1000个单词的句子。另外，我们知道句子的长度因句而异，所以训练时间也因句而异。因为在反向传播梯度时，我们必须为每个输入句子展开LSTM网络，并在每个时间步长计算梯度，因此这导致很长的训练时间。</p><p id="fcca" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，为了解决所有这些问题，研究人员提出了强大而简单的网络架构，<strong class="iw hj">“变压器”</strong>，这是一种基于注意力的机制，具有与递归模型相同的特征。最重要的是，我们可以在可行的时间内对训练应用并行化。</p><p id="eb93" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">名为<strong class="iw hj">“注意力是你所需要的一切”</strong>的研究论文引入了名为“自我注意力”的概念，这种概念可以浏览整个输入句子，并创建单词依赖，甚至对长句也很有效。注意机制在完成阅读理解、机器翻译、提问和回答建模等任务时是成功的。它是一种简单的循环注意机制，具有端到端的记忆网络。它不需要序列比对的RNNs或卷积网络，但给出了更好的结果。</p><p id="26c6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">完整的变压器架构如下所示</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/6cf5548222c2b4a19b5f960ffd139f2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*r4ToT27gU0kb45zcEFN4Gw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">The Transformer architecture — <a class="ae lm" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">(source-link)</a></figcaption></figure><p id="dcd5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">不要被上面的架构吓到。我将把这个架构分成几个部分，并对每个内部部分给出一个简单的解释。</p><p id="bb1f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">整个架构分为编码器和解码器两部分。左半部分是<strong class="iw hj">【编码器】</strong>，右半部分是<strong class="iw hj">【解码器】</strong>。</p><p id="af9a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">编码器:</strong>它<strong class="iw hj"> </strong>有N个堆叠的相同层，其中N可以是超参数。它分为两部分，即多头机构和位置前馈网络。对于每个叠加层，来自位置编码的输入向量并行通过多头和快捷连接，多头的输出加上快捷连接，然后进行层归一化。然后，输出将通过前馈网络，该网络独立且相同地应用于每个位置。残差网络被引入到每个子层中，以便在反向传播时易于收敛。</p><p id="23d2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">解码器:- </strong>也是N个堆叠的相同层，其中N可以是超参数。它分为三部分，即掩蔽多头机构、2D多头机构和位置式前馈网络。对于每个叠加层，来自位置编码的输入向量并行通过屏蔽多头和快捷连接，屏蔽多头的输出加上快捷连接，然后进行层归一化。然后，输出将通过下一个2D多头关注，在这里，来自编码器层的输出也被引入。然后，输出将通过前馈网络，该网络被分别且相同地应用于每个位置。残差网络被引入到每个子层中，以便在反向传播时易于收敛。</p><p id="a063" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们按时间顺序讨论整个架构。为简单起见，我们假设一个编码器和一个解码器层。</p><p id="f7b4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">不像RNN模型，我们按顺序传递输入单词，我们不需要做同样的程序。我们将一次传递整个一个句子或一批句子，然后进行单词嵌入。</p><p id="59b8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">单词嵌入</strong>将为每个边学习边训练的单词分配一个“d”维向量。为了确保每个单词都按顺序排列，嵌入层的输出将经过位置编码。</p><p id="e298" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">位置编码</strong>确保每个单词都在其位置上。它管理输入句子或一批句子的顺序模式。</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/2ab771a6c6be8837216402750b3cef5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*DR8EGRPcij0FYRUhcZxpTA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Visualization of positional encoding —<a class="ae lm" href="https://www.tensorflow.org/text/tutorials/transformer" rel="noopener ugc nofollow" target="_blank"> (source-link)</a></figcaption></figure><p id="0af9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">x轴是单词位置，y轴是每个单词的512维。如果我们放大上面的图片，我们会发现每个单词的位置都各不相同。位置编码的输出输入到多头注意和快捷连接。</p><p id="e33e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">多头注意力</strong>是“m”头注意力机制，其中m是超参数。在研究论文中，他们使用了8个定标的点积注意，该注意在内部为每个单词给出8个512维向量，并且来自每个定标的点积的结果被连接，并且经历具有((8 * 512) * k)维权重矩阵的点积。这些权重通过反向传播来学习。</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/05ece13a1dd5185ff0c637c2f9219cfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LYzA5Sh-N3dENzYo-O2rvw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx"><a class="ae lm" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">Source — link</a></figcaption></figure><p id="0ed6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">把多头注意力想象成一个内部有8个标度的点积注意力的函数，它需要的参数是3个向量。3矢量只是前一层的输出，所有三个矢量都是相同的。它们被称为查询、键和值。比例点积注意力的输出是</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/36fdba2fa40be5c68542244aeb4106cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*qNHTwJSpeAx4zGUKqtbYIA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx"><a class="ae lm" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">Source — link</a></figcaption></figure><p id="cfdd" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">详细解释可以参考<a class="ae lm" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">杰伊·阿拉玛的</a>博客。</p><p id="a849" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">多头注意力的输出将添加一个快捷连接，然后进行图层归一化。然后，它通过逐位置前馈网络，接着是层归一化，因此它是来自1个编码器的最终输出。</p><p id="d344" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在让我们来谈谈解码器，</p><p id="06d5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">与RNN不同，我们将解码器输入一次发送到单词嵌入层。这是一种<strong class="iw hj">教学力</strong>技术，这意味着我们从softmax获得的输出不会反馈到解码器，而是假设它已经预测了正确的序列并要求预测下一个单词序列的模型。它允许模型快速训练并且具有更少的计算成本。</p><p id="e375" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">解码器</strong>层有几乎相同的子层，除了一个额外的多头关注。第一个注意层是屏蔽的多头注意，其中<strong class="iw hj">屏蔽的</strong>指的是预测屏蔽，这意味着它限制单词序列预测该单词，因为我们必须预测下一个单词。编码器的输出将反馈给第二个多头注意，其余过程保持不变。解码器的输出通过最后一个2D密集层，然后是一个大小等于vocab大小的softmax层。</p><p id="8264" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我使用了两个模型来成功提取字符串。我将详细讨论这两种模型。</p><h1 id="96d4" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak"> 7。型号:一个</strong></h1><p id="cc00" class="pw-post-body-paragraph iu iv hi iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr hb bi translated"><strong class="iw hj">ResNet作为编码器和Transformer作为解码器相结合的架构简述:</strong></p><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/c0cd3006718dbabd94310627f8f2d480.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*s_Qn4VHVkaF-bnp2bqJeaw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Model 1 architecture —<a class="ae lm" href="https://arxiv.org/pdf/1904.01375.pdf" rel="noopener ugc nofollow" target="_blank"> (source -link)</a></figcaption></figure><p id="b82e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">整个架构分为两个部分。左半部分是编码器，右半部分是解码器。</p><p id="f354" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们首先进入编码器的细节。</p><h1 id="4a17" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">编码器:</strong></h1><p id="f372" class="pw-post-body-paragraph iu iv hi iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr hb bi translated">ResNet type 34用作特征映射和特征提取机制。修改后的ResNet34输出三维特征图。在我的实验中，我尝试使用改进的ResNet50用于更深层次的网络，与ResNet34相比，它可以给出更好的结果。特征映射进一步同时通过两个网络，即(1 * 1) conv层和瓶颈。来自(1 * 1) conv层的输出馈入解码器子层，即第二多重注意机制，并将其作为查询和关键向量。</p><p id="9750" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在论文中，研究人员使用了六个堆叠的正常ResNet34作为具有剩余连接的瓶颈。最后一个堆栈瓶颈的输出进一步通过平均池，然后是大小为512的完全连接的密集层。来自密集层的输出是二维的，其被视为输入图像的文字嵌入。</p><h1 id="1828" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">解码器:</h1><p id="cbd1" class="pw-post-body-paragraph iu iv hi iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr hb bi translated">解码器嵌入层的输入是字符串。输入字符串是字符标记化的，带有附加的<strong class="iw hj"> ' &lt;结尾&gt; ' </strong>作为字符串的结尾。<strong class="iw hj"> </strong>我没有使用<strong class="iw hj"> ' &lt; start &gt; ' </strong>，因为来自编码器最后一个密集层的输出在位置编码后被引入字符嵌入，作为字符串的开始。在论文中，他们将密集层图像字嵌入的编码器输出与位置编码连接在一起，但我在连接后进行了位置编码，以确保图像字嵌入首先出现，并作为<strong class="iw hj"> ' &lt; start &gt; ' </strong>索引。</p><p id="2f03" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">来自前一层的输出被馈送到掩蔽的多层注意力模型，随后通过添加残差网络进行层归一化。掩码与前瞻掩码相关联。然后，输出与来自特征映射的输出一起被馈送到二维注意层，随后通过添加残差网络进行层标准化。来自层归一化的输出被馈送到位置式前馈网络，随后通过添加残差网络进行层归一化，并最终通过softmax激活的2维密集层。</p><h1 id="9351" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">实验:</h1><p id="896d" class="pw-post-body-paragraph iu iv hi iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr hb bi translated">我用修改过的ResNet50试过上面的架构，用普通的ResNet50试过瓶颈。从最后一个瓶颈层出来的输出，接着是平均池，被重新整形为二维，然后传递到大小为512的密集层。我还尝试使用一个自定义的学习率和一个等于4000的步长，以及Adam作为优化器。我还尝试了<strong class="iw hj">光束搜索</strong>来预测更好的输出。我用232个时期训练了这个模型，发现该模型预测准确率为87%,损失减少到0.0903。</p><p id="8107" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下面是关于第一个模型架构的代码。</p><figure class="ln lo lp lq fd ij"><div class="bz dy l di"><div class="mn mo l"/></div></figure><p id="2c98" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">预测样本如下所示:</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/81226e5aecb5d747c9afe7d84112948c.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*mPizuppksOHiO-1F1dtV8Q.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">source-IIIT 5K</figcaption></figure><p id="cd17" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">而相应的关注度图如下图所示:</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mq"><img src="../Images/83fb9c57e73c8211ed0f2f18e499df4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eW63yiwERVtDDt08BdjwPw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">source by Dipak</figcaption></figure><h1 id="39b8" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">8.型号:两个</h1><p id="784e" class="pw-post-body-paragraph iu iv hi iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr hb bi translated"><strong class="iw hj">简要说明ResNet101作为变压器编码器的输入和变压器作为解码器的组合架构:</strong></p><figure class="ln lo lp lq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mr"><img src="../Images/172631b4288976c0d291bf776a4b312b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*083joGULJUX-MQSYt_kxbw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Model 2 architecture -<a class="ae lm" href="https://neurohive.io/en/news/transformer-model-used-to-recognize-text-in-images/" rel="noopener ugc nofollow" target="_blank">(source — link)</a></figcaption></figure><p id="dd82" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们在第一个模型中已经看到，ResNet被视为编码器，而transformer被视为解码器。第二个模型的模型架构完全不同。这里，ResNet用于特征图提取，来自图像字嵌入的输出被输入到编码器变换器。除此之外，一切都和我们在变压器架构基础中讨论的一样。</p><p id="7305" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">术语部分结果101指的是瓶颈模型，其可以进一步减少到期望的层，从而我们得到三维卷积特征图。它被进一步整形为一个2维特征图，随后是一个完全连接的2维密集层。最终输出被视为输入到编码器层的每个图像的字嵌入。我们使用4层堆叠的编码器和解码器以及8头多头注意力机制。</p><p id="b614" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这里，我也尝试使用一个自定义的学习率，一个温暖的步骤等于4000，以及亚当作为一个优化器。我还尝试了<strong class="iw hj">光束搜索</strong>来预测更好的输出。我已经用500个时期训练了这个模型，并且发现该模型以51%的准确度预测，损失减少到0.37，这意味着与第一个模型相比，它不能预测。</p><p id="9077" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下面是关于第一个模型架构的代码。</p><figure class="ln lo lp lq fd ij"><div class="bz dy l di"><div class="mn mo l"/></div></figure><p id="59c6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">预测样本如下所示:</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/82982887b0122a7cf1474fe3b55ad725.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*tho6x-qISir2P5ZYmqw2Zg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">source — IIIT 5K</figcaption></figure><p id="e3f8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">而相应的关注度图如下图所示:</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/768ea776743ec36a052491d6171e75bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*mXa8iQJf7SEi4Npb7bThog.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">source by Dipak</figcaption></figure><p id="80f4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我只展示了前两个结果。有关详细信息，您可以查看GitHub资源库。</p><h1 id="1fa0" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak"> 9。未来工作:</strong></h1><p id="5be8" class="pw-post-body-paragraph iu iv hi iw b ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn lc jp jq jr hb bi translated">我想尝试不同的ResNet架构，并运行更多时代的模型。我没有做过我想尝试的数据扩充。</p><h1 id="5283" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">10.参考:</h1><ol class=""><li id="1641" class="ld le hi iw b ix ky jb kz jf lf jj lg jn lh jr li lj lk ll bi translated"><a class="ae lm" href="https://www.tensorflow.org/text/tutorials/transformer" rel="noopener ugc nofollow" target="_blank">用于语言理解的变压器模型</a></li><li id="14e0" class="ld le hi iw b ix mt jb mu jf mv jj mw jn mx jr li lj lk ll bi translated"><a class="ae lm" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的一切</a></li><li id="c3ef" class="ld le hi iw b ix mt jb mu jf mv jj mw jn mx jr li lj lk ll bi translated"><a class="ae lm" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">图示变压器</a></li><li id="b2fe" class="ld le hi iw b ix mt jb mu jf mv jj mw jn mx jr li lj lk ll bi translated"><a class="ae lm" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">用于图像识别的深度残差学习</a></li><li id="6f65" class="ld le hi iw b ix mt jb mu jf mv jj mw jn mx jr li lj lk ll bi translated"><a class="ae lm" href="https://arxiv.org/pdf/1904.01375.pdf" rel="noopener ugc nofollow" target="_blank">用于场景文本识别的整体表征引导的注意网络</a></li><li id="e24d" class="ld le hi iw b ix mt jb mu jf mv jj mw jn mx jr li lj lk ll bi translated"><a class="ae lm" href="https://neurohive.io/en/news/transformer-model-used-to-recognize-text-in-images/" rel="noopener ugc nofollow" target="_blank">用于识别图像中文本的变压器型号</a></li></ol><p id="ad38" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">你可以从下面我的GitHub链接中查看所有细节:</strong></p><div class="my mz ez fb na nb"><a href="https://github.com/tiwaridipak103/Scene-Text-Recognition" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hj fi z dy ng ea eb nh ed ef hh bi translated">tiwaridipak 103/场景-文本-识别</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">通过在GitHub上创建帐户，为tiwaridipak 103/场景文本识别开发做出贡献。</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">github.com</p></div></div><div class="nk l"><div class="nl l nm nn no nk np io nb"/></div></div></a></div><p id="c5ec" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">我的LinkedIn </strong>:</p><div class="my mz ez fb na nb"><a href="https://www.linkedin.com/in/dipak-kr-tiwari/" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hj fi z dy ng ea eb nh ed ef hh bi translated">迪帕克Kr。Tiwari -学生应用人工智能课程| LinkedIn</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">在一个可以增长知识、提高技能的组织中工作。此外，为…提供机会</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">www.linkedin.com</p></div></div><div class="nk l"><div class="nq l nm nn no nk np io nb"/></div></div></a></div></div></div>    
</body>
</html>
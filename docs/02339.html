<html>
<head>
<title>BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伯特</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/bert-842e956b8c0f?source=collection_archive---------15-----------------------#2021-05-11">https://medium.com/geekculture/bert-842e956b8c0f?source=collection_archive---------15-----------------------#2021-05-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/1ff29e125f43bff1c9dee48f6b129e55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/0*Y7TTBqjoJERYan9p.jpg"/></div><figcaption class="im in et er es io ip bd b be z dx">Source:<a class="ae iq" href="https://seofutura.com/what-is-new-with-google-bert-update/" rel="noopener ugc nofollow" target="_blank">https://seofutura.com/what-is-new-with-google-bert-update/</a></figcaption></figure><blockquote class="ir"><p id="84c1" class="is it hi bd iu iv iw ix iy iz ja jb dx translated">据估计，在2025年，全球范围内使用NLP将产生43289.9美元的收入。</p></blockquote><h2 id="9a38" class="jc jd hi bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">自然语言处理中的BERT是什么？</h2><p id="dbd8" class="pw-post-body-paragraph ka kb hi kc b kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt jb hb bi translated"><strong class="kc hj"> BERT(来自变形金刚的双向编码器表示)是谷歌AI语言研究人员发表的论文。</strong>它在各种各样的NLP任务中展示了最先进的结果，包括问题回答、自然语言推理等，在机器学习社区中引起了轰动。</p><p id="1cea" class="pw-post-body-paragraph ka kb hi kc b kd ku kf kg kh kv kj kk jn kw km kn jr kx kp kq jv ky ks kt jb hb bi translated">BERT的关键技术创新是将Transformer(一种流行的注意力模型)的双向训练应用于语言建模。这与以前从左到右或者结合从左到右和从右到左训练来查看文本序列的努力形成对比。</p><p id="5ebc" class="pw-post-body-paragraph ka kb hi kc b kd ku kf kg kh kv kj kk jn kw km kn jr kx kp kq jv ky ks kt jb hb bi translated">论文的研究结果表明，双向训练的语言模型比单向训练的语言模型具有更深的语境感和语流感。在论文中，研究人员详细介绍了一种名为<strong class="kc hj">掩蔽LM (MLM) </strong>的新技术，这种技术允许在模型中进行双向训练，这在以前是不可能的。</p><h2 id="b472" class="jc jd hi bd je jf kz jh ji jj la jl jm jn lb jp jq jr lc jt ju jv ld jx jy jz bi translated">伯特是如何工作的？？</h2><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es le"><img src="../Images/e438305af41d393b21fb7644c26095a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*x6aqL9d1rP7cAyBI.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx">Source: Towards Data Science</figcaption></figure><p id="86e1" class="pw-post-body-paragraph ka kb hi kc b kd ku kf kg kh kv kj kk jn kw km kn jr kx kp kq jv ky ks kt jb hb bi translated"><strong class="kc hj">伯特利用了一种转换器，这是一种学习文本中单词之间上下文关系的注意力机制</strong>。一般来说，Transformer包括两个独立的机制——一个读取文本输入的编码器和一个为任务生成预测的解码器。由于BERT的目标是生成一个语言模型，所以只有编码器机制是必要的。</p><p id="6b98" class="pw-post-body-paragraph ka kb hi kc b kd ku kf kg kh kv kj kk jn kw km kn jr kx kp kq jv ky ks kt jb hb bi translated">与顺序读取文本输入的方向模型相反，Transformer编码器一次读取整个单词序列。因此，它被认为是双向的，虽然说它是非定向的会更准确。这一特性允许模型基于单词的所有环境来学习单词的上下文。</p><p id="f341" class="pw-post-body-paragraph ka kb hi kc b kd ku kf kg kh kv kj kk jn kw km kn jr kx kp kq jv ky ks kt jb hb bi translated">输入是一系列标记，这些标记首先被嵌入到向量中，然后在神经网络中进行处理。输出是大小为H的向量序列，其中每个向量对应于一个具有相同索引的输入令牌。</p><p id="5bd6" class="pw-post-body-paragraph ka kb hi kc b kd ku kf kg kh kv kj kk jn kw km kn jr kx kp kq jv ky ks kt jb hb bi translated">当训练语言模型时，存在定义预测目标的挑战。许多模型预测序列中的下一个单词，这是一种方向性的方法，固有地限制了上下文学习。为了克服这个挑战，<strong class="kc hj"> BERT使用了屏蔽LM </strong>:</p><h2 id="c568" class="jc jd hi bd je jf kz jh ji jj la jl jm jn lb jp jq jr lc jt ju jv ld jx jy jz bi translated"><strong class="ak">蒙面LM (MLM) </strong></h2><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ln"><img src="../Images/4518cd15163e668f4e0ddcd7709582a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5D7KSGLKYldpJ6IU9O5SMA.jpeg"/></div></div><figcaption class="im in et er es io ip bd b be z dx">Source: Towards Data Science</figcaption></figure><p id="0a17" class="pw-post-body-paragraph ka kb hi kc b kd ku kf kg kh kv kj kk jn kw km kn jr kx kp kq jv ky ks kt jb hb bi translated"><strong class="kc hj">在将单词序列输入BERT之前，每个序列中有15%的单词被替换为一个【掩码】标记。</strong>然后，该模型试图根据序列中其他未屏蔽单词提供的上下文来预测屏蔽单词的原始值。在技术术语中，输出字的预测需要:</p><p id="d6de" class="pw-post-body-paragraph ka kb hi kc b kd ku kf kg kh kv kj kk jn kw km kn jr kx kp kq jv ky ks kt jb hb bi translated">在编码器输出之上添加分类层。<br/>将输出向量乘以嵌入矩阵，将其转换到词汇维度。<br/>用SoftMax计算词汇中每个词的概率。</p><blockquote class="lo lp lq"><p id="2dce" class="ka kb lr kc b kd ku kf kg kh kv kj kk ls kw km kn lt kx kp kq lu ky ks kt jb hb bi translated"><em class="hi">“计算机快得不可思议，准确得不可思议，愚蠢得不可思议；人类慢得不可思议，不准确又聪明；他们在一起的力量超乎想象。”——阿尔伯特·爱因斯坦</em></p></blockquote></div></div>    
</body>
</html>
<html>
<head>
<title>X-Means — A Complement to the K-Means Clustering Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">x-Means-K-Means聚类算法的补充</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/x-means-algorithm-a-complement-to-the-k-means-algorithm-b087ae88cf88?source=collection_archive---------9-----------------------#2021-05-27">https://medium.com/geekculture/x-means-algorithm-a-complement-to-the-k-means-algorithm-b087ae88cf88?source=collection_archive---------9-----------------------#2021-05-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d0bb012e46c16db6ff52731162789367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qu9vtRFFrQ1_4jj4Nu__BQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Image Source: <a class="ae iu" href="https://www.pinterest.com/" rel="noopener ugc nofollow" target="_blank">https://www.pinterest.com</a></figcaption></figure><h1 id="958f" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">介绍</h1><p id="fcf1" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">聚类是将数据分组到几个簇或组中的过程，因此一个簇中的数据具有最大的相似性，而簇之间的数据具有最小的相似性。k-均值(杜达&amp;哈特，1973；Bishop，1995)一直是度量数据的主力。它的吸引力在于它的简单性，以及它的局部最小收敛性质。聚类的中心或质心是K-Means算法中聚类组的起点。数据是通过计算到初始聚类中心点的最近距离作为每个组或聚类形成中的中心点来完成的。</p><h1 id="15da" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">K-均值的弱点和X-均值的必要性</h1><p id="1d1c" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">但是在这种情况下，初始聚类中心点的确定是K-Means算法的弱点。这是因为没有用于选择和确定聚类中心点的方法。从一组数据中任意或随机选择聚类中心点。K-Means算法的聚类结果通常不是最优的，并且在每个进行的实验中都不是最优的。所以可以说聚类结果的好坏取决于聚类的中心点或者初始质心。</p><p id="564a" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">整体而言，K-means受到以下限制:</p><ol class=""><li id="c6c8" class="kw kx hi jv b jw kr ka ks ke ky ki kz km la kq lb lc ld le bi translated">K-means很慢，并且相对于完成每次迭代所花费的时间，它的伸缩性很差。</li><li id="79ca" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">簇的数量“K”必须由用户预先确定和提供。</li><li id="3252" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">当局限于以固定的K值运行时，它会根据经验找到比动态改变K值时更差的局部最优解。</li></ol><p id="0f70" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">前两个问题的解决方案和第三个问题的部分补救方法是X-means。这种有效估计星团数量的方法是由美国匹兹堡卡内基梅隆大学的Dan Pelleg和Andrew Moore提出的。</p><h1 id="28e6" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">X均值的解释</h1><blockquote class="lk ll lm"><p id="3339" class="jt ju ln jv b jw kr jy jz ka ks kc kd lo kt kg kh lp ku kk kl lq kv ko kp kq hb bi translated">在每次运行K-means后，X-means开始起作用，对当前质心的哪个子集应该分裂以获得更好的拟合做出局部决定。分裂决策是通过计算贝叶斯信息标准(BIC)来完成的。</p></blockquote><p id="487d" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">X-Means通过交替应用两种操作来工作——K-Means算法(改进参数)来针对选定的K值最佳地检测聚类，以及聚类分裂(改进结构)来根据信息标准优化K值。在这种方法中，K的实际值是以一种不被监控并且仅基于数据集的方式来估计的。<em class="ln"> Kmax </em>和<em class="ln"> Kmin </em>作为X可能取值的上下限，在X-means分组的第一步，知道目前X = <em class="ln"> Xmin </em>，X-means找到初始结构和质心。在下一步中，估计结构中的每个聚类被视为父聚类，父聚类可以被分成两组。</p><h1 id="d8db" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">用Python实现</h1><p id="7b77" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">有一个强大而令人惊叹的库可以实现X-means来确定最佳的集群数量—<a class="ae iu" href="https://pypi.org/project/pyclustering/" rel="noopener ugc nofollow" target="_blank">py clustering . cluster . X means</a></p><p id="4236" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">PyClustering库实现了K-Means++初始化算法，该算法在最佳中心位置的已知范围内选择初始中心。PyClustering的文档显示了如何在那个包中调用K++初始化。K++初始化(及其更具可伸缩性的对应物K||)随机选择彼此远离的初始点。他们随机均匀地选取初始对象，然后以与其距离成比例的概率从已经选择的对象中选择下一个点。</p><p id="a67a" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在实施时，我们需要给它一个初始的中心列表。我们可以选择以下初始中心之一:</p><ol class=""><li id="f2de" class="kw kx hi jv b jw kr ka ks ke ky ki kz km la kq lb lc ld le bi translated">两个随机的物体。</li><li id="bc1a" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">两个最远的物体。</li><li id="8da1" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">一个随机的物体和它最远的邻居。</li><li id="34f4" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">沿着PCA的第一特征向量的第一个和最后一个对象。</li></ol><h1 id="22d8" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">改进的余地</h1><p id="6084" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">X-means的一个主要问题是，它假设数据具有相同的球面高斯分布。因此，它倾向于过度拟合椭圆聚类中的数据，或者具有不同聚类大小的数据的输入集中的数据。G-Means和PG-Means算法试图通过将数据投影到一维上，并运行统计拟合优度测试来解决这个问题。这种方法可以为非球形分布带来更好的性能，但是，投影可能无法为所有数据集提供最佳效果。投影可以将许多聚类的数据折叠在一起，忽略密度的差异。这需要多次预测以确保准确性。</p><p id="5500" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我希望这篇文章能让你深入了解X-Means算法，它是K-Means算法的一个增强。</p><p id="5f8b" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">感谢阅读！注意安全！</p></div></div>    
</body>
</html>
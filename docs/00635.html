<html>
<head>
<title>Build Your Own Custom Road Damage Detector</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">建立您自己的自定义道路损坏检测器</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/build-your-own-custom-road-damage-detector-ddcfcc405771?source=collection_archive---------17-----------------------#2021-03-08">https://medium.com/geekculture/build-your-own-custom-road-damage-detector-ddcfcc405771?source=collection_archive---------17-----------------------#2021-03-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="10b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇博客中，我解释了我工作的点点滴滴，同时在一个自定义数据集(RDD)上工作，执行对象检测任务。</p><p id="24c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以首先让我们从数据集开始，这是<strong class="ih hj">道路损坏检测</strong>数据集。每个人都知道道路在我们生活中的重要性。这些不仅对交通和旅行至关重要，而且对许多事情都至关重要，我们都使用道路来完成我们的日常事务。但是维护道路同样重要。这些道路维护会消耗大量的资金，要么来自当地市政当局，要么来自政府。这笔钱可以用来调查损失，调查车辆，承包商，劳工和其他许多事情。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/011fadb98fe03d43894546dac29089aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E6370gxkBBi1p7_tuWajGA.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Source : <a class="ae jt" href="https://wallpaperaccess.com/beautiful-roads" rel="noopener ugc nofollow" target="_blank">https://wallpaperaccess.com/beautiful-roads</a></figcaption></figure><p id="0511" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于道路损坏调查至关重要，因此我们需要了解当地管理机构(市政当局)检查损坏的技术。为了调查损坏情况，有三种主要的技术。</p><ul class=""><li id="fe19" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated"><strong class="ih hj">手动方法— </strong>这种方法被认为是传统方法。在手动<strong class="ih hj"> </strong>方法中，一组调查人员通过步行或坐在缓慢行驶的车辆上来检测损伤。这种视觉检查受到调查者主观判断的影响，并且这一过程非常耗时且不安全。</li><li id="286c" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><strong class="ih hj">半自动方法— </strong>该方法也被认为是传统方法。在半自动<strong class="ih hj"> </strong>方法中，使用快速移动的车辆来拍摄损坏的照片，但是对于重要的损坏，仍然需要人工检查。但是与手动方法相比，这种方法最终是相当安全的，但不是完全安全的。</li><li id="f751" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><strong class="ih hj">全自动方法— </strong>该方法是检查损伤的现代方法。<strong class="ih hj"> </strong>在全自动<strong class="ih hj"> </strong>方法中，一辆快速移动的车辆配备了精密而昂贵的传感器。然后通过图像处理和模式识别对采集的图像进行处理。但是这种方法太贵了，因为要使用高质量的传感器和摄像头。所以财力薄弱的地方机构负担不起。<strong class="ih hj">那么接下来是什么。</strong></li></ul><p id="83f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在是时候理解我们试图解决的<strong class="ih hj">问题，</strong>了，记住一件事，我们必须最小化成本。因此，其中一个很好的想法是使用<strong class="ih hj">深度学习</strong>模型对道路图像进行训练，然后预测损坏类型。我们也可以想一想另一件事，我们可以使用智能手机来拍摄道路图像，而不是使用昂贵的相机来最小化成本。这项技术是在<a class="ae jt" href="http://scholar.google.co.in/scholar_url?url=https://www.researchgate.net/profile/Toshikazu_Seto/publication/326087983_Road_Damage_Detection_and_Classification_Using_Deep_Neural_Networks_with_Smartphone_Images_Road_damage_detection_and_classification/links/5c51051aa6fdccd6b5d3381e/Road-Damage-Detection-and-Classification-Using-Deep-Neural-Networks-with-Smartphone-Images-Road-damage-detection-and-classification.pdf&amp;hl=en&amp;sa=X&amp;ei=rl_jX5iEGY7gygSm-KCACw&amp;scisig=AAGBfm3hRxmGM0MksjxOlQOKro5PhZmBPg&amp;nossl=1&amp;oi=scholarr" rel="noopener ugc nofollow" target="_blank">的这篇</a>研究论文上提出的。</p><p id="88e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该想法是将智能手机安装在汽车的仪表板上，然后以40公里/小时的速度行驶车辆(在研究论文中提到)。这款智能手机配备了一个应用程序，可以每秒钟捕捉一幅图像。所以最终我们必须建立一个<strong class="ih hj">实时物体检测器</strong>，它甚至可以为移动设备工作。因为速度是至关重要的，所以YOLO模型可能会工作得很好。对于这个问题，我用的是<strong class="ih hj"> YOLOv3 </strong>型号。在进一步深入之前，我们先非常快速地了解一下数据。</p><h1 id="8a09" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">数据:</h1><p id="27b4" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">我正在使用的<strong class="ih hj">数据</strong>也被称为<strong class="ih hj"> RDD2020 </strong>，可以从<a class="ae jt" href="https://mycityreport.s3-ap-northeast-1.amazonaws.com/02_RoadDamageDataset/public_data/IEEE_bigdata_RDD2020/train.tar.gz" rel="noopener ugc nofollow" target="_blank">这个</a>链接下载。在此数据集之前，还有RDD2019和RDD2018可用。prior RDD和RDD2020之间的主要区别是—</p><ul class=""><li id="15a1" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">RDD2020包含3个国家(印度、日本和捷克共和国)的图像，而以前的版本只包含日本的图像。</li><li id="f35c" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">在RDD2020中提到了4种损坏类型，但在之前的RDD中提到了8种损坏类型。下图描述了RDD2020的4种损坏类型—</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ll"><img src="../Images/d103636d416a288df1dfb3dc233b73c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*FXdqqRu8jeVLiHXrq8_T-A.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Damage types in RDD2020</figcaption></figure><p id="1090" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们从一个快速的EDA部分开始——</p><h1 id="dc16" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">EDA:</h1><p id="9e6d" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">让我们从可视化一些带有损伤类型的图像开始，只是为了理解损伤类型及其外观。</p><div class="je jf jg jh fd ab cb"><figure class="lm ji ln lo lp lq lr paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><img src="../Images/b150cc35ae2d96163912232deb6dea9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*RcWkdTeKmTv4jTP10-bjMw.png"/></div></figure><figure class="lm ji ls lo lp lq lr paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><img src="../Images/3dd5b7536a12e86578884612e307c3b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*1QUlmHJuiA-QFzbqpOxR_g.png"/></div></figure></div><div class="ab cb"><figure class="lm ji lt lo lp lq lr paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><img src="../Images/afe11701045c8a6d2c80752c5b708555.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*Lmvc8vnG3_o-6SjElb1Rlg.png"/></div></figure><figure class="lm ji lu lo lp lq lr paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><img src="../Images/60ebc001bc2eeba0e8551b1b5d8eb4b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*uUd9D-65Mh8zwwLfJJ-Smw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx lv di lw lx">The 4 damage types mentioned in RDD2020</figcaption></figure></div><p id="6083" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们来看看损伤类型在数据集中的分布。为此，我使用了<strong class="ih hj">计数图</strong>和百分比。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ly"><img src="../Images/4c10c12bcc557f9cae634344b5d8970d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*qThsQ7aDY5m7IqZX3pfTUg.png"/></div></figure><p id="1e4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的图中我们可以很容易地看到，没有太多的不平衡，我们有来自所有损坏类型的数据集的大量数据。</p><p id="7bdc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里要注意的一点是，图像不包含边界框，这些边界框信息在一个单独的文件中。包含图像对应的包围盒信息的文件可以用<strong class="ih hj"> COCO </strong>格式或者<strong class="ih hj"> PASCAL VOC </strong>格式表示，通常这些格式被称为<strong class="ih hj">注解</strong>。实际上两种格式是一样的，不同的是，COCO格式看起来像典型的“json”或python字典，而PASCAL VOC格式看起来像xml文件。对于RDD问题，他们提供了PASCAL VOC格式。并且两种表示都呈现在下面。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="3e5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于数据集RDD2020只是先前RDD2019和RDD2018的扩展，因此它还包含与RDD2020中未提及的损坏类型(如D01、D43、D44、D50等)相对应的图像和边界框注释。所以我们可以很容易地在预处理步骤中删除它们。</p><h1 id="f9ae" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">预处理:</h1><p id="b252" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">首先我们必须创建一个pandas dataframe，它只包含两列<em class="mb">图像</em>和<em class="mb">注释</em>。在创建它之前，我们需要确保在每一行中，我们有相同的文件名和xml文件名。为此，我们可以使用以下代码片段。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="ebd4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">运行上面的代码片段后，我们可以得到如下所示的数据帧，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mc"><img src="../Images/dba62f7d37061a498f51f65c8b104f77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*kXrkMxU6IyPpNlBC8S_Qew.png"/></div></figure><p id="e7c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于进一步的<strong class="ih hj">预处理</strong>，我在解释这个文件中<a class="ae jt" href="https://github.com/rahulchamoli916/RDD2020/blob/main/Dataset_Preparation.ipynb" rel="noopener ugc nofollow" target="_blank">提到的步骤。因此，作为一个良好的预处理步骤，我们可以首先删除图像和相应的xml文件，其中损坏类型不是D00，D10，D20和D40。为了删除不必要的图片和它们的xml文件，我使用了下面的代码片段。</a></p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="d697" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以最初我们有25046张图片，其中有12851张图片的类别不是我们想要的。因此，我们可以使用它们的索引轻松地将它们从数据帧中删除。</p><p id="6520" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">移除12851张图片后，我们只剩下12195张图片和注释。但问题是，这些注释也可能包含我们不想要的类(例如，除了损坏类型D00、D10、D20和D40之外，还有可能是一些损坏类型，如D43、D44、D50等。也存在于xml文件中)。所以我们需要从xml文件中删除这些类细节。在下面的xml文件中，有3个对象，其中两个是D40，一个是D50。所以我们想通过从xml文件中删除D50来保留两个D40对象。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="08cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了执行这个预处理步骤，我们可以使用下面提到的代码片段—</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="2484" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是我们问题的解决方案，对于预处理步骤，现在我们需要为YOLOv3模型准备数据。但首先要快速了解YOLOv3的功能。</p><h1 id="f7f5" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">YOLO v3:</h1><p id="c670" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">如上所述，我们必须建立一个<strong class="ih hj">实时对象检测器</strong>，它可以非常快速地检测损坏。这就是我们使用YOLOv3的原因。Joseph Redmon等人已经介绍了YOLOv3的概念，关于YOLOv3的所有细节都可以在他的<a class="ae jt" href="https://pjreddie.com/darknet/yolo/" rel="noopener ugc nofollow" target="_blank">网站</a>上获得。如果你想知道我是如何在自定义RDD数据集上训练YOLOv3的，你可以跳过这一节。</p><p id="ff8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就速度而言，YOLOv3比以前的对象检测模型快得多，但它的mAP(平均精度)也可以与一些最先进的对象检测模型相媲美。正如我们在下图中看到的—</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es md"><img src="../Images/1cb210e66dedac781088e35b043a25b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CYhC4eEU02lvkyFf5-4UFg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Source : <a class="ae jt" href="https://pjreddie.com/darknet/yolo/" rel="noopener ugc nofollow" target="_blank">https://pjreddie.com/darknet/yolo/</a></figcaption></figure><p id="1dc6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们看一下YOLOv3–608的性能，那么它的mAP@IoU0.5可以与最先进的FPN·FRCN模型相媲美，但yolov 3花费的时间比FPN·FRCN模型少得多。所以现在让我们深入到YOLOv3本身，它使用了哪种革命性的思想。</p><ul class=""><li id="1b7d" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated"><strong class="ih hj">特征提取器— </strong>最初他们为YOLOv2提供了<strong class="ih hj"> darknet-19 </strong>作为特征提取器，但他们没有为YOLOv3使用它，而是提出了新的特征提取器，称为<strong class="ih hj"> darknet-53 </strong>。darknet-53的架构是—</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es me"><img src="../Images/2d7b04438f35d616b383a3b8bd993c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*zQf8WYhCXjt2ueFT2Yugjg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx">darknet — 53 architecture</figcaption></figure><p id="833e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">darknet-53是完全卷积网络，没有任何池。它包含<strong class="ih hj"> 53个卷积层</strong>、<em class="mb">，每个卷积层之后是批量归一化层和泄漏relu激活</em>。并在ImageNet数据集上进行了预训练。实际上，创造者也将darknet-53的性能与现有的特征提取器进行了比较。结果显示在下图中—</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mf"><img src="../Images/6880febe70d849e0c6cb33f458cb958c.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*TYZ9BzzGJt8isojxJM8I-Q.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx">darknet-53 performance</figcaption></figure><p id="7c02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上表中,“Top-1”(Top-1分类器的准确度)和“Top-5”(Top-5分类器的准确度)用于表示在ImageNet任务上的<strong class="ih hj">性能，我们可以看到所有特征提取器在ImageNet任务上的性能是相等的。Bn Ops(预测时需要数十亿次运算模型)比darknet-19多两倍，但不如ResNet模型快。BFLOP/s '(每秒十亿次浮点运算)darknet-53是一个明显的赢家。与ResNet模型相比，FPS(每秒帧数)darknet-53更快，但不等同于darknet-19。综上所述，作者选择了<strong class="ih hj"> darknet-53作为YOLOv3 </strong>的特征提取器。</strong></p><p id="9540" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有3种不同的图像尺寸，我们可以传给YOLOv3型号，320X320，416X416，608X608。如果我将416X416的图像作为darknet-53的输入，那么在全局平均池层之前，输出由13X13X1024表示，<strong class="ih hj">为什么？</strong> <em class="mb">由于我们在卷积层上有5倍步幅为2，所以可以简单的用2⁵除416，所以就变成了416/32=13，1024是最后一个卷积层拥有的滤波器个数。</em></p><p id="246c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在它可以把整个416X416的图像划分成13X13的网格，每个网格都可以包含包围盒信息，但是怎么存储包围盒信息呢？那是一个谜…这并不神秘。</p><p id="61a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个13*13的网格的深度是1024，所以它没有使用整个深度。相反，它使用85维向量来存储单个边界框。在这个85维向量中，前4维表示<strong class="ih hj">边界框坐标</strong> (tx，ty，tw和th)。第五维度代表<strong class="ih hj">物体存在性得分</strong>，表示物体是否存在(潜在概率)。最后，剩下的80个维度对应于COCO数据集中提到的<strong class="ih hj">类(同样是潜在概率)。正如我们在下图中看到的—</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mg"><img src="../Images/5a740ba9abf5b78f0780bc12268c22b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*5aAwON5YMxbI3d1mi37NHw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Source : <a class="ae jt" href="https://pylessons.com/YOLOv3-introduction/" rel="noopener ugc nofollow" target="_blank">https://pylessons.com/YOLOv3-introduction/</a></figcaption></figure><p id="9fa2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是根据这篇论文，他们在每个13X13的网格上使用了3个边框细节。为什么每个网格有多个边界框？ <em class="mb">因为每个网格可以容纳多个对象，所以他们使用了多个边界框的概念</em>。由于85个维度用于表示单个边界框，我们需要3个这样的框，因此我们需要85*3=255个维度。最初，我们从darknet-53(在移除全局平均池层之后)得到的输出是13X13X1024，因此我们可以使用1X1卷积和255个过滤器将13X13X1024转换为13X13X255。</p><p id="f5c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最初，他们试图直接预测图像的边界框，但是这些预测的边界框对于对象检测来说看起来并不好。为了解决这个问题，他们想出了一个非常巧妙的主意，被称为<strong class="ih hj">锚盒</strong>。</p><ul class=""><li id="e6cb" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated"><strong class="ih hj">锚框— </strong>被认为是预定义的框，模型在其上轻微移动以到达正确的预测框。通过使用锚盒，它们到达使用下述数学公式的预测盒——</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mh"><img src="../Images/e247baab0d0293968309351530dc0ef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*8JpnWReGKXlc-sn8236YHQ.png"/></div></figure><p id="a4fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上述公式中，tx、ty、tw和th是原始边界框坐标，cx、cy、pw和ph是锚框坐标。最后，bx，by，bw和bh是预测的边界框坐标。现在，另一个有趣的问题可能是— <strong class="ih hj">他们是如何产生5个边界框的想法的？</strong> <em class="mb">为此，他们从COCO数据集中获取训练数据，为它们预测边界框，然后在预测的边界框上应用</em> <strong class="ih hj"> <em class="mb"> K-Means聚类</em></strong><em class="mb"/><strong class="ih hj"><em class="mb">K = 5</em></strong><em class="mb">。这5个边界框获得对象的可能性非常高。</em></p><ul class=""><li id="733a" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated"><strong class="ih hj">每类Sigmoid — </strong>如我们所知，对于416X416大小的输入图像，来自darknet-53(在移除全局平均池层和SoftMax分类器之后)的结果是13X13X1024。所以YOLO可以把整个图像分成13X13的网格。并且每个13×13的网格用于表示一个85维的边界框信息。如下所示—</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mi"><img src="../Images/7fa8f04d65f701f595cf5e65268877f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*Vf16Aod9QlyXqNZVmy9tVA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx">85 dimensional vector corresponding to a single bounding box</figcaption></figure><p id="efd6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们看<strong class="ih hj"> Po </strong>，它代表客体性得分(概率)，也就是说客体是否存在。所以我们可以把它看作是一个二元分类，因此我们可以使用sigmoid函数进行分类。所以到目前为止，这是好的，但接下来的问题开始为其余80维向量…</p><p id="fc54" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在看<strong class="ih hj">类分数</strong> (P1到Pc，其中c=80)，分别对应COCO数据集中提到的各自的类。对于YOLOv2，他们使用SoftMax分类器来了解潜在的概率。但让我们先来看一个场景，我们有一个女性的图像，并假设COCO类中的C5代表“女性”, COCO类中的C10代表“人”,因此通过使用SoftMax，它可以为女性或个人实现更高的概率，但不能同时为两者。但是直觉上，<em class="mb">女人就是人，对吧？</em>因此，在YOLOv3中，他们没有使用SoftMax分类器来了解潜在的概率，而是对每个类使用sigmoid函数，这种想法被称为<strong class="ih hj">每类Sigmoid </strong>。通过使用per-class sigmoid YOLOv3可以为女性和个人实现更高的概率。</p><ul class=""><li id="48a4" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated"><strong class="ih hj">多尺度预测— </strong> YOLO v3跨3个不同尺度进行预测。检测层用于在三个不同大小的特征图上进行检测，分别具有跨度32、16、8。这意味着，使用416 x 416的输入，我们可以在13X13、26X26和52X52的比例上进行检测。事实上，YOLO的作者想到了这个想法，可以轻松地探测到更小的物体，只是因为他们从之前的版本中得到了抱怨。所以问题是，这个巧妙的想法是如何运作的？这很简单…</li></ul><p id="9290" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先模型预测52×52网格的边界框(3个框)，然后将网格向下采样到26×26，再次预测边界框(3个框)，再次将网格向下采样到13×13，以预测最终的边界框(3个框)。所以最终，我们将在每个单元/网格上有3*3=9个边界框。</p><ul class=""><li id="a2fc" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated"><strong class="ih hj">过滤— </strong>对于416X416大小的输入图像，YOLOv3预测((52X52) + (26*26) + (13X13)) * 3 =总共10647个框。但是一幅图像怎么可能包含这么多物体呢，对吧？这就是为什么YOLOv3需要<strong class="ih hj">过滤</strong>的原因。</li></ul><p id="f6ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">过滤意味着它将找出边界框的IoU，对于IoU大于0.5(阈值)的边界框，它只取那些并丢弃其余的。但是仍然可能存在许多边界框。那么如何去除呢？为此，YOLOv3使用了下述技术——</p><ul class=""><li id="69ac" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">非最大抑制-可能有多个框覆盖同一个对象。所以YOLOv3型号，必须将它们移除。为此，它找到覆盖同一对象的所有边界框的IoU，并只取一个具有最高IoU的边界框。如下图所示——</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mj"><img src="../Images/2ea2b9c0b20ec7d3bb66b2a2d75e0ff0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N2IkemoYAKLD1pINHsDOzw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">NMF</figcaption></figure><p id="d2b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对YOLOv3来说，有一个术语可能有些读者不知道，那就是IoU，它代表并上的<strong class="ih hj">交集，如下图所示</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mk"><img src="../Images/3da89ac38e98086630304e8b9cc1a0a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*6jh2R_ZVhtwoZmineJdTBQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx">IoU</figcaption></figure><h1 id="a976" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">在RDD数据集上训练YOLOv3模型:</h1><p id="bb9c" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">YOLO模型首先以适当的方式期望数据集，这意味着它必须在pandas数据帧中包含6个字段(图像名称、xmin、ymin、xmax、ymax和类标签)。为此，我们需要所有的xml文件。然后我们可以用下面的片段—</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="cf91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">结果是一个熊猫数据帧，看起来和下面提到的一模一样</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ml"><img src="../Images/e141f27c622a9b46d2d95e5d2a1c9977.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*Gyrr5JM5fBnqf9U2JYttzQ.png"/></div></figure><p id="d043" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在保存这个熊猫数据帧，对于我的例子，我使用了名字<strong class="ih hj"/></p><p id="dfdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为我在google colab中使用了Keras实现，所以我跟踪了这个<a class="ae jt" href="https://github.com/AntonMu/TrainYourOwnYOLO" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>。我在这里解释的步骤可以在我的笔记本里找到。要在YOLOv3上训练RDD数据集，可以使用<a class="ae jt" href="https://github.com/rahulchamoli916/RDD2020/tree/main/YOLO/YOLOv3" rel="noopener ugc nofollow" target="_blank">这个</a>文件夹。获得这个文件夹(YOLOv3)后，目录结构将如下所示—</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mm"><img src="../Images/2cfdff5293488e7b812bfe76b31cacb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*b-6meWVwotT45pii_EQWpw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Directory Structure for YOLO</figcaption></figure><blockquote class="mn mo mp"><p id="b6e8" class="if ig mb ih b ii ij ik il im in io ip mq ir is it mr iv iw ix ms iz ja jb jc hb bi translated">在继续之前，请确保使用Tensorflow版本2.3.1和keras版本2.4.3</p></blockquote><p id="171c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们看看目录结构，大部分内容都是一样的，正如我在<a class="ae jt" href="https://github.com/AntonMu/TrainYourOwnYOLO" rel="noopener ugc nofollow" target="_blank">这个</a> GitHub仓库中提到的。有两件重要的事情需要改变。首先，在Data→Source _ Images→Training_Images下，复制所有训练图像，并创建一个文件夹“files_to_train”(在Training _ Images中只包含Annotation-export.csv)。其次，在Data → Source_Images → Test_Images下，复制你的所有测试图像。其余一切保持不变。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="15af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的代码片段获取Annotation-export.csv(从数据→源_图像→训练_图像→文件_到_训练)并将其转换为YOLO格式，通过生成两个文件，一个是data_train.txt(在数据→源_图像→训练_图像→文件_到_训练中)，第二个是data_classes.txt(在数据→模型_权重中)。</p><p id="6a46" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，是下载预训练的YOLOv3重量，并将其转换为keras格式。为此，预训练模型权重在ImageNet数据集上进行预训练。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="77b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">运行上面的代码片段后，我们可以训练我们的模型，为此我们只需运行下面的代码片段。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="090f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这将运行到批次大小为32的11个时期，并且再次运行到批次大小为4的11个时期。结合时间将需要大约略多于7个小时在谷歌Colab与特斯拉T4 GPU。然后保存的模型将存储为trained_weight_final.h5，它将保存在Data → Model_Weights下。训练到此结束…下一步…推论。</p><h1 id="1e47" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">对图像的测试(推断):</h1><p id="48e5" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">现在是时候在测试图像上运行我们训练过的YOLOv3模型了。为此，我使用了下面的片段。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="2e3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的代码片段将获取测试图像(来自数据→源_图像→测试_图像)和保存的模型权重(来自数据→模型_权重),并尝试检测测试图像的边界框，然后将结果图像存储到数据→源_图像→测试_图像_检测_结果。现在让我们看看模型工作良好的一些结果，以及模型失败(不完全)的一些情况。</p><h1 id="10ec" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">结果:</h1><ul class=""><li id="2a1b" class="ju jv hi ih b ii lg im lh iq mt iu mu iy mv jc jz ka kb kc bi translated"><strong class="ih hj">井预测案例— </strong></li></ul><div class="je jf jg jh fd ab cb"><figure class="lm ji mw lo lp lq lr paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><img src="../Images/8a46bb61cb8af5c3f986bc7b33c74675.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*aOC2kEc6Ow497MmRKe_DsQ.png"/></div></figure><figure class="lm ji mx lo lp lq lr paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><img src="../Images/88b4e0e96c37073a55cba4ae87254ea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*afzkyJC9VQfq7P58Lj7eZg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx lv di lw lx">Test images for which model worked well</figcaption></figure></div><ul class=""><li id="0519" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated"><strong class="ih hj">不良预测案例— </strong></li></ul><div class="je jf jg jh fd ab cb"><figure class="lm ji my lo lp lq lr paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><img src="../Images/60cee73557f5b37755a997db7eecdfe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*VAbFZQk5a88MK5K7UajGsA.png"/></div></figure><figure class="lm ji mz lo lp lq lr paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><img src="../Images/920fa54c8d78dea06e9538c01d3948b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*VraXssGBrx_V2HuWIQ3cBg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx lv di lw lx">Test images for which model doesn’t worked well</figcaption></figure></div><h1 id="11e8" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">数据管道:</h1><p id="6ff0" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">数据管道对于部署我们的模型至关重要。通过创建管道，我们知道代码中的哪些片段对于测试图像的预测是有用的。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="86ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们谈一谈与这个项目有关的一些<strong class="ih hj">未来工作</strong>，首先在一个像样的系统上训练更多时代的模型，现在YOLOv3不是最先进的，所以我们可以尝试YOLOv4，YOLOv5来训练相同的数据集。</p><h1 id="73f8" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">参考资料:</h1><ul class=""><li id="fb43" class="ju jv hi ih b ii lg im lh iq mt iu mu iy mv jc jz ka kb kc bi translated">约洛夫3的原始研究论文—<a class="ae jt" href="https://arxiv.org/pdf/1804.02767" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1804.02767</a></li><li id="38d7" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">数据—<a class="ae jt" href="https://github.com/sekilab/RoadDamageDetector/" rel="noopener ugc nofollow" target="_blank">https://github.com/sekilab/RoadDamageDetector/</a></li><li id="50b1" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">Redmon的网站—<a class="ae jt" href="https://pjreddie.com/darknet/yolo/" rel="noopener ugc nofollow" target="_blank">https://pjreddie.com/darknet/yolo/</a></li><li id="31c3" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">前田等人——http://scholar.google.co.in/scholar_url?URL = https://arxiv . org/pdf/1801.09454&amp;HL = en&amp;sa = X&amp;ei = qokaypvib 4 tsyqstxkyybq&amp;sci SIG = aagbfm 3 b 0 dnk ZJ 8 co 0 vvv7 owhjjjqxz 0773 q&amp;nossl = 1&amp;oi = schol arr</li><li id="6696" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">艾莉亚等人——https://arxiv.org/pdf/2008.13101</li><li id="ec4a" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><a class="ae jt" href="https://pylessons.com/YOLOv3-introduction/" rel="noopener ugc nofollow" target="_blank">https://pylessons.com/YOLOv3-introduction/</a></li><li id="e2ae" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><a class="ae jt" href="https://github.com/asetkn/Tutorial-Image-and-Multiple-Bounding-Boxes-Augmentation-for-Deep-Learning-in-4-Steps" rel="noopener ugc nofollow" target="_blank">https://github . com/aset kn/Tutorial-Image-and-multi-Bounding-Boxes-Augmentation-for-Deep-Learning-in-4-Steps</a></li><li id="9e4d" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">https://github.com/AntonMu/TrainYourOwnYOLO<a class="ae jt" href="https://github.com/AntonMu/TrainYourOwnYOLO" rel="noopener ugc nofollow" target="_blank"/></li></ul><p id="1335" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你想看完整的代码，那就访问这个GitHub库——</p><div class="na nb ez fb nc nd"><a href="https://github.com/rahulchamoli916/RDD2020" rel="noopener  ugc nofollow" target="_blank"><div class="ne ab dw"><div class="nf ab ng cl cj nh"><h2 class="bd hj fi z dy ni ea eb nj ed ef hh bi translated">rahulchamoli916/RDD2020</h2><div class="nk l"><h3 class="bd b fi z dy ni ea eb nj ed ef dx translated">完整的解释，请访问我的博客…</h3></div><div class="nl l"><p class="bd b fp z dy ni ea eb nj ed ef dx translated">github.com</p></div></div><div class="nm l"><div class="nn l no np nq nm nr jn nd"/></div></div></a></div><p id="0b9a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你想在LinkedIn上联系我，这是我的个人资料</p><div class="na nb ez fb nc nd"><a href="https://www.linkedin.com/in/rahul-chamoli-076b7b14b" rel="noopener  ugc nofollow" target="_blank"><div class="ne ab dw"><div class="nf ab ng cl cj nh"><h2 class="bd hj fi z dy ni ea eb nj ed ef hh bi translated">Rahul Chamoli -新德里，德里，印度|职业简介| LinkedIn</h2><div class="nk l"><h3 class="bd b fi z dy ni ea eb nj ed ef dx translated">查看拉胡尔·查莫利在全球最大的职业社区LinkedIn上的个人资料。拉胡尔有两个工作列在他们的…</h3></div><div class="nl l"><p class="bd b fp z dy ni ea eb nj ed ef dx translated">www.linkedin.com</p></div></div><div class="nm l"><div class="ns l no np nq nm nr jn nd"/></div></div></a></div></div></div>    
</body>
</html>
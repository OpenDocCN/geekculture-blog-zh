<html>
<head>
<title>AI and Calculus: The Vanishing Gradient</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能和微积分:消失的梯度</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/ai-and-calculus-the-vanishing-gradient-927a46646154?source=collection_archive---------7-----------------------#2022-05-03">https://medium.com/geekculture/ai-and-calculus-the-vanishing-gradient-927a46646154?source=collection_archive---------7-----------------------#2022-05-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="ced5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">简介:</strong></p><p id="cd8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在微积分课上学过基本的导数、积分等。在2018年AP微积分BC考试中，我们不得不写一个方程来求解一个土豆在时间t分钟的内部温度的切线……绝对是微积分的一大用处(<em class="jd">笑</em>)。微积分有很多应用，其中一个很大的应用就是用在人工智能上。让我们来看看关键的微积分概念，并将其应用到现实世界中！</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/b5c6c7df2bfcdf36017aa02967ad9aaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*Gv8H7SRBE7MWDWgN8LQ2Tg.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Image 1: Famous AP Calc Potato Problem, from: <a class="ae jq" href="https://www.reddit.com/r/MemeEconomy/comments/6a90jg/quick_buy_ap_calc_bc_potato_memes_before_its_too/" rel="noopener ugc nofollow" target="_blank">Reddit</a></figcaption></figure><p id="57b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">人工智能与微积分:</strong></p><p id="3de8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降是一种寻找函数的局部最小值和最大值的算法。神经网络(NN)梯度使用反向传播，它通过模型的各个层，使用链规则找到初始层的导数(或误差)。神经网络采用权重和偏差参数，并使用梯度下降算法更新这些值。该算法用于人工智能/人工智能、工程和工业领域。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jr"><img src="../Images/5602d0a403785fbd316c19a1e2bd4c6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*sy6P_dYyCFDaAEINQWnS0w.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Image 2: Demonstrating Backpropagation, from: <a class="ae jq" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">Matt Mazur</a></figcaption></figure><p id="9041" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">消失渐变问题:</strong></p><p id="7a57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用反向传播，随着更多层的倍增，梯度呈指数下降并接近0。这不会改变初始层的权重和偏差，只会改变外层。如果导数输出小于1，它将收敛得更快，但不是很快。这使得我们很难训练模型，因为初始层没有更新，即使在最佳纪元编号下也产生低精度验证。</p><p id="4fbc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">芝诺悖论:</strong></p><p id="98f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">芝诺悖论指出</p><blockquote class="js jt ju"><p id="328b" class="if ig jd ih b ii ij ik il im in io ip jv ir is it jw iv iw ix jx iz ja jb jc hb bi translated">“如果一个人想从A点旅行到B点，他必须先在有限的时间内走完这段距离的一半。此后，他们必须在另一段有限的时间内再走完剩余距离的一半。然后，他们必须再次走完剩余距离的一半，以此类推。通过不断地将剩余的距离减半，这个人将会走无限长的距离，并且仍然稍微远离他们的最终目的地”(<a class="ae jq" href="https://www.futurescienceleaders.com/blog/2021/03/zenos-dichotomy-paradox/#:~:text=Once%20upon%20a%20time%2C%20an,a%20finite%20amount%20of%20time." rel="noopener ugc nofollow" target="_blank">迈克尔·莫</a>)</p></blockquote><p id="e09c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">消失梯度问题是芝诺悖论在现实生活中的表现，因为两种情况都试图接近渐近线，但永远不会达到它。</p><p id="c3df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">微积分:</strong></p><p id="c223" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不要害怕微积分！对我们写梯度下降算法是一个助力。现在让我们学习将在梯度下降算法中使用的关键微积分概念。然后，我们将探索如何找到一个令人讨厌的梯度发散的解决方案。</p><p id="dd84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降只对可微且上凹的函数有效。该算法通过获取当前点的梯度来寻找下一个点，然后应用某个足够小的比率来引起收敛，以从当前点增加或减去该值，最终目标是最大化或最小化该函数。这是很多话，看看下面的伪代码来了解它是如何工作的！</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jy"><img src="../Images/98f019d9c46fffb507f7095172d2ebb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/0*eUtBnaDsse4iwErJ"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Image 3: of the Gradient Descent Algorithm, from: <a class="ae jq" href="https://stats.stackexchange.com/questions/166575/what-happens-when-i-use-gradient-descent-over-a-zero-slope" rel="noopener ugc nofollow" target="_blank">Stack Exchange</a></figcaption></figure><p id="2bea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">坡度</em></p><p id="cb6a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">多元函数的梯度是导数的向量。我们将使用偏导数，因为它给出了函数在某个方向(特定轴)上某一点的导数。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jz"><img src="../Images/7616303109ecfa0ae62505aba501c4d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/0*8p6V8kJdZvFGuY_7"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Image 4: Gradient, from: Stack Exchange</figcaption></figure><p id="7a55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">可微</em></p><p id="0b62" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">函数是连续的，左边的导数等于右边的导数。为了使函数连续，函数在左右两边的极限必须等于该点的值。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es ka"><img src="../Images/9ded5c3d7cb050be85c443cda75aadc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z0GLOUMLUs581gre9l9v_A.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx">Image 5: Differentiable, from: Author</figcaption></figure><p id="3057" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">凹度</em></p><p id="5bf5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了确保我们的函数是上凹的，我们必须对函数求二阶导数，并检查它是否大于0。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kf"><img src="../Images/727dbfbfd05ce86fbef89e9e60aeadda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*VDT1HP0c_aRIQcq6dBf-6w.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Image 6: Concavity, from: Author</figcaption></figure><p id="6419" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们的函数是上凹的</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kg"><img src="../Images/3a72c1f30e65bac11dbf67d3c5a339f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/0*U17XDlwWCt0rywbX"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Image 7: Domain, from: Author</figcaption></figure><p id="a917" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">偏导数</em></p><p id="1de0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降算法在方程中使用偏导数，所以我们来看看什么是偏导数。对于多元函数，偏导数只能找到其中一个独立变量的变化率。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kh"><img src="../Images/4714ffcbba1abecd59daa27dca7feebc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*xBzBUiW12atWQMHHVfIHfA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Image 8: Example of PD , from: Author</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ki"><img src="../Images/1b7634c49abd153d00f24684d8f1ab89.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*DNSJM6XtUwXqeFQMk7O9eg.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Image 9: PD in Algorithm, from: <a class="ae jq" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">Matt Mazur</a></figcaption></figure><p id="8471" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">消失渐变问题的解决方案:</strong></p><p id="f9e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">既然我们已经了解了梯度下降算法如何使用微积分概念，我们仍然需要讨论消失梯度的解决方案。我们要关注的一个解决方案是整流线性单元(ReLU)。</p><p id="e304" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ReLU是一个非饱和激活函数，用于创建AI模型，输出范围为0到正无穷大。ReLU允许导数为0或1。值1没有帮助进一步最小化偏导数。这允许整体更大的导数值，这有助于确保当我们将许多层相乘时梯度不会发散。不幸的是，当你得到一个连续的数字0作为输出时，这意味着神经元已经死亡:死亡。除了ReLU之外，还有许多其他解决方案有助于解决消失梯度问题，如LReLU、残差网络和批量归一化。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kj"><img src="../Images/c2c9283a6182273d2e56fff3ef3a6876.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*roUc4rSII0k8vM_DhjG-Vg.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Image 9: ReLU Graph, from: <a class="ae jq" href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">Machine Learning Mastery</a></figcaption></figure><p id="8e40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在来看看我用ReLU构建模型的一些代码。下面的代码是一个卷积神经网络(CNN)模型。这比我们之前讨论的NN更复杂。与具有多个隐藏层的NN不同，CNN具有宽度、高度和深度参数的三维。ReLU用于代码第3行的隐藏层，帮助解决渐变消失的问题。您可以看到还使用了其他激活函数，如SoftMax它在输出图层中用于输出标注类的概率分布。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es kk"><img src="../Images/eddb4bbc88921bc82b6977b5ffd028bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p0G6Q5AmPHuI-vdwUvWiDA.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx">Image 10: Code of AI Model, from: Author</figcaption></figure><p id="8539" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是在将街道图像的一些数据输入到上面的模型中之后的输出，我们发现验证精度大约为85.17%。调整隐藏层和所使用的激活函数的类型将有助于提高最佳纪元编号的最终验证精度。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es kl"><img src="../Images/9f016cc41585262d734a8b5a1b2f1940.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BD3YxdXWQo70VHDJ-V1L1g.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx">Image 11: Output of AI Model, from: Author</figcaption></figure><p id="e33c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">结论:</strong></p><p id="2842" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文对算法中涉及的梯度下降和微积分做了深入的分析。我们还讨论了如何在NN/CNN和不同的解决方案中使用它来提高最佳历元值的验证准确性。</p><p id="2878" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我在创建各种人工智能模型后写了这篇文章，并注意到一些阻碍验证准确性的东西。我进一步研究了是什么导致了一个降低的值，并且遇到了渐变消失的问题。消失梯度和AI/ML通常在数学中很重要，这起初令人生畏，但在学习了一些基本的微积分概念后(在我的老师Wernau先生的帮助下)，你可以开始看到它们是如何联系在一起的。你在课堂上学到的东西被积极地应用到现实世界中，这是非常令人着迷的！:)</p><p id="debd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">继续学习:</em></p><div class="km kn ez fb ko kp"><a href="https://www.youtube.com/channel/UCUpxD3z4-CDdHXHjnKCY2Jg" rel="noopener  ugc nofollow" target="_blank"><div class="kq ab dw"><div class="kr ab ks cl cj kt"><h2 class="bd hj fi z dy ku ea eb kv ed ef hh bi translated">托马斯·韦诺</h2><div class="kw l"><h3 class="bd b fi z dy ku ea eb kv ed ef dx translated">与朋友、家人和全世界分享您的视频</h3></div><div class="kx l"><p class="bd b fp z dy ku ea eb kv ed ef dx translated">www.youtube.com</p></div></div><div class="ky l"><div class="kz l la lb lc ky ld jk kp"/></div></div></a></div><div class="km kn ez fb ko kp"><a href="https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/" rel="noopener  ugc nofollow" target="_blank"><div class="kq ab dw"><div class="kr ab ks cl cj kt"><h2 class="bd hj fi z dy ku ea eb kv ed ef hh bi translated">深度神经网络中的消失梯度和爆炸梯度</h2><div class="kw l"><h3 class="bd b fi z dy ku ea eb kv ed ef dx translated">这篇文章是作为数据科学博客网络和可用于…</h3></div><div class="kx l"><p class="bd b fp z dy ku ea eb kv ed ef dx translated">www.analyticsvidhya.com</p></div></div><div class="ky l"><div class="le l la lb lc ky ld jk kp"/></div></div></a></div><div class="km kn ez fb ko kp"><a href="https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21" rel="noopener follow" target="_blank"><div class="kq ab dw"><div class="kr ab ks cl cj kt"><h2 class="bd hj fi z dy ku ea eb kv ed ef hh bi translated">梯度下降算法——深度探索</h2><div class="kw l"><h3 class="bd b fi z dy ku ea eb kv ed ef dx translated">梯度下降法为机器学习和深度学习技术奠定了基础。</h3></div><div class="kx l"><p class="bd b fp z dy ku ea eb kv ed ef dx translated">towardsdatascience.com</p></div></div><div class="ky l"><div class="lf l la lb lc ky ld jk kp"/></div></div></a></div></div></div>    
</body>
</html>
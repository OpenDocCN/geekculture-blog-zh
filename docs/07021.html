<html>
<head>
<title>Batch Renormalization-Why and How?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">批量重整化-为什么和如何？</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/batch-renormalization-why-and-how-46ebbdb2ae96?source=collection_archive---------8-----------------------#2021-09-05">https://medium.com/geekculture/batch-renormalization-why-and-how-46ebbdb2ae96?source=collection_archive---------8-----------------------#2021-09-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="bcfd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">名为<a class="ae jd" href="https://arxiv.org/abs/1702.03275" rel="noopener ugc nofollow" target="_blank">批次重整化:减少批次标准化模型中的小批次依赖性</a>的论文摘要。</p><p id="1848" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在继续讨论批处理重整化时，我假设您非常熟悉批处理规范化(BN)。它如何帮助更快地收敛到手头问题的最优解？如果没有，请阅读<a class="ae jd" rel="noopener" href="/geekculture/batchnormalization-a-technique-that-enhances-training-5d44966c22c0">批量标准化——一种增强培训的技术</a>。</p><p id="8c6d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们简单总结一下BN:</p><ul class=""><li id="370c" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">它有助于减少内部协变量移位(ICS ),因此激活的输入分布保持更稳定。</li><li id="7b83" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">BN使我们对参数的规模及其初始化不那么小心。</li><li id="8ab5" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">它允许我们使用更高的学习率，帮助我们加快训练。</li></ul><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es js"><img src="../Images/1c0c97caf571ea4bcd52b5df80cd90aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*mJ0eoRoPTcaR3dfBxuSh6w.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx"><a class="ae jd" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="39dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们在上面看到的，BN在加速收敛到最优解的过程中是非常有用和有效的。但是，标准化激活的过程有什么缺点。我们将通过这篇文章来了解它。理解批量重整化如何帮助解决这个问题？</p><p id="3af0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如Sergey Ioffe在<a class="ae jd" href="https://arxiv.org/abs/1702.03275" rel="noopener ugc nofollow" target="_blank">批处理重整:减少批处理标准化模型中的小批处理依赖</a>中总结的那样，“它提供了改善任何使用批处理范式的模型的性能的承诺。”我们试图理解为什么会这样。</p><p id="ebd2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们知道，BN转换不能在每个训练示例中处理激活，因为BN对训练示例和小批量 中的其他示例都有<strong class="ih hj"> <em class="ke">依赖性。虽然它让BN变得强大；也是其弊端的来源。</em></strong></p><p id="e56e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为，随着我们减小迷你批次的大小，我们用来归一化输入的平均值和方差每维度的任何层都变得不太准确。并且这些不准确性与深度相混合，这影响了模型的质量。并指出非iid小批量对批量模型有不良影响。</p><p id="f3b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我想现在，我们明白了BN可能会失败的地方和原因。因此，为了解决这个问题，BN<a class="ae jd" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">论文的作者之一向我们介绍了批量重整化，它通过保留BN的优点(如对参数初始化不敏感和训练效率)来消除批量标准化的上述差异。</a></p><p id="72d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">批量重正化和BN有什么不同？</strong></p><p id="97be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们所知，在BN中，移动平均值是在训练期间对最后几个小批次进行计算的，并且仅用于推断。但是Batch Renorm在训练过程中使用这些移动平均值和方差进行校正。</p><p id="3af3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">批量重正化是网络的扩充，它包含批量规范化层，每维仿射变换应用于规范化激活。</p><p id="ecdd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们有一个小批，并希望使用小批统计或它们的移动平均值来归一化特定的节点x，那么两个归一化的结果通过仿射变换相关。</p><p id="79d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如同</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kf"><img src="../Images/05251ab4cda9fb57ef214e9a933484fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/0*8dThf9DIYDftKw2z"/></div></figure><p id="1ca2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在哪里</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kg"><img src="../Images/e18a11e6efb90ba747bb3fbf15ee22a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/0*EWk5tjep0pi_dNjl"/></div></figure><p id="d80d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">实际上，我们将参数r和d视为固定值。在训练阶段，我们通过保持r = 1和d=0单独开始batchnorm一定次数的迭代，然后在一定范围内逐渐改变这些参数。下面是批量重正化算法:</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kh"><img src="../Images/f716c23bbccbd00fb2e8e2352b2c625f.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*y7kF8O9CCNmVxyMgF2jpSw.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx">Batch-Renormalization Algorithm</figcaption></figure><p id="62d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们说过，当我们减小小批的尺寸时，批量重正化比使用BN的网络更能提高网络的精度。此外，该论文还指出，当Minibatch中的样本不是相同且独立地采样(iid)时，BN可能表现不佳。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es ki"><img src="../Images/301d9486acd9c64b1b86ab5befd61729.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OXsPrhitV8Rfwm8g82QcQw.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">Right Side Image: Shows validation accuracy of InceptionV3 model on ImageNet dataset as we reduce the size of minibatches, Left side: shows validation accuracy for non-iid samples</figcaption></figure><h2 id="3de0" class="kn ko hi bd kp kq kr ks kt ku kv kw kx iq ky kz la iu lb lc ld iy le lf lg lh bi translated">总结:</h2><ul class=""><li id="c61c" class="je jf hi ih b ii li im lj iq lk iu ll iy lm jc jj jk jl jm bi translated">批量重正化降低了小批量中每个示例的处理激活对其他示例的依赖性，并保留了BN的优点。</li><li id="9778" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">在使用迷你电池时，它的效果非常好。</li><li id="4d47" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">它为BN上的非iid例子提供了重要的结果。</li></ul><p id="3313" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望这能帮助你理解BN什么时候会倒闭？批量重正化什么时候会有帮助？</p><p id="539a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请随时告诉我任何概念的错误或误解。这对我帮助很大。如果你认为这会帮助你的朋友理解这个概念；请与他们分享。</p><p id="7b82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">快乐学习！</p></div></div>    
</body>
</html>
<html>
<head>
<title>How To Use “Model Stacking” To Improve Machine Learning Predictions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用“模型堆叠”来改进机器学习预测</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/how-to-use-model-stacking-to-improve-machine-learning-predictions-d113278612d4?source=collection_archive---------5-----------------------#2021-06-14">https://medium.com/geekculture/how-to-use-model-stacking-to-improve-machine-learning-predictions-d113278612d4?source=collection_archive---------5-----------------------#2021-06-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="6fc7" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">什么是模型堆叠？</h1><p id="4a42" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">模型堆叠是一种通过组合多个模型的输出并通过另一个称为元学习器的机器学习模型运行它们来改善模型预测的方法。这是一种用来赢得kaggle竞赛的流行策略，但尽管它们很有用，但它们很少在数据科学文章中被提及——我希望改变这种情况。</p><p id="7bf1" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">本质上，堆叠模型通过“元学习者”(通常是线性回归器/分类器，但也可以是决策树等其他模型)运行多个模型的输出来工作。元学习者试图最小化每个模型的缺点，最大化每个模型的优点。结果通常是一个非常健壮的模型，可以很好地概括看不见的数据。</p><p id="fb68" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">下图说明了堆叠模型的体系结构:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/1467bea49f95959546dee5f4c0fbd52c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*27vV5Pit7XEwKHfi6s5Q1w.png"/></div></div></figure><h1 id="b5b5" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">如何建立堆叠模型？</h1><p id="1d05" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">通过使用sklearn的stacking regressor/分类器库，构建堆叠模型是最容易完成的。下面我将导入所有必要的库，创建一个神经网络架构，然后向您展示如何创建堆叠模型。</p><pre class="kh ki kj kk fd ks kt ku kv aw kw bi"><span id="504e" class="kx ig hi kt b fi ky kz l la lb"># First import necessary libraries<br/>import pandas as pd<br/>from sklearn.ensemble import StackingRegressor</span><span id="ef3c" class="kx ig hi kt b fi lc kz l la lb"># Decision trees<br/>from catboost import CatBoostRegressor<br/>from xgboost import XGBRegressor</span><span id="f6c7" class="kx ig hi kt b fi lc kz l la lb"># Neural networks<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Add, Input, Dense, Dropout<br/>from tensorflow.keras.layers import BatchNormalization, Embedding<br/>from tensorflow.keras.layers import Flatten, Concatenate<br/>from tensorflow.keras import regularizers<br/>from keras.regularizers import l1<br/>from keras.regularizers import l2</span><span id="f528" class="kx ig hi kt b fi lc kz l la lb">from tensorflow.keras import regularizers</span><span id="dbcb" class="kx ig hi kt b fi lc kz l la lb"># Wrapper to make neural network compitable with StackingRegressor<br/>from tensorflow.keras.wrappers.scikit_learn import KerasRegressor</span><span id="bae4" class="kx ig hi kt b fi lc kz l la lb"># Linear model as meta-learn<br/>from sklearn.linear_model import LinearRegression</span><span id="046e" class="kx ig hi kt b fi lc kz l la lb"># Create generic dataset for regression<br/>from sklearn.datasets import make_regression<br/>from sklearn.model_selection import train_test_split</span><span id="9408" class="kx ig hi kt b fi lc kz l la lb"># Create regression dataset<br/>X, y = make_regression(n_targets=1, random_state=42)</span><span id="5f48" class="kx ig hi kt b fi lc kz l la lb"># Convert to pandas<br/>X = pd.DataFrame(X)<br/>y = pd.DataFrame(y)</span><span id="bd62" class="kx ig hi kt b fi lc kz l la lb">#Rename column<br/>y = y.rename(columns={0: 'target'})</span><span id="788e" class="kx ig hi kt b fi lc kz l la lb"># Split into validation set<br/>X_train, X_val, y_train, y_val = train_test_split(X, y,<br/>                                                  test_size=0.2,<br/>                                                  random_state=42)</span></pre><p id="52a5" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">浏览我们的训练数据:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ld"><img src="../Images/e5df94776c134270754b3c1909a6bb0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*tLjnjP7OWcN8l5L88m7yYw.png"/></div></figure><p id="cbff" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们的目标变量:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es le"><img src="../Images/0ee16ed78129b69a4ddb67ce46063c30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*1UDlKHz4I3LZCt76kNQ3zg.png"/></div></figure><p id="dfed" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">好了，现在我们已经定义了训练数据集，我们可以开始构建实际的模型了。对于模型，我们只是要创建一个CatBoostRegessor，XGBRegressor，LinearRegression和一些神经网络。</p><p id="44f6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">首先是一些构建神经网络的代码。我们稍后将在堆叠模型中使用它。(理解神经网络对于本教程来说不是必需的，但它涉及到创建跳过连接和批量规范化，这有助于提高正确数据的性能)</p><pre class="kh ki kj kk fd ks kt ku kv aw kw bi"><span id="9e38" class="kx ig hi kt b fi ky kz l la lb">def create_neural_network(input_shape=510, depth=10, batch_mod=2, num_neurons=250, drop_rate=0.1, learn_rate=.001,<br/>                      r1_weight=0.02,<br/>                      r2_weight=0.02):<br/>    '''A neural network architecture built using keras functional API'''<br/>    act_reg = l1(r2_weight)<br/>    kern_reg = l1(r1_weight)<br/>    <br/>    inputs = Input(shape=(input_shape,))</span><span id="2ac7" class="kx ig hi kt b fi lc kz l la lb">batch1 = BatchNormalization()(inputs)</span><span id="ec31" class="kx ig hi kt b fi lc kz l la lb">hidden1 = Dense(num_neurons, activation='relu', kernel_regularizer=kern_reg, activity_regularizer=act_reg)(batch1)<br/>    dropout1 = Dropout(drop_rate)(hidden1)<br/>    hidden2 = Dense(int(num_neurons/2), activation='relu', kernel_regularizer=kern_reg, activity_regularizer=act_reg)(dropout1)<br/>    <br/>    skip_list = [batch1]<br/>    last_layer_in_loop = hidden2<br/>    <br/>    for i in range(depth):<br/>        added_layer = concatenate(skip_list + [last_layer_in_loop])<br/>        skip_list.append(added_layer)</span><span id="98c7" class="kx ig hi kt b fi lc kz l la lb">b1 = None<br/>        #Apply batch only on every i % N layers<br/>        if i % batch_mod == 2:<br/>            b1 = BatchNormalization()(added_layer)<br/>        else:<br/>            b1 = added_layer<br/>        <br/>        h1 = Dense(num_neurons, activation='relu', kernel_regularizer=kern_reg, activity_regularizer=act_reg)(b1)<br/>        d1 = Dropout(drop_rate)(h1)<br/>        h2 = Dense(int(num_neurons/2), activation='relu', kernel_regularizer=kern_reg, activity_regularizer=act_reg)(d1)<br/>        d2 = Dropout(drop_rate)(h2)<br/>        h3 =  Dense(int(num_neurons/2), activation='relu', kernel_regularizer=kern_reg, activity_regularizer=act_reg)(d2)<br/>        d3 = Dropout(drop_rate)(h3)<br/>        h4 =  Dense(int(num_neurons/2), activation='relu', kernel_regularizer=kern_reg, activity_regularizer=act_reg)(d3)</span><span id="37dd" class="kx ig hi kt b fi lc kz l la lb">last_layer_in_loop = h4</span><span id="31bf" class="kx ig hi kt b fi lc kz l la lb">c1 = concatenate(skip_list + [last_layer_in_loop])<br/>    output = Dense(1, activation='sigmoid')(c1)<br/>    <br/>    model = Model(inputs=inputs, outputs=output)</span><span id="fba6" class="kx ig hi kt b fi lc kz l la lb">optimizer = Adam()<br/>    optimizer.learning_rate = learn_rate<br/>    <br/>    model.compile(optimizer=optimizer,<br/>                  loss='mse',<br/>                  metrics=['accuracy'])</span><span id="dc40" class="kx ig hi kt b fi lc kz l la lb">return model</span></pre><p id="3846" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在是构建堆叠模型的一些代码:</p><pre class="kh ki kj kk fd ks kt ku kv aw kw bi"><span id="be3d" class="kx ig hi kt b fi ky kz l la lb">def get_stacking(input_shape=None):<br/>    '''A stacking model that consists of CatBoostRegressor,<br/>    XGBRegressor, a linear model, and some neural networks'''<br/>    # First we create a list called "level0", which consists of our base models"<br/>    # These models will get passed down to the meta-learner later<br/>    level0 = list()</span><span id="df6f" class="kx ig hi kt b fi lc kz l la lb">level0.append(('cat', CatBoostRegressor(verbose=False)))<br/>    level0.append(('cat2', CatBoostRegressor(verbose=False, learning_rate=.0001)))<br/>    level0.append(('xgb', XGBRegressor()))<br/>    level0.append(('xgb2', XGBRegressor(max_depth=5, learning_rate=.0001)))<br/>    level0.append(('linear', LinearRegression()))</span><span id="91ad" class="kx ig hi kt b fi lc kz l la lb">#Create 5 neural networks using our function above<br/>    for i in range(5):<br/>        # Wrap our neural network in a Keras Regressor to make it<br/>        #compatible with StackingRegressor<br/>        keras_reg = KerasRegressor(<br/>                create_neural_network, # Pass in function<br/>                input_shape=input_shape, # Pass in the dimensions to above function<br/>                epochs=6,<br/>                batch_size=32,<br/>                verbose=False)<br/>        keras_reg._estimator_type = "regressor"<br/>        # Append to our list<br/>        level0.append(('nn_{num}'.format(num=i), keras_reg))</span><span id="8674" class="kx ig hi kt b fi lc kz l la lb"># The "meta-learner" designated as the level1 model<br/>    # In my experience Linear Regression performs best<br/>    # but feel free to experiment with other models<br/>    level1 = LinearRegression()</span><span id="6389" class="kx ig hi kt b fi lc kz l la lb"># Create the stacking ensemble<br/>    model = StackingRegressor(estimators=level0, final_estimator=level1, cv=2, verbose=1)<br/>    return model</span></pre><p id="f133" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在我们可以把它们放在一起:</p><pre class="kh ki kj kk fd ks kt ku kv aw kw bi"><span id="5a07" class="kx ig hi kt b fi ky kz l la lb">#Get our input dimensions for neural network<br/>input_dimensions = len(X_train.columns)</span><span id="c032" class="kx ig hi kt b fi lc kz l la lb"># Create stacking model<br/>model = get_stacking(input_dimensions)</span><span id="3e56" class="kx ig hi kt b fi lc kz l la lb">model.fit(X_train, y_train.values.ravel())</span><span id="388d" class="kx ig hi kt b fi lc kz l la lb"># Creating a temporary dataframe so we can see how each of our models performed<br/>temp = pd.DataFrame(y_val)</span><span id="979a" class="kx ig hi kt b fi lc kz l la lb"># The stacked models predictions, which should perform the best<br/>temp['stacking_prediction'] = model.predict(X_val)</span><span id="9759" class="kx ig hi kt b fi lc kz l la lb"># Get each model in the stacked model to see how they individually perform<br/>for m in model.named_estimators_:<br/>        temp[m] = model.named_estimators_[m].predict(X_val)</span><span id="128c" class="kx ig hi kt b fi lc kz l la lb"># See how each of our models correlate with our target<br/>print(temp.corr()['target'])</span><span id="dfaa" class="kx ig hi kt b fi lc kz l la lb"># See what our meta-learner is thinking (the linear regression)<br/>for coef in zip(model.named_estimators_, model.final_estimator_.coef_):<br/>    print(coef)</span></pre><p id="e2d8" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">下面我们可以看到，通过最小的微调，我们的堆叠模型的性能大大高于任何其他模型，相关性高达0.909。尽管没有一个模型被优化，有些甚至对整体模型有害，具有负相关性，但在元学习者的帮助下，这不是问题，我们得到了奇妙的结果。通过对表现不佳的模型进行更多的微调和修剪，您可以轻松获得更好的结果。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lf"><img src="../Images/6b2a53f285d6af46c024536d67be9908.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*pF3Sc2Tlc_NDl-XjHt8sJA.png"/></div></figure><p id="03a9" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这里你可以看到每个模型是如何影响元学习者的预测的。所有这些系数的组合有助于掩盖或提升每个单独模型的优势/劣势。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lg"><img src="../Images/8de0e99efa8458a485077217c047ffc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*l0xgY92iXG8YegTte_bLSQ.png"/></div></figure><h1 id="57a0" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结论:</h1><p id="8d19" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">创建堆叠模型可以使从模型中“挤出”每一点点性能变得微不足道。在一些数据科学问题中，每一点点的性能都非常重要，因此堆叠模型是实现这一点的快速而方便的解决方案。</p><p id="1612" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">但是，请记住，堆叠模型通常需要长得多的训练时间，并且比其他模型的延迟要慢得多。因此，如果您需要将快速预测发送给用户，那么堆叠模型可能并不理想。</p><p id="503f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">无论如何，感谢阅读！如果你喜欢我的简短教程，记得鼓掌。</p><p id="590f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">下面是完全可复制的笔记本:</p><p id="a9b0" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><a class="ae lh" href="https://colab.research.google.com/drive/11TUd7Yc6hEyAotGMBoAG0xB_NWQXOkD6#scrollTo=qimy45csuiAf" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/drive/11 tu D7 YC 6 heyaotgmboag 0 XB _ nwqxokd 6 # scroll to = qimy 45 csuiaf</a></p></div></div>    
</body>
</html>
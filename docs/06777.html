<html>
<head>
<title>BatchNormalization- a technique that enhances training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">批量标准化——一种强化训练的技术</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/batchnormalization-a-technique-that-enhances-training-5d44966c22c0?source=collection_archive---------17-----------------------#2021-08-29">https://medium.com/geekculture/batchnormalization-a-technique-that-enhances-training-5d44966c22c0?source=collection_archive---------17-----------------------#2021-08-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f102" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为什么这是神经架构中最常用的标准化技术？</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/1c0c97caf571ea4bcd52b5df80cd90aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*mJ0eoRoPTcaR3dfBxuSh6w.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx"><a class="ae jp" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="8cd6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在读Sergey Ioffe和Christian Szegedy写的<a class="ae jp" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank"> BatchNormalization </a> (BN)论文的时候。我偶然发现它直到现在还被29.5k左右引用。此外，我注意到，我认为我们作为ML从业者经常使用批量规范化技术。</p><ul class=""><li id="2a55" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">但是现在我想到了一个问题<strong class="ih hj">为什么我们要使用这种技术</strong>？</li><li id="22f6" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated"><strong class="ih hj">在我们构建的神经架构中使用这样的技术有什么好处</strong>？</li></ul><p id="c5c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在论文中，作者自己描述了为什么他们使用批处理规范化技术来加速深度网络训练。现在想到的问题是如何以及哪些因素制约着网络加速学习过程。我们再次从论文本身得到答案。</p><p id="3e02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">训练深度神经网络(DNN)是复杂的，因为在对前一层输出应用非线性作为下一层的输入之后；在训练过程中，输入的分布会发生变化，因为前几层的参数会发生变化。这一事实通过要求较低的学习速率以及仔细的参数初始化而减慢了训练。这叫做<strong class="ih hj"> <em class="ke">【内部协变移位(ICS) </em> </strong>。这个事实减缓了训练。作者认为国阵试图抑制它，加速DNN的训练过程。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kf"><img src="../Images/3d9d7859a6c53633a0b3c5c7b5ec4065.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*BSssXFdw2MWR3SqdGF-BoQ.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx"><a class="ae jp" href="https://www.diva-portal.org/smash/get/diva2:955562/FULLTEXT01.pdf" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="9df3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了优化手头的问题，我们使用不同版本的梯度下降算法。正如我们所知，随机梯度总是有效的，但它需要仔细调整超参数，特别是学习率和参数初始化。但是由于内部协变量变化，训练变得复杂，因为这种变化随着我们深入而放大。所以每一层都需要适应变化。这就是为什么它需要较低的学习率，增加了培训时间。</p><p id="8b5b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BN试图减少内部协变量偏移，以便非线性输入的分布保持更稳定，并且在训练时不太可能停留在饱和区域。并且这导致加速训练以优化最优解。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kg"><img src="../Images/b8c4195d24c612929c758bcf4e3daa51.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*Am5DO-PQWOIzJKJKk5ljqw.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx"><a class="ae jp" href="https://e2eml.school/batch_normalization.html" rel="noopener ugc nofollow" target="_blank">Reduction in internal covariate shift after BN</a></figcaption></figure><p id="a9c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是除了这些好处，还有其他好处，例如:</p><ul class=""><li id="86f2" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">BN对通过网络的梯度流具有有益的影响，因为梯度变得独立于参数/权重(W)的比例或它们的初始值。</li><li id="1662" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">随后，它允许我们使用更高的学习率，而没有发散的风险。</li><li id="9eae" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">通过调整模型来减少辍学的需要。</li></ul><p id="99c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如LeCun等人。艾尔。并且Wiesler等人建议，如果输入被白化，即输入被线性变换以具有零均值和单位方差并且去相关，则网络训练收敛得更快。因此，由于体系结构中的每一层都从其前一层获取输入，因此白化它的输入将是有利的。</p><p id="d696" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了对输入进行白化，需要注意的是，当我们在GD步骤之外计算归一化参数时，模型会爆炸，因此虽然发生了归一化，但归一化无助于降低损耗。这里的问题是GD优化没有考虑发生的规范化。</p><p id="5d79" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，<em class="ke">每一层输入的完全白化是昂贵的，因此我们通过使标量特征具有零均值和单位方差来独立地归一化它。这意味着如果我们有一个d维输入向量x</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kh"><img src="../Images/c6aa3f3a999ae1548fc6cfcf93d9a4dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/0*7VSHgc94KU0vYz2w"/></div></figure><p id="2edf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将每个维度标准化为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ki"><img src="../Images/4c30c1bb025500ceda4958870289672a.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/0*a-F5u8ehf6wgGrKw"/></div></figure><p id="8bca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是在规范化输入图层时，图层的表示可能会发生变化，因此为了保持表示的完整性，我们必须确保网络中插入的变换可以表示身份变换。这就是为什么引入</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kj"><img src="../Images/d18e502d41175533c6947d7a337250a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:88/0*pbWVhLE-AY8YBJ_h"/></div></figure><p id="4aec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这可以缩放和移动归一化值并恢复网络的表示能力。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kk"><img src="../Images/f09efaf1c71d5229dc1fda2927cb223d.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/0*BLY7SA47yzIOWKj3"/></div></figure><p id="9846" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是每维引入这两个参数抑制ICS并加速训练的原因。</p><h2 id="5b15" class="kl km hi bd kn ko kp kq kr ks kt ku kv iq kw kx ky iu kz la lb iy lc ld le lf bi translated">总结:</h2><ul class=""><li id="e750" class="jq jr hi ih b ii lg im lh iq li iu lj iy lk jc jv jw jx jy bi translated">由于要求较低的学习速率和仔细的参数初始化，ICS减慢了训练。</li><li id="4ba8" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">BN通常用于神经架构中，以减少最终加速训练过程的ICS。</li><li id="57c1" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">但是为了使GD意识到网络的归一化和完整的表示能力，我们需要在每个维度的网络中引入两个参数。</li></ul><p id="d545" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我解释错了，请随时纠正我。并与他人分享。</p><p id="7147" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">快乐学习！</p></div></div>    
</body>
</html>
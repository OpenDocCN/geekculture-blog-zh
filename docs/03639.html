<html>
<head>
<title>ML algorithms 1.02: Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最大似然算法1.02:逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/ml-algorithms-1-02-logistic-regression-f2ca619c97ff?source=collection_archive---------21-----------------------#2021-06-12">https://medium.com/geekculture/ml-algorithms-1-02-logistic-regression-f2ca619c97ff?source=collection_archive---------21-----------------------#2021-06-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/2064099ed618b2bd49403a211fc5b892.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8_R5GZO5XoNAm543x3faqg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx"><a class="ae iu" href="https://unsplash.com/photos/KEIrdasbkhw?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditShareLink" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h1 id="3246" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">介绍</h1><p id="3742" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">当使用<a class="ae iu" rel="noopener" href="/geekculture/ml-algorithms-1-01-linear-regression-5829a9698aa9">线性回归</a>时，如果你对自己说，“天哪，我怎么用它来分类呢？”，你读对了文章。逻辑回归从线性回归中借用最佳拟合线的概念，以OVR(一与其余)的方式划分类别。由于所需的输出是一个预测，因此该模型使用sigmoid变换将输出范围保持在[0，1]内。此外，损失函数从线性回归中看到的连续凸损失函数变为铰链损失。缺失值需要在线性回归中进行估算或剔除。离群值影响最佳拟合线的形成。应该使用箱线图或任何其他方法对它们进行过滤。缩放对于模型是有益的。这里值得一提的是，由于输出是一个概率，逻辑回归可以用作bagging/boosting算法中的基本估计量。</p><h1 id="9f0b" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">假设</h1><ul class=""><li id="a988" class="kr ks hi jv b jw jx ka kb ke kt ki ku km kv kq kw kx ky kz bi translated">预测值和目标正类对数比数之间的线性关系</li><li id="58a2" class="kr ks hi jv b jw la ka lb ke lc ki ld km le kq kw kx ky kz bi translated">预测值之间很少或没有相关性</li></ul><h1 id="31ae" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">优势</h1><ul class=""><li id="1e37" class="kr ks hi jv b jw jx ka kb ke kt ki ku km kv kq kw kx ky kz bi translated">高模型可解释性</li><li id="a90a" class="kr ks hi jv b jw la ka lb ke lc ki ld km le kq kw kx ky kz bi translated">当数据是线性可分时，性能非常好</li><li id="9e02" class="kr ks hi jv b jw la ka lb ke lc ki ld km le kq kw kx ky kz bi translated">输出类别概率</li></ul><h1 id="b14d" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">不足之处</h1><ul class=""><li id="f1dc" class="kr ks hi jv b jw jx ka kb ke kt ki ku km kv kq kw kx ky kz bi translated">数据几乎不符合假设</li><li id="e785" class="kr ks hi jv b jw la ka lb ke lc ki ld km le kq kw kx ky kz bi translated">在极少数问题中，数据是线性可分的，没有核技巧</li><li id="9473" class="kr ks hi jv b jw la ka lb ke lc ki ld km le kq kw kx ky kz bi translated">模型无法解释复杂的关系</li></ul><h1 id="2abb" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">模型</h1><p id="2604" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">设<strong class="jv hj"> X </strong>为具有<em class="lf"> m </em>个样本和<em class="lf"> n </em>个特征的特征集。设<strong class="jv hj"> y </strong>为类响应。</p><p id="3024" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">模型的参数表示为:</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/0ce3f49dc34bea18f910278def5c6e59.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/0*TM34_X8aPRLUZNId.png"/></div></figure><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/72acc180eecdd2c0028c7b63e1fa308f.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/0*dDUGf4VzYVfVeomT.png"/></div></figure><p id="118a" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">我们可以将模型的初始参数𝜷设置为接近0。让我们定义模型的损失(<em class="lf">成本</em>)函数:</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/3ccf8235f1f274f680aa07e2d2b4c3b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/0*Yi_EEUmH5UKt-atE.png"/></div></figure><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/9a233c1dd4f43c46a69b758d01eca0f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/0*n1f0QV6DnbnDg1YX.png"/></div></figure><p id="2bfe" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">上面的等式代表了sigmoid变换。</p><p id="152a" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">让我们理解为什么我们在损失函数中使用了log()。</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/5114862f01a6f86c7bd132abb350c691.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/0*6SVT_kueg4v2oYK3.png"/></div></figure><p id="b42e" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">对于一次观察，例1和例4中的误差约为0；案例2和3中的误差= 1。</p><p id="0d76" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">病例1和4分类正确，病例2和3分类错误。</p><p id="078d" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">如下图所示，单个对数损失函数惩罚与预期输出的高偏差:</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/8513bddb8f672fa2ff1f38e740204bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-J9AgsrY0D09I6xqEYtSqg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx"><a class="ae iu" href="https://www.desmos.com/calculator/cmghmic8rz" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="c429" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">红线代表log(ŷ){which对应class=1}，蓝线代表log(1-ŷ){which对应class=0}。注意红线射向0附近的无穷大，反之亦然。</p><p id="8f71" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">这就是损失函数如何惩罚错误的类预测。</p><p id="2625" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">∑前的负号是因为小数值的对数为负。</p><p id="30e6" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">请注意，我没有将y或(1-y)乘以曲线，因为我们可以<em class="lf">说</em>它用于选择损失函数中的相关对数项。对于特定的观察，它也是恒定的。</p><h2 id="a3d7" class="lv iw hi bd ix lw lx ly jb lz ma mb jf ke mc md jj ki me mf jn km mg mh jr mi bi translated">设置阈值</h2><p id="f152" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">模型输出是一个概率，而不是直接的类<strong class="jv hj">。</strong>对于二分类问题，当这个概率大于某个阈值(一般为0.5)时，输出被预测为正类。这个阈值可以使用ROC曲线来设置。期望的阈值是给出TPR相对于FPR的最大变化的阈值，即阈值= argmax(TPR-FPR)。这在不平衡的班级问题中特别有用。</p><h2 id="2a6e" class="lv iw hi bd ix lw lx ly jb lz ma mb jf ke mc md jj ki me mf jn km mg mh jr mi bi translated">一对多是如何工作的</h2><p id="1e26" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在二元分类中，建立一个超平面来分离点。</p><p id="2794" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">对于n个类，将构建n个超平面(<em class="lf">或2个特征的线</em>)。每一个都将被用来积极地预测一个特定的阶层<strong class="jv hj"/>。</p><p id="6be6" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">设一个问题中有a、b、c三类要分类。</p><p id="dea5" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">超平面1:1 = a；0=b，c</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/02788298d79b2a26bc388ee91fe41f12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*iDxgMA9xCN5BvLnzwwJHNw.png"/></div></figure><p id="a584" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">超平面2:1 = b；0=a，c</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/15e59288b7bf1f3eb297f41d0b831a67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*YWEGwB__swXd6tVIMQJwxA.png"/></div></figure><p id="1b86" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">超平面3:1 = c；0=a，b</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/271d5d8573449907f744c51223858f16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*qEIewVktX6_Ov0xGX56zhw.png"/></div></figure><p id="783d" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">使用超平面1，我们获得类别a的概率，类似地，我们获得类别b和c的概率。这三个类别中最高的类别概率用于输出相应的类别作为模型输出。</p><p id="499d" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *<em class="lf">示例代码:</em></p><p id="ddb7" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">来自sklearn.linear_model导入逻辑回归为LR</p><p id="f2d5" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">lr =LR()</p><p id="e86a" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">lr.fit(X_train，y_train)</p><p id="2a5e" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">y_hat = lr.predict(X_test)</p><p id="02c5" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi">*****************************************************************</p><p id="531d" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">参考资料:</p><div class="mm mn ez fb mo mp"><a href="https://www.coursera.org/learn/machine-learning?utm_source=gg&amp;utm_medium=sem&amp;utm_campaign=07-StanfordML-IN&amp;utm_content=07-StanfordML-IN&amp;campaignid=1950458127&amp;adgroupid=70479331563&amp;device=c&amp;keyword=andrew%20ng%20machine%20learning&amp;matchtype=e&amp;network=g&amp;devicemodel=&amp;adpostion=&amp;creativeid=351348153032&amp;hide_mobile_promo" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hj fi z dy mu ea eb mv ed ef hh bi translated">机器学习</h2><div class="mw l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">4，182，163已注册机器学习是让计算机在没有明确…</h3></div><div class="mx l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">www.coursera.org</p></div></div><div class="my l"><div class="mz l na nb nc my nd io mp"/></div></div></a></div><div class="mm mn ez fb mo mp"><a href="https://github.com/krishnaik06/Interview-Prepartion-Data-Science/blob/master/Interview%20Preparation-%20Day%205-Logistic%20Regression.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hj fi z dy mu ea eb mv ed ef hh bi translated">krishnaik 06/采访-准备-数据-科学</h2><div class="mw l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">在GitHub上创建一个帐户，为krishnaik 06/采访准备数据科学的发展做出贡献。</h3></div><div class="mx l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">github.com</p></div></div><div class="my l"><div class="ne l na nb nc my nd io mp"/></div></div></a></div></div></div>    
</body>
</html>
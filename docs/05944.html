<html>
<head>
<title>Actor-Critic: Implementing Actor-Critic Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">行动者-批评家:实现行动者-批评家方法</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/actor-critic-implementing-actor-critic-methods-82efb998c273?source=collection_archive---------5-----------------------#2021-08-03">https://medium.com/geekculture/actor-critic-implementing-actor-critic-methods-82efb998c273?source=collection_archive---------5-----------------------#2021-08-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="624b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将使用我以前文章中的<a class="ae jd" rel="noopener" href="/nerd-for-tech/reinforcement-learning-introduction-to-policy-gradients-aa2ff134c1b">策略梯度方法</a>和<a class="ae jd" rel="noopener" href="/geekculture/actor-critic-value-function-approximations-b8c118dbf723">价值函数近似法</a>来实现一些行动者-批评家方法。我不会太关注理论和细节，但更多的是结合演员和评论家以前的想法。</p><h1 id="0e2d" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">演员-评论家算法</h1><p id="06fb" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">首先，让我们看看萨顿&amp;巴尔托给出的一步演员-评论家算法:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kh"><img src="../Images/94cf5f376f7b43c5e24daac629ca46de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y3GuuYG9Lt27G9RBn8b3Yg.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">Taken from Sutton&amp;Barto 2017</figcaption></figure><p id="d2d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们已经学习了如何更新“批评”当前状态的批评家(价值函数)和按照批评家告诉我们的方向更新的参与者(政策函数)。这里的单步演员-评论家算法完全在线，评论家使用TD(0)算法来更新价值函数的参数w。回想一下TD(0)更新公式:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/2c9e0a2d07252b3962df285d921f4935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*d9yrGRk3r_nAfq0j.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">Taken from David Silver’s UCL Lecture 7</figcaption></figure><p id="367b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，我们使用目标值和近似值之间的时间差来更新梯度方向上的值函数。因为TD(0)算法在每个时间步长更新，所以通过自举来估计目标值。</p><p id="edd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们来看看演员。参与者使用批评家的评估来更新策略参数θ。回想一下我们从策略梯度定理导出的策略更新等式:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ky"><img src="../Images/f0159414fce79c2fa15b086b2ae4f582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/0*hY3cW5UxbpEEAYa3.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Taken from David Silver’s UCL Lecture 7</figcaption></figure><p id="b33d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个策略更新方程被用在加强算法中，该算法在对整个轨迹进行采样之后进行更新。由于Sutton&amp;Barto的一步行动者-批评家使用在线更新，我们在由价值函数近似的状态-行动值的方向上更新政策参数，而不是使用累积报酬Gₜ.现在，如果我们只使用值函数来确定更新的方向，这将导致一个相当有偏差的结果。我们可以用减去基线的概念来使它无偏。你可以在这里阅读更多关于我用基线<a class="ae jd" rel="noopener" href="/nerd-for-tech/policy-gradients-reinforce-with-baseline-6c871a3a068">加固的实现。</a></p><p id="f11a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在用基线强化中，我们使用价值函数作为基线，所以自然地，我们在这里也可以。我们不是在近似状态-动作值的方向上更新策略，而是通过在该状态下选择特定动作的优势来调整它。我们可以通过查看状态值和状态-动作值之间的差异来计算这种优势。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kz"><img src="../Images/c91e42f7b3c73ef518292c80e4e8dcd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*QFsP_WJM7wXeLXw6qxR-Fw.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Taken from David Silver’s UCL Lecture 7</figcaption></figure><p id="97fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">快速回顾一下:Q(s，a)是采取行动a后一个状态s的值而V(s)只是一个状态s的值。</p><p id="6118" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单的方法是通过学习具有不同参数的状态-动作值函数Q和状态值函数V。这样做会使我们的算法更加复杂，因为我们必须跟踪Q、V和策略参数。萨顿和巴尔托在这里使用了一种更简单的方法。我们可以用(R + V(s ')作为估计的状态-动作值来计算优势，和TD误差一样！</p><p id="6604" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您查看Actor-Critic算法的不同实现，您可能会注意到一些差异。例如，<a class="ae jd" href="https://keras.io/examples/rl/actor_critic_cartpole/" rel="noopener ugc nofollow" target="_blank"> Keras </a>和<a class="ae jd" href="https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py" rel="noopener ugc nofollow" target="_blank"> Pytorch </a>使用蒙特卡罗方法更新演员和评论家。虽然萨顿&amp;巴尔托并不认为蒙特卡罗方法是一个真正的行动者-批评家方法，因为批评家只是用来作为减少方差的基线，这些方法仍然是有效的。</p><p id="3c59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你现在可能有的一个问题是，评论家可以使用不同的价值近似值方法吗？答案是肯定的。回想一下我们讨论过的不同的<a class="ae jd" rel="noopener" href="/geekculture/actor-critic-value-function-approximations-b8c118dbf723">近似方法</a>:</p><p id="9ffd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">蒙特卡洛:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es la"><img src="../Images/0c83a4a04a940badffb987ec69440497.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*l0zbxiX4cPj_My2GPI2iEg.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Taken from David Silver’s UCL Lecture 7</figcaption></figure><p id="2244" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">萨顿&amp;巴尔托所用的TD(0):</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lb"><img src="../Images/eb168ff9fc3353a4ec9e4329ea70e02f.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*ZyHjxefdBChXNrPyDOp2lw.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Taken from David Silver’s UCL Lecture 7</figcaption></figure><p id="4dab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">前视TD(λ):</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es lc"><img src="../Images/51233abbe53b4ca59d62597852559e15.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*loT7M24lr7hK4DyrG_E2qw.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">Taken from David Silver’s UCL Lecture 7</figcaption></figure><p id="aa60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">后视TD(λ):</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ld"><img src="../Images/8c4815924015ab4f0571e8c33f3ec42b.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*OL7iTntB_WXdWA3dpe-RxA.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Taken from David Silver’s UCL Lecture 7</figcaption></figure><p id="bda6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们甚至可以为演员的优势函数做同样的事情。简单的用ϕ(s = ∇lnπ(a | s，θ)代替演员来代替评论家ϕ(s) = ∇v(S，w)。</p><p id="385d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是萨顿&amp;巴尔托为《演员和评论家》使用后视TD(λ)的在线演员评论家的伪代码:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es le"><img src="../Images/c538f0b89043dfcd69b945e10d091769.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BBbKhynXKckOpljjFzmOLg.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">Taken from Sutton&amp;Barto 2017</figcaption></figure><p id="ad55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还可以为演员和评论家实现一个前视TD(λ),但类似于蒙特卡罗方法，我们必须对整个轨迹进行采样，并不定期地执行离线更新。</p><h1 id="e25f" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">履行</h1><p id="f7aa" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">在这篇文章的其余部分，我将展示我的演员-评论家方法的Pytorch实现，并谈论一些挑战和我做的事情。我已经使用TD(0)、后视TD(λ)和前视TD(λ)实现了Actor-Critic。对于蒙特卡罗实现，请参考我的帖子<a class="ae jd" rel="noopener" href="/nerd-for-tech/policy-gradients-reinforce-with-baseline-6c871a3a068">用基线</a>加强。以下是我用来参数化我的策略和价值函数的神经网络:</p><pre class="ki kj kk kl fd lf lg lh li aw lj bi"><span id="88c9" class="lk jf hi lg b fi ll lm l ln lo">class PolicyNetwork(nn.Module):<br/>    <br/>    #Takes in observations and outputs actions<br/>    def __init__(self, observation_space, action_space):<br/>        super(PolicyNetwork, self).__init__()<br/>        self.input_layer = nn.Linear(observation_space, 128)<br/>        self.output_layer = nn.Linear(128, action_space)<br/>    <br/>    #forward pass<br/>    def forward(self, x):<br/>        #input states<br/>        x = self.input_layer(x)<br/>        <br/>        #relu activation<br/>        x = F.relu(x)<br/>        <br/>        #actions<br/>        actions = self.output_layer(x)<br/>        <br/>        #get softmax for a probability distribution<br/>        action_probs = F.softmax(actions, dim=1)<br/>        <br/>        return action_probs</span></pre><p id="7a03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">国家价值网络:</p><pre class="ki kj kk kl fd lf lg lh li aw lj bi"><span id="4f6d" class="lk jf hi lg b fi ll lm l ln lo">class StateValueNetwork(nn.Module):<br/>    <br/>    #Takes in state<br/>    def __init__(self, observation_space):<br/>        super(StateValueNetwork, self).__init__()<br/>        <br/>        self.input_layer = nn.Linear(observation_space, 128)<br/>        self.output_layer = nn.Linear(128, 1)<br/>        <br/>    def forward(self, x):<br/>        #input layer<br/>        x = self.input_layer(x)<br/>        <br/>        #activiation relu<br/>        x = F.relu(x)<br/>        <br/>        #get state value<br/>        state_value = self.output_layer(x)<br/>        <br/>        return state_value</span></pre><p id="73ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是我在实现中使用的超参数:</p><ul class=""><li id="76a4" class="lp lq hi ih b ii ij im in iq lr iu ls iy lt jc lu lv lw lx bi translated">γ(折扣系数):0.99</li><li id="60ee" class="lp lq hi ih b ii ly im lz iq ma iu mb iy mc jc lu lv lw lx bi translated">剧集数量:1000</li><li id="50dd" class="lp lq hi ih b ii ly im lz iq ma iu mb iy mc jc lu lv lw lx bi translated">最大步数:10000</li><li id="e06b" class="lp lq hi ih b ii ly im lz iq ma iu mb iy mc jc lu lv lw lx bi translated">α(系数LR): 0.01</li><li id="6172" class="lp lq hi ih b ii ly im lz iq ma iu mb iy mc jc lu lv lw lx bi translated">β(评论家LR): 0.01</li><li id="add0" class="lp lq hi ih b ii ly im lz iq ma iu mb iy mc jc lu lv lw lx bi translated">λ(演员):0.8</li><li id="2fa2" class="lp lq hi ih b ii ly im lz iq ma iu mb iy mc jc lu lv lw lx bi translated">λ(评论家):0.8</li></ul><p id="a0c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于每个实现，我将使用OpenAI在最近100集的平均分数为195的解决条件。</p><h1 id="7dfc" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated"><strong class="ak"> TD(0)实现</strong></h1><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kh"><img src="../Images/94cf5f376f7b43c5e24daac629ca46de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y3GuuYG9Lt27G9RBn8b3Yg.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">Taken from Sutton&amp;Barto 2017</figcaption></figure><p id="9e90" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于TD(0) AC，我使用了上面萨顿和巴尔托给出的伪代码。在我的实现中，我使用MSE作为评论家的损失函数，这与上面应用链规则一次的伪代码是一样的。我还使用随机梯度下降(SGD)作为我的优化器，正如伪代码中所描述的，并且在不同的实现之间进行公平的比较。</p><p id="d1b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看我的结果的训练历史:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es md"><img src="../Images/8cc23bd0ec5940f8ae89a46c93364fa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*UDTEiMzbXhn3X6VeV3Is3w.png"/></div></figure><p id="16b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的代理在5次试玩中，50次游戏的平均得分为61.828。在训练历史中，我们可以看到，尽管方差很高，但分数仍有稳步上升的趋势。相比于人类基准(me)的30.8分和随机策略的21.48分，61.8分肯定要好得多。</p><p id="0dfa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">高差异的一个解释可能是因为回报。对于基线强化，我可以使用白化奖励来标准化奖励，以减少差异。这是因为该算法在应用更新之前对整个轨迹进行了采样，所以我能够应用归一化。差异的另一个原因是分数的突然下降。这是因为一种叫做灾难性遗忘的现象，在这种现象中，主体“忘记”过去的状态，并需要再次重新学习模式。</p><p id="9832" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是使用Adam优化器时的训练结果:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es md"><img src="../Images/838a261edafb274e9627816487a3d1b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*D9zaQCfpGMTTNaGBborNBQ.png"/></div></figure><p id="1783" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有了Adam optimizer，该代理可以在50次播放中获得平均500分。代理也在仅仅443集的训练中达到了我们解决的条件。</p><h1 id="d289" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated"><strong class="ak">前视TD(λ)实现</strong></h1><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es me"><img src="../Images/e7361cdea8f93afd04d1f511fa399621.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6kdraNRNufKeW2FvEgFO7A.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">Taken from Sutton&amp;Barto 2017</figcaption></figure><p id="fa00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于前视TD(λ) AC，没有任何伪代码，但这个概念类似于萨顿和巴尔托在上面描述的用基线加固。我们不计算来自步骤t的Gₜ回报，而是计算来自步骤t的gₜλ-回报，它对所有ns的n步回报进行平均，并为每个n步回报分配衰减权重。我用随机梯度下降(SGD)作为我的优化器，用MSE作为我的损失函数来学习价值函数。我还用了白化奖励来对抗渐变爆炸，减少方差。</p><p id="0dff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看我的结果的训练历史:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es md"><img src="../Images/391c4828cca6372a1934639578c2cfd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*3-TUd4EMekMywWw8uDSBZQ.png"/></div></figure><p id="0484" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的代理人在3次试玩的50次游戏中取得了146.27的平均分数。由于算法运行时间较长，我只进行了3次试验，这足以确保我没有异常值。与TD(0)相比，我们的代理能够取得146.27的更好成绩，而不是61.83。我们还可以看到，由于组合了来自所有Ns的不同N步目标值的更稳健的目标值，因此没有太多的差异。还有一个灾难性遗忘的例子发生在第160集左右，在代理人达到500的高分之后。</p><p id="82ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我计算N步回报时，我的解决方案的运行时间是O(n ),我可能会将其减少到O(n ),因此它的运行时间比TD(0)的2分钟长得多，约为16分钟。这是前视TD(λ)的缺点，前视TD(λ)需要更高的时间复杂度来计算每个时间步长上所有ns的N步回报，并且还需要更高的空间复杂度来存储轨迹。这也是为什么大多数算法选择使用后视TD(λ)而不是前视的原因，因为这不是合格跟踪的问题。</p><p id="ff8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是使用Adam优化器时的训练结果:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es md"><img src="../Images/f26e6e5f7a43227601e4b4ba7a01a5b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*GDVPf2QQBvIFh_ViLbjUUg.png"/></div></figure><p id="76eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有了Adam optimizer，该代理可以在50次播放中获得平均500分。在仅仅262集的训练中，代理也达到了我们解决的条件。</p><h1 id="2e9a" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated"><strong class="ak">后视TD(λ)实现</strong></h1><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es le"><img src="../Images/c538f0b89043dfcd69b945e10d091769.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BBbKhynXKckOpljjFzmOLg.png"/></div></div></figure><p id="192b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于后视TD(λ) AC，我遵循上面萨顿&amp;巴尔托给出的伪代码。在我的实现中，我没有使用优化器，而是对网络参数进行手动更新，这样我就可以应用资格跟踪。这也是为什么我在其他实现中使用SGD来获得更公平的结果比较的原因。</p><p id="1062" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来看看我的成绩的训练历史:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es md"><img src="../Images/cd3705f7912dd1838006991952250b97.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*-rNE5_PbFiSjHHteXXLqvA.png"/></div></figure><p id="3bd0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的代理在5次试玩中，50次游戏的平均得分为334.116。代理也达到了平均266.2集的解决条件。与TD(0)相比，我们的代理能够取得更好的成绩。理论上，前视和后视TD(λ)是等价的，但在强化学习中，由于高方差，总是存在复制结果的问题。后视TD(λ)的训练时间与TD(0)大致相同，因为两者都进行在线更新。与具有高时间和空间复杂度的前视TD(λ)相比，后视TD(λ)在每一步累积一个合格轨迹，这允许每次更新考虑所有过去的更新。有趣的是，轨迹的空间复杂度仅为O(|θ| + |w|) = O(1)，这是演员和评论家网络参数的大小。计算目标值回报时，我们只需要计算1步TD误差，一集O(n)。</p><p id="0b91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于我们必须手动执行更新，Adam优化器在这种情况下无法工作。</p><h1 id="354f" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">摘要</h1><p id="e46d" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">以下是结果汇总，包括来自<a class="ae jd" href="https://chengxi600.medium.com/policy-gradients-reinforce-with-baseline-6c871a3a068" rel="noopener">的蒙特卡洛模拟，用基线</a>进行强化:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es mf"><img src="../Images/c47b020907330f13ce1c1c54b4503096.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WlGNOM4Nle-q5GKsjeUfvA.png"/></div></div></figure><p id="6048" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，由于我只对亚当优化程序进行了一次试验，因此解决问题的次数可能会有所不同。值得注意的一点是，对于已解决的实现，使用SGD优化器的实现获得了大约300分，而使用Adam优化器的实现获得了大约500分。这可能是因为Adam优化器引入了更新动量和加速，使代理能够比使用SGD时更快地达到局部最大值。另一件要注意的事情是，CartPole环境的选择对每个算法的有效性有巨大的影响。CartPole在非终结状态下对每个时间步长只给出+1的统一奖励，所以TD(λ)算法并不是特别好。</p><h1 id="1d98" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">最后的想法</h1><p id="47d7" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">这三个实现花费的时间比预期的要长，这主要是因为我误解了伪代码和Pytorch库。虽然TD(0)和前视TD(λ)对我来说相当简单，但我在后视TD(λ)上停留了一段时间。我使用预建的优化器，不确定在哪里应用我的资格跟踪。在看了这个<a class="ae jd" href="https://www.youtube.com/watch?v=mdKjMPmcWjY" rel="noopener ugc nofollow" target="_blank">视频</a>之后，我意识到我应该执行手动更新，这个视频简要概述了不同的优化算法。</p><p id="8189" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我学会使用的另一件事是在形成张量时将required_grad属性设置为True，我需要对该张量执行反向传播，这将有助于跟踪张量操作。当在具有重叠叶张量的张量上多次调用backward()时，我需要使用backward(retain_graph=True ),这样在调用backward()后，计算图形不会从内存中释放。</p><p id="9f79" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总的来说，这是一项具有挑战性的任务，但也是值得的，我学到了很多东西，并对更难的算法更有信心。由于没有太多关于资格追踪的资源，我最终在它们上面停留了一段时间。在以后的文章中，我想讨论连续动作空间、自然行动者-评价者、异步优势行动者-评价者(A3C)、确定性策略梯度和近似策略优化的行动者-评价者算法。</p><p id="621a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代码:</p><ul class=""><li id="7dc5" class="lp lq hi ih b ii ij im in iq lr iu ls iy lt jc lu lv lw lx bi translated">TD(0)演员-评论家:<a class="ae jd" href="https://github.com/chengxi600/RLStuff/blob/master/Actor-Critic/Actor-Critic_TD_0.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/cheng i600/rl stuff/blob/master/演员-评论家/演员-评论家_TD_0.ipynb </a></li><li id="b0dc" class="lp lq hi ih b ii ly im lz iq ma iu mb iy mc jc lu lv lw lx bi translated">前视TD(λ)演员-评论家:<a class="ae jd" href="https://github.com/chengxi600/RLStuff/blob/master/Actor-Critic/Actor-Critic_TD_Lambda_Forward.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/cheng i600/rl stuff/blob/master/Actor-Critic/Actor-Critic _ TD _ Lambda _ forward . ipynb</a></li><li id="08b6" class="lp lq hi ih b ii ly im lz iq ma iu mb iy mc jc lu lv lw lx bi translated">后视TD(λ)Actor-Critic:<a class="ae jd" href="https://github.com/chengxi600/RLStuff/blob/master/Actor-Critic/Actor-Critic_TD_Lambda_Backward.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/cheng i600/rl stuff/blob/master/Actor-Critic/Actor-Critic _ TD _ Lambda _ backward . ipynb</a></li></ul><p id="6890" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><ul class=""><li id="2a96" class="lp lq hi ih b ii ij im in iq lr iu ls iy lt jc lu lv lw lx bi translated"><a class="ae jd" href="https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf" rel="noopener ugc nofollow" target="_blank">政策梯度(大卫·西尔弗的UCL讲座7) </a></li><li id="2fd3" class="lp lq hi ih b ii ly im lz iq ma iu mb iy mc jc lu lv lw lx bi translated"><a class="ae jd" href="http://incompleteideas.net/book/bookdraft2017nov5.pdf" rel="noopener ugc nofollow" target="_blank">强化学习:导论(萨顿&amp;巴尔托2017) </a></li><li id="405e" class="lp lq hi ih b ii ly im lz iq ma iu mb iy mc jc lu lv lw lx bi translated"><a class="ae jd" href="https://www.youtube.com/watch?v=mdKjMPmcWjY" rel="noopener ugc nofollow" target="_blank">优化者解释(CodeEmporium) </a></li></ul></div></div>    
</body>
</html>
<html>
<head>
<title>Simple Chatbot using BERT and Pytorch: Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用BERT和Pytorch的简单聊天机器人:第2部分</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/simple-chatbot-using-bert-and-pytorch-part-2-ef48506a4105?source=collection_archive---------3-----------------------#2021-06-27">https://medium.com/geekculture/simple-chatbot-using-bert-and-pytorch-part-2-ef48506a4105?source=collection_archive---------3-----------------------#2021-06-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8104" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文分为三个部分。</p><p id="e762" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">零件(1/3): <a class="ae jd" rel="noopener" href="/@shrinidhi.rm1990/simple-chatbot-using-bert-and-pytorch-part-1-2735643e0baa">简介及安装</a></p><p id="a1e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">部分(2/3): <a class="ae jd" rel="noopener" href="/@shrinidhi.rm1990/simple-chatbot-using-bert-and-pytorch-part-2-ef48506a4105">数据准备</a></p><p id="07f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第(3/3)部分:<a class="ae jd" rel="noopener" href="/@shrinidhi.rm1990/simple-chatbot-using-bert-and-pytorch-part-3-a6832c50b8d1">模型微调</a></p><p id="8190" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上一篇文章中，我们看到了Transformer和Pytorch概念的简要介绍。我们已经安装了所有必要的库。现在让我们进入下一部分，即数据准备。</p><p id="d73b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本例中，我们尝试使用<strong class="ih hj"> Bert-base-uncased、Roberta-base </strong>和<strong class="ih hj"> distilbert-base-uncased </strong>模型进行训练。</p><p id="08c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于我们的训练数据，<strong class="ih hj">distil Bert-base-un cased</strong>模型给出了更好的结果。</p><h2 id="3f3f" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated"><strong class="ak">伯特模型</strong></h2><p id="fc41" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">我们可以导入<strong class="ih hj"> Bert </strong>模型如下。</p><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="39b7" class="je jf hi kj b fi kn ko l kp kq">from transformers import AutoModel, BertTokenizerFast</span><span id="e0e9" class="je jf hi kj b fi kr ko l kp kq"># Load the BERT tokenizer<br/>tokenizer = BertTokenizerFast.from_pretrained(‘bert-base-uncased’)</span><span id="5cdc" class="je jf hi kj b fi kr ko l kp kq"># Import BERT-base pretrained model<br/>bert = AutoModel.from_pretrained(‘bert-base-uncased’)</span></pre><h2 id="8af2" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">罗伯塔模型</h2><p id="d487" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">我们可以导入<strong class="ih hj"> Roberta </strong>模型，如下所示。</p><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="29e3" class="je jf hi kj b fi kn ko l kp kq">from transformers import RobertaTokenizer, RobertaModel</span><span id="047c" class="je jf hi kj b fi kr ko l kp kq"># Load the Roberta tokenizer<br/>tokenizer = RobertaTokenizer.from_pretrained(‘roberta-base’)</span><span id="f891" class="je jf hi kj b fi kr ko l kp kq"># Import Roberta pretrained model<br/>bert = RobertaModel.from_pretrained(‘roberta-base’)</span></pre><h2 id="7edd" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">蒸馏模型</h2><p id="d200" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">我们可以如下导入<strong class="ih hj"> DistilBert </strong>模型。</p><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="f5b3" class="je jf hi kj b fi kn ko l kp kq">from transformers import DistilBertTokenizer, DistilBertModel</span><span id="32d7" class="je jf hi kj b fi kr ko l kp kq"># Load the DistilBert tokenizer<br/>tokenizer = DistilBertTokenizer.from_pretrained(‘distilbert-base-uncased’)</span><span id="7e5c" class="je jf hi kj b fi kr ko l kp kq"># Import the DistilBert pretrained model<br/>bert = DistilBertModel.from_pretrained(“distilbert-base-uncased”)</span></pre><p id="89c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这个例子，我们将使用DistilBert模型。</p><p id="be92" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">distilbert-base-uncased标记器的示例数据</p><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="dc16" class="je jf hi kj b fi kn ko l kp kq">text = ["this is a distil bert model.","data is oil"]</span><span id="299d" class="je jf hi kj b fi kr ko l kp kq"># Encode the text</span><span id="d4d1" class="je jf hi kj b fi kr ko l kp kq">encoded_input = tokenizer(text, padding=True,truncation=True, return_tensors='pt')</span><span id="48a8" class="je jf hi kj b fi kr ko l kp kq">print(encoded_input)</span><span id="15b9" class="je jf hi kj b fi kr ko l kp kq">In input_ids:<br/>101 - Indicates beginning of the sentence<br/>102 - Indicates end of the sentence</span><span id="fc01" class="je jf hi kj b fi kr ko l kp kq">In attention_mask:<br/>1 - Actual token<br/>0 - Padded token</span></pre><figure class="ke kf kg kh fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ks"><img src="../Images/c20a250c44d0cf0f0654ecd2c77a8bba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ONk9vldNVw6zP3q1gyclfQ.png"/></div></div></figure><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="2a4f" class="je jf hi kj b fi kn ko l kp kq"># get length of all the messages in the train set</span><span id="99ae" class="je jf hi kj b fi kr ko l kp kq">seq_len = [len(i.split()) for i in train_text]</span><span id="5483" class="je jf hi kj b fi kr ko l kp kq">pd.Series(seq_len).hist(bins = 10)</span><span id="09e3" class="je jf hi kj b fi kr ko l kp kq"># Based on the histogram we are selecting the max len as 8<br/>max_seq_len = 8</span></pre><figure class="ke kf kg kh fd kt er es paragraph-image"><div class="er es la"><img src="../Images/5caa2eb301d6f9e642dfb503ee027585.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*LZ83epLFUnGL05kzAMhOZw.png"/></div></figure><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="c6b3" class="je jf hi kj b fi kn ko l kp kq"># tokenize and encode sequences in the training set</span><span id="16c1" class="je jf hi kj b fi kr ko l kp kq">tokens_train = tokenizer(<br/>    train_text.tolist(),<br/>    max_length = max_seq_len,<br/>    pad_to_max_length=True,<br/>    truncation=True,<br/>    return_token_type_ids=False<br/>)</span></pre><p id="f452" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们将把整数序列转换成张量。</p><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="8922" class="je jf hi kj b fi kn ko l kp kq"># for train set</span><span id="0cce" class="je jf hi kj b fi kr ko l kp kq">train_seq = torch.tensor(tokens_train['input_ids'])<br/>train_mask = torch.tensor(tokens_train['attention_mask'])<br/>train_y = torch.tensor(train_labels.tolist())</span></pre><p id="1e56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将为训练集创建数据加载器。在训练阶段，这些数据加载器将成批的训练数据作为输入传递给模型。</p><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="bfc5" class="je jf hi kj b fi kn ko l kp kq">from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler</span><span id="c128" class="je jf hi kj b fi kr ko l kp kq">#define a batch size<br/>batch_size = 16</span><span id="8c86" class="je jf hi kj b fi kr ko l kp kq"># wrap tensors<br/>train_data = TensorDataset(train_seq, train_mask, train_y)</span><span id="4a18" class="je jf hi kj b fi kr ko l kp kq"># sampler for sampling the data during training<br/>train_sampler = RandomSampler(train_data)</span><span id="9276" class="je jf hi kj b fi kr ko l kp kq"># DataLoader for train set<br/>train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)</span></pre><h2 id="4ade" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">定义模型架构</h2><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="4c8b" class="je jf hi kj b fi kn ko l kp kq">class BERT_Arch(nn.Module):</span><span id="05ec" class="je jf hi kj b fi kr ko l kp kq">   def __init__(self, bert):      <br/>       super(BERT_Arch, self).__init__()</span><span id="e543" class="je jf hi kj b fi kr ko l kp kq">       self.bert = bert <br/>      <br/>       # dropout layer<br/>       self.dropout = nn.Dropout(0.2)<br/>      <br/>       # relu activation function<br/>       self.relu =  nn.ReLU()</span><span id="552c" class="je jf hi kj b fi kr ko l kp kq">       # dense layer</span><span id="7452" class="je jf hi kj b fi kr ko l kp kq">       self.fc1 = nn.Linear(768,512)</span><span id="0f71" class="je jf hi kj b fi kr ko l kp kq">       self.fc2 = nn.Linear(512,256)</span><span id="57aa" class="je jf hi kj b fi kr ko l kp kq">       self.fc3 = nn.Linear(256,5)</span><span id="c0ac" class="je jf hi kj b fi kr ko l kp kq">       #softmax activation function<br/>       self.softmax = nn.LogSoftmax(dim=1)</span><span id="120c" class="je jf hi kj b fi kr ko l kp kq">       #define the forward pass<br/>   def forward(self, sent_id, mask):</span><span id="bc75" class="je jf hi kj b fi kr ko l kp kq">      #pass the inputs to the model  <br/>      cls_hs = self.bert(sent_id, attention_mask=mask)[0][:,0]<br/>      <br/>      x = self.fc1(cls_hs)<br/>      x = self.relu(x)<br/>      x = self.dropout(x)<br/>      <br/>      x = self.fc2(x)<br/>      x = self.relu(x)<br/>      x = self.dropout(x)</span><span id="5a4f" class="je jf hi kj b fi kr ko l kp kq">      # output layer<br/>      x = self.fc3(x)<br/>   <br/>      # apply softmax activation<br/>      x = self.softmax(x)</span><span id="089e" class="je jf hi kj b fi kr ko l kp kq">      return x</span></pre></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><pre class="ki kj kk kl aw km bi"><span id="10e8" class="je jf hi kj b fi li lj lk ll lm ko l kp kq"># freeze all the parameters. This will prevent updating of model weights during fine-tuning.</span><span id="fd09" class="je jf hi kj b fi kr ko l kp kq">for param in bert.parameters():<br/>      param.requires_grad = False</span><span id="4d28" class="je jf hi kj b fi kr ko l kp kq">model = BERT_Arch(bert)</span><span id="99ca" class="je jf hi kj b fi kr ko l kp kq"># push the model to GPU<br/>model = model.to(device)</span><span id="94df" class="je jf hi kj b fi kr ko l kp kq">from torchinfo import summary<br/>summary(model)</span></pre><figure class="ke kf kg kh fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ln"><img src="../Images/2badaf0e66c2ab237302dbaaebfbbab5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*obZCLOIy0VptaaS3E4eLAg.png"/></div></div></figure></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><h2 id="51c7" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">点击此处进入下一部分:第三部分:<a class="ae jd" rel="noopener" href="/@shrinidhi.rm1990/simple-chatbot-using-bert-and-pytorch-part-3-a6832c50b8d1">模型微调</a></h2></div></div>    
</body>
</html>
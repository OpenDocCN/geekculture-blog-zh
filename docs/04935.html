<html>
<head>
<title>Facial Image Reconstruction using Autoencoders in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Keras中使用自动编码器的人脸图像重建</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/face-image-reconstruction-using-autoencoders-in-keras-69a35cde01b0?source=collection_archive---------6-----------------------#2021-07-06">https://medium.com/geekculture/face-image-reconstruction-using-autoencoders-in-keras-69a35cde01b0?source=collection_archive---------6-----------------------#2021-07-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="4283" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用自动编码器恢复噪声图像的完整指南</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/b255f916aef8dbf266c19f1df8615ec0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1TNkPB6qIyc85Qdy"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Photo by <a class="ae jn" href="https://unsplash.com/@itfeelslikefilm?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">🇸🇮 Janko Ferlič</a> on <a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5408" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">自动编码器被用作无监督的深度学习技术，用于学习<em class="kk">数据编码</em>。它们的工作原理是从给定的未标记数据中学习一种表示，并尽可能准确地从该表示中重建数据。学习到的表征状态也被称为<strong class="jq hj">潜在空间</strong>或<strong class="jq hj">代码</strong>。自动编码器分两部分工作。<strong class="jq hj">编码器</strong>(第一部分)用于学习给定图像的重要和有代表性的特征，并将其表示到潜在空间中。<strong class="jq hj">解码器</strong>(第二部分)用于通过去除图像中的噪声和不重要的特征，从潜在空间重建图像。这将产生一个压缩的图像。压缩可能是<em class="kk">有损的</em>，因为一些特征在压缩中丢失了，结果图像可能会模糊。</p><p id="0f6b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="kk">自动编码器可用于图像去噪、信息检索、数据压缩、图像彩色化和图像增强等应用。</em></p><p id="affa" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">下图展示了自动编码器工作的高级架构。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es kl"><img src="../Images/88427bb6ad3b837a3cdd878f3b0e5074.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2p_5kNHGkYCUoHMk.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Source: <a class="ae jn" href="https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798" rel="noopener" target="_blank">https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798</a></figcaption></figure><blockquote class="km kn ko"><p id="8927" class="jo jp kk jq b jr js ij jt ju jv im jw kp jy jz ka kq kc kd ke kr kg kh ki kj hb bi translated">出于重建目的，输入层和输出层中的节点数量必须相同。一般模型由一个输入、一个输出和一个隐藏层组成，但是具有多个隐藏层使它们成为<strong class="jq hj">深度自动编码器</strong>。</p></blockquote><p id="2181" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">数据通过输入层提供给编码器。在通过隐藏层之后，数据被压缩到一个代码/潜在空间中，其中相似的数据点彼此更接近。然后，潜在状态被传递到解码器，在解码器中，数据的必要模式和特征被拾取并被重新转换成原始图像。</p><h1 id="0687" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated">自动编码器的数学理解</h1><p id="4cd1" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">下图显示了数学表示。这里<em class="kk"> W </em>和<em class="kk"> V </em>分别代表编码器和解码器部分的权重。z是取编码器权重和输入的乘积，然后通过函数得到的潜在空间。之后，在解码器中，潜在空间向量乘以解码器权重以获得重建图像。均方误差用于寻找重建损失。这种损失会在训练模型以获得准确结果时传播回来。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es kl"><img src="../Images/472ac03538b8145ae72c3fbc8e496347.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*feZUg84wSSBaxNzN.jpg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">source: <a class="ae jn" href="https://www.jeremyjordan.me/autoencoders/" rel="noopener ugc nofollow" target="_blank">https://www.jeremyjordan.me/autoencoders/</a></figcaption></figure><h1 id="61b3" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated">面部图像重建示例</h1><p id="48d3" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">让我们使用人脸图像构建一个自动编码器，并尽可能精确地重建它们。我们将从<strong class="jq hj"> </strong> <a class="ae jn" href="https://github.com/NVlabs/ffhq-dataset" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj"> Flickr-Faces-HQ数据集(FFHQ) </strong> </a>数据集获取图像，并通过有意放置充当噪声的黑盒来移除它们的一些部分。原始数据集包含大小为1024 x 1024的图像，但我们只拍摄了128 x 128的图像。我们的自动编码器将尝试重建图像的丢失部分。</p><h1 id="61c7" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated"><strong class="ak">第一步:导入库… </strong></h1><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lp lq l"/></div></figure><h1 id="e30a" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated">步骤2:数据生成和预处理…</h1><p id="be94" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">将包含70，000幅图像的整个数据集一次性加载到内存中几乎是不可能的。因此，我们将实现一个<strong class="jq hj">自定义生成器函数</strong>来批量加载图像。这里，我们将从生成器返回元组<em class="kk"> (corrupted_images_batch，original_images_batch) </em>，而不是返回图像及其标签，其中损坏的图像与原始图像相同，但从中删除了一个小方块。</p><p id="0446" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在我们将创建一个函数，通过在图像上绘制一个黑色方框来移除图像的一部分。这里画的正方形在图像中的任意位置都是28x28。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lp lq l"/></div></figure><blockquote class="km kn ko"><p id="147e" class="jo jp kk jq b jr js ij jt ju jv im jw kp jy jz ka kq kc kd ke kr kg kh ki kj hb bi translated">我们使用PIL  <em class="hi">库</em> <strong class="jq hj">的<strong class="jq hj"> ImageDraw </strong> <em class="hi">函数来生成盒子。</em></strong></p></blockquote><p id="ab2f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在，我们将为训练目的生成批量数据。在这里，我们将继续迭代数据集以检索图像批次。图像从存储在文件变量中的路径中读取。我已经在Colabs目录中存储了图像的路径。读取图像后，我们会将原始图像添加到<em class="kk"> original_images_batch </em>列表中，并将预处理后的损坏图像添加到<em class="kk"> corrupted_images_batch </em>列表中。最后，我们将以元组的形式返回原始图像和损坏的图像。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lp lq l"/></div></figure><p id="206e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在批量处理数据和获取图像后，我们损坏的数据看起来会像这样。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lr"><img src="../Images/72539531fee56e7824b0650f6ba02ede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yyt9XFM6E4Rv68yNZW3SGg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by Author</figcaption></figure><h1 id="7f84" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated">第3步:模型构建…</h1><p id="b477" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">接下来，我们将通过首先创建编码器和解码器的架构来构建我们的模型。之后，我们将合并它们，得到完整的自动编码器。</p><h2 id="c2b1" class="ls kt hi bd ku lt lu lv ky lw lx ly lc jx lz ma le kb mb mc lg kf md me li mf bi translated"><strong class="ak">编码器架构</strong></h2><p id="7eb9" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">编码器架构由一堆卷积层组成，后跟一个密集(全连接)层，该层输出一个大小为<strong class="jq hj"> Z_DIM </strong>(潜在空间维度)的向量。尺寸为128x128x3的整个图像解码成这个尺寸为<strong class="jq hj"> Z_DIM </strong>的潜在空间向量。</p><p id="73ce" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">你可以参考这个<a class="ae jn" href="https://blog.keras.io/building-autoencoders-in-keras.html" rel="noopener ugc nofollow" target="_blank">链接</a>来看看如何在<em class="kk"> Keras </em>中创建Autoencoder模型。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lp lq l"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mg"><img src="../Images/5498c7585c07a655506d5b1a1f04db1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*oeYb0k9QpyXmwZSzXWAw6g.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by Author</figcaption></figure><h2 id="ad60" class="ls kt hi bd ku lt lu lv ky lw lx ly lc jx lz ma le kb mb mc lg kf md me li mf bi translated">解码器架构</h2><p id="5022" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">解码器模型通常与编码器相反，但不是强制性的。由于编码器使用卷积层来解压缩图像，为了在解码器中实现相反的效果，我们将使用<a class="ae jn" href="https://keras.io/layers/convolutional/#conv2dtranspose" rel="noopener ugc nofollow" target="_blank"> Conv2DTransponse </a>层。这一层产生的输出张量在高度和宽度上都是输入张量的两倍。解码器的输入是尺寸为<strong class="jq hj"> Z_DIM </strong>的向量，输出将是尺寸为<strong class="jq hj"> INPUT_DIM </strong> (128x128x3)的图像。</p><blockquote class="km kn ko"><p id="7503" class="jo jp kk jq b jr js ij jt ju jv im jw kp jy jz ka kq kc kd ke kr kg kh ki kj hb bi translated"><em class="hi">与编码器不同，解码器将有激活功能，因为它将输出图像。我们希望像素值在0到1之间。这里我们使用</em> <strong class="jq hj"> sigmoid </strong> <em class="hi">作为我们的激活函数。</em></p></blockquote><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lp lq l"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mh"><img src="../Images/3ede7120e4cf9d213f490b5b3666dce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*ErrMJ8OoJWaQTATY1OkRuA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by author</figcaption></figure><h2 id="21be" class="ls kt hi bd ku lt lu lv ky lw lx ly lc jx lz ma le kb mb mc lg kf md me li mf bi translated">合并编码器和解码器</h2><p id="970b" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">作为最后一步，我们将编码器附加到我们的解码器，以获得我们的自动编码器的最终版本。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lp lq l"/></div></figure><h1 id="bbc1" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated">步骤4:训练模型…</h1><p id="23be" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">快好了！现在我们只剩下训练部分了。我们可以改变各种参数，找到准确的结果。参数列表包括:</p><ul class=""><li id="488d" class="mi mj hi jq b jr js ju jv jx mk kb ml kf mm kj mn mo mp mq bi translated">学习率</li><li id="ee45" class="mi mj hi jq b jr mr ju ms jx mt kb mu kf mv kj mn mo mp mq bi translated">训练时代</li><li id="6ba9" class="mi mj hi jq b jr mr ju ms jx mt kb mu kf mv kj mn mo mp mq bi translated">批量</li><li id="e76b" class="mi mj hi jq b jr mr ju ms jx mt kb mu kf mv kj mn mo mp mq bi translated">潜在向量大小</li><li id="2420" class="mi mj hi jq b jr mr ju ms jx mt kb mu kf mv kj mn mo mp mq bi translated">误差函数</li><li id="0c4b" class="mi mj hi jq b jr mr ju ms jx mt kb mu kf mv kj mn mo mp mq bi translated">【计算机】优化程序</li></ul><p id="0b7b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我已经用<em class="kk">学习率= [0.01，0.001]，优化器= [Adam，SGD]，损失= mse，批量= 64，历元= 15，潜在空间大小= 300 </em>训练了模型</p><p id="0eb0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">训练代码如下。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lp lq l"/></div></figure><h1 id="deaf" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated">第五步:结果…</h1><p id="a9e0" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">终于到了公布结果的时候了！我的训练模型花费了大约4-5个小时在Google Colabs上讨论每个参数。在训练<strong class="jq hj">损失:0.0111 </strong>和<strong class="jq hj">准确度:85.10% </strong>的情况下，给出最佳结果的参数如下:</p><p id="7bba" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> <em class="kk">潜在空间:300，历元:50，学习率:0.001，批量:64，优化器:亚当，损耗:mse </em> </strong></p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lp lq l"/></div></figure><h2 id="a6d4" class="ls kt hi bd ku lt lu lv ky lw lx ly lc jx lz ma le kb mb mc lg kf md me li mf bi translated">图像结果</h2><p id="3f29" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">结果我得到的图像很模糊。这是因为损失函数<em class="kk"> MSE </em>平均了像素值并导致模糊。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lr"><img src="../Images/eec625019dcfbdb8a6b1ddfed35c78b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OPxSLaANbfN_GLI6w-H_FA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Given Test Images Batch</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lr"><img src="../Images/1c8f31208735a16677f74dfd809be074.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g_ZjCsWUwjhlmgv-jVmFYg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Reconstructed Images.</figcaption></figure><h2 id="cf4d" class="ls kt hi bd ku lt lu lv ky lw lx ly lc jx lz ma le kb mb mc lg kf md me li mf bi translated">其他结果！</h2><p id="3ed1" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">我用不同的参数组合得到的一些其他结果如下:</p><pre class="iy iz ja jb fd mw mx my mz aw na bi"><span id="ba44" class="ls kt hi mx b fi nb nc l nd ne"><em class="kk">Parameters: epoch:15, learning rate:0.01,  batch size:64, optimizer:Adam, loss:mse</em></span><span id="849a" class="ls kt hi mx b fi nf nc l nd ne">Results: loss_value: 0.0121 - accuracy: 0.8392</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lr"><img src="../Images/4a01ab69fdf9fc45d58161a0c48589d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K-P0Kc7F4sPKjiMUacAo4Q.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by Author</figcaption></figure><pre class="iy iz ja jb fd mw mx my mz aw na bi"><span id="e44d" class="ls kt hi mx b fi nb nc l nd ne"><em class="kk">Parameters: </em>epoch:15, learning rate:0.01,batch size:64, optimizer:SGD, loss:mse</span><span id="290b" class="ls kt hi mx b fi nf nc l nd ne">Results: loss_value: 0.0118 - accuracy: 0.8433</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lr"><img src="../Images/c10b878a08ae71d684b6efcef30f2611.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xn5DfEqhtGn_dO43DhG49A.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by Author</figcaption></figure><pre class="iy iz ja jb fd mw mx my mz aw na bi"><span id="901d" class="ls kt hi mx b fi nb nc l nd ne"><em class="kk">Parameters:</em>epoch:15, learning rate:0.001, batch size:64, optimizer:Adam, loss:mse</span><span id="637d" class="ls kt hi mx b fi nf nc l nd ne">Results: loss_value: 0.0111 - accuracy: 0.8475</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lr"><img src="../Images/4b1df20f6a09a4bc8f9d88fe8c4c5017.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gZ_tATuQoqdITfVuFYQ6iQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by Author</figcaption></figure><pre class="iy iz ja jb fd mw mx my mz aw na bi"><span id="0b25" class="ls kt hi mx b fi nb nc l nd ne"><em class="kk">Parameters:</em>epoch:15, learning rate:0.001, batch size:64, optimizer:SGD, loss:mse</span><span id="e347" class="ls kt hi mx b fi nf nc l nd ne">Results: loss_value: 0.0111 - accuracy: 0.8469</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lr"><img src="../Images/d9160569251b3a9d796abb4e33991f73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GIeLDypSmzgXydkYgIAPKw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by Author</figcaption></figure><pre class="iy iz ja jb fd mw mx my mz aw na bi"><span id="1f70" class="ls kt hi mx b fi nb nc l nd ne"><em class="kk">Parameters:</em>epoch:30, learning rate:0.0001, batch size:32, optimizer:Adam, loss:mse</span><span id="fd60" class="ls kt hi mx b fi nf nc l nd ne">Results: loss_value: 0.0125 - accuracy: 0.8357</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lr"><img src="../Images/731fa6b8c5f15c0d70f740f765a999ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JxzxNOll8UIpRR2t_zOUmg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by Author</figcaption></figure><pre class="iy iz ja jb fd mw mx my mz aw na bi"><span id="a972" class="ls kt hi mx b fi nb nc l nd ne"><em class="kk">Parameters:</em>epoch:10, learning rate:0.01, batch size:64, optimizer:RMSprop, loss:binary_crossentropy</span><span id="aaf6" class="ls kt hi mx b fi nf nc l nd ne">Results: loss_value: 0.5712 - accuracy: 0.8022</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lr"><img src="../Images/7d5750ea4e69cdb925e2644732f081d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*em7L6jbeS7Y1cmalZ7kzXg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by Author</figcaption></figure><h1 id="1523" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated">第六步:结论…</h1><p id="3fb9" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">对于图像去噪、重建和异常检测，我们可以使用自动编码器，但它们在生成图像时并不太有效，因为它们会变得模糊。它们效率低下的最大原因是自动编码器的<em class="kk"> </em> <strong class="jq hj"> <em class="kk">潜在空间不连续，因为它们只学习数据</em> </strong>的单个潜在表示。</p><h1 id="cc09" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated">参考资料…</h1><p id="ff1e" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">我在撰写本文时使用的一些参考资料如下:</p><ul class=""><li id="4988" class="mi mj hi jq b jr js ju jv jx mk kb ml kf mm kj mn mo mp mq bi translated"><a class="ae jn" href="https://medium.com/r?url=https%3A%2F%2Fwww.jeremyjordan.me%2Fautoencoders%2F" rel="noopener">https://medium.com/r?URL = https % 3A % 2F % 2fwww . Jeremy Jordan . me % 2 fauto encoders % 2F</a></li><li id="1be9" class="mi mj hi jq b jr mr ju ms jx mt kb mu kf mv kj mn mo mp mq bi translated"><a class="ae jn" href="https://medium.com/r?url=https%3A%2F%2Ftowardsdatascience.com%2Fapplied-deep-learning-part-3-autoencoders-1c083af4d798" rel="noopener">https://medium.com/r?URL = https % 3A % 2F % 2 ftowardsdata science . com % 2f applied-deep-learning-part-3-auto encoders-1c 083 af 4d 798</a></li><li id="6a3b" class="mi mj hi jq b jr mr ju ms jx mt kb mu kf mv kj mn mo mp mq bi translated"><a class="ae jn" href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" rel="noopener" target="_blank">https://towards data science . com/understanding-variable-auto encoders-vaes-f 70510919 f 73</a></li></ul><p id="868c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">那都是关于自动编码器的。如果你觉得它有帮助、有趣，或者想提出一些反馈意见，请随时给我发电子邮件至emanijaz583@gmail.com<strong class="jq hj"><em class="kk"/></strong><em class="kk">或者通过</em><a class="ae jn" href="http://www.linkedin.com/in/eman-ijaz" rel="noopener ugc nofollow" target="_blank"><strong class="jq hj"><em class="kk">Linkedin</em></strong></a><em class="kk">与我联系。</em>你可以在Google Colab <a class="ae jn" href="https://drive.google.com/file/d/1Cz4md6kgwxC7ncw9IbcrhcVpKb21M-we/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里</a>找到完整的代码。我将很快尝试写另一篇关于variable Autoencoder(一种更高级的Autoencoder)及其与auto encoder的比较的文章。</p></div></div>    
</body>
</html>
<html>
<head>
<title>Deploying GPU-based Models on SageMaker using ‘Multi-Model’ Endpoint (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用“多模型”端点在SageMaker上部署基于GPU的模型(第1部分)</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/deploying-gpu-based-models-on-sagemaker-using-multi-model-endpoint-part-1-da68cbbf3d04?source=collection_archive---------5-----------------------#2022-10-24">https://medium.com/geekculture/deploying-gpu-based-models-on-sagemaker-using-multi-model-endpoint-part-1-da68cbbf3d04?source=collection_archive---------5-----------------------#2022-10-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/24c881effa8bd02ff7dd81d351f36b64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3z3WB_t-bj2DLF2n"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Photo by <a class="ae iu" href="https://unsplash.com/@shots_of_aspartame?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Julia Joppien</a> on <a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="iv iw ix"><p id="d824" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">本文是duology <strong class="jb hj">“使用‘多模型’端点在SageMaker上部署基于GPU的模型”</strong>的第一部分。你可以在本文底部找到第二部分的链接。</p><p id="e8c2" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jb hj">2022年26月10日更新:</strong>我刚刚发现<a class="ae iu" href="https://developer.nvidia.com/blog/run-multiple-ai-models-on-same-gpu-with-sagemaker-mme-powered-by-triton/" rel="noopener ugc nofollow" target="_blank"> <strong class="jb hj">一篇来自NVIDIA </strong> </a>的文章，提到使用NVIDIA Triton推理服务器在亚马逊SageMaker的多模型端点上运行基于GPU的模型。我自己还没有检查这个功能，但如果这是真的，这是一个游戏改变者。</p><p id="d684" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jb hj">2022年17月11日更新:</strong>我从<a class="ae iu" href="https://www.linkedin.com/feed/update/urn:li:activity:6990336296409858048?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A6990336296409858048%2C6998715078984302592%29&amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%286998715078984302592%2Curn%3Ali%3Aactivity%3A6990336296409858048%29" rel="noopener ugc nofollow" target="_blank">一位AWS SageMaker员工</a>那里得到消息，AWS终于在SageMaker上发布了多模型端点GPU。酷！你可以在这里 阅读文章<a class="ae iu" href="https://aws.amazon.com/about-aws/whats-new/2022/10/amazon-sagemaker-cost-effectively-host-1000s-gpu-multi-model-endpoint/" rel="noopener ugc nofollow" target="_blank"> <strong class="jb hj">。</strong></a></p></blockquote><h1 id="0034" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">目录:</h1><p id="0e9c" class="pw-post-body-paragraph iy iz hi jb b jc kv je jf jg kw ji jj kx ky jm jn kz la jq jr lb lc ju jv jw hb bi translated">第一部分的文章安排如下:</p><ul class=""><li id="2929" class="ld le hi jb b jc jd jg jh kx lf kz lg lb lh jw li lj lk ll bi translated">简介<br/> -优化实时推理的成本<br/> -关于多模型端点</li><li id="fce7" class="ld le hi jb b jc lm jg ln kx lo kz lp lb lq jw li lj lk ll bi translated">你需要什么</li><li id="631c" class="ld le hi jb b jc lm jg ln kx lo kz lp lb lq jw li lj lk ll bi translated">准备工具和环境</li><li id="25d9" class="ld le hi jb b jc lm jg ln kx lo kz lp lb lq jw li lj lk ll bi translated">数据准备和模型训练<br/> - PyTorch模型样本<br/> - TensorFlow模型样本</li><li id="dcf4" class="ld le hi jb b jc lm jg ln kx lo kz lp lb lq jw li lj lk ll bi translated">模型预测<br/> - PyTorch <br/> - TensorFlow</li><li id="abc9" class="ld le hi jb b jc lm jg ln kx lo kz lp lb lq jw li lj lk ll bi translated">部署<br/> -目录结构<br/> -编写推理. py脚本</li></ul><h1 id="73ea" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">介绍</h1><p id="7625" class="pw-post-body-paragraph iy iz hi jb b jc kv je jf jg kw ji jj kx ky jm jn kz la jq jr lb lc ju jv jw hb bi translated"><a class="ae iu" href="https://aws.amazon.com/sagemaker/" rel="noopener ugc nofollow" target="_blank">亚马逊SageMaker </a>是AWS的托管服务之一，提供从数据管道、ML/AI项目、模型部署到MLOps的端到端解决方案。SageMaker是数据科学家和ML工程师的一体化工具。</p><p id="8c14" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">当你要将最终训练好的模型投入生产时，在SageMaker中，通常你可以有两种选择:<a class="ae iu" href="https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html" rel="noopener ugc nofollow" target="_blank">实时推理</a>和<a class="ae iu" href="https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html" rel="noopener ugc nofollow" target="_blank">批量转换</a>。对于实时推理，您的模型将被部署在一个类似REST-API的端点上，您可以在任何时候实时访问该端点。对于批量转换，您的模型将以一种方式部署，在这种方式下，它不会24/7在线，但是，相反，当您使用批量/批量数据到达端点时，它可以被激活。</p><p id="9949" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">对于前者，AWS将根据您部署模型的实例的正常运行时间向您收费。对于后者，当您的模型处理大量数据时，您只需支付正常运行时间的费用。</p><h2 id="e910" class="lr jy hi bd jz ls lt lu kd lv lw lx kh kx ly lz kl kz ma mb kp lb mc md kt me bi translated">优化实时推理的成本</h2><p id="3395" class="pw-post-body-paragraph iy iz hi jb b jc kv je jf jg kw ji jj kx ky jm jn kz la jq jr lb lc ju jv jw hb bi translated">基于此，我们可以推断，实时推理的代价一定比批量转换高。AWS有几种解决方案来克服昂贵的实时预测，例如:</p><ul class=""><li id="7370" class="ld le hi jb b jc jd jg jh kx lf kz lg lb lh jw li lj lk ll bi translated"><a class="ae iu" href="https://docs.aws.amazon.com/elastic-inference/latest/developerguide/what-is-ei.html" rel="noopener ugc nofollow" target="_blank">亚马逊弹性推论</a></li><li id="26b6" class="ld le hi jb b jc lm jg ln kx lo kz lp lb lq jw li lj lk ll bi translated"><a class="ae iu" href="https://aws.amazon.com/machine-learning/inferentia/" rel="noopener ugc nofollow" target="_blank"> AWS推理</a></li><li id="2da8" class="ld le hi jb b jc lm jg ln kx lo kz lp lb lq jw li lj lk ll bi translated"><a class="ae iu" href="https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html" rel="noopener ugc nofollow" target="_blank">无服务器推理</a></li><li id="0ab8" class="ld le hi jb b jc lm jg ln kx lo kz lp lb lq jw li lj lk ll bi translated"><a class="ae iu" href="https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html" rel="noopener ugc nofollow" target="_blank">多模型端点</a></li><li id="4d51" class="ld le hi jb b jc lm jg ln kx lo kz lp lb lq jw li lj lk ll bi translated">等等。</li></ul><p id="effb" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">然而，在本文中，我们将关注多模型端点的方法。如<a class="ae iu" href="https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html" rel="noopener ugc nofollow" target="_blank"> AWS网页</a>所述:</p><blockquote class="mf"><p id="bba5" class="mg mh hi bd mi mj mk ml mm mn mo jw dx translated">若要创建可以承载多个模型的终结点，请使用多模型终结点。多模型端点为部署大量模型提供了一个可扩展且经济高效的解决方案。</p></blockquote><p id="1f94" class="pw-post-body-paragraph iy iz hi jb b jc mp je jf jg mq ji jj kx mr jm jn kz ms jq jr lb mt ju jv jw hb bi translated">因此，简单地说，通过使用多模型端点方法，我们可以在单个端点中部署多个模型。这确实会优化所需的成本，因为我们只需要为我们使用的终端付费，即只有一个终端，而我们部署了许多模型。</p><h2 id="542d" class="lr jy hi bd jz ls lt lu kd lv lw lx kh kx ly lz kl kz ma mb kp lb mc md kt me bi translated">关于多模型端点</h2><p id="4baf" class="pw-post-body-paragraph iy iz hi jb b jc kv je jf jg kw ji jj kx ky jm jn kz la jq jr lb lc ju jv jw hb bi translated">关于多模型端点方法，我有两条消息:一条坏消息和一条好消息。</p><ul class=""><li id="f97f" class="ld le hi jb b jc jd jg jh kx lf kz lg lb lh jw li lj lk ll bi translated">坏消息</li></ul><p id="78a7" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">h̶o̶w̶e̶v̶e̶r̶,̶̶s̶a̶d̶l̶y̶,̶̶w̶h̶e̶n̶̶t̶h̶i̶s̶̶a̶r̶t̶i̶c̶l̶e̶̶i̶s̶̶w̶r̶i̶t̶t̶e̶n̶,̶̶a̶w̶s̶̶o̶f̶f̶i̶c̶i̶a̶l̶l̶y̶̶s̶t̶a̶t̶e̶s̶̶t̶h̶a̶t̶̶t̶h̶e̶̶m̶u̶l̶t̶i̶-̶m̶o̶d̶e̶l̶̶e̶n̶d̶p̶o̶i̶n̶t̶̶a̶p̶p̶r̶o̶a̶c̶h̶̶i̶s̶̶n̶o̶t̶̶s̶u̶p̶p̶o̶r̶t̶e̶d̶̶o̶n̶̶g̶p̶u̶̶i̶n̶s̶t̶a̶n̶c̶e̶̶t̶y̶p̶e̶s̶̶(̶y̶e̶t̶？̶̶i̶̶d̶o̶n̶'̶t̶̶k̶n̶o̶w̶)̶.̶̶p̶e̶r̶h̶a̶p̶s̶,̶̶t̶h̶i̶s̶̶c̶o̶r̶r̶e̶l̶a̶t̶e̶s̶̶t̶o̶̶g̶p̶u̶̶i̶n̶s̶t̶a̶n̶c̶e̶̶r̶e̶s̶o̶u̶r̶c̶e̶̶m̶a̶n̶a̶g̶e̶m̶e̶n̶t̶s̶̶t̶h̶a̶t̶̶a̶r̶e̶̶m̶o̶r̶e̶̶c̶h̶a̶l̶l̶e̶n̶g̶i̶n̶g̶̶t̶h̶a̶n̶̶m̶a̶n̶a̶g̶i̶n̶g̶̶r̶e̶s̶o̶u̶r̶c̶e̶s̶̶f̶o̶r̶̶c̶p̶u̶̶i̶n̶s̶t̶a̶n̶c̶e̶s̶；̶̶b̶u̶t̶̶i̶̶d̶o̶n̶'̶t̶̶k̶n̶o̶w̶；̶̶i̶t̶'̶s̶̶j̶u̶s̶t̶̶m̶y̶̶s̶p̶e̶c̶u̶l̶a̶t̶i̶o̶n̶.̶<strong class="jb hj">请阅读我上面提到的更新信息。</strong></p><ul class=""><li id="ef11" class="ld le hi jb b jc jd jg jh kx lf kz lg lb lh jw li lj lk ll bi translated"><strong class="jb hj">好消息</strong></li></ul><p id="0af7" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">然而，幸运的是，我们仍然可以拥有自己的“多模型”端点，能够托管基于GPU的模型。要做到这一点，我们得在实时推理中黑掉几样东西。总之，我从<a class="ae iu" href="https://de.linkedin.com/in/philipp-schmid-a6a2bb196" rel="noopener ugc nofollow" target="_blank"> Philipp Schmid(抱脸技术负责人)</a>在抱脸论坛<a class="ae iu" href="https://discuss.huggingface.co/t/when-to-use-sagemaker-multi-model-endpoint/18781/2" rel="noopener ugc nofollow" target="_blank">的回答中得到启发</a>。</p><blockquote class="iv iw ix"><p id="3636" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jb hj">免责声明:</strong>我不会深入讨论如何开发一个性能良好的模型，因为本文更侧重于使用“多模型”端点的部署。因此，我们将只使用具有几个训练时期的简单模型。</p></blockquote><h1 id="7b60" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">你需要什么</h1><p id="1161" class="pw-post-body-paragraph iy iz hi jb b jc kv je jf jg kw ji jj kx ky jm jn kz la jq jr lb lc ju jv jw hb bi translated">在本文中，我们将使用Keras-TensorFlow和PyTorch部署两个基于GPU的模型。因此，除了很好地理解Python之外，如果你也有一些使用<a class="ae iu" href="https://keras.io/getting_started/" rel="noopener ugc nofollow" target="_blank"> Keras </a>、<a class="ae iu" href="https://www.tensorflow.org/tutorials/quickstart/beginner" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>和<a class="ae iu" href="https://pytorch.org/get-started/locally/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>的知识将会很有帮助。</p><p id="f826" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">当我们在SageMaker上工作时，您还需要一个至少具有以下<a class="ae iu" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html" rel="noopener ugc nofollow" target="_blank"> IAM策略/角色</a>的<a class="ae iu" href="https://aws.amazon.com/console/" rel="noopener ugc nofollow" target="_blank"> AWS帐户</a>:</p><ul class=""><li id="be4b" class="ld le hi jb b jc jd jg jh kx lf kz lg lb lh jw li lj lk ll bi translated">使用<a class="ae iu" href="https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html" rel="noopener ugc nofollow" target="_blank"> SageMaker笔记本实例</a>或<a class="ae iu" href="https://aws.amazon.com/sagemaker/studio/" rel="noopener ugc nofollow" target="_blank">SageMaker Studio</a>；</li><li id="c172" class="ld le hi jb b jc lm jg ln kx lo kz lp lb lq jw li lj lk ll bi translated">访问(上传和下载)将用于存储<code class="du mu mv mw mx b">model.tar.gz</code>文件的<a class="ae iu" href="https://aws.amazon.com/s3/" rel="noopener ugc nofollow" target="_blank"> S3桶</a>；</li><li id="7771" class="ld le hi jb b jc lm jg ln kx lo kz lp lb lq jw li lj lk ll bi translated">将模型部署到<a class="ae iu" href="https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html" rel="noopener ugc nofollow" target="_blank"> SageMaker推理端点</a>。</li></ul><p id="f3e0" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">下面的网页有助于定义上述IAM政策/角色。</p><div class="my mz ez fb na nb"><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hj fi z dy ng ea eb nh ed ef hh bi translated">SageMaker角色</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">作为一项托管服务，Amazon SageMaker代表您在AWS硬件上执行操作，该硬件由…</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">docs.aws.amazon.com</p></div></div></div></a></div><h1 id="9762" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">准备工具和环境</h1><p id="e3e9" class="pw-post-body-paragraph iy iz hi jb b jc kv je jf jg kw ji jj kx ky jm jn kz la jq jr lb lc ju jv jw hb bi translated">当你决定使用SageMaker笔记本时，你可以按照下面的说明来设置一个。</p><div class="my mz ez fb na nb"><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hj fi z dy ng ea eb nh ed ef hh bi translated">创建笔记本实例</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">Amazon SageMaker notebook实例是运行Jupyter Notebook应用程序的ML计算实例。SageMaker管理…</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">docs.aws.amazon.com</p></div></div></div></a></div><div class="my mz ez fb na nb"><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hj fi z dy ng ea eb nh ed ef hh bi translated">步骤1:创建Amazon SageMaker笔记本实例</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">亚马逊SageMaker笔记本实例是一个完全托管的机器学习(ML)亚马逊弹性计算云(亚马逊…</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">docs.aws.amazon.com</p></div></div><div class="nk l"><div class="nl l nm nn no nk np io nb"/></div></div></a></div><p id="54e2" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">否则，如果您更喜欢使用SageMaker Studio，您可以按照下面的说明来设置一个。</p><div class="my mz ez fb na nb"><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-create-open.html" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hj fi z dy ng ea eb nh ed ef hh bi translated">创建或打开一个亚马逊SageMaker工作室笔记本</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">当您在Amazon SageMaker Studio中创建笔记本或首次在Studio中打开非共享笔记本时，您…</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">docs.aws.amazon.com</p></div></div></div></a></div><blockquote class="iv iw ix"><p id="9bdd" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jb hj">注意:</strong>您可以决定是否在您的笔记本环境中使用GPU实例。但是对于本文，我不会使用GPU实例，因为使用的模型非常简单，即使使用了PyTorch和TensorFlow。</p></blockquote><p id="9f0c" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">无论您选择哪种笔记本来源，您都必须确保PyTorch和Tensorflow都已成功安装在您的笔记本系统中。只需按照下面列出的他们网站上的说明。</p><div class="my mz ez fb na nb"><a href="https://pytorch.org/" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hj fi z dy ng ea eb nh ed ef hh bi translated">PyTorch</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">使用PyTorch、TorchServe和AWS Inferentia降低71%的推理成本并推动横向扩展。推动…的状态</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">pytorch.org</p></div></div><div class="nk l"><div class="nq l nm nn no nk np io nb"/></div></div></a></div><div class="my mz ez fb na nb"><a href="https://www.tensorflow.org/install" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hj fi z dy ng ea eb nh ed ef hh bi translated">安装TensorFlow 2</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">了解如何在您的系统上安装TensorFlow。下载一个pip包，在Docker容器中运行，或者从…</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">www.tensorflow.org</p></div></div><div class="nk l"><div class="nr l nm nn no nk np io nb"/></div></div></a></div><p id="09f2" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">您可以将这两个库安装在它们各自的conda虚拟环境中，但是我建议您只在一个conda环境中安装这两个库，以便在创建<code class="du mu mv mw mx b">model.tar.gz</code>文件之前进行调试。</p><h1 id="46a9" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">数据准备和模型训练</h1><p id="a1b2" class="pw-post-body-paragraph iy iz hi jb b jc kv je jf jg kw ji jj kx ky jm jn kz la jq jr lb lc ju jv jw hb bi translated">在本文中，出于示例的目的，我们将为每个库使用两个简单的模型。</p><ul class=""><li id="0841" class="ld le hi jb b jc jd jg jh kx lf kz lg lb lh jw li lj lk ll bi translated">PyTorch模型样本</li></ul><p id="d0ee" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">对于PyTorch，我们将使用一个使用MNIST数据集的图像分类模型。我们可以用来自<a class="ae iu" href="https://github.com/pytorch/examples/tree/main/mnist" rel="noopener ugc nofollow" target="_blank"> PyTorch的GitHub官方回购</a>的例子。您可以使用您需要的任何参数运行<code class="du mu mv mw mx b">main.py</code> Python文件(不要忘记设置<code class="du mu mv mw mx b">save-model</code>参数，因为我们需要部署模型)。之后，您将得到与下面类似的输出。</p><figure class="nt nu nv nw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ns"><img src="../Images/0fabd16ddbebc0538eb380c8ec23748e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YlXDblRMBnzDg7fIIH2gQA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Training a model using PyTorch</figcaption></figure><p id="ec57" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">在培训过程完成之后，请记住您保存的模型的路径，因为您稍后将需要它来进行部署。</p><ul class=""><li id="0350" class="ld le hi jb b jc jd jg jh kx lf kz lg lb lh jw li lj lk ll bi translated">张量流模型示例</li></ul><p id="c7bc" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">对于TensorFlow，我们将使用一个使用IMBD数据集的文本分类模型。我们可以用来自<a class="ae iu" href="https://www.tensorflow.org/tutorials/keras/text_classification_with_hub" rel="noopener ugc nofollow" target="_blank"> TensorFlow网站</a>的例子。只需按照这里列出的说明进行操作，并保存您训练好的模型和标记器。稍后部署时会用到它们。</p><figure class="nt nu nv nw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nx"><img src="../Images/2774d346a948be21c3221a8a36a30b2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8tUPxDH_WdcvpPxjQsx5Zg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Training a model using TensorFlow</figcaption></figure><figure class="nt nu nv nw fd ij"><div class="bz dy l di"><div class="ny nz l"/></div></figure><h1 id="f313" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">模型预测法</h1><p id="6544" class="pw-post-body-paragraph iy iz hi jb b jc kv je jf jg kw ji jj kx ky jm jn kz la jq jr lb lc ju jv jw hb bi translated">训练过程完成后，我们应该尝试测试两个模型的预测，以检查两个模型是否都有效。</p><ul class=""><li id="846b" class="ld le hi jb b jc jd jg jh kx lf kz lg lb lh jw li lj lk ll bi translated">PyTorch</li></ul><p id="417f" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">对于PyTorch模型，我们首先必须定义模型架构来加载训练好的权重。对于测试集，我们简单地使用PyTorch库提供的MNIST数据集。结果将是指示由输入图像显示的数字的预测数字。</p><figure class="nt nu nv nw fd ij"><div class="bz dy l di"><div class="ny nz l"/></div></figure><p id="a696" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">下面是结果:</p><figure class="nt nu nv nw fd ij er es paragraph-image"><div class="er es oa"><img src="../Images/949ba25e53256c62a5fa1bea3822695d.png" data-original-src="https://miro.medium.com/v2/resize:fit:80/format:webp/1*VGo1dCs9R1_RcFH3Hj6eQw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Result from PyTorch model prediction</figcaption></figure><ul class=""><li id="6000" class="ld le hi jb b jc jd jg jh kx lf kz lg lb lh jw li lj lk ll bi translated">张量流</li></ul><p id="119d" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">对于张量流模型，我们应该定义几个有用的变量，例如<code class="du mu mv mw mx b">label_mapping</code>和<code class="du mu mv mw mx b">max_len</code>，这对我们的预处理步骤有帮助。我们必须加载训练好的模型和标记器。输出将是输入文本的情感。</p><figure class="nt nu nv nw fd ij"><div class="bz dy l di"><div class="ny nz l"/></div></figure><p id="c556" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">下面是结果:</p><figure class="nt nu nv nw fd ij er es paragraph-image"><div class="er es ob"><img src="../Images/4a92c878b63ccd75996901af09b55b03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*15sCwMaexr8pf9C1NoQk1g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Results from TensorFlow model prediction</figcaption></figure><h1 id="318d" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">部署</h1><h2 id="9507" class="lr jy hi bd jz ls lt lu kd lv lw lx kh kx ly lz kl kz ma mb kp lb mc md kt me bi translated">目录结构</h2><p id="6c06" class="pw-post-body-paragraph iy iz hi jb b jc kv je jf jg kw ji jj kx ky jm jn kz la jq jr lb lc ju jv jw hb bi translated">为了部署到SageMaker端点推断，我们必须遵循AWS定义的目录结构，如这里的<a class="ae iu" href="https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#model-directory-structure" rel="noopener ugc nofollow" target="_blank">或下面的</a>所示。</p><figure class="nt nu nv nw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es oc"><img src="../Images/2e657d931bfebf9d7602318db3fb500a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vueuT3E5yZonFjg9ForfYQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">The basic directory structure for deploying to SageMaker Endpoint Inference</figcaption></figure><p id="8356" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">简单地说，您必须创建一个目录(任何名称),其中包含使用您的模型进行加载和预测所需的子目录和文件。然后，您必须将这样一个目录压缩成一个名为<code class="du mu mv mw mx b">model.tar.gz</code>的<code class="du mu mv mw mx b">.tar.gz</code>文件扩展名。</p><p id="bb27" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">在我们创建自己的“多模型”端点的例子中，在主目录中(这将是<code class="du mu mv mw mx b">model.tar.gz</code>文件)，我们将遵循如下内容:</p><figure class="nt nu nv nw fd ij er es paragraph-image"><div class="er es od"><img src="../Images/84b785630e13aafd3696ad8144c1f8f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*tu4d3iC2dfZTSTizd0iHeg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Proposed directory structure</figcaption></figure><p id="f3ea" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">因此，我们将创建两个子目录，即一个用于已训练PyTorch模型，另一个用于已训练TensorFlow模型。在每个文件中，我们将放置与每个文件相关的所有文件(和文件夹，如果有的话)。</p><p id="3da3" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">通过这样做，我的<code class="du mu mv mw mx b">model.tar.gz</code>看起来如下:</p><figure class="nt nu nv nw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es oe"><img src="../Images/d30b3dcaf1b8e71c4acd1e84016c86c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2PsKzUl2q13S-iBIcKxjyA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">My directory structure</figcaption></figure><h2 id="eb21" class="lr jy hi bd jz ls lt lu kd lv lw lx kh kx ly lz kl kz ma mb kp lb mc md kt me bi translated">写作<code class="du mu mv mw mx b">inference.py script</code></h2><p id="00b4" class="pw-post-body-paragraph iy iz hi jb b jc kv je jf jg kw ji jj kx ky jm jn kz la jq jr lb lc ju jv jw hb bi translated">在我们的例子中，我们将使用所谓的<a class="ae iu" href="https://sagemaker-examples.readthedocs.io/en/latest/frameworks/pytorch/get_started_mnist_deploy.html#Entry-Point-for-the-Inference-Image" rel="noopener ugc nofollow" target="_blank">【自带模型】</a> (BYOM)。因此，我们必须编写一个脚本，即<code class="du mu mv mw mx b">inference.py</code>，以告诉SageMaker如何处理我们自己的模型(在SageMaker培训工作之外培训的)，用于模型的输入(通过REST API)，加载模型，模型预测，以及模型的输出(通过REST API)。</p><p id="e106" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated"><code class="du mu mv mw mx b">inference.py</code>文件脚本读取文档<a class="ae iu" href="https://sagemaker-examples.readthedocs.io/en/latest/frameworks/pytorch/get_started_mnist_deploy.html" rel="noopener ugc nofollow" target="_blank">中所述的4个主要函数，此处为</a>:</p><ul class=""><li id="3542" class="ld le hi jb b jc jd jg jh kx lf kz lg lb lh jw li lj lk ll bi translated"><code class="du mu mv mw mx b">model_fn()</code></li></ul><p id="4d1c" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">该函数用于告诉推理映像如何加载模型检查点。该函数有一个参数，即<code class="du mu mv mw mx b">model_dir</code>(推理图像中静态模型检查点的目录)。该函数返回加载模型的变量<code class="du mu mv mw mx b">model</code>。</p><ul class=""><li id="8291" class="ld le hi jb b jc jd jg jh kx lf kz lg lb lh jw li lj lk ll bi translated"><code class="du mu mv mw mx b">input_fn()</code></li></ul><p id="22b5" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">该函数在推理入口点被调用。它处理来自请求输入的数据解码。它有两个参数:<code class="du mu mv mw mx b">request_body</code>(传入请求的有效负载)和<code class="du mu mv mw mx b">request_content_type</code>(传入请求的内容类型)。该函数返回一个对象(即加载模型的输入)，该对象可以传递给<code class="du mu mv mw mx b">predict_fn()</code>函数。</p><ul class=""><li id="5308" class="ld le hi jb b jc jd jg jh kx lf kz lg lb lh jw li lj lk ll bi translated"><code class="du mu mv mw mx b">predict_fn()</code></li></ul><p id="e1a1" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">该函数使用加载的模型消耗<code class="du mu mv mw mx b">input_fn()</code>函数的即时输出，该模型是<code class="du mu mv mw mx b">model_fn()</code>函数的返回值。该函数有两个参数:<code class="du mu mv mw mx b">input_object</code>(来自<code class="du mu mv mw mx b">input_fn()</code>函数的返回值)和<code class="du mu mv mw mx b">model</code>(来自<code class="du mu mv mw mx b">model_fn()</code>函数的返回值)。该函数返回要传递给<code class="du mu mv mw mx b">output_fn()</code>函数的第一个参数。</p><ul class=""><li id="9393" class="ld le hi jb b jc jd jg jh kx lf kz lg lb lh jw li lj lk ll bi translated"><code class="du mu mv mw mx b">output_fn()</code></li></ul><p id="2459" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">该函数用于对由<code class="du mu mv mw mx b">predict_fn()</code>函数产生的预测进行数据后处理。该函数有两个参数，即<code class="du mu mv mw mx b">prediction</code>(<code class="du mu mv mw mx b">predict_fn()</code>函数的返回值)和<code class="du mu mv mw mx b">content_type</code>(响应的内容类型)。函数的返回应该是序列化到<code class="du mu mv mw mx b">content_type</code>的数据的字节数组，用于将预测编码到响应的内容类型中。</p></div><div class="ab cl of og gp oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="hb hc hd he hf"><p id="a0af" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">对于一个简单的BYOM情况，通常，我们可以只使用上述四个函数中的一个，即<code class="du mu mv mw mx b">model_fn()</code>函数。对于这种情况，我们只改变推理服务器加载模型的方式；我们让其余的函数作为默认函数。</p><p id="3ad4" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">然而，在我们想要拥有自己的“多模式”终端的情况下，我们需要定制所有四个功能。</p><ul class=""><li id="cc6a" class="ld le hi jb b jc jd jg jh kx lf kz lg lb lh jw li lj lk ll bi translated"><code class="du mu mv mw mx b">model_fn()</code></li></ul><p id="8d23" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">该函数接受一个参数<code class="du mu mv mw mx b">model_dir</code>，它是<code class="du mu mv mw mx b">model.tar.gz</code>文件中的位置。由于我们已经将两个模型目录放入其中，即前面定义的目录结构中提到的<code class="du mu mv mw mx b">image_model/</code>和<code class="du mu mv mw mx b">text_model/</code>，我们将在这个<code class="du mu mv mw mx b">model_fn()</code>函数中加载这两个目录。</p><p id="db45" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">如下面的代码片段所示，首先，我们加载图像模型，即经过训练的PyTorch模型，然后我们还加载文本模型，即经过训练的TensorFlow模型，以及经过训练的tokenizer。最后，我们将函数设置为返回两个模型(使用标记器),其输出将是一个元组。</p><figure class="nt nu nv nw fd ij"><div class="bz dy l di"><div class="ny nz l"/></div></figure><ul class=""><li id="d799" class="ld le hi jb b jc jd jg jh kx lf kz lg lb lh jw li lj lk ll bi translated"><code class="du mu mv mw mx b">input_fn()</code></li></ul><p id="3bb2" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">在这个函数中，我们将真正创建一个定制函数。由于SageMaker端点推断作为REST API工作，我们必须设置该函数在REST API中工作。我们可以使用多种类型的<a class="ae iu" href="https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html" rel="noopener ugc nofollow" target="_blank"/><code class="du mu mv mw mx b"><a class="ae iu" href="https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html" rel="noopener ugc nofollow" target="_blank">content_type</a></code>，但是我们将在这里使用<code class="du mu mv mw mx b">application/json</code>，因为我们也将使用JSON数据访问服务器。</p><p id="28d0" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">我们将让我们的推理服务器从客户端接收以下JSON请求:</p><pre class="nt nu nv nw fd om mx on oo aw op bi"><span id="baaa" class="lr jy hi mx b fi oq or l os ot">{<br/>  "model_type": "text OR image",<br/>  "inputs": "(a text IF 'model_type' == 'text') OR<br/>             (a list of numbers with shape (any, 1, 28, 28) IF<br/>             'model_type' == 'image')"<br/>}</span></pre><p id="7e8c" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">为此，我们可以定义<code class="du mu mv mw mx b">input_fn()</code>函数，如下面的代码片段所示。我实现了下面的异常处理<code class="du mu mv mw mx b">TypeError</code>,因为有时我发现只使用一次<code class="du mu mv mw mx b">json.loads()</code>是不够的，会导致代码试图访问字典的元素时出错。随后，当来自客户端请求的<code class="du mu mv mw mx b">"model_type”</code>字段被设置为<code class="du mu mv mw mx b">“image”</code>时，我们期望得到一个形状为(any，1，28，28)的数字列表，然后将其转换为<code class="du mu mv mw mx b">torch.Tensor</code>数据类型。最后，我们返回处理过的数据。</p><figure class="nt nu nv nw fd ij"><div class="bz dy l di"><div class="ny nz l"/></div></figure><ul class=""><li id="c282" class="ld le hi jb b jc jd jg jh kx lf kz lg lb lh jw li lj lk ll bi translated"><code class="du mu mv mw mx b">predict_fn()</code></li></ul><p id="7cba" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">由于该函数接收两个参数，我们将输入来自<code class="du mu mv mw mx b">input_fn()</code>的输出和来自<code class="du mu mv mw mx b">model_fn()</code>的输出。由于后者是一个元组(因为我们已经这样做了),它由图像模型、文本模型和标记器组成，我们必须根据它们的需要来分割它。</p><p id="f382" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">然后，我们可以根据每个模型类型应用预测部分。有关更多详细信息，请参见下面的代码片段。</p><figure class="nt nu nv nw fd ij"><div class="bz dy l di"><div class="ny nz l"/></div></figure><ul class=""><li id="55af" class="ld le hi jb b jc jd jg jh kx lf kz lg lb lh jw li lj lk ll bi translated"><code class="du mu mv mw mx b">output_fn()</code></li></ul><p id="4468" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">正如在<code class="du mu mv mw mx b">input_fn()</code>函数中定义的，我们使用<code class="du mu mv mw mx b">application/json</code>作为内容类型，我们将在这个<code class="du mu mv mw mx b">output_fn()</code>函数中使用相同的内容类型。如下面的代码片段所示，我们根据模型类型来处理推理服务器的最终输出(将作为响应返回给客户机)。如果预测数据属于<code class="du mu mv mw mx b">torch.Tensor</code>数据类型(或者换句话说，它是来自图像模型的数据)，我们将把它转换成一个原始数字。否则，最终输出只是文本模型生成文本时的预测数据。最后，我们以JSON的形式返回带有字段<code class="du mu mv mw mx b">“pred”</code>的最终输出。</p><figure class="nt nu nv nw fd ij"><div class="bz dy l di"><div class="ny nz l"/></div></figure></div><div class="ab cl of og gp oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="hb hc hd he hf"><p id="58a3" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">对于支持上面四个主要函数所需的其他变量和对象，你可以在这里找到我的<code class="du mu mv mw mx b">inference.py</code> <a class="ae iu" href="https://github.com/utomoreza/MultimodelEndpoint_hacked/blob/main/model/code/inference.py" rel="noopener ugc nofollow" target="_blank">的例子。</a></p><p id="30ef" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated"><strong class="jb hj">加法</strong></p><p id="9a29" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">对于驻留在<code class="du mu mv mw mx b">code/</code>目录中的<code class="du mu mv mw mx b">requirements.txt</code>文件，我们可以编写下面的库列表。</p><pre class="nt nu nv nw fd om mx on oo aw op bi"><span id="6bc5" class="lr jy hi mx b fi oq or l os ot">numpy<br/>tensorflow</span></pre><p id="886c" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj kx jl jm jn kz jp jq jr lb jt ju jv jw hb bi translated">尽管我们使用PyTorch模型，但我没有在上面列出<code class="du mu mv mw mx b">torch</code>库的原因是因为我们将使用PyTorch推理映像来进行部署(我们稍后将对此进行讨论)。因此，<code class="du mu mv mw mx b">torch</code>库已经包含在这样的映像中。</p><blockquote class="iv iw ix"><p id="0e16" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">我们将在第二部分讨论接下来的部分，即调试、上传到S3桶、使用<code class="du mu mv mw mx b">sagemaker</code>库部署、调用部署的端点和约束。您可以在下面找到第2部分的链接。</p></blockquote><div class="my mz ez fb na nb"><a href="https://utomorezadwi.medium.com/deploying-gpu-based-models-on-sagemaker-using-multi-model-endpoint-part-2-final-6e05cf10142f" rel="noopener follow" target="_blank"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hj fi z dy ng ea eb nh ed ef hh bi translated">使用“多模型”端点在SageMaker上部署基于GPU的模型(第2部分-最终版)</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">本文是最后一部分。如果您可以在SageMaker上部署一个基于GPU的多模型端点会怎么样？</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">utomorezadwi.medium.com</p></div></div><div class="nk l"><div class="ou l nm nn no nk np io nb"/></div></div></a></div></div><div class="ab cl of og gp oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="hb hc hd he hf"><h1 id="0a06" class="jx jy hi bd jz ka ov kc kd ke ow kg kh ki ox kk kl km oy ko kp kq oz ks kt ku bi translated">关于作者</h1><p id="3ac1" class="pw-post-body-paragraph iy iz hi jb b jc kv je jf jg kw ji jj kx ky jm jn kz la jq jr lb lc ju jv jw hb bi translated">雷扎是一名专门从事数据驱动分析的工程师。他目前在Tokopedia担任高级数据科学家。业余时间喜欢在<a class="ae iu" rel="noopener" href="/@utomorezadwi">媒体</a>上写文章，学点新东西，或者<a class="ae iu" href="https://github.com/utomoreza" rel="noopener ugc nofollow" target="_blank">创作副业</a>。</p></div></div>    
</body>
</html>
<html>
<head>
<title>Deep Learning — A Beginners Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习——初学者指南</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/deep-learning-a-to-z-part-1-1d5bd4e9944c?source=collection_archive---------3-----------------------#2020-11-22">https://medium.com/geekculture/deep-learning-a-to-z-part-1-1d5bd4e9944c?source=collection_archive---------3-----------------------#2020-11-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/03c4f5af2ff7afd1260c37402563464d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cx8P2_tLOhWPvXjIc2xP9A.jpeg"/></div></div></figure><p id="ae05" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">曾经想知道什么是深度学习，以及它如何改变我们做事的方式。在这一系列教程中，我将深入探讨深度学习领域中使用的术语。一个完整的数学背后的激活函数，损失，函数，优化等等。一路上，我也会分享一些我觉得有用的链接，而不是在文章结尾才提到。</p><p id="1167" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">深度学习:</strong>简而言之，<strong class="is hj"> <em class="jo">“深度学习就是让机器像人脑一样思考和学习”。</em></strong> <em class="jo"> </em>为了做到这一点科学家们想出了神经网络的概念。术语“深”是指网络的深度。</p><p id="0e03" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jp" href="https://en.wikipedia.org/wiki/Andrew_Ng" rel="noopener ugc nofollow" target="_blank">吴恩达</a>是<a class="ae jp" href="https://en.wikipedia.org/wiki/Google_Brain" rel="noopener ugc nofollow" target="_blank">谷歌大脑</a>的联合创始人和领导者，他在2013年关于<a class="ae jp" href="https://www.youtube.com/watch?v=n1ViNeWhC24" rel="noopener ugc nofollow" target="_blank">深度学习、自学和无监督特征学习</a>的演讲中很好地解释了这个概念。下图说明了深度学习的必要性。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es jq"><img src="../Images/092b1fc12a8a2ec9d19b9c1f91f889d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*J55pauUQcPZn1-ulZgleDw.jpeg"/></div><figcaption class="jv jw et er es jx jy bd b be z dx">Figure 1 — Need of Deep Learning</figcaption></figure><p id="5aa0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">神经网络:</strong>尝试在谷歌中搜索神经网络，你会得到定义<strong class="is hj"> <em class="jo">“一个模仿人脑和神经系统的计算机系统”。</em> </strong> <em class="jo"> </em>我发现这个定义对于一个门外汉来说更加贴切和简单。神经网络是一系列相互连接的神经元，它们一起工作以产生输出。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jz"><img src="../Images/707dd016f5d366232d8349fe326359ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*gtgY9vkWM0zhFsZ4jIczbg.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx">Figure 2 — A Simple Neural Network</figcaption></figure><p id="c878" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第一层是输入层。这一层中的每个节点接受一个输入或一个特征，然后将其输出传递给下一层中的每个节点。同一层内的节点不相连。最后一层产生输出。隐藏层具有与输入或输出无关的神经元。它们被来自前一层节点的输入激活。</p><p id="cde2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们了解一些与神经网络功能相关的微小细节。我在下面放了一个简化的图表，以便解释概念，使图表不那么笨拙。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ka"><img src="../Images/f593967b694f8a086c7a987dd50d2bf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XPxkfdZmQgQ0-GXW5DjgCA.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx">Figure 3 — Neural Network Details</figcaption></figure><p id="706b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上图中有不少术语。让我们一个一个来看</p><p id="a2d4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">输入(x1，x2，x3): </strong>输入是预测输出所基于的输入值或特征。例如，“通过/失败输出(y)将根据输入的学习时间(x1)、玩耍时间(x2)和睡眠时间(x3)来决定”</p><p id="6872" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">权重(w1，w2，w3): </strong>权重表示输入的强度。换句话说，权重决定了输入对输出的影响程度。考虑到上面的例子，学习时间可能比其他两个时间更重要。</p><p id="8c5a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> Bias (b1): </strong>在神经网络中加入Bias，照顾零输入。偏置单元确保神经元即使在全零输入的情况下也将被激活。值得注意的是，这个值不受先前层的影响。如果你知道线性函数<em class="jo"> y = mx + c，</em>，你可以将偏差与常数<em class="jo">‘c’联系起来。</em>偏置值允许向左或向右移动激活功能。在这篇<a class="ae jp" href="https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks" rel="noopener ugc nofollow" target="_blank"> stackoverflow帖子</a>中解释得非常好。</p><h1 id="0762" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">神经元内的操作</strong></h1><p id="a0ac" class="pw-post-body-paragraph iq ir hi is b it kz iv iw ix la iz ja jb lb jd je jf lc jh ji jj ld jl jm jn hb bi translated">每个神经元完成的操作总是2步。</p><p id="8569" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">加法器或预激活函数:</strong>不确定是否有明确的名称，但我打算将第一个操作称为加法器。在此步骤中，计算输入和权重乘积的总和。在这一步骤中，我们还考虑了偏差。注意，当权重和输入可能变化时，偏差保持不变。它由下面的函数定义:</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es le"><img src="../Images/f71064ed22c91b09782ae350b2c1736b.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*V8b9duQPAPgg1iSQavBJwQ.png"/></div></figure><p id="94b4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">考虑到我们上面的例子，它可以写成:</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/5be98041e05dd9bad8838b5d08b72ba2.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*K5sBHRsKNYUDbyCg3HoV9g.png"/></div></figure><p id="395f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">激活函数:</strong>激活函数取加法器函数计算出的值，将其转换为0到1之间的数(激活(0)，去激活(1))。该函数根据与模型预测相关的神经元输入，确定是否应该激活(激发)神经元。有许多不同的激活功能，我将在后续文章中介绍它们。下面是一个Sigmoid激活函数的描述。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/c8a2151dd55a9d9562ef2b40435e40df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*WhhWPRcHdemZVUpubkfAKw.png"/></div><figcaption class="jv jw et er es jx jy bd b be z dx">Figure 4 — Sigmoid Activation Function</figcaption></figure><h1 id="e9c7" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">正向传播</h1><p id="39e1" class="pw-post-body-paragraph iq ir hi is b it kz iv iw ix la iz ja jb lb jd je jf lc jh ji jj ld jl jm jn hb bi translated">计算隐藏层和输出层中的值的完整过程称为正向传播。这种网络也被称为<em class="jo">前馈网络</em>。请注意，我刚刚显示了一个隐藏层中的计算，其中有一个神经元。在真实的神经网络中，可以有多个隐藏层，每个层中有多个神经元。在产生最终输出之前，在所有层中重复相同的过程。</p><h1 id="7c58" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">损失函数、反向传播和优化器</h1><p id="8038" class="pw-post-body-paragraph iq ir hi is b it kz iv iw ix la iz ja jb lb jd je jf lc jh ji jj ld jl jm jn hb bi translated">在我介绍新概念之前，理解我们为什么需要它们是很重要的。为此，让我们举个例子。我取一条输出(y)值为1的记录。我们将建立一个简单的神经网络来根据输入特征x1、x2、x3预测该输出。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lh"><img src="../Images/cf75db28fccd92f8193bd303f1bbb0c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C4ThfYuflsJ1p3jPDrLPVw.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx">Figure 5— Neural Network Example</figcaption></figure><figure class="jr js jt ju fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es li"><img src="../Images/4247be9831b782c492cb3daebec9af28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*19xgIclfELDKxyxDVPIkUg.png"/></div></div></figure><p id="79e4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如上所述，第一个操作是隐藏层中的加法器功能(<em class="jo"> a11 </em>)，随后是激活功能(<em class="jo"> h11 </em>)，在这种情况下转换为:</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/fba8dae0de60822765b8fb0a7c2298d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*csj-LOEEVI_GnGBNQS3ASw.png"/></div></figure><p id="5ff1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在输出层重复相同的计算，如下所示。注意<em class="jo"> h11 </em>是来自隐藏层的输入。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/f6d97cacc61a3e1023d9ad0e3ad41b64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*rvw7qWov2ixaOkpiRYCFVQ.png"/></div></figure><p id="4fc4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">h21是输出，它只不过是我们的Y-hat。Y的真实值是1。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/5dd69f68a748fa6c50d1207d7488f310.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/format:webp/1*rzORoN0faRgB_tKaTDzImg.png"/></div></figure><p id="9199" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">目标应该是使Y和Y-hat更接近，以便我们在给定一组输入参数的情况下准确预测输出。现在是时候理解关于<em class="jo">损失函数、反向传播</em>和<em class="jo">优化器的新概念了。</em></p><p id="4bb1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">损失函数:</strong>简单来说，损失就是神经网络的预测误差，计算损失的方法称为损失函数。有许多不同的损失函数，我将在后续主题中介绍。现在，让我们使用均方误差(MSE)损失函数。考虑我们上面的例子，MSE是:</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/2f64e9444c661625f3e40c780eaba4c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*uJ9XGp-5py-vIEXQD_WkMQ.png"/></div></figure><p id="7e62" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">优化器:</strong>优化器是负责减少损失(Y-Y-hat)的算法或方法。减少损失的方法是更新权重和偏置参数。还有一个参数叫做“学习率”,当我写优化器的时候我会介绍这个参数。有不同的优化器，如梯度下降(GD)，随机梯度下降(SGD)，阿达格拉德，亚当等。</p><h1 id="0207" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">反向传播</h1><p id="1e63" class="pw-post-body-paragraph iq ir hi is b it kz iv iw ix la iz ja jb lb jd je jf lc jh ji jj ld jl jm jn hb bi translated">反向传播的目的是更新权重以减少损失(Y-Y-hat)。它通过考虑损失函数和使用优化器来更新权重来做到这一点。一旦我们完成一组正向传播和反向传播，我们称之为迭代。我们不断重复这一过程，直到我们减少(Y-Y-hat ),以便获得更准确的结果。新术语可以想象如下。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lh"><img src="../Images/3d04a7c2a43e43e4e5ee111e2070fa20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zTAsdLVqz3BoEJtDJNRCfQ.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx">Figure 6 — Back Propagation</figcaption></figure><p id="2537" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以给你。这是人工神经网络(ANN)的要点。下面的图表描述了行动的顺序，可能有助于消化一些东西。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ln"><img src="../Images/cf73bd0707ae8678f6edb8f67f0be762.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w5JP73IkQBUnVI8LvTgQ_A.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx">Figure 7 — Learning Cycle</figcaption></figure><p id="332f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我的下一篇文章中，我将探索不同的激活功能，并讨论何时使用什么。我希望您对这个广阔而令人兴奋的领域有一个快速的了解。不断学习。</p><p id="6890" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">继续阅读本系列的下一篇文章:</p><p id="5515" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jp" href="https://srinivas-kulkarni.medium.com/deep-learning-a-to-z-part-2-mnist-the-hello-world-of-neural-networks-2429c4367086" rel="noopener">https://srinivas-kulkarni . medium . com/deep-learning-a-to-z-part-2-Mn ist-the-hello-world-of-neural-networks-2429 c 4367086</a></p></div></div>    
</body>
</html>
<html>
<head>
<title>Computer Vision using Mediapipe</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Mediapipe的计算机视觉</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/computer-vision-using-mediapipe-c0933b743cf5?source=collection_archive---------19-----------------------#2021-06-21">https://medium.com/geekculture/computer-vision-using-mediapipe-c0933b743cf5?source=collection_archive---------19-----------------------#2021-06-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/70edb5d0b3229c1d5cb53dd167809303.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0pjZL6xQIcoP8-UQyGidhA.png"/></div></div></figure><h1 id="6e10" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">💻我们对计算机视觉的理解是什么？</strong></h1><figure class="jo jp jq jr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b18ac9f4b8fa42d7eb49616a65382679.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hVgPeJATyDpg8bX9xcxIvw.png"/></div></div></figure><p id="7160" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated"><strong class="ju hj">计算机视觉</strong>可以定义为训练计算机解释和理解视觉世界的人工智能领域。使用来自相机和视频的数字图像以及深度学习模型，机器可以准确地识别和分类对象，然后对他们“看到的”做出反应。<br/>计算机视觉是一个跨学科的科学领域，研究计算机如何从数字图像或视频中获得高层次的理解。<br/>在当今世界，计算机视觉在许多领域非常有用，例如:</p><p id="406b" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated"><strong class="ju hj"> <em class="kq"> *库存管理— : </em> </strong>在库存管理的情况下，应用可以在<strong class="ju hj">安全摄像机图像分析</strong>的领域中，其中计算机视觉算法可以生成对商店中可用物品的非常准确的估计。另一个领域是<strong class="ju hj">分析货架空间的使用情况，以确定次优配置</strong>。</p><p id="bc62" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">* <strong class="ju hj">制造— : </strong>在制造领域，计算机视觉可以帮助机器的<strong class="ju hj">预测性维护</strong>。</p><p id="7ca6" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated"><strong class="ju hj"> *医疗保健— : </strong>在医疗保健领域，计算机视觉可用于<strong class="ju hj">医学图像分析。</strong>来自CT扫描和X射线的图像被分析以发现异常，如肿瘤或寻找神经疾病的迹象。</p><p id="b9bc" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">* <strong class="ju hj">自动驾驶汽车— : </strong>计算机视觉领域在自动驾驶汽车领域发挥着核心作用，因为它允许它们感知和理解周围的环境，以便正确操作。计算机视觉中最令人兴奋的挑战之一是图像和视频中的目标检测。这涉及到定位不同数量的对象以及对它们进行分类的能力，以便区分对象是交通灯、汽车还是人，如下视频所示。</p></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="4ec9" class="iq ir hi bd is it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn bi translated">💻什么是mediaPipe？</h1><p id="a6f5" class="pw-post-body-paragraph js jt hi ju b jv ld jx jy jz le kb kc kd lf kf kg kh lg kj kk kl lh kn ko kp hb bi translated">MediaPipe是PyPI上预先构建的Python包。它还为用户提供了构建自己的解决方案的工具。MediaPipe Python包在PyPI上适用于Linux、macOS和Windows。</p><p id="fa42" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在MediaPipe的帮助下我们可以做的事情</p><ul class=""><li id="192f" class="li lj hi ju b jv jw jz ka kd lk kh ll kl lm kp ln lo lp lq bi translated">人脸检测和人脸网格</li><li id="d035" class="li lj hi ju b jv lr jz ls kd lt kh lu kl lv kp ln lo lp lq bi translated">姿态和整体检测</li><li id="07a4" class="li lj hi ju b jv lr jz ls kd lt kh lu kl lv kp ln lo lp lq bi translated">目标检测和跟踪</li></ul><p id="cfcc" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在这篇博客中，我解释了以下项目:</p><ul class=""><li id="5f5b" class="li lj hi ju b jv jw jz ka kd lk kh ll kl lm kp ln lo lp lq bi translated">手部识别</li><li id="d488" class="li lj hi ju b jv lr jz ls kd lt kh lu kl lv kp ln lo lp lq bi translated">姿态估计</li><li id="8c69" class="li lj hi ju b jv lr jz ls kd lt kh lu kl lv kp ln lo lp lq bi translated">手指计数</li><li id="60d6" class="li lj hi ju b jv lr jz ls kd lt kh lu kl lv kp ln lo lp lq bi translated">卷宗集合</li></ul></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="9ae2" class="iq ir hi bd is it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn bi translated">💻手部识别</h1><figure class="jo jp jq jr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/a24721e5153a9dfa4203d27dff69c298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fMBLvkdLbg0MEfv7KbJZjQ.png"/></div></div></figure><p id="a18b" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在手识别中，我首先创建了一个检测和跟踪手的脚本，最初我设置了手的阈值，但我们也可以编辑它，增加需要检测的手的数量。我还为这个项目创建了一个模块，这样我们就不必在“手指计数”等其他项目中一遍又一遍地编写整个代码过程。</p><pre class="jo jp jq jr fd lx ly lz ma aw mb bi"><span id="906f" class="mc ir hi ly b fi md me l mf mg">import cv2<br/>import mediapipe as mp<br/>import time<br/>import Handsrecognition_Module as htm</span><span id="9391" class="mc ir hi ly b fi mh me l mf mg">cap = cv2.VideoCapture(0)</span><span id="3954" class="mc ir hi ly b fi mh me l mf mg">mpHands = mp.solutions.hands<br/>hands = mpHands.Hands()<br/>mpDraw = mp.solutions.drawing_utils</span><span id="4a0c" class="mc ir hi ly b fi mh me l mf mg">pTime = 0<br/>cTime = 0</span><span id="4ee6" class="mc ir hi ly b fi mh me l mf mg">while True:<br/>    success, img = cap.read()<br/>    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br/>    results = hands.process(imgRGB)<br/>    print(results.multi_hand_landmarks)<br/>    <br/>    if results.multi_hand_landmarks:<br/>        for handLms in results.multi_hand_landmarks:<br/>            for id,lm in results.enumerate(handLms.landmarks):<br/>                #print(id,lm)<br/>                h, w, c =img.shape<br/>                cx, cy =int(lm.x*w),int(lm.y*h)<br/>                print(id, cx, cy)<br/>                if id==0:<br/>                    cv2.circle(img,(cx,cy),15,(255,0,255),cv2.FILLED)<br/>                <br/>            mpDraw.draw_landmarks(img, handLms,mpHands.HAND_CONNECTIONS)<br/>    cTime = time.time()<br/>    fps = 1/(cTime-pTime)<br/>    pTime = cTime<br/>    cv2.putText(img, str(int(fps)),(10,70),cv2.FONT_HERSHEY_PLAIN,3,(255,0,255),3)<br/>    <br/>    cv2.imshow("Image",img)<br/>    cv2.waitKey(1)</span></pre><figure class="jo jp jq jr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/f12993a388a8c55ce79d7ac2ad95d125.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*36mLCYmsVQFvVC4rnL1rPA.gif"/></div></div></figure><h1 id="8855" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">💻姿态估计</h1><figure class="jo jp jq jr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mj"><img src="../Images/2f8f1c9f31b7c63f0d3d9bb63247f602.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qk-H_QYyhBhGR5UbDZzweA.jpeg"/></div></div></figure><p id="c988" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">这个项目检测和跟踪物体的位置。我已经在摄像机和mp4文件上进行了测试，它们看起来都运行良好。姿势估计基于手和腿跟踪的基本概念。基本上，它的工作原理是，首先我们必须为系统提供姿势标志，这样它就可以检测个人的手和腿，除了面部，我们还提供鼻子、眼睛等的标志</p><figure class="jo jp jq jr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/a5ac53f44fcd61f314f8d267eff12f96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f4QmESby6G0SzME1kvscJw.png"/></div></div></figure><pre class="jo jp jq jr fd lx ly lz ma aw mb bi"><span id="3445" class="mc ir hi ly b fi md me l mf mg">import cv2<br/>import mediapipe as mp<br/>import time<br/>mpDraw = mp.solutions.drawing_utils<br/>mpPose = mp.solutions.pose<br/>pose = mpPose.Pose()</span><span id="c8bc" class="mc ir hi ly b fi mh me l mf mg">cap =cv2.VideoCapture('3.mp4')<br/>pT = 0 <br/>while True:<br/>   success, img = cap.read()<br/>   imgRGB=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br/>   results = pose.process(imgRGB)<br/>   #print(results.pose_landmarks)<br/>   if results.pose_landmarks:<br/>       mpDraw.draw_landmarks(img,results.pose_landmarks,mpPose.POSE_CONNECTIONS )<br/>       for id,lm in enumerate(results.pose_landmarks.landmark):<br/>           h,w,c=img.shape<br/>           cx, cy =int(lm.x*w),int(lm.y*h)<br/>           cv2.circle(img,(cx,cy),5,(255,0,0),cv2.FILLED)<br/>           #print(id, cx, cy)<br/>   cT =time.time()<br/>   fps = 1/(cT-pT)<br/>   pT = cT<br/>   cv2.putText(img, str(int(fps)),(10,70),cv2.FONT_HERSHEY_PLAIN,3,(255,0,255),3)<br/>   cv2.imshow("Image",img)<br/>   cv2.waitKey(1)</span></pre><p id="4359" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在位置估计的情况下，我们有“姿势”功能，如果我们想要模型有所不同，我们必须提供所有的数据。</p><figure class="jo jp jq jr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/46ed5bc75c2b145677915c3e3acb55de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*yGa1Nk3d9XM50_8Vcux2jg.gif"/></div></div></figure><h1 id="fec9" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">💻手指计数</h1><figure class="jo jp jq jr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/c426d712c49c488ba15e0a608ee47a93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e7CogvoSv4zlC73xrYipUw.png"/></div></div></figure><p id="8624" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">这是我在计算机视觉领域最喜欢的学习之一<br/>在这个项目中，我创建了一个计算机视觉脚本，它可以识别手的运动，并确定用户正在指示的数字。</p><pre class="jo jp jq jr fd lx ly lz ma aw mb bi"><span id="5ba1" class="mc ir hi ly b fi md me l mf mg">import cv2<br/>import time<br/>import os<br/>import Handsrecognition_Module as htm</span><span id="26a8" class="mc ir hi ly b fi mh me l mf mg">wCam, hCam = 640, 480</span><span id="4ac3" class="mc ir hi ly b fi mh me l mf mg">cap = cv2.VideoCapture(0)<br/>cap.set(3, wCam)<br/>cap.set(4, hCam)</span><span id="7c48" class="mc ir hi ly b fi mh me l mf mg">folderPath = "My_image"<br/>myList = os.listdir(folderPath)<br/>print(myList)<br/>overlayList = []<br/>for imPath in myList:<br/>    image = cv2.imread(f'{folderPath}/{imPath}')<br/>    # print(f'{folderPath}/{imPath}')<br/>    overlayList.append(image)</span><span id="60ee" class="mc ir hi ly b fi mh me l mf mg">print(len(overlayList))<br/>pTime = 0</span><span id="140c" class="mc ir hi ly b fi mh me l mf mg">detector = htm.handDetector(detectionCon=0.75)</span><span id="adea" class="mc ir hi ly b fi mh me l mf mg">tipIds = [4, 8, 12, 16, 20]</span><span id="c219" class="mc ir hi ly b fi mh me l mf mg">while True:<br/>    success, img = cap.read()<br/>    img = detector.findHands(img)<br/>    lmList = detector.findPosition(img, draw=False)<br/>    # print(lmList)</span><span id="48d0" class="mc ir hi ly b fi mh me l mf mg">if len(lmList) != 0:<br/>        fingers = []</span><span id="48b4" class="mc ir hi ly b fi mh me l mf mg"># Thumb<br/>        if lmList[tipIds[0]][1] &gt; lmList[tipIds[0] - 1][1]:<br/>            fingers.append(1)<br/>        else:<br/>            fingers.append(0)</span><span id="c0e9" class="mc ir hi ly b fi mh me l mf mg"># 4 Fingers<br/>        for id in range(1, 5):<br/>            if lmList[tipIds[id]][2] &lt; lmList[tipIds[id] - 2][2]:<br/>                fingers.append(1)<br/>            else:<br/>                fingers.append(0)</span><span id="f6e1" class="mc ir hi ly b fi mh me l mf mg"># print(fingers)<br/>        totalFingers = fingers.count(1)<br/>        print(totalFingers)</span><span id="a9f7" class="mc ir hi ly b fi mh me l mf mg">h, w, c = overlayList[totalFingers - 1].shape<br/>        img[0:h, 0:w] = overlayList[totalFingers - 1]</span><span id="de62" class="mc ir hi ly b fi mh me l mf mg">cv2.rectangle(img, (20, 225), (170, 425), (0, 255, 0), cv2.FILLED)<br/>        cv2.putText(img, str(totalFingers), (45, 375), cv2.FONT_HERSHEY_PLAIN,<br/>                    10, (255, 0, 0), 25)</span><span id="d658" class="mc ir hi ly b fi mh me l mf mg">cTime = time.time()<br/>    fps = 1 / (cTime - pTime)<br/>    pTime = cTime</span><span id="1619" class="mc ir hi ly b fi mh me l mf mg">cv2.putText(img, f'FPS: {int(fps)}', (400, 70), cv2.FONT_HERSHEY_PLAIN,<br/>                3, (255, 0, 0), 3)</span><span id="665c" class="mc ir hi ly b fi mh me l mf mg">cv2.imshow("Image", img)<br/>    cv2.waitKey(1)</span></pre><p id="a11f" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">为了让这个脚本最初工作，我们必须创建一个“手识别”模块，这样我们就可以将它的特性导入到我们的代码中。然后，我们将为我们可以创建列表的手定义参数。在我的例子中，我已经创建了一个名为TipID的列表，在这个列表中，作为输入，我输入了手的每个指尖的界标号(作为参考，我们可以参考手识别的手界标图)。</p><pre class="jo jp jq jr fd lx ly lz ma aw mb bi"><span id="3e7e" class="mc ir hi ly b fi md me l mf mg">import cv2<br/>import time<br/>import os<br/>import Handsrecognition_Module as htm</span><span id="8fd1" class="mc ir hi ly b fi mh me l mf mg">wCam, hCam = 640, 480</span><span id="bbbc" class="mc ir hi ly b fi mh me l mf mg">cap = cv2.VideoCapture(0)<br/>cap.set(3, wCam)<br/>cap.set(4, hCam)</span><span id="7073" class="mc ir hi ly b fi mh me l mf mg">folderPath = "My_image"<br/>myList = os.listdir(folderPath)<br/>print(myList)<br/>overlayList = []<br/>for imPath in myList:<br/>    image = cv2.imread(f'{folderPath}/{imPath}')<br/>    # print(f'{folderPath}/{imPath}')<br/>    overlayList.append(image)</span><span id="e658" class="mc ir hi ly b fi mh me l mf mg">print(len(overlayList))<br/>pTime = 0</span><span id="dabe" class="mc ir hi ly b fi mh me l mf mg">detector = htm.handDetector(detectionCon=0.75)</span><span id="fabb" class="mc ir hi ly b fi mh me l mf mg">tipIds = [4, 8, 12, 16, 20]</span><span id="08a0" class="mc ir hi ly b fi mh me l mf mg">while True:<br/>    success, img = cap.read()<br/>    img = detector.findHands(img)<br/>    lmList = detector.findPosition(img, draw=False)<br/>    # print(lmList)</span><span id="a582" class="mc ir hi ly b fi mh me l mf mg">if len(lmList) != 0:<br/>        fingers = []</span><span id="f171" class="mc ir hi ly b fi mh me l mf mg"># Thumb<br/>        if lmList[tipIds[0]][1] &gt; lmList[tipIds[0] - 1][1]:<br/>            fingers.append(1)<br/>        else:<br/>            fingers.append(0)</span><span id="e255" class="mc ir hi ly b fi mh me l mf mg"># 4 Fingers<br/>        for id in range(1, 5):<br/>            if lmList[tipIds[id]][2] &lt; lmList[tipIds[id] - 2][2]:<br/>                fingers.append(1)<br/>            else:<br/>                fingers.append(0)</span><span id="0a5e" class="mc ir hi ly b fi mh me l mf mg"># print(fingers)<br/>        totalFingers = fingers.count(1)<br/>        print(totalFingers)</span><span id="c196" class="mc ir hi ly b fi mh me l mf mg">h, w, c = overlayList[totalFingers - 1].shape<br/>        img[0:h, 0:w] = overlayList[totalFingers - 1]</span><span id="838e" class="mc ir hi ly b fi mh me l mf mg">cv2.rectangle(img, (20, 225), (170, 425), (0, 255, 0), cv2.FILLED)<br/>        cv2.putText(img, str(totalFingers), (45, 375), cv2.FONT_HERSHEY_PLAIN,<br/>                    10, (255, 0, 0), 25)</span><span id="23a5" class="mc ir hi ly b fi mh me l mf mg">cTime = time.time()<br/>    fps = 1 / (cTime - pTime)<br/>    pTime = cTime</span><span id="c2c2" class="mc ir hi ly b fi mh me l mf mg">cv2.putText(img, f'FPS: {int(fps)}', (400, 70), cv2.FONT_HERSHEY_PLAIN,<br/>                3, (255, 0, 0), 3)</span><span id="3824" class="mc ir hi ly b fi mh me l mf mg">cv2.imshow("Image", img)<br/>    cv2.waitKey(1)</span></pre><figure class="jo jp jq jr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/e5ab30fe0a4eaf0d92bb3e2514588b7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*T0tcdfIQF1EFXzxKsnYMCA.gif"/></div></div></figure><h1 id="5156" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">💻卷宗集合</h1><figure class="jo jp jq jr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/3096d1d0c10472a200764108c3587e7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*MaRWIH2OlDE9NkO6RYUuEA.gif"/></div></div></figure><p id="b8e1" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在这个音量设置项目中，我将OpenCV与操作系统集成在一起，这样我就可以在手势的帮助下改变系统的音量。所以为了完成这个项目，我已经确定并创建了食指和拇指的音量设置标志，因为这两个手指之间的距离相对来说是最大的。</p><pre class="jo jp jq jr fd lx ly lz ma aw mb bi"><span id="cdc0" class="mc ir hi ly b fi md me l mf mg">import cv2<br/>import time<br/>import numpy as np<br/>import Handsrecognition_Module as htm<br/>import math<br/>from ctypes import cast, POINTER<br/>from comtypes import CLSCTX_ALL<br/>from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume</span><span id="edf2" class="mc ir hi ly b fi mh me l mf mg">wCam, hCam = 640, 480</span><span id="a7e6" class="mc ir hi ly b fi mh me l mf mg">cap = cv2.VideoCapture(0)<br/>cap.set(3, wCam)<br/>cap.set(4, hCam)<br/>pTime = 0</span><span id="b2ce" class="mc ir hi ly b fi mh me l mf mg">detector = htm.handDetector(detectionCon=0.7)</span><span id="0743" class="mc ir hi ly b fi mh me l mf mg">devices = AudioUtilities.GetSpeakers()<br/>interface = devices.Activate(<br/>    IAudioEndpointVolume._iid_, CLSCTX_ALL, None)<br/>volume = cast(interface, POINTER(IAudioEndpointVolume))<br/># volume.GetMute()<br/># volume.GetMasterVolumeLevel()<br/>volRange = volume.GetVolumeRange()<br/>minVol = volRange[0]<br/>maxVol = volRange[1]<br/>vol = 0<br/>volBar = 400<br/>volPer = 0<br/>while True:<br/>    success, img = cap.read()<br/>    img = detector.findHands(img)<br/>    lmList = detector.findPosition(img, draw=False)<br/>    if len(lmList) != 0:<br/>        # print(lmList[4], lmList[8])</span><span id="88f3" class="mc ir hi ly b fi mh me l mf mg">x1, y1 = lmList[4][1], lmList[4][2]<br/>        x2, y2 = lmList[20][1], lmList[20][2]<br/>        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2</span><span id="8a55" class="mc ir hi ly b fi mh me l mf mg">cv2.circle(img, (x1, y1), 10, (255, 0, 255), cv2.FILLED)<br/>        cv2.circle(img, (x2, y2), 10, (255, 0, 255), cv2.FILLED)<br/>        cv2.line(img, (x1, y1), (x2, y2), (255, 0, 255), 3)<br/>        cv2.circle(img, (cx, cy), 10, (255, 0, 255), cv2.FILLED)</span><span id="f2f8" class="mc ir hi ly b fi mh me l mf mg">length = math.hypot(x2 - x1, y2 - y1)<br/>        # print(length)</span><span id="4551" class="mc ir hi ly b fi mh me l mf mg"># Hand range 50 - 300<br/>        # Volume Range -65 - 0</span><span id="0c12" class="mc ir hi ly b fi mh me l mf mg">vol = np.interp(length, [50, 300], [minVol, maxVol])<br/>        volBar = np.interp(length, [50, 300], [400, 150])<br/>        volPer = np.interp(length, [50, 300], [0, 100])<br/>        print(int(length), vol)<br/>        volume.SetMasterVolumeLevel(vol, None)</span><span id="681c" class="mc ir hi ly b fi mh me l mf mg">if length &lt; 50:<br/>            cv2.circle(img, (cx, cy), 15, (0, 255, 0), cv2.FILLED)</span><span id="c434" class="mc ir hi ly b fi mh me l mf mg">cv2.rectangle(img, (50, 150), (85, 400), (255, 0, 0), 3)<br/>    cv2.rectangle(img, (50, int(volBar)), (85, 400), (255, 0, 0), cv2.FILLED)<br/>    cv2.putText(img, f'{int(volPer)} %', (40, 450), cv2.FONT_HERSHEY_COMPLEX,<br/>                1, (255, 0, 0), 3)<br/>while True:<br/>    success, img = cap.read()<br/>    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<br/>    results = hands.process(imgRGB)<br/>    print(results.multi_hand_landmarks)<br/>    <br/>    if results.multi_hand_landmarks:<br/>        for handLms in results.multi_hand_landmarks:<br/>            for id,lm in results.enumerate(handLms.landmarks):<br/>                #print(id,lm)<br/>                h, w, c =img.shape<br/>                cx, cy =int(lm.x*w),int(lm.y*h)<br/>                print(id, cx, cy)<br/>                if id==0:<br/>                    cv2.circle(img,(cx,cy),15,(255,0,255),cv2.FILLED)<br/>                <br/>            mpDraw.draw_landmarks(img, handLms,mpHands.HAND_CONNECTIONS)<br/>    cTime = time.time()<br/>    fps = 1/(cTime-pTime)<br/>    pTime = cTime<br/>    cv2.putText(img, str(int(fps)),(10,70),cv2.FONT_HERSHEY_PLAIN,3,(255,0,255),3)<br/>    <br/>    cv2.imshow("Image",img)<br/>    cv2.waitKey(1)</span></pre><p id="d3fa" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">这里是我学到的一些关于MediaPipe和OpenCV <br/>的概念，希望你喜欢这个博客😊<br/>快乐学习🎇🎇🎇</p></div></div>    
</body>
</html>
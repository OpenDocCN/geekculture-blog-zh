<html>
<head>
<title>Facebook AI and UC Berkley pick a fight with Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">脸书·艾和加州大学伯克利分校与变形金刚开战</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/facebook-ai-picks-a-fight-with-transformers-5e0f511b4383?source=collection_archive---------0-----------------------#2022-04-27">https://medium.com/geekculture/facebook-ai-picks-a-fight-with-transformers-5e0f511b4383?source=collection_archive---------0-----------------------#2022-04-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="3f57" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">随着围绕GPT3、DALLE、PaLM和更多产品的疯狂炒作，现在是报道本文的最佳时机。</h2></div><p id="8177" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">浏览这些天的机器学习新闻，你会到处看到变形金刚(<a class="ae jt" href="https://www.youtube.com/watch?v=ZXiruGOCn9s&amp;ab_channel=IBMTechnology" rel="noopener ugc nofollow" target="_blank">观看这个视频IBM Technology快速概述</a>这个想法)。而且理由很充分。自推出以来，变形金刚已经席卷了深度学习领域。虽然它们传统上与自然语言处理相关联，但变形金刚现在也用于计算机视觉管道中。就在最近几周，我们已经看到了变形金刚在计算机视觉中的一些疯狂应用。因此，变压器似乎将取代卷积神经网络(CNN)来完成一般的计算机视觉任务。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es ju"><img src="../Images/96b5002c46c05a1cff213c97f6583ab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LskUBUhrwgPnqujgK-IjvQ.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx"><a class="ae jt" href="https://openai.com/blog/dall-e/" rel="noopener ugc nofollow" target="_blank">DALL·E: Creating Images from Text</a> really pushed a lot of boundaries with what was considered possible.</figcaption></figure><p id="50d2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，脸书人工智能的研究人员有一些补充。在他们的论文“<a class="ae jt" href="https://arxiv.org/abs/2201.03545" rel="noopener ugc nofollow" target="_blank">2020年代的ConvNet】”中，作者假设变形金刚在视觉相关任务中表现优于CNN的很大一部分原因是变形金刚使用的高级训练协议(这是一种较新的架构)。因此，他们认为，通过改善模型周围的管道，我们可以缩小变压器和CNN之间的性能差距。用他们的话说，</a></p><blockquote class="kk kl km"><p id="19b2" class="ix iy kn iz b ja jb ij jc jd je im jf ko jh ji jj kp jl jm jn kq jp jq jr js hb bi translated">在这项工作中，我们重新审视了设计空间，并测试了纯ConvNet所能达到的极限。我们逐渐将标准ResNet“现代化”,用于视觉转换器的设计，并在此过程中发现了影响性能差异的几个关键组件。</p></blockquote><p id="a30a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">结果相当有趣，它们表明CNN甚至可以在某些任务中胜过<strong class="iz hj">和</strong>变形金刚。这更证明了你的深度学习管道可以通过更好的训练来改善，而不是简单地去寻找更大的模型。在这篇文章中，我将介绍他们论文中一些有趣的发现。但首先介绍一下变形金刚和CNN，以及每种架构在计算机视觉任务中的优势。</p><h1 id="6bf4" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">CNN:OG计算机视觉网络</h1><p id="cf82" class="pw-post-body-paragraph ix iy hi iz b ja lj ij jc jd lk im jf jg ll ji jj jk lm jm jn jo ln jq jr js hb bi translated">卷积神经网络从一开始就是计算机视觉的架构。<a class="ae jt" href="http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf" rel="noopener ugc nofollow" target="_blank">其实CNN的根基都比我老</a>。CNN实际上是为视觉而建的。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lo"><img src="../Images/1207a184ff3b00b0617943ff7b6e66c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*wRxBsPLhb7jlrP6qxVEbaQ.png"/></div><figcaption class="kg kh et er es ki kj bd b be z dx">The feature extraction is the true CNN revolution. Taken from <a class="ae jt" href="https://www.ibm.com/cloud/learn/convolutional-neural-networks" rel="noopener ugc nofollow" target="_blank">IBM’s Writeup on ConvNets</a></figcaption></figure><p id="4a43" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">那么CNN有什么好的呢？卷积神经网络背后的主要思想是，它们逐段遍历图像，并从中提取主要特征。CNN的早期层通常提取更粗糙的特征，例如边缘和颜色。然而，添加更多的层允许以非常高的细节分辨率提取特征。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es lp"><img src="../Images/8de209edbe86bae0cfaeb27484bea7d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Ch23Xi-vJt_hDSBGDAx5PA.gif"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx">CNNs use the sliding window technique to build their feature maps. As you can see, Good Machine Learning requires good software engineering. <a class="ae jt" href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53" rel="noopener" target="_blank">Image Source</a></figcaption></figure><p id="5c0b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这篇文章更详细地介绍了CNN。对于我们的目的来说，有一件事很重要:<strong class="iz hj">CNN一直是计算机视觉的首选，主要是因为它们能够构建特征地图。</strong></p><h1 id="30f8" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">变形金刚——新一代？</h1><p id="091f" class="pw-post-body-paragraph ix iy hi iz b ja lj ij jc jd lk im jf jg ll ji jj jk lm jm jn jo ln jq jr js hb bi translated">变形金刚是CNN的表亲。它们是对传统递归神经网络的改进。创建rnn是为了处理时态数据，即过去跟随未来。我们举个简单的例子。想象一下那句话，“别吃我的_”。为了填充_，我们需要从句子中前面的单词中找出上下文。随意填写单词对我们没有任何好处。与传统网络不同，rnn从先前的输入中获取信息。<a class="ae jt" href="https://youtu.be/Y2wfIKQyd1I" rel="noopener ugc nofollow" target="_blank">code basics的这个视频很好地介绍了这个想法，对于那些想了解更多的人来说</a>。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lq"><img src="../Images/08f2d890eb62329d6029a01c3da9f816.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*hVmfu7BijCnRf52eNzTxQg.png"/></div><figcaption class="kg kh et er es ki kj bd b be z dx">RNN hidden layers feedback into themselves, allowing them to use prior input in their predictions. <a class="ae jt" href="https://www.ibm.com/cloud/learn/recurrent-neural-networks" rel="noopener ugc nofollow" target="_blank">Image Source</a></figcaption></figure><p id="535b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，rnn有一个缺陷。因为它们处理的是顺序数据，所以数据顺序很重要。这使得它们不可能并行化，因为我们必须按顺序输入。变形金刚就是为了解决这个问题而产生的。变压器利用注意力来识别输入的重要部分，并将其存储在内存中。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es lr"><img src="../Images/26b87d8247537c013a879cf70be4b3de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ws5MjbMD8ZHPyeZjT78F3A.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx">How Transformers are adapted to different tasks. <a class="ae jt" href="https://www.researchgate.net/figure/Some-applications-of-transformers-in-different-fields-of-machine-learning-For-NLP-a_fig1_356159551" rel="noopener ugc nofollow" target="_blank">Image Source</a></figcaption></figure><p id="2efd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于转换器可以并行化，我们已经看到一些巨大的数据集被训练。Googles的<a class="ae jt" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank"> BERT </a>和OpenAI的<a class="ae jt" href="https://openai.com/blog/gpt-3-apps/" rel="noopener ugc nofollow" target="_blank"> GPT3 </a>就是一些显著的例子。我们已经看到他们实现了一些疯狂的功能。然而，这留下了一个问题——变形金刚是为NLP设计的，为什么它们对计算机视觉有好处？仅仅是盲目的运气+大量的训练吗？</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es ls"><img src="../Images/6e05e7967c927a5ab7d2cf752cbef170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8_df4gxo_WG3XFwCA2CK5w.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx">Encoder-Decoder pairs have a lot of use in translation, reconstruction, deepfakes, and a bunch of other cool ideas. <a class="ae jt" href="https://showmecyber.com/deepfakes-or-ai-generated-synthetic-media-not-just-for-porn-anymore/" rel="noopener ugc nofollow" target="_blank">Image Source</a></figcaption></figure><p id="b23c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">变压器非常有效，因为它们处理输入的方式。变压器利用编码器和解码器。编码器接收你的输入，并将其编码到一个潜在的空间。解码器从潜在空间中提取矢量，并将其转换回来。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es lt"><img src="../Images/35854f4b5eca6a4a207cfb6dfb00bc81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P_1sAauS7CgPn0tQ0N61Ng.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx">DALL-E truly is amazing. This is another one of their functionalities.</figcaption></figure><p id="d633" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这在计算机视觉中有多种用途。对抗性学习、重建、图像存储和生成是一些显著的例子。它在DALL-E中也起着至关重要的作用。我们将文本输入编码到潜在空间中。然后我们可以用一个解码器把潜在向量解码成图像。这就是我们如何能够从文本描述中生成图像。脸书·艾的<a class="ae jt" href="https://arxiv.org/abs/2203.13131" rel="noopener ugc nofollow" target="_blank">制作场景:基于场景的人类先验文本到图像生成</a></p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es lu"><img src="../Images/bb49441089739544c58bdd835844ba35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tlm50QYlaFqlxNCRHduFgA.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx">Taken from Make-A-Scene. The quality of images that Meta’s AI generates is stunning. <a class="ae jt" rel="noopener" href="/geekculture/machine-learning-for-the-metaverse-why-metas-ai-lab-is-so-random-42975ab28a26">Read about how this ties into their MetaVerse aspiration</a></figcaption></figure><p id="a7ed" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我为这个视频做研究的时候，我实际上了解到了视觉变形金刚如此有效的另一个原因。这关系到他们的注意力机制。这是足够有趣的担保它自己的文章，但这里是一个总结。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es lv"><img src="../Images/9905e77f2c5e352e848cc900f67eac64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-KBdBM4m85v03lqVZVDJqg.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx">I sent an earlier draft of this article to some experts. One of them actually taught me about the following properties of vision transformers. Goes to show you how deep and complex AI is. Shoutout to <a class="ae jt" href="https://www.linkedin.com/in/arjunjain/" rel="noopener ugc nofollow" target="_blank">Dr. Ajun Jain</a> for his constant assistance and input on my drafts.</figcaption></figure><p id="809c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae jt" href="https://machinelearningmastery.com/the-transformer-attention-mechanism/" rel="noopener ugc nofollow" target="_blank">变形金刚的注意力机制</a>让它们能够识别句子中重要的部分。注意力让变形金刚过滤掉噪音，捕捉相距甚远的单词之间的关系。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es lw"><img src="../Images/acb01ee40d773c8c1107350c70e8c3b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E7jaHVyiZVwrsi_-b1nmrw.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx">Taken from the legendary Google Paper, <a class="ae jt" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">Attention Is All You Need</a>. Not needing sequence is a big deal.</figcaption></figure><p id="a06d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在NLP的上下文中，大家已经知道了这一点。我不知道的是，这甚至适用于CV。注意力机制允许变形金刚保持“图像的全局视图”,允许它们提取与ConvNets非常不同的特征。记住，CNN使用核来提取特征，这意味着它们只能找到局部特征。注意让变压器绕过这个。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es lx"><img src="../Images/8906ddb03714311155fbaefc1c5b59bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OUM2CjMp9DXH9kvJyMDzmw.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx">Since Transformers also use Conv Priors, this is a best of both worlds sort of deal.</figcaption></figure><p id="a9ed" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上图取自非常有趣的，<a class="ae jt" href="https://arxiv.org/abs/2108.08810" rel="noopener ugc nofollow" target="_blank">视觉变形金刚看起来像卷积神经网络吗？</a>有趣的是，我稍后会对这篇论文进行分解。重要的方面是下面的引用，也来自该文件。</p><blockquote class="ly"><p id="2dbd" class="lz ma hi bd mb mc md me mf mg mh js dx translated">证明获取更多的全球信息也会导致与ResNet下层的局部感受野所计算的特征在数量上不同的特征</p></blockquote><p id="d19a" class="pw-post-body-paragraph ix iy hi iz b ja mi ij jc jd mj im jf jg mk ji jj jk ml jm jn jo mm jq jr js hb bi translated">很明显，变形金刚非常强大。注意力机制、大规模训练和现代建筑使它们成为许多视觉任务的支柱。所以问题是，纯CNN有机会吗？这里有一个新的CNN训练方法与视觉变形金刚的比较，摘自论文。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es mn"><img src="../Images/8e2b3edb84a75add5774279549a2416f.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*Ff7sFJHnE6zFCirsRW4cog.png"/></div><figcaption class="kg kh et er es ki kj bd b be z dx">As we can see by this diagram, ConvNeXt is able to even beat Swin Transformers.</figcaption></figure><p id="e037" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">显然，这是非常令人兴奋的东西。通过改进围绕CNN架构的训练管道，我们可以媲美SOTA变形金刚。这显示了建立良好的机器学习训练管道的力量。它们可以弥补使用较弱模型的不足，通常更具成本效益。</p><blockquote class="ly"><p id="a3f2" class="lz ma hi bd mb mc md me mf mg mh js dx translated">ConvNeXts完全由标准ConvNet模块构建而成，在精度和可扩展性方面与变压器不相上下，实现了87.8%的ImageNet top-1精度，在COCO检测和ADE20K分段方面优于Swin变压器，同时保持了标准conv net的简单性和效率。</p></blockquote><h1 id="ecc9" class="kr ks hi bd kt ku kv kw kx ky kz la lb io mo ip ld ir mp is lf iu mq iv lh li bi translated">CNN基础设施的现代化</h1><p id="23b0" class="pw-post-body-paragraph ix iy hi iz b ja lj ij jc jd lk im jf jg ll ji jj jk lm jm jn jo ln jq jr js hb bi translated">显然，我们可以改进我们的标准模型，实现惊人的性能。让我们来谈谈他们为实现这一目标所做的一些有趣的设计调整(要了解所有这些调整，请务必阅读本文)。我也将回顾一下我在阅读这篇论文时的一些见解，我很乐意与你们大家讨论。通过IG/LinkedIn/Twitter(本文末尾的链接)在评论/与我分享你的想法。</p><h1 id="abda" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">增加内核大小</h1><p id="1125" class="pw-post-body-paragraph ix iy hi iz b ja lj ij jc jd lk im jf jg ll ji jj jk lm jm jn jo ln jq jr js hb bi translated">转换器使用比ConvNets更大的内核大小。作者指出，“<em class="kn">虽然Swin Transformers重新将局部窗口引入了自我注意块，但窗口大小至少是7×7，明显大于ResNe(X)t内核大小33。在这里，我们再次讨论大内核大小的卷积在卷积网中的应用。</em>“作者对内核大小进行了实验，发现更大的大小可以提高性能。然而，他们发现增加内核大小的好处在达到7x7内核后就消失了。</p><blockquote class="kk kl km"><p id="5c49" class="ix iy kn iz b ja jb ij jc jd je im jf ko jh ji jj kp jl jm jn kq jp jq jr js hb bi translated">有了所有这些准备，采用更大内核大小的卷积的好处是显著的。我们试验了几种内核大小，包括3、5、7、9和11。网络的性能从79.9% (33)提高到80.6% (77)，而网络的失败次数大致保持不变。</p></blockquote><h1 id="23f6" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">倒置瓶颈设计</h1><p id="7032" class="pw-post-body-paragraph ix iy hi iz b ja lj ij jc jd lk im jf jg ll ji jj jk lm jm jn jo ln jq jr js hb bi translated">瓶颈设计在神经网络中很常见。许多编码器-解码器对倾向于对潜在空间的输入进行下采样，并从潜在空间进行上采样。然而，作者发现使用一个倒置的瓶颈设计更优越(这也是变形金刚所使用的)。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es mr"><img src="../Images/1837330366e52cd415ee5468d47f59e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*uSiEBGASczLPL_UMgiFVLQ.png"/></div></figure><p id="563c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这种设计模式的好处如下所示-</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es ms"><img src="../Images/e4f22f92614a78df7ffa40099b3828fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*BMWBFV_FZ2EOl-Q09HIUag.png"/></div></figure><p id="1b7a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">大型网络中性能的提高是一个有趣的现象。我很想知道这种改进的性能是否会随着更大的型号继续扩展。这在今天已经成为趋势的超大规模语言模型中有很大的潜力。如果有人知道为什么会这样，请分享。我很想学。</p><h1 id="1eb8" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">调整激活功能</h1><p id="6019" class="pw-post-body-paragraph ix iy hi iz b ja lj ij jc jd lk im jf jg ll ji jj jk lm jm jn jo ln jq jr js hb bi translated"><a class="ae jt" rel="noopener" href="/codex/learnings-from-googles-comprehensive-research-into-activation-functions-4a9d6566ba2c">正如我之前提到的，激活功能是一件大事</a>。作者对ResNet块进行了一些修改，以模仿变形金刚。第一个是将激活函数从ReLU改为GeLU(高斯误差)</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es mt"><img src="../Images/1e2ea8ea10f7482a0f0dce6dae87b2c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*r6n6UwVWn3yfAoJxoAz6zw.png"/></div><figcaption class="kg kh et er es ki kj bd b be z dx">GeLU is a much smoother function than ReLU. This is why Google and OpenAI use it. <a class="ae jt" href="https://www.quora.com/What-are-some-good-Activation-Functions-other-than-ReLu-or-Leaky-ReLu" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="59cc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作者还改变了一个模块中激活函数的数量。如图4所示，除了两个1x1层之间的一个层之外，我们从剩余块中消除了所有的GELU层，复制了变压器块的样式。 " <strong class="iz hj">这导致了0.7%的改善。</strong></p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es mu"><img src="../Images/6e4e658de39051e6b72a7f4ee57cfe3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*iBsLkTrAjUuzr-oh01K9Tg.png"/></div><figcaption class="kg kh et er es ki kj bd b be z dx">While the results were impressive, look at the ROI for accuracy compared to the amount of memory used. The reason I recommend most people stick to the basics is that they don’t have Google’s resources</figcaption></figure><h1 id="d0e7" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">分隔缩减取样图层</h1><p id="409f" class="pw-post-body-paragraph ix iy hi iz b ja lj ij jc jd lk im jf jg ll ji jj jk lm jm jn jo ln jq jr js hb bi translated">另一个来自变形金刚的灵感，作者决定分离下采样。与在一个阶段的开始(像传统的ResNet)做所有这些不同，它们也在层之间进行缩减采样(像视觉变形金刚)。ConvNeXt块看起来是这样的</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es mv"><img src="../Images/98dedaeefb505a84f6c071aa1fe25031.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*QqVgL_Gab1iFcjatLbJEfg.png"/></div><figcaption class="kg kh et er es ki kj bd b be z dx">LN- Layer Normalization. Notice our new network uses LN instead of BN (batch normalization) like traditional CNNs (ResNet)</figcaption></figure><p id="a466" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这有相当大的影响，作者指出，通过遵循这一策略“<em class="kn">，我们可以将准确率提高到82.0%，大大超过Swin-T的81.3% </em>。”</p><h1 id="05b7" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">关于AGI的笔记</h1><p id="3225" class="pw-post-body-paragraph ix iy hi iz b ja lj ij jc jd lk im jf jg ll ji jj jk lm jm jn jo ln jq jr js hb bi translated">我所涉及的变化绝不是全面的。为了让他们的网络更加“现代”，作者们做了大量的改变。幸运的是，他们在论文的附录中列出了这些变化，以及每个变化带来的好处。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es mw"><img src="../Images/43fd3befdc99ec41b4364311cbd1cdd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S0Dwo164xHmrWBT7xVmrXA.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx">That, my lovely reader, is why we should always read the appendix. This gem is perfect.</figcaption></figure><p id="002f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当我阅读这篇论文时，它让我想到了人工通用智能(AGI)。许多这些变化都是受到其他模式/网络的成功的启发(作者从《变形金刚》和其他CNN获得了灵感)。给出对数据插补、学习率、批量、架构等的研究结果。这让我想到一个问题——AGI的关键在于“一个完美的训练协议”。有没有跨任务训练的完美配置？<a class="ae jt" href="https://youtu.be/L48-g85Kg6A" rel="noopener ugc nofollow" target="_blank">我会在这段6分钟的视频中更详细地介绍这一点</a>，但我很想听听你的想法。</p><p id="9877" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">本文到此为止。撇开有趣的AGI假设不谈，这篇论文再次向我们展示了不断学习机器学习基础研究的重要性。为了将你的基本ML技能发展到足以与论文互动的程度，请查看我的文章<a class="ae jt" rel="noopener" href="/geekculture/how-to-learn-machine-learning-in-2022-9ef2ea904986">如何在2022年学习机器学习</a>。它介绍了免费的在线资源，你可以利用这些资源来提高你的机器学习水平。</p><p id="b18d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了帮助我写更好的文章和了解你<a class="ae jt" href="https://forms.gle/7MfQmKhEhyBTMDUD7" rel="noopener ugc nofollow" target="_blank">填写这份调查(匿名)</a>。最多花3分钟，让我提高工作质量。请务必使用我的社交媒体链接来获得更多反馈。所有反馈都有助于我提高。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es mx"><img src="../Images/d97a6f78983b421d141ba2709b2a7f2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/0*vMpG0_rou7zB9liI.png"/></div></figure><p id="5d69" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于机器学习来说，软件工程的基础至关重要。它将帮助你概念化，建立和优化你的ML。我的每日时事通讯，<a class="ae jt" href="https://codinginterviewsmadesimple.substack.com/" rel="noopener ugc nofollow" target="_blank">Coding interview make simpled</a>涵盖了算法设计、数学、最近的技术事件、软件工程等主题，让你成为更好的开发人员。<a class="ae jt" href="https://codinginterviewsmadesimple.substack.com/subscribe?coupon=1e0532f2" rel="noopener ugc nofollow" target="_blank"> <strong class="iz hj">我目前正在进行全年八折优惠，所以一定要去看看。</strong>T5】</a></p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es my"><img src="../Images/96eedd554d7e6e63ecc74bd8fbc63b0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/0*yP63LKJaH6JWW1pz.png"/></div></figure><p id="f52e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我创建了<a class="ae jt" href="https://codinginterviewsmadesimple.substack.com/p/faqs-and-about-this-newsletter?r=4tnbw&amp;s=w&amp;utm_campaign=post&amp;utm_medium=web" rel="noopener ugc nofollow" target="_blank">编码面试，使用通过指导多人进入顶级科技公司而发现的新技术，使面试变得简单</a>。时事通讯旨在帮助你成功，避免你在Leetcode上浪费时间。<a class="ae jt" href="https://codinginterviewsmadesimple.substack.com/p/faqs-and-about-this-newsletter?r=4tnbw&amp;s=w&amp;utm_campaign=post&amp;utm_medium=web" rel="noopener ugc nofollow" target="_blank">您可以阅读常见问题解答，并在此了解更多信息</a></p><p id="ae69" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你也有任何有趣的工作/项目/想法给我，请随时联系我。总是很乐意听你说完。</p><p id="26fa" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下是我的Venmo和Paypal对我工作的金钱支持。任何数额都值得赞赏，并有很大帮助。捐赠解锁独家内容，如论文分析、特殊代码、咨询和特定辅导:</p><p id="07ea" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">https://account.venmo.com/u/FNU-Devansh</p><p id="ab69" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">贝宝:<a class="ae jt" href="https://www.paypal.com/paypalme/ISeeThings" rel="noopener ugc nofollow" target="_blank">paypal.me/ISeeThings</a></p><h1 id="0a22" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">向我伸出手</h1><p id="a4d4" class="pw-post-body-paragraph ix iy hi iz b ja lj ij jc jd lk im jf jg ll ji jj jk lm jm jn jo ln jq jr js hb bi translated">使用下面的链接查看我的其他内容，了解更多关于辅导的信息，或者只是打个招呼。另外，查看免费的罗宾汉推荐链接。我们都得到一个免费的股票(你不用放任何钱)，对你没有任何风险。<strong class="iz hj">所以不使用它只是损失免费的钱。</strong></p><p id="fed1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">查看我在Medium上的其他文章。https://rb.gy/zn1aiu</p><p id="5a3c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我的YouTube:<a class="ae jt" href="https://rb.gy/88iwdd" rel="noopener ugc nofollow" target="_blank">https://rb.gy/88iwdd</a></p><p id="3ff3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在LinkedIn上联系我。我们来连线:<a class="ae jt" href="https://rb.gy/f7ltuj" rel="noopener ugc nofollow" target="_blank">https://rb.gy/m5ok2y</a></p><p id="b76d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我的insta gram:<a class="ae jt" href="https://rb.gy/gmvuy9" rel="noopener ugc nofollow" target="_blank">https://rb.gy/gmvuy9</a></p><p id="b3c9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我的推特:<a class="ae jt" href="https://twitter.com/Machine01776819" rel="noopener ugc nofollow" target="_blank">https://twitter.com/Machine01776819</a></p><p id="57ab" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你正在准备编码/技术面试:【https://codinginterviewsmadesimple.substack.com/ T2】</p><p id="37c0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">获得罗宾汉的免费股票:【https://join.robinhood.com/fnud75 T4】</p></div></div>    
</body>
</html>
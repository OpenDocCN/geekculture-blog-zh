<html>
<head>
<title>Simple Chatbot using BERT and Pytorch: Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用BERT和Pytorch的简单聊天机器人:第1部分</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/simple-chatbot-using-bert-and-pytorch-part-1-2735643e0baa?source=collection_archive---------2-----------------------#2021-06-27">https://medium.com/geekculture/simple-chatbot-using-bert-and-pytorch-part-1-2735643e0baa?source=collection_archive---------2-----------------------#2021-06-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/59f7a7e6e3ace3f02b5683fb6d280658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e8v1xC0NTgoduh_ei9F7Pw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Source: <a class="ae iu" href="https://images.app.goo.gl/PXgomMwAU3x6CYRu8" rel="noopener ugc nofollow" target="_blank">https://chatbotsmagazine.com/</a></figcaption></figure><p id="3784" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">人工智能正在迅速进入各行各业许多企业的工作流程。由于自然语言处理(NLP)、自然语言理解(NLU)和深度学习(DL)的进步，我们现在能够开发能够模仿类似人类的交互的技术，包括识别语音和文本。</p><p id="7177" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我们将使用Transformer和Pytorch构建一个聊天机器人。</p><p id="a206" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我把这篇文章分成三部分。</p><p id="e762" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">零件(1/3): <a class="ae iu" rel="noopener" href="/@shrinidhi.rm1990/simple-chatbot-using-bert-and-pytorch-part-1-2735643e0baa">简介及安装</a></p><p id="a1e6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">部分(2/3): <a class="ae iu" rel="noopener" href="/@shrinidhi.rm1990/simple-chatbot-using-bert-and-pytorch-part-2-ef48506a4105">数据准备</a></p><p id="07f8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第(3/3)部分:<a class="ae iu" rel="noopener" href="/@shrinidhi.rm1990/simple-chatbot-using-bert-and-pytorch-part-3-a6832c50b8d1">模型微调</a></p><h1 id="05cb" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">变压器</h1><p id="1710" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">Google在论文《注意力是你所需要的全部》中介绍了transformer架构。变换器采用自我注意机制，适合语言理解。</p><p id="8752" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">比如说“今年夏天我去了喜马拉雅山。我真的很享受在那里的时光”。最后一个词“那里”指的是喜马拉雅山。但是要理解这一点，记住前几个部分是必不可少的。为了实现这一点，注意力机制在输入序列的每一步决定序列的哪些其他部分是重要的。</p><p id="5df7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">变压器具有编码器-解码器架构。它们由包含前馈和注意层的模块组成。</p><p id="0224" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> BERT(变压器的双向编码器表示)</strong></p><p id="8a07" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">它是Google开发的基于transformer的机器学习技术，用于自然语言处理预训练。BERT由来自谷歌的雅各布·德夫林(Jacob Devlin)及其同事于2018年创建并发布。</p><p id="5ae1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">BERT使用双向训练，即从两个方向阅读句子，以理解句子的上下文。</p><p id="b922" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注意，BERT只是一个<strong class="ix hj">编码器</strong>。它没有解码器。</p><p id="d2d1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最近发布的几个预训练语言模型的参数计数。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/1eaad51e216cfeef1a88bdda2b66a54f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MlGtcvEOGJvYQI_WrAvaog.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Source: Internet</figcaption></figure></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><h1 id="796a" class="jt ju hi bd jv jw li jy jz ka lj kc kd ke lk kg kh ki ll kk kl km lm ko kp kq bi translated">Pytorch:</h1><p id="dca2" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">PyTorch是一个基于Python的科学计算包，它使用图形处理单元(GPU)的能力。自2016年1月发布以来，许多研究人员继续越来越多地采用PyTorch。由于它在构建极其复杂的神经网络方面的便利性，它很快成为了人们的首选库。它给TensorFlow带来了激烈的竞争，尤其是在用于研究工作时。</p><p id="2db7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">PyTorch的一些主要亮点包括:</p><p id="bcbb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">简单接口</strong>:提供易于使用的API。</p><p id="8bb1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">本质上的Python化</strong>:这个库是Python化的，可以与Python数据科学栈顺利集成。</p><p id="1361" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">张量</strong>:基本和NumPy数组一样。要在GPU上运行操作，只需将张量转换为Cuda数据类型。</p><p id="4549" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">计算图</strong> : PyTorch提供了一个优秀的平台，提供动态计算图。<br/> <br/> <strong class="ix hj">亲笔签名(自动微分)</strong>:这个类是计算导数的引擎。</p></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><h1 id="542f" class="jt ju hi bd jv jw li jy jz ka lj kc kd ke lk kg kh ki ll kk kl km lm ko kp kq bi translated"><strong class="ak">数据</strong></h1><p id="8836" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">作为第一步，我们需要建立一个intents JSON文件来定义聊天机器人用户的意图。<br/>例如:<br/>用户可能希望知道我们聊天机器人的名字；因此，我们创建了一个名为name的意图。用户可能希望知道我们聊天机器人的年龄；因此，我们创造了一个叫做年龄的意图。</p><p id="906c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这个聊天机器人中，我们使用了5个意图:姓名、年龄、日期、问候和再见。我们已经使用了具有属于这些意图中的每一个的话语的训练集。当用户输入任何输入时，机器人都会识别出其意图。</p><p id="5b3c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这个intents JSON文件中，每个intents标签旁边都有响应。对于我们的聊天机器人，一旦识别出意图，将从与每个意图相关的静态响应集中随机选择响应。</p><pre class="kx ky kz la fd ln lo lp lq aw lr bi"><span id="c662" class="ls ju hi lo b fi lt lu l lv lw"># used a dictionary to represent an intents JSON file</span><span id="8fa7" class="ls ju hi lo b fi lx lu l lv lw">data = {"intents": [</span><span id="64fd" class="ls ju hi lo b fi lx lu l lv lw">{"tag": "greeting",<br/> "responses": ["Howdy Partner!", "Hello", "How are you doing?",   "Greetings!", "How do you do?"]},</span><span id="368f" class="ls ju hi lo b fi lx lu l lv lw">{"tag": "age",<br/> "responses": ["I am 25 years old", "I was born in 1998", "My birthday is July 3rd and I was born in 1998", "03/07/1998"]},</span><span id="7377" class="ls ju hi lo b fi lx lu l lv lw">{"tag": "date",<br/> "responses": ["I am available all week", "I don't have any plans",  "I am not busy"]},</span><span id="b3ba" class="ls ju hi lo b fi lx lu l lv lw">{"tag": "name",<br/> "responses": ["My name is James", "I'm James", "James"]},</span><span id="3e49" class="ls ju hi lo b fi lx lu l lv lw">{"tag": "goodbye",<br/> "responses": ["It was nice speaking to you", "See you later", "Speak soon!"]}<br/>]}</span></pre><h2 id="a242" class="ls ju hi bd jv ly lz ma jz mb mc md kd jg me mf kh jk mg mh kl jo mi mj kp mk bi translated"><strong class="ak">包的安装</strong></h2><p id="2d4d" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated"><strong class="ix hj">变形金刚</strong>:这个库汇集了超过40个最先进的预训练NLP模型(伯特，GPT-2，罗伯塔等..)</p><p id="5ec8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> Torchinfo </strong>:打印模型架构。</p><pre class="kx ky kz la fd ln lo lp lq aw lr bi"><span id="3c4f" class="ls ju hi lo b fi lt lu l lv lw"># Install Transformers<br/>!pip install transformers==3</span><span id="8147" class="ls ju hi lo b fi lx lu l lv lw"># To get model summary<br/>!pip install torchinfo</span></pre><h2 id="e532" class="ls ju hi bd jv ly lz ma jz mb mc md kd jg me mf kh jk mg mh kl jo mi mj kp mk bi translated">导入库</h2><p id="6383" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">导入对数据集执行操作所需的库。</p><pre class="kx ky kz la fd ln lo lp lq aw lr bi"><span id="4abd" class="ls ju hi lo b fi lt lu l lv lw">import numpy as np<br/>import pandas as pd<br/>import re<br/>import torch<br/>import random<br/>import torch.nn as nn<br/>import transformers<br/>import matplotlib.pyplot as plt</span><span id="a2be" class="ls ju hi lo b fi lx lu l lv lw"># specify GPU<br/>device = torch.device(“cuda”)</span></pre><h2 id="dd3f" class="ls ju hi bd jv ly lz ma jz mb mc md kd jg me mf kh jk mg mh kl jo mi mj kp mk bi translated"><strong class="ak">加载数据集</strong></h2><p id="0f13" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">我们在这里加载训练数据集</p><pre class="kx ky kz la fd ln lo lp lq aw lr bi"><span id="0860" class="ls ju hi lo b fi lt lu l lv lw"># We have prepared a chitchat dataset with 5 labels<br/>df = pd.read_excel(“/content/drive/MyDrive/Datasets/chitchat.xlsx”)</span><span id="946e" class="ls ju hi lo b fi lx lu l lv lw">df.head()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/6761ff621f469e29f8a54d37819fcaa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*8ZBChrayKkaTgg8uqv5_Ww.png"/></div></div></figure><pre class="kx ky kz la fd ln lo lp lq aw lr bi"><span id="63df" class="ls ju hi lo b fi lt lu l lv lw">df[‘label’].value_counts()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/8c5458ff3894477ed5bb627432c30f20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*95wt7VIo4teu6WeZ1CX-rg.png"/></div></figure><p id="4965" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了将这些分类标签转换成数字编码，我们使用了LabelEncoder。</p><pre class="kx ky kz la fd ln lo lp lq aw lr bi"><span id="cfef" class="ls ju hi lo b fi lt lu l lv lw"># Converting the labels into encodings<br/>from sklearn.preprocessing import LabelEncoder<br/>le = LabelEncoder()<br/>df['label'] = le.fit_transform(df['label'])</span><span id="5bc0" class="ls ju hi lo b fi lx lu l lv lw"># check class distribution<br/>df['label'].value_counts(normalize = True)</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/d12faabcc10dca8799f9a08de1071ee3.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*_crRD0BwVHQw1kcUcvvM9Q.png"/></div></figure><pre class="kx ky kz la fd ln lo lp lq aw lr bi"><span id="440d" class="ls ju hi lo b fi lt lu l lv lw"># In this example we have used all the utterances for training purpose<br/>train_text, train_labels = df[‘text’], df[‘label’]</span></pre></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><h2 id="f5da" class="ls ju hi bd jv ly lz ma jz mb mc md kd jg me mf kh jk mg mh kl jo mi mj kp mk bi translated">点击此处进入下一部分:Part(2/3): <a class="ae iu" rel="noopener" href="/@shrinidhi.rm1990/simple-chatbot-using-bert-and-pytorch-part-2-ef48506a4105">资料准备</a></h2></div></div>    
</body>
</html>
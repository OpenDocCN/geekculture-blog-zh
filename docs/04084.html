<html>
<head>
<title>Everything about Ensemble Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于集成学习的一切</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/everything-about-ensemble-learning-46cd96d5498e?source=collection_archive---------25-----------------------#2021-06-21">https://medium.com/geekculture/everything-about-ensemble-learning-46cd96d5498e?source=collection_archive---------25-----------------------#2021-06-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/f53ccc8e84e4e997f19153b7b70cbebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*t89wfgpv_fwwhu5YInP3Sg.png"/></div><figcaption class="im in et er es io ip bd b be z dx">Table of Contents</figcaption></figure><h1 id="8c3f" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak"> 1。什么是集成学习？</strong></h1><p id="deb4" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi km translated"><span class="l kn ko kp bm kq kr ks kt ku di"> T </span>“整体”的一般含义是“作为一个整体而不是单独来看的一组项目”。在机器学习(ML)中也是同样的意思。集成学习是ML的一部分，它使用多种学习算法来获得比单独从任何基本或本质学习算法获得的预测性能更好的预测性能。</p><p id="49da" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">我们用一个例子来理解集成学习。当你想去一家餐馆时，你会试图从各种资源中找到关于那家餐馆的信息，比如问朋友，在谷歌上搜索评论，打电话给餐馆询问食物选择等。在考虑了所有这些意见后，你会做出决定。集合模型也以同样的方式工作。它们将来自多个模型的决策结合起来以提高性能。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es la"><img src="../Images/d2f8f92f437f86e1351673c26e80a09e.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*LKLLSCfkJh31iULxvy3JWw.png"/></div><figcaption class="im in et er es io ip bd b be z dx">Fig. 1 Ensemble Learning example</figcaption></figure><h1 id="bc83" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak"> 2。为什么我们需要集合模型？</strong></h1><p id="4883" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">在理解我们为什么需要系综模型之前，让我们理解什么是偏差和方差。</p><blockquote class="lf lg lh"><p id="efbf" class="jo jp li jq b jr kv jt ju jv kw jx jy lj kx kb kc lk ky kf kg ll kz kj kk kl hb bi translated"><strong class="jq hj">维基百科:</strong></p><p id="cb4b" class="jo jp li jq b jr kv jt ju jv kw jx jy lj kx kb kc lk ky kf kg ll kz kj kk kl hb bi translated">偏差误差是来自学习算法中错误假设的误差。高偏差会导致算法错过特征和目标输出之间的相关关系(欠拟合)。</p><p id="72c6" class="jo jp li jq b jr kv jt ju jv kw jx jy lj kx kb kc lk ky kf kg ll kz kj kk kl hb bi translated">方差是对训练集中的小波动的敏感性的误差。对训练数据中的随机噪声进行建模的算法可能会导致高方差(过拟合)。</p></blockquote><p id="d0e3" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">偏差-方差权衡是监督学习的核心问题。优选地，我们想要选择一个模型，该模型准确地捕捉其训练数据中的规律性(由固定比率表征的质量),并且很好地推广到看不见的数据。通常不可能同时做到这两点。因此，我们必须保持偏差和方差的最佳平衡。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/50a153ff258a23e173fc855a6f22a886.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*wXxnfJPPAmQjgogzLfy03Q.png"/></div><figcaption class="im in et er es io ip bd b be z dx">Fig. 2 Bias Variance Trade-Off</figcaption></figure><blockquote class="lf lg lh"><p id="1bae" class="jo jp li jq b jr kv jt ju jv kw jx jy lj kx kb kc lk ky kf kg ll kz kj kk kl hb bi translated">总误差=偏差+方差+不可约误差</p></blockquote><p id="d441" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">当您尝试使用任何ML技术预测目标变量时，实际值和预测值之间差异的主要因素是噪声、方差和偏差。集成学习有助于减少除噪声之外的这些因素，噪声是不可减少的误差。减少偏差和方差使这些模型更加稳健。</p><h1 id="269f" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak"> 3。集成学习方法的类型</strong></h1><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es ln"><img src="../Images/eb7862de06b22754cfb010d41d9af2f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z0rKkdLMTWjms6bKvrZcog.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx">Fig. 3 Types of Ensemble Models</figcaption></figure><h2 id="2557" class="ls ir hi bd is lt lu lv iw lw lx ly ja jz lz ma je kd mb mc ji kh md me jm mf bi translated"><strong class="ak">装袋</strong></h2><p id="13ed" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">Bagging是一种并行集成元算法，是Bootstrap和Aggregation的组合，开发该算法是为了减少方差并提高ML问题的准确性，可以在回归和分类中使用。</p><p id="e514" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><strong class="jq hj">自举</strong>是一种通过从原始数据集中随机选择替换的数据点来生成样本的技术。这些样本被称为Bootstrap样本，其大小小于原始数据集。</p><p id="6694" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">要从引导中获得最大收益，必须满足两个假设:</p><p id="bec5" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">首先，数据集(N)的大小应该足够大，以捕捉基本分布的大部分复杂性，以便从数据集进行采样是从真实分布进行采样的良好近似。</p><p id="c516" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">第二，原始数据集的大小(N)应该远大于引导样本大小(B ),因为样本可以相关。</p><p id="20af" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><strong class="jq hj">聚合</strong>是将拟合在自举样本上的每个模型的输出进行组合的过程。对于回归，对所有预测的输出进行平均，对于分类，使用最大投票。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/a2f5c4ea0ca64c0b8f540f71584465f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*PyUNs0SV9Jm2RYeNZV1e0g.png"/></div><figcaption class="im in et er es io ip bd b be z dx">Fig. 4 Bagging Equation</figcaption></figure><p id="509b" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">在上面的等式中，X是我们想要生成预测的记录，LHS(fbag)是袋装预测，RHS是来自单个基础学习者的预测。</p><p id="103e" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">让我们举个例子，</p><p id="9617" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">给定一个大小为N的标准训练集D，bagging通过从D均匀采样并替换，生成M个新的训练集Di，每个训练集的大小为B。短语“置换抽样”是指在每个Di中可以重复一些观察结果。</p><p id="78ce" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">然后，使用这M个自举样本来拟合Mi模型，并通过平均输出(用于回归)和投票(用于分类)来组合Mi模型</p><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es mh"><img src="../Images/670ca28a01d261198d4d350d49ae744d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aH0DdwaOq3YmpyU2dDH8qw.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx">Fig. 5 Example for Bagging</figcaption></figure><p id="9bb5" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">通常，对于bagging，基础模型通常是具有低偏差和高方差的模型。最终模型的方差减少了，因为当数据改变时，只有一部分改变的数据被发送到每个Mi，所以大多数Mi不会受到太大影响，因此总体方差减少了。</p><p id="ca67" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><strong class="jq hj">优势</strong></p><ul class=""><li id="daf0" class="mi mj hi jq b jr kv jv kw jz mk kd ml kh mm kl mn mo mp mq bi translated">许多弱学习者聚集在一起通常在整个集合中胜过单个学习者，并且具有较少的过拟合。</li><li id="286e" class="mi mj hi jq b jr mr jv ms jz mt kd mu kh mv kl mn mo mp mq bi translated">减少高方差数据集中的方差</li><li id="5236" class="mi mj hi jq b jr mr jv ms jz mt kd mu kh mv kl mn mo mp mq bi translated">培训和评估的并行处理。</li></ul><p id="241f" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><strong class="jq hj">缺点</strong></p><ul class=""><li id="31b6" class="mi mj hi jq b jr kv jv kw jz mk kd ml kh mm kl mn mo mp mq bi translated">它在高偏差数据集中可能表现不佳。</li><li id="7cff" class="mi mj hi jq b jr mr jv ms jz mt kd mu kh mv kl mn mo mp mq bi translated">模型可解释性的丧失。</li><li id="f9dc" class="mi mj hi jq b jr mr jv ms jz mt kd mu kh mv kl mn mo mp mq bi translated">根据数据的不同，计算量可能很大。</li></ul><h2 id="cfaa" class="ls ir hi bd is lt lu lv iw lw lx ly ja jz lz ma je kd mb mc ji kh md me jm mf bi translated"><strong class="ak">增压</strong></h2><p id="b15d" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">在ML中，boosting是一种顺序集成元算法，主要用于减少偏差。它是一个ML算法家族，将弱学习者转化为强学习者。Boosting背后的基本直觉是依次训练弱学习者<strong class="jq hj"/>，每个人都试图纠正其前任，这将最终减少偏差。通常，像浅层决策树一样，选择具有低方差和高偏差的弱学习器。我们必须在降低方差的同时减少偏差。</p><p id="ae83" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">数学上，</p><p id="18cc" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">1.使决策树适合数据:F1(x) = y</p><p id="0550" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">2.然后，我们将下一个决策树与上一个决策树的残差进行拟合:</p><p id="ede5" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">h1(x) = y — F1(x)</p><p id="becb" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">3.将这个新树添加到我们的算法中:F2(x) = F1(x) + h1(x)</p><p id="5080" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">4.将下一个决策树拟合到F2:H2(x)= y-F2(x)的残差</p><p id="35a6" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">5.将这个新树添加到我们的算法中:F3(x) = F2(x) + h2(x)</p><p id="1337" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">6.继续这个过程，直到某种机制(即交叉验证)告诉我们停止。</p><p id="37c8" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">这里的最后一个模型是b棵个体树的<strong class="jq hj">阶段加性模型</strong>:</p><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/1a30acc5f3af4bdb03c1bf3bfe6d1b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*vlmycQDhASmQhZS4feA1nA.png"/></div><figcaption class="im in et er es io ip bd b be z dx">Fig 6. Equation for Boosting</figcaption></figure><p id="762d" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">这些模型被称为加性加权模型，因为每个单独的模型都被训练以适应其前一阶段结束时的残差(被忽略的误差)。</p><p id="d974" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">随着b值的增加，最终的模型最终具有较低的剩余误差，因为在每个阶段，我们都在根据误差拟合模型。随着训练误差的减少，偏差也会减少。</p><p id="84ab" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><strong class="jq hj">优势</strong></p><ul class=""><li id="1d10" class="mi mj hi jq b jr kv jv kw jz mk kd ml kh mm kl mn mo mp mq bi translated">boosting的结果永远比单基础学习者好。</li></ul><p id="f6d2" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><strong class="jq hj">劣势</strong></p><ul class=""><li id="08d7" class="mi mj hi jq b jr kv jv kw jz mk kd ml kh mm kl mn mo mp mq bi translated">boosting的一个缺点是它对异常值很敏感，因为每个分类器都必须修复前一个分类器中的错误。</li></ul><h2 id="147e" class="ls ir hi bd is lt lu lv iw lw lx ly ja jz lz ma je kd mb mc ji kh md me jm mf bi translated">堆垛</h2><p id="f56d" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">堆叠的基本思想是考虑<strong class="jq hj">异构</strong>弱学习器，在<strong class="jq hj">并行</strong>中学习它们，并通过训练元模型来组合它们，以基于不同弱模型的预测输出预测。实际上，模型越不同越好。所有这些模型都是并行构建的，彼此独立。</p><p id="aa0a" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">步骤1-在原始数据集d上训练M个模型。</p><p id="8c58" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">步骤2-制作新的数据集D ’,该数据集由步骤1中M个模型的预测组成。</p><p id="8db2" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">步骤3-现在构建第二级模型或元模型，以对新数据集D '进行预测。</p><p id="9be8" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><strong class="jq hj">优点</strong></p><ul class=""><li id="8146" class="mi mj hi jq b jr kv jv kw jz mk kd ml kh mm kl mn mo mp mq bi translated">可以通过控制各种性能良好的模型的能力来提高性能。</li></ul><p id="1829" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><strong class="jq hj">缺点</strong></p><ul class=""><li id="7ba0" class="mi mj hi jq b jr kv jv kw jz mk kd ml kh mm kl mn mo mp mq bi translated">高训练时间</li><li id="a760" class="mi mj hi jq b jr mr jv ms jz mt kd mu kh mv kl mn mo mp mq bi translated">高评估时间</li></ul><p id="dae1" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><strong class="jq hj">代码:</strong></p><p id="26c3" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">有关<a class="ae mx" href="http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/" rel="noopener ugc nofollow" target="_blank">代码</a>的更多解释</p><figure class="lb lc ld le fd ij"><div class="bz dy l di"><div class="my mz l"/></div><figcaption class="im in et er es io ip bd b be z dx">Code for Stacking algorithm for classification problem</figcaption></figure><h1 id="b4b1" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak"> 4。基于Bagging和Boosting方法的最常用算法</strong></h1><p id="e4ee" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated"><strong class="jq hj">随机森林</strong></p><p id="eede" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">随机森林，也称为随机决策森林，是一种广泛用于分类和回归的bagging集成学习方法。我们发现许多树的地方是一片森林。在随机森林中，我们使用许多决策树作为基础模型，因此命名为森林。随机来自“随机”引导抽样。随机森林是<strong class="jq hj">决策树+装袋+列抽样</strong>(特征装袋)的组合。</p><p id="c864" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">列采样只不过是从原始数据集中选取随机要素/列进行采样。随机森林的核心思想是通过选取随机的行和列来制作样本，然后在这些样本上训练模型并聚合结果。</p><p id="5037" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">从原始数据集中采样一组点后，该样本中未被考虑的点称为袋外<strong class="jq hj">点</strong>点，被考虑的点称为袋内样本。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es na"><img src="../Images/6154809c9c39db6bd4d6f07aa428ab48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c5f5beubFbEZq9407t6tng.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx">Fig. 7 Process of Random Forest Algorithm</figcaption></figure><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es nb"><img src="../Images/ca5911dbdc401ab92646a8156f08b773.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*On2etk4-JUUnZd75Hbn9og.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx">Fig. 8 Sample Data of In-Bag and Out-of-Bag points</figcaption></figure><p id="3e23" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">这些OOB点可用于测试根据袋内采样数据训练的模型。许多库向OOB提供错误，以了解基于原始数据集中的样本构建的每个模型。</p><p id="5b27" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><strong class="jq hj">代码:</strong></p><p id="18d7" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">关于<a class="ae mx" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" rel="noopener ugc nofollow" target="_blank">代码</a>的更多信息</p><figure class="lb lc ld le fd ij"><div class="bz dy l di"><div class="my mz l"/></div><figcaption class="im in et er es io ip bd b be z dx">Code for Random Forest with test scores</figcaption></figure><p id="1d0c" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">n _估计量、bootstrap、max_features称为超参数，我们必须调整它们以避免过度拟合并提高精度。</p><p id="a383" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><strong class="jq hj">梯度推进</strong></p><p id="dad8" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">梯度推进是一种用于回归和分类的集成机器学习算法，它以弱预测模型(通常是决策树)的集成的形式产生预测模型。它像其他boosting方法一样以<strong class="jq hj">分阶段的方式</strong>构建模型，并通过允许优化任意可微分损失函数来推广它们。</p><p id="48cb" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">梯度推进建立在三个支柱之上:-</p><p id="c0c1" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">(一)损失函数</p><p id="4bb9" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">选择一个完美的损失函数取决于需要解决的问题的类型。例如；对于回归，可以使用平方误差，对于分类，可以使用对数损失。所选择的损失函数必须被优化以获得更好的结果。</p><p id="171f" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">(二)学习能力差</p><p id="ed44" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">典型地，具有合理深度的决策树被用作弱学习器。这些树是贪婪地构建的，我们必须根据纯度分数(如基尼系数或熵)选择最佳分裂点。</p><p id="9042" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">(三)加法模型</p><p id="04ff" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">新树的输出被添加到现有树序列的输出，以校正或改进模型的最终输出。这些树一次添加一个，模型中现有的树不变。添加树时，采用梯度下降过程来按比例降低损失。</p><p id="3d53" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">由于这种算法是一种贪婪的算法，它会<strong class="jq hj">过拟合</strong>训练数据集。对于这个算法，有4种方法来调节这个装配问题</p><p id="5e01" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">1.树约束:-该算法中使用的树需要被控制以避免过度拟合。以下是控制树的最常用参数:</p><p id="7ac9" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">树木数量</p><p id="005c" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">树的深度</p><p id="f38b" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">节点数或叶数</p><p id="c0cc" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">每次分割的观察次数</p><p id="f812" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">损失的最小改善</p><p id="7248" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">2.加权更新:-每当添加新树时，可以通过控制学习速率来加权它的贡献。通常，学习率使用0:1到0.3范围内的小值。</p><p id="39fa" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">3.随机梯度推进:-在装袋中制作样品的优势也可以在推进中。这降低了树之间的相关性。在每一次迭代中，从原始数据集中抽取一个样本，这个样本使用了fit基学习器。</p><p id="d355" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">4.惩罚梯度增强:-调整最终学习的权重有助于避免过度拟合。常用的正则化函数有:</p><p id="d122" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">L1正规化</p><p id="8cca" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">L2正规化</p><p id="666a" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><strong class="jq hj">代码:</strong></p><p id="0477" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">关于<a class="ae mx" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html" rel="noopener ugc nofollow" target="_blank">代码</a>的更多信息</p><figure class="lb lc ld le fd ij"><div class="bz dy l di"><div class="my mz l"/></div><figcaption class="im in et er es io ip bd b be z dx">Gradient Boosting Code on sample data</figcaption></figure><h1 id="3490" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak"> 5。装袋、增压和堆垛之间的比较</strong></h1><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es nc"><img src="../Images/105366a2e45c09ad2f86efafe6ca2980.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Js4OAZvzBaKjuO-s1W_QeA.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx">Fig. 9 Comparision between various ensemble learning methods</figcaption></figure><h1 id="e6eb" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">6.结论</h1><p id="25d9" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">每个集合模型都有自己的优点和缺点。选择正确的模型取决于问题和ML爱好者处理的数据集。</p><p id="f0ef" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><strong class="jq hj">参考文献:</strong></p><p id="9542" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><a class="ae mx" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . ensemble . gradientboostingclassifier . html</a></p><p id="8a67" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><a class="ae mx" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . ensemble . randomforestclassifier . html # sk learn . ensemble . randomforestclassifie</a>r</p><p id="a459" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><a class="ae mx" rel="noopener" href="/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f">https://medium . com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c 98 fffa 5489 f</a></p><p id="88a6" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><a class="ae mx" href="https://bradleyboehmke.github.io/HOML/gbm.html" rel="noopener ugc nofollow" target="_blank">https://bradleyboehmke.github.io/HOML/gbm.html</a></p><p id="3562" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><a class="ae mx" href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/gentle-introduction-gradient-boosting-algorithm-machine-learning/</a></p><p id="e3d2" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><a class="ae mx" href="https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2018/06/comprehensive-guide-for-ensemble-models/</a></p><p id="f728" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><a class="ae mx" rel="noopener" href="/greyatom/a-quick-guide-to-boosting-in-ml-acf7c1585cb5">https://medium . com/grey atom/a-quick-guide-to-boosting-in-ml-acf7c 1585 CB 5</a></p><p id="b347" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><a class="ae mx" href="https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205" rel="noopener" target="_blank">https://towards data science . com/ensemble-methods-bagging-boosting-and-stacking-c 9214 a10a 205</a></p><p id="76dd" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><a class="ae mx" href="https://en.m.wikipedia.org/wiki/Boosting_(machine_learning)#Boosting_algorithms." rel="noopener ugc nofollow" target="_blank">https://en . m . Wikipedia . org/wiki/Boosting _(机器学习)#Boosting_algorithms。</a></p><p id="d138" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><a class="ae mx" href="https://en.m.wikipedia.org/wiki/Random_forest#Algorithm" rel="noopener ugc nofollow" target="_blank">https://en.m.wikipedia.org/wiki/Random_forest#Algorithm</a></p><p id="f575" class="pw-post-body-paragraph jo jp hi jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated"><a class="ae mx" href="http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/" rel="noopener ugc nofollow" target="_blank">http://rasbt . github . io/mlx tend/user _ guide/classifier/stacking classifier/</a></p></div></div>    
</body>
</html>
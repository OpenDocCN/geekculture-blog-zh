<html>
<head>
<title>NLP With TensorFlow/Keras: Explanation and Tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用TensorFlow/Keras的NLP:说明和教程</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/nlp-with-tensorflow-keras-explanation-and-tutorial-cae3554b1290?source=collection_archive---------0-----------------------#2021-04-01">https://medium.com/geekculture/nlp-with-tensorflow-keras-explanation-and-tutorial-cae3554b1290?source=collection_archive---------0-----------------------#2021-04-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f774d3605f3a4d537f4f8c0c6d7a4105.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ltDjXiuhKLABp4La.jpg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx"><a class="ae iu" href="https://www.blumeglobal.com/learning/natural-language-processing/" rel="noopener ugc nofollow" target="_blank">https://www.blumeglobal.com/learning/natural-language-processing/</a></figcaption></figure><h1 id="a823" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">什么是NLP？</h1><p id="ebf2" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">自然语言处理(NLP)是机器学习和人工智能的一个分支，专注于从人类语言中获取意义并自动处理它。在我们的现代社会中，它有各种各样的使用案例，包括聊天机器人、来自客户评论的情感分析以及识别假新闻。在本文中，您将了解NLP背后最重要的概念，以及如何使用TensorFlow和Keras实现情感分析。</p></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="7a2d" class="iv iw hi bd ix iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js bi translated">主要概念</h1><h2 id="b5ca" class="ld iw hi bd ix le lf lg jb lh li lj jf ke lk ll jj ki lm ln jn km lo lp jr lq bi translated">标记化</h2><p id="967e" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">标记化是将句子分割成各种“标记”的过程，这些标记很可能是单个单词。此外，这个过程还包括去掉某些不太重要的字符，比如标点符号。这对于理解上下文和发展模型的理解是极其重要的。这是每个NLP模型的重要组成部分。</p><h2 id="8ba8" class="ld iw hi bd ix le lf lg jb lh li lj jf ke lk ll jj ki lm ln jn km lo lp jr lq bi translated">停止单词删除</h2><p id="2d85" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">这是删除不相关的词，如“和”、“到”和“的”。这种常见的单词不会为NLP模型提供任何值，在将文本传递给模型之前会被过滤掉。虽然没有官方的停用词列表，但需要注意的是，该列表会根据模型的目的而变化。这对于在训练期间增加模型的准确性非常有用。</p><h2 id="c1d9" class="ld iw hi bd ix le lf lg jb lh li lj jf ke lk ll jj ki lm ln jn km lo lp jr lq bi translated">堵塞物</h2><p id="ea03" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">词干化是另一个过程，它有助于使模型更容易理解文本。它包括通过将单词缩减到词干来缩短文本。比如“waiting”和“waited”都简称为“wait”。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/1bec180e790fcd6ea63e91e487a9e340.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/0*Eu_uxDt3XZ9lzqi5.jpg"/></div><figcaption class="iq ir et er es is it bd b be z dx"><a class="ae iu" href="https://www.c-sharpcorner.com/blogs/stemming-in-natural-language-processing" rel="noopener ugc nofollow" target="_blank">https://www.c-sharpcorner.com/blogs/stemming-in-natural-language-processing</a></figcaption></figure><h2 id="aa06" class="ld iw hi bd ix le lf lg jb lh li lj jf ke lk ll jj ki lm ln jn km lo lp jr lq bi translated">词汇化</h2><p id="9998" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">词汇化是将一个单词转化为其基本形式并将同义词标准化为其词根的过程。比如“去了”变成“去了”，“欢喜了”变成“好了”。虽然它有点类似于词干，但它采用了不同的方法来简化文本。</p><h1 id="9de4" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">主题建模</h1><p id="af88" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">NLP中的另一个重要主题是主题建模。它本质上是一种无监督的机器学习技术，用于将不同的文本分组到某些主题下。例如，它被用于Gmail等电子邮件系统中，在这些系统中，涉及某些主题的电子邮件被分组在一起。当使用主题建模时，我上面列出的所有技术也可以用来更好地训练您的模型。</p></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="1d04" class="iv iw hi bd ix iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js bi translated">Tensorflow/Keras教程</h1><p id="91e5" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">现在我们知道了NLP是什么，以及用来提高模型准确性的各种工具，我们将处理一个经典的NLP问题:<strong class="jv hj">检测文本的情感</strong>。对于数据集，我们将使用一组英语Twitter消息，这些消息被分为六种基本情绪:愤怒、恐惧、快乐、爱、悲伤和惊讶。您可以在这里查看数据集:【https://huggingface.co/datasets/emotion<a class="ae iu" href="https://huggingface.co/datasets/emotion" rel="noopener ugc nofollow" target="_blank">。请注意，您不必下载它，因为我们将使用“nlp”模块来导入它。</a></p><h2 id="d874" class="ld iw hi bd ix le lf lg jb lh li lj jf ke lk ll jj ki lm ln jn km lo lp jr lq bi translated">进口</h2><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="3749" class="ld iw hi lx b fi mb mc l md me">!pip install nlp</span><span id="2b94" class="ld iw hi lx b fi mf mc l md me">import tensorflow as tf</span><span id="da49" class="ld iw hi lx b fi mf mc l md me">import numpy as np</span><span id="7d2a" class="ld iw hi lx b fi mf mc l md me">import matplotlib.pyplot as plt</span><span id="acf9" class="ld iw hi lx b fi mf mc l md me">import nlp</span><span id="ea82" class="ld iw hi lx b fi mf mc l md me">import random</span><span id="555f" class="ld iw hi lx b fi mf mc l md me">from tensorflow.keras.preprocessing.text import Tokenizer</span><span id="d075" class="ld iw hi lx b fi mf mc l md me">from tensorflow.keras.preprocessing.sequence import pad_sequences</span></pre><h2 id="92d0" class="ld iw hi bd ix le lf lg jb lh li lj jf ke lk ll jj ki lm ln jn km lo lp jr lq bi translated">导入和准备数据</h2><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="771a" class="ld iw hi lx b fi mb mc l md me">dataset = nlp.load_dataset('emotion')</span><span id="023a" class="ld iw hi lx b fi mf mc l md me">train = dataset['train']</span><span id="e9f9" class="ld iw hi lx b fi mf mc l md me">val = dataset['validation']</span><span id="c613" class="ld iw hi lx b fi mf mc l md me">test = dataset['test']</span></pre><p id="38cc" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">这将导入数据集，并将其分为定型集、验证集和测试集。</p><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="ff1b" class="ld iw hi lx b fi mb mc l md me">def get_tweet(data):</span><span id="c262" class="ld iw hi lx b fi mf mc l md me">tweets = [x['text'] for x in data]</span><span id="48a0" class="ld iw hi lx b fi mf mc l md me">labels = [x['label'] for x in data]</span><span id="2eca" class="ld iw hi lx b fi mf mc l md me">return tweets, labels</span><span id="621e" class="ld iw hi lx b fi mf mc l md me">tweets, labels = get_tweet(train)</span></pre><p id="91a3" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">这将我们的训练数据分成两个数组:“tweets”和“labels”。</p><p id="4f80" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">您可以运行此代码来更好地了解您的数据是什么样的:</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/9d7611e146fb4e2996777ca90aa48e8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FmZ_c5i2Wn_z4oTBO1aBlQ.png"/></div></div></figure><h2 id="e74d" class="ld iw hi bd ix le lf lg jb lh li lj jf ke lk ll jj ki lm ln jn km lo lp jr lq bi translated">标记化</h2><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="a392" class="ld iw hi lx b fi mb mc l md me">tokenizer = Tokenizer(num_words=10000, oov_token='&lt;UNK&gt;')</span><span id="c362" class="ld iw hi lx b fi mf mc l md me">tokenizer.fit_on_texts(tweets)</span></pre><p id="ae7e" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">这段代码将初始化一个标记器，并将其校准到我们的训练数据上。这将根据单词在数据集中出现的频率给每个单词分配一个编号。</p><p id="8bff" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">正如你在下面看到的，当我们在数据集的第一条tweet上运行“texts_to_sequences”时，我们将得到一个由四个数字组成的数组。每个数字对应于推文中的一个单词，并由该单词的常见程度决定。例如，你可以看到单词“I”与数字“2”相对应，因为它是一个非常常见的单词。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/5315fb8dd0c3a9bbaca9f9ac732604dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hqLR6OxnhQoB3Ms8eZrPsA.png"/></div></div></figure><h2 id="303b" class="ld iw hi bd ix le lf lg jb lh li lj jf ke lk ll jj ki lm ln jn km lo lp jr lq bi translated">使所有序列形状相同</h2><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="21a7" class="ld iw hi lx b fi mb mc l md me">maxlen=50</span><span id="79c3" class="ld iw hi lx b fi mf mc l md me">def get_sequences(tokenizer, tweets):</span><span id="304c" class="ld iw hi lx b fi mf mc l md me">sequences = tokenizer.texts_to_sequences(tweets)</span><span id="2acf" class="ld iw hi lx b fi mf mc l md me">padded = pad_sequences(sequences, truncating = 'post', padding='post', maxlen=maxlen)</span><span id="cf14" class="ld iw hi lx b fi mf mc l md me">return padded</span></pre><p id="3666" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">上面的代码将简单地将数据集中的所有tweetss转换成相同的长度，并将每个tweet的长度设置为50个单词(这个数字可能会根据不同数据集中的文本长度而变化)。空格将被添加，多余的单词将在结尾被删除。这是一个必要的步骤，因为ML模型期望输入是固定的形状和长度。</p><p id="1130" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">当在我们的tweet上运行“get_sequences()”并从该集合中取出第一条tweet时，您可以看到我们有与上面相同的四个序列的集合，只是它的长度被扩展到了50。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/20d95c7330216bb482eec2c7770b1267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hw7j_y7e-3qYuhLB1REbfg.png"/></div></div></figure><h2 id="d8f3" class="ld iw hi bd ix le lf lg jb lh li lj jf ke lk ll jj ki lm ln jn km lo lp jr lq bi translated">为模型准备数据</h2><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="0438" class="ld iw hi lx b fi mb mc l md me">classes = set(labels)</span><span id="9fbe" class="ld iw hi lx b fi mf mc l md me">class_to_index = dict((c,i) for i, c in enumerate(classes))</span><span id="0ebe" class="ld iw hi lx b fi mf mc l md me">index_to_class = dict((v,k) for k, v in class_to_index.items())</span><span id="939f" class="ld iw hi lx b fi mf mc l md me">names_to_ids = lambda labels: np.array([class_to_index.get(x) for x in labels])</span><span id="06f4" class="ld iw hi lx b fi mf mc l md me">train_labels = names_to_ids(labels)</span></pre><p id="a7b7" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">这将创建一个包含所有标签的集合，以及一个字典，我们可以在将类转换为它们的索引以及将索引转换为类时使用它。这在将模型返回的值转换成我们更容易理解的内容时特别有用。此外，我们创建一个名为“names_to_ids”的lambda函数，并使用它将训练数据中的所有标签转换为它们各自的索引。</p><p id="4db9" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">您可以在下面查看我们的每个变量，以便更好地理解它们的用途。请注意，您的单个变量的索引可能略有不同，这是完全正常的。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mn"><img src="../Images/4a604885c602fbb77377f5be063ad589.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W04GssYslb1on0wV4L9sCg.png"/></div></div></figure><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mo"><img src="../Images/32f18bb145eeb1e7c2c24ff64bf2bfe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YC3M2EV_L1Z58AvStbkyoQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Corresponds classes to indexes</figcaption></figure><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/417d23711587ef8fc9172ffdbb3e91a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WnjSamjhVsqsLZc6fZr69Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Values of index_to_class switch places from class_to_index</figcaption></figure><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/99910d11ff351d533a0b076c7915f810.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*vygtKW3aBz9YTNGo6WUJqw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">This shows that the first tweet has a class of index “1”, which corresponds to “sadness”</figcaption></figure><h2 id="9d0c" class="ld iw hi bd ix le lf lg jb lh li lj jf ke lk ll jj ki lm ln jn km lo lp jr lq bi translated">创建模型</h2><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="9c63" class="ld iw hi lx b fi mb mc l md me">model = tf.keras.models.Sequential([</span><span id="3e51" class="ld iw hi lx b fi mf mc l md me">tf.keras.layers.Embedding(10000,16,input_length=maxlen),</span><span id="5b28" class="ld iw hi lx b fi mf mc l md me">tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, return_sequences=True)),</span><span id="1519" class="ld iw hi lx b fi mf mc l md me">tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),</span><span id="3a90" class="ld iw hi lx b fi mf mc l md me">tf.keras.layers.Dense(6, activation='softmax')</span><span id="bf50" class="ld iw hi lx b fi mf mc l md me">])</span><span id="42b6" class="ld iw hi lx b fi mf mc l md me">model.compile(</span><span id="1dfb" class="ld iw hi lx b fi mf mc l md me">     loss='sparse_categorical_crossentropy',</span><span id="7aeb" class="ld iw hi lx b fi mf mc l md me">     optimizer='adam',</span><span id="e875" class="ld iw hi lx b fi mf mc l md me">     metrics=['accuracy']</span><span id="3c21" class="ld iw hi lx b fi mf mc l md me">)</span></pre><p id="895b" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">这里，我们创建了一个非常简单的模型，包括一个嵌入层、两个双向LSTM层和一个用于输出的密集层。要了解更多关于嵌入层的信息，请访问此<a class="ae iu" href="https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/" rel="noopener ugc nofollow" target="_blank">链接</a>。双向层允许双向通信，并使用长期短期记忆层，这是一种能够学习长期依赖性的RNN体系结构。要了解更多信息，请访问此<a class="ae iu" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/." rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="99ed" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">我们还编译了模型及其损失函数、优化器，并通过其准确性来衡量它。要了解更多关于Adam优化器的信息，请点击<a class="ae iu" href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/" rel="noopener ugc nofollow" target="_blank">这里</a>。要了解更多关于稀疏分类交叉熵的信息，点击<a class="ae iu" href="https://datascience.stackexchange.com/questions/41921/sparse-categorical-crossentropy-vs-categorical-crossentropy-keras-accuracy" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h2 id="0b3b" class="ld iw hi bd ix le lf lg jb lh li lj jf ke lk ll jj ki lm ln jn km lo lp jr lq bi translated">培训模式</h2><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="cd8c" class="ld iw hi lx b fi mb mc l md me">val_tweets, val_labels = get_tweet(val)</span><span id="bb31" class="ld iw hi lx b fi mf mc l md me">val_seq = get_sequences(tokenizer, val_tweets)</span><span id="562e" class="ld iw hi lx b fi mf mc l md me">val_labels= names_to_ids(val_labels)</span><span id="8746" class="ld iw hi lx b fi mf mc l md me">h = model.fit(</span><span id="8172" class="ld iw hi lx b fi mf mc l md me">     padded_train_seq, train_labels,</span><span id="56c3" class="ld iw hi lx b fi mf mc l md me">     validation_data=(val_seq, val_labels),</span><span id="3c91" class="ld iw hi lx b fi mf mc l md me">     epochs=20,</span><span id="4644" class="ld iw hi lx b fi mf mc l md me">     callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)]</span><span id="4aa8" class="ld iw hi lx b fi mf mc l md me">)</span></pre><p id="821f" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">终于可以开始训练模型了。请注意，当我们在验证集上的准确性没有上升超过2个时期时，请使用回调来停止训练。</p><p id="ce2d" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">它应该是这样的:</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mr"><img src="../Images/e3067fd8049fa1e93ce25d90b4d4f5e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E9lU9v_RugXcYyBaG-gnxA.png"/></div></div></figure><h2 id="ace8" class="ld iw hi bd ix le lf lg jb lh li lj jf ke lk ll jj ki lm ln jn km lo lp jr lq bi translated">评估和测试模型</h2><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="f3d2" class="ld iw hi lx b fi mb mc l md me">test_tweets, test_labels=get_tweet(test)</span><span id="cccb" class="ld iw hi lx b fi mf mc l md me">test_seq = get_sequences(tokenizer, test_tweets)</span><span id="ef5c" class="ld iw hi lx b fi mf mc l md me">test_labels=names_to_ids(test_labels)</span><span id="4ff9" class="ld iw hi lx b fi mf mc l md me">model.evaluate(test_seq, test_labels)</span></pre><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ms"><img src="../Images/d913b73a9d49b8e7188591a75dd8a339.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1I4pdMfe5KS_e1u0LgtPiw.png"/></div></div></figure><p id="b30d" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">这将分离您的测试数据并获得它们的序列。它还允许模型根据测试数据评估其准确性。</p><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="6c82" class="ld iw hi lx b fi mb mc l md me">i = random.randint(0,len(test_labels)-1)</span><span id="4a7c" class="ld iw hi lx b fi mf mc l md me">print('Sentence:', test_tweets[i])</span><span id="e346" class="ld iw hi lx b fi mf mc l md me">print('Emotion:', index_to_class[test_labels[i]])</span><span id="579f" class="ld iw hi lx b fi mf mc l md me">p = model.predict(np.expand_dims(test_seq[i], axis=0))[0]</span><span id="6691" class="ld iw hi lx b fi mf mc l md me">print(test_seq[i])</span><span id="3d5b" class="ld iw hi lx b fi mf mc l md me">pred_class=index_to_class[np.argmax(p).astype('uint8')]</span><span id="a9f8" class="ld iw hi lx b fi mf mc l md me">print('Predicted Emotion: ', pred_class)</span></pre><p id="9c87" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">这段代码只是生成一条随机的tweet，并让模型预测它属于哪个类。它还预测推文及其标签，以便您可以比较预测和正确答案。</p><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="5694" class="ld iw hi lx b fi mb mc l md me">sentence = 'i am not sure what to do'</span><span id="5fce" class="ld iw hi lx b fi mf mc l md me">sequence = tokenizer.texts_to_sequences([sentence])</span><span id="9b15" class="ld iw hi lx b fi mf mc l md me">paddedSequence = pad_sequences(sequence, truncating = 'post', padding='post', maxlen=maxlen)</span><span id="c949" class="ld iw hi lx b fi mf mc l md me">p = model.predict(np.expand_dims(paddedSequence[0], axis=0))[0]</span><span id="e3d9" class="ld iw hi lx b fi mf mc l md me">pred_class=index_to_class[np.argmax(p).astype('uint8')]</span><span id="6d80" class="ld iw hi lx b fi mf mc l md me">print('Sentence:', sentence)</span><span id="4d35" class="ld iw hi lx b fi mf mc l md me">print('Predicted Emotion: ', pred_class)</span></pre><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/3c89df2bbc64ed33913e71b5f07bd88e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*zxf625ukfab6Jn3bz_i0YA.png"/></div></figure><p id="d432" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">这段代码允许您输入自己的句子，并让模型预测其情绪。</p><h2 id="2afa" class="ld iw hi bd ix le lf lg jb lh li lj jf ke lk ll jj ki lm ln jn km lo lp jr lq bi translated">保存模型</h2><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="15c7" class="ld iw hi lx b fi mb mc l md me">from google.colab import drive</span><span id="e2c7" class="ld iw hi lx b fi mf mc l md me">drive.mount('/content/drive')</span><span id="5428" class="ld iw hi lx b fi mf mc l md me">model.save("/content/drive/My Drive/TweetEmotionRecognition/h5/tweet_model.h5")</span></pre><p id="0dbd" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">这允许您以. h5文件类型将模型保存到Google Drive。请注意，我使用的是Google Collab，所以如果您在本地运行代码，路径会改变。</p><h2 id="099e" class="ld iw hi bd ix le lf lg jb lh li lj jf ke lk ll jj ki lm ln jn km lo lp jr lq bi translated">装载模型</h2><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="978a" class="ld iw hi lx b fi mb mc l md me">load_model = tf.keras.models.load_model("/content/drive/My Drive/TweetEmotionRecognition/h5/tweet_model.h5")</span><span id="52a3" class="ld iw hi lx b fi mf mc l md me">print(load_model.summary())</span></pre><p id="777d" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">如果您想从某个文件路径加载您的模型，您可以使用上面的代码来完成。从那时起，您可以使用您的模型运行您想要的所有函数，除了将单词“model”替换为“load_model”(例如，“load_model.predict()”)。</p></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="4083" class="iv iw hi bd ix iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js bi translated">结论</h1><p id="6b7e" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">话虽如此，我希望你喜欢我的文章，并了解自然语言处理！请随意查看我的其他文章，更多文章即将发布！</p><p id="f3f7" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">如果你想看完整的代码，你可以在我的Github库<a class="ae iu" href="https://github.com/AlexanderChow9333/TweetEmotionRecognition" rel="noopener ugc nofollow" target="_blank">这里</a>查看。</p><p id="cc82" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">如果你有任何问题或想联系，随时给我发电子邮件:alexander.chow911@gmail.com</p><p id="16e4" class="pw-post-body-paragraph jt ju hi jv b jw mg jy jz ka mh kc kd ke mi kg kh ki mj kk kl km mk ko kp kq hb bi translated">了解更多关于我的信息:<a class="ae iu" href="https://www.linkedin.com/in/alexander-chow-6539771b3/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a></p></div></div>    
</body>
</html>
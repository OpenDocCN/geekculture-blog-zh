<html>
<head>
<title>Things to Remember in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习需要记住的事情</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/things-to-remember-in-deep-learning-eca746ed29c8?source=collection_archive---------66-----------------------#2021-05-27">https://medium.com/geekculture/things-to-remember-in-deep-learning-eca746ed29c8?source=collection_archive---------66-----------------------#2021-05-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="3af3" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated"><strong class="ak"> <em class="ix">深度学习快速游</em> </strong></h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es iy"><img src="../Images/d9349a8f530f1279fa3afd4f80a4767a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t7lx34vYoJ8lAeNU6un7_g.jpeg"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx">Photo by <a class="ae jo" href="https://www.pexels.com/@agk42?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Alex Knight</a> from <a class="ae jo" href="https://www.pexels.com/photo/high-angle-photo-of-robot-2599244/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><h2 id="a873" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">介绍</h2><p id="fb72" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ka kw kx ky ke kz la lb ki lc ld le lf hb bi translated">当有人为任何数据科学或机器学习职位面试你时，第一个问题会是:告诉我你知道的机器学习技术。然后你将开始回答这个问题，陈述你已经研究和工作过的许多算法。面试官会在你解释概念的时候挑选一些关键词，问你一些相关的问题。</p><p id="c223" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">我也会用那种方式解释事情，这样我们就能把事情联系起来。</p><p id="75b1" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated"><strong class="kp hj">深度学习</strong></p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ll"><img src="../Images/489cbc86e924832747c6c78191a27c84.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*vzIQkT0FH0Ck7Yny8nqUEg.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">image by author</figcaption></figure><p id="64de" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">深度学习是机器学习的一个子集。在机器学习中，我们给出输入和输出，并使用一些指定的模型，而在深度学习中，神经网络根据给定的数据进行训练，以找出自身有用的模式。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es et"><img src="../Images/736f646da80f81e28d87e923e5f265eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-18XKN-Zx-42gZBJaGMFuA.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx">Example for Neural Network</figcaption></figure><p id="d334" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">神经网络模拟输出变量和输入信号之间的关系。在我们的例子中，输入是购物车中的产品、以前的订单和薪水。</p><p id="6361" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">神经网络将试图从输入中找出一些特征，并通过权重来证明输入(独立变量)的强度。也许购物车中的产品和以前的订单告诉我们这些产品之间的相似性，所以在这种情况下，给予薪金的权重将非常小。</p><p id="d940" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">以前的订单和工资告诉我们客户的可支配收入。根据产品和可支配收入的相似性，我们可以预测客户是否会购买该产品。</p><p id="1302" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">注意:在上面的例子中，输入变量之间的特征是产品和可支配收入的相似性，在神经网络中可以是任何东西。这个想法是神经网络自己学习这些特征</p><p id="8ac3" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated"><strong class="kp hj">神经网络中的步骤</strong></p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es et"><img src="../Images/a726ed08c970598d8533d925ea8a65e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uY-BC0P-VqznYSPd4jfxjQ.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx">Steps Neural Network</figcaption></figure><p id="afb0" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">需要记住的要点</p><p id="fd36" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated"><strong class="kp hj"> 1。</strong> <strong class="kp hj">为什么随机赋权，为什么不赋零？</strong></p><p id="d7d1" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">如果w和b等于0而不是一个随机数。</p><p id="0b3c" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">第一个隐藏层中的每个神经元将执行相同的计算，因此即使在梯度下降的多次迭代之后，该层中的每个神经元将与其他神经元计算相同的东西。</p><p id="7eab" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">现在的问题是什么是梯度下降？</p><p id="bc9b" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">梯度下降是一种寻找最佳权重和偏差的方法，其中梯度意味着斜率。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es lm"><img src="../Images/34b0dc80e766e5de534b11695ec54a82.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*9cG475qSwDVuJElVIxYW9g.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Gradient Descent</figcaption></figure><p id="c730" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">假设我们的成本函数只是w的函数，我们的权重和成本函数的曲线将如上所示。我们的优化问题是找到点5，这是全局最小值。</p><p id="73d0" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">在向前移动之前，用于更新权重的公式是:</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ln"><img src="../Images/d73c37975cf9afb460f8f187329c8440.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*plhVVvhsr-ItwCv0NVOoVQ.png"/></div></figure><p id="e4a0" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">注意:同样的公式也适用于偏差，但在这种情况下，J将是w和b的函数。</p><p id="18e9" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">当我们分配随机权重时(设1为pt。)，我们观察到梯度，即dJ(w)/dw为正，因此重量将减少。</p><p id="0851" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">另一方面，如果我们在点3，梯度将是负的，因此重量将增加。这个过程将继续下去，直到我们接近全局最小值。</p><p id="1e63" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">梯度下降也用于逻辑回归以优化成本函数</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es lo"><img src="../Images/2a75f20dae4d5a628bfb12442f3fa0a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*qYANlQ7ixqZvmPSTiTh-yQ.png"/></div></figure><p id="c1a8" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated"><strong class="kp hj"> 2。</strong> <strong class="kp hj">为什么我们在逻辑回归中使用上述成本函数而不是MSE？</strong></p><p id="05ed" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">首先，记住逻辑回归是最简单的神经网络之一，也没有隐藏层。</p><p id="bbe3" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">我们在逻辑回归中不使用均方误差，因为当我们对权重绘制MSE损失函数时，获得的曲线是<em class="lp">而不是凸曲线</em>，这使得很难找到全局最小值。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es lq"><img src="../Images/0cfd3220d65112070d2055b280f079e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*7dfTyObt6e65sG9IGK6IpQ.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Non-Convex Function</figcaption></figure><p id="7266" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">逻辑回归的均方误差的非凸性质是因为非线性是以sigmoid函数的形式引入的(非线性)，</p><p id="90f4" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">这使得权重和误差之间的关系非常复杂。</p><p id="951b" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated"><strong class="kp hj"> 3。</strong> <strong class="kp hj">什么是乙状结肠功能？</strong></p><p id="b1a3" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">sigmoid函数是非线性的，并且sigmoid函数中的值存在于0和1之间。它有助于发现分类问题中的概率。</p><p id="8634" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">sigmoid函数是可微的，我们可以计算斜率。</p><p id="75af" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">用于神经网络时，Sigmoid函数会减慢学习过程。记住我们的体重更新公式:</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ln"><img src="../Images/d73c37975cf9afb460f8f187329c8440.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*plhVVvhsr-ItwCv0NVOoVQ.png"/></div></figure><p id="4311" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">如图所示，如果我们在那些梯度接近0的位置，w(重量)将变化非常缓慢，因为我们的α也很小。因此使得<em class="lp">α* dJ(w)/dw</em>为非常小的值。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es lr"><img src="../Images/bf9e5d583f4743d8f34e88df5caff794.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*dkh2PC_M9T3XDtbl-Xt_wg.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Sigmoid Function</figcaption></figure><p id="49a1" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">这里的学习非常缓慢，因为斜率(梯度接近于零)</p><p id="96ab" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">4.如何克服以上挑战(因为乙状结肠学的慢)？</p><p id="6658" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">我们使用ReLU函数作为替代</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ls"><img src="../Images/1d198e4dde386895dbcf0c6c4df69fec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*Q9wPijez6EU_qBnQDWYzDg.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">ReLU</figcaption></figure><p id="ed40" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">ReLU函数有R(z) = max {0，z} <strong class="kp hj"> </strong>在z = 0处不可微，其他任何地方梯度都不小。所以，权重更新和学习会很快。</p><p id="7167" class="pw-post-body-paragraph kn ko hi kp b kq lg ij ks kt lh im kv ka li kx ky ke lj la lb ki lk ld le lf hb bi translated">爱情！活下去！笑！干杯</p></div></div>    
</body>
</html>
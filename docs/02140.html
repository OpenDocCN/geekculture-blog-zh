<html>
<head>
<title>Logistic Regression From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/logistic-regression-from-scratch-59e88bea2ba2?source=collection_archive---------10-----------------------#2021-05-05">https://medium.com/geekculture/logistic-regression-from-scratch-59e88bea2ba2?source=collection_archive---------10-----------------------#2021-05-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/bafe894f844299a976c0f6f611b1a4dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NOlXEdpKgOn6rkOzBXvxUA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Visualization for the Iris Dataset (Setosa vs rest)</figcaption></figure></div><div class="ab cl iu iv gp iw" role="separator"><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz"/></div><div class="hb hc hd he hf"><h2 id="90ba" class="jb jc hi bd jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy bi translated"><strong class="ak">简介</strong></h2><p id="1822" class="pw-post-body-paragraph jz ka hi kb b kc kd ke kf kg kh ki kj jm kk kl km jq kn ko kp ju kq kr ks kt hb bi translated">本文的目标是为读者提供对逻辑回归技术的坚实理解。它假设熟悉概率论、线性代数和微分学的基础知识。它的第一部分探索逻辑回归的基本原理以及必要的数学细节。第二部分集中在实现上，为此我使用Python。</p></div><div class="ab cl iu iv gp iw" role="separator"><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz"/></div><div class="hb hc hd he hf"><h2 id="0131" class="jb jc hi bd jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy bi translated"><strong class="ak">为什么使用逻辑回归？</strong></h2><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/9edf67c25482ab37edb04308423d43a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eoJkP5fYpG4rzzZxCG9N6g.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Linear model vs logistic model</figcaption></figure><p id="d5e7" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">即使目标变量是二分的(也称为二元的)，它也诱使人们求助于旧的熟悉的线性回归，然而它将被证明在建模概率和适当的分类方面是无效的。原因是这样的模型具有无限的范围，这意味着它从-inf一直延伸到+inf。发明了一种解决方案，它涉及一个在概率建模中表现出色的函数，即sigmoid函数:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/b96bd28815a257a18643f708966733a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/0*sLs3aSnnJ2vQ1XPe"/></div></figure><p id="2973" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">你也可能偶尔碰到它的不同形式:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es le"><img src="../Images/95f409df6eb558e92cc308ff2a3034b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/0*gQaPJLVKlxsecXik"/></div></figure><p id="f61d" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">因此，当x走向+inf时，函数值接近1，当x走向-inf时，函数值接近0。逻辑回归在进行概率假设时利用了这种行为。</p></div><div class="ab cl iu iv gp iw" role="separator"><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz"/></div><div class="hb hc hd he hf"><h2 id="7eff" class="jb jc hi bd jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy bi translated"><strong class="ak">理论</strong></h2><p id="bf30" class="pw-post-body-paragraph jz ka hi kb b kc kd ke kf kg kh ki kj jm kk kl km jq kn ko kp ju kq kr ks kt hb bi translated">与线性回归一样，逻辑回归的输入是与其相关的特征和权重(系数)的线性组合:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/4721dd854059d2987c154b4228bdca1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/0*v2SwW2kfAD_QPsAU"/></div></figure><p id="23e1" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">我们将改写为:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/091a25861c512691f5ca8a15823c41bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/0*zSCbS3O3whzY04LY"/></div><figcaption class="iq ir et er es is it bd b be z dx">If you feel uncomfortable with multiplying matrices, see <a class="ae lh" href="http://matrixmultiplication.xyz/" rel="noopener ugc nofollow" target="_blank">http://matrixmultiplication.xyz</a> for an excellent visualization</figcaption></figure><p id="c266" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">零w和1的出现可能看起来令人困惑，但是它是零β所代表的相同的y截距。在权重的上下文中，w-零通常被称为偏差项。</p><p id="9212" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">将输入代入sigmoid函数，我们得到逻辑回归的概率形式:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es li"><img src="../Images/4d600eba07369e57c4f1eca9f923aa10.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/0*oVjuhFANMVTX3n7s"/></div></figure><p id="961f" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">然而，在所谓的逻辑回归的logit形式中，也经常看到赔率的对数:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/1c45ba0e5cf5d84e8847ee6cc72942a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/0*r9RK3loEl7yHxhmK"/></div></figure><p id="80dd" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">但是我们为什么要用这种奇怪的方法呢？原因是需要表达特征和权重的线性组合。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/9c14a5125a0f50aad037d2e984203f06.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/0*1wDHQ-_H20MadRXx"/></div></figure><p id="3623" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">通过概率对数表达线性组合的决定有两个基础:首先，它带回了熟悉的“直线”符号，其次，系数变得更容易解释。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/cb7bb01e3efadb3e72070fdb033896c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oIUxih1psbyoo56ldLR4Bw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">In the left plot, the classes (0 and 1) are at -inf and +inf because the logarithm is undefined at P ∈ {0, 1}</figcaption></figure><p id="153b" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">因此，这就是我们如何画出概率曲线的问题。问题出现了:我们如何选择权重来输出最佳拟合曲线？</p></div><div class="ab cl iu iv gp iw" role="separator"><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz"/></div><div class="hb hc hd he hf"><h2 id="ff94" class="jb jc hi bd jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy bi translated"><strong class="ak">最大可能性</strong></h2><p id="134a" class="pw-post-body-paragraph jz ka hi kb b kc kd ke kf kg kh ki kj jm kk kl km jq kn ko kp ju kq kr ks kt hb bi translated">我们用来优化线性回归参数的OLS(普通最小二乘法)方法不适用于logit函数，问题在于，无论哪条线，到数据点的距离总是无穷大。相反，我们引入了最大似然的概念，其中似然是在假设模型为真的情况下观察到我们的数据的概率:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/bf3379b0d3b8deb901631c787a41ed47.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/0*oAdCPYzG7QloMVKA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">When y_j =1, the right factor becomes 1 and we get the probability of observing 1 when the sample belongs to class 1. When y_j = 0, the left factor becomes 1 and we get the probability of observing 0 when the sample belongs to class 0.</figcaption></figure><p id="64d4" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">请注意，MLE(最大似然估计)要求观测值独立，这允许乘以概率:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/b1ebef083a6f52aec6022a26f5fbdd07.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/0*PtWVDWQ21zggz2Rz.png"/></div></figure><p id="4ba6" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">然而，由于浮点数的舍入问题，我们在乘以概率时有下溢的风险，无论我们选择什么样的权重，这几乎肯定会以零可能性结束。这就是为什么在做MLE时，似然函数是对数变换的，它把概率的乘积转换成它们对数的和。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/866f19cf8a5cf0f954d6cd1c66ef53c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/0*QdMWaGw-9aig0grv"/></div></figure></div><div class="ab cl iu iv gp iw" role="separator"><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz"/></div><div class="hb hc hd he hf"><h2 id="2616" class="jb jc hi bd jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy bi translated"><strong class="ak">梯度下降</strong></h2><p id="a3a7" class="pw-post-body-paragraph jz ka hi kb b kc kd ke kf kg kh ki kj jm kk kl km jq kn ko kp ju kq kr ks kt hb bi translated">梯度下降是一种数值优化技术，它迭代地逼近成本函数J(w)的最小值。这是算法的概要:</p><ul class=""><li id="38be" class="lo lp hi kb b kc ky kg kz jm lq jq lr ju ls kt lt lu lv lw bi translated">选择学习率η(通常在. 001和. 1之间的数字)和迭代次数(建议至少取1000次);</li><li id="bde3" class="lo lp hi kb b kc lx kg ly jm lz jq ma ju mb kt lt lu lv lw bi translated">生成初始权重，通常是随机数；</li><li id="6699" class="lo lp hi kb b kc lx kg ly jm lz jq ma ju mb kt lt lu lv lw bi translated">相对于所涉及的每个参数w_i计算梯度(导数);</li><li id="d08c" class="lo lp hi kb b kc lx kg ly jm lz jq ma ju mb kt lt lu lv lw bi translated">使用以下公式计算新的权重:</li></ul><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/21eea223faeedb4d9f1d8310a87208c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/0*KUZnT7PorSGvTP1S"/></div><figcaption class="iq ir et er es is it bd b be z dx">As the value of gradient decreases, smaller and smaller steps are taken towards the minimum</figcaption></figure><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="ab fe cl md"><img src="../Images/6d643acd460de50045bb092bb25d02d1.png" data-original-src="https://miro.medium.com/v2/0*fU8XFt-NCMZGAWND."/></div><figcaption class="iq ir et er es is it bd b be z dx">source: <a class="ae lh" href="https://www.kdnuggets.com/2018/06/intuitive-introduction-gradient-descent.html" rel="noopener ugc nofollow" target="_blank">https://www.kdnuggets.com/2018/06/intuitive-introduction-gradient-descent.html</a></figcaption></figure><p id="53b0" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">注意:注意你选择的学习速度。如果你选择一个相对较大的值，你的权重将会无限反弹而不收敛。相反，如果η太小，在梯度值达到容许阈值(任意选择的接近零的参数，在该阈值上算法停止)之前，你可能会变老。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es me"><img src="../Images/b6cb5187c7523bdb28b6d834a273ecfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/0*xlUOJxiDAyLmdRFQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">source: <a class="ae lh" href="https://srdas.github.io/DLBook/GradientDescentTechniques.html" rel="noopener ugc nofollow" target="_blank">https://srdas.github.io/DLBook/GradientDescentTechniques.html</a></figcaption></figure><p id="1477" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">为了遵守某些约定，我们将成本函数J定义为-LL，因为否则它将是梯度上升，这很少被提及。下面是J的梯度的完整推导:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/af76de4024747586d9d3dd45ea582ce3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/0*3AWNtBTVDiOJ-snb"/></div></figure><p id="c250" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">让我们分解这个问题，首先导出总和中各项的梯度:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/a67477b1db05d0f91881b271b7bcf62f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eRvF5aGUWvPIN1S6"/></div></div></figure><p id="2f1a" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">然后，将所有内容联系在一起，消除冗余:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/96c97a54f59933e1d96404275ac01e19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-grphParm9vzveyK"/></div></div></figure><p id="a195" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">因此，我们的目标是导出所有权重的梯度，并使其接近于零。虽然从理论上来说，这个任务可以通过分析方程组来完成，但是随着特征数量的增加，事情很快就会变得疯狂。想象一下，教一台计算机找到一个包含10 000个特征的解析解，因此10 001个(记住截距)导数的系统都设置为零。看到了吗？数字方法拯救了世界。</p></div><div class="ab cl iu iv gp iw" role="separator"><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz"/></div><div class="hb hc hd he hf"><h2 id="aa77" class="jb jc hi bd jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy bi translated"><strong class="ak">多类逻辑回归</strong></h2><p id="9bc7" class="pw-post-body-paragraph jz ka hi kb b kc kd ke kf kg kh ki kj jm kk kl km jq kn ko kp ju kq kr ks kt hb bi translated">尽管在本质上，逻辑回归的目的只是区分两类，但它也可以用于多类(n &gt; 2)分类。本文涵盖了One-vs-Rest方法，这意味着用n个不同的预处理数据集来拟合n个二元逻辑模型。这个概念可以通过一个例子得到最好的理解:</p><p id="8a6e" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">考虑我们要从Iris数据集区分Setosa、Versicolour和Virginica，这对熟悉机器学习的人来说一定非常熟悉。我们必须建立3种不同的物流模型:</p><ul class=""><li id="54c4" class="lo lp hi kb b kc ky kg kz jm lq jq lr ju ls kt lt lu lv lw bi translated">Setosa (1)对Versicolour和Virginica(0)；</li><li id="9c07" class="lo lp hi kb b kc lx kg ly jm lz jq ma ju mb kt lt lu lv lw bi translated">Versicolour (1) vs Setosa和Virginica(0)；</li><li id="4f01" class="lo lp hi kb b kc lx kg ly jm lz jq ma ju mb kt lt lu lv lw bi translated">Virginica (1)对Setosa和Versicolour (0)，</li></ul><p id="7ee1" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">然后我们汇总结果，最后的预测是观察到1的概率最高的类。比方说，对于一次观测，我们得到Setosa的概率等于. 035，Versicolour的概率等于. 23，Virginica的概率等于. 0006。因此，Versicolour将成为预测的职业。</p></div><div class="ab cl iu iv gp iw" role="separator"><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz"/></div><div class="hb hc hd he hf"><h2 id="95fb" class="jb jc hi bd jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy bi translated">Python中的逻辑回归</h2><p id="f789" class="pw-post-body-paragraph jz ka hi kb b kc kd ke kf kg kh ki kj jm kk kl km jq kn ko kp ju kq kr ks kt hb bi translated">恭喜你掌握了理论，到达了文章的第二部分。这里我们要用Python来构建逻辑回归。我们将以一种面向对象的方式来做这件事，所以如果你觉得你对OOP理解不深，考虑一下你是在死记硬背。对于基础知识，我建议参观科里·斯查费的迷你球场。</p><p id="53fd" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">对于这个实现，您还需要安装numpy。</p><p id="aaa0" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">我们将开始声明类并指定初始化时设置的参数:</p><pre class="ku kv kw kx fd mh mi mj mk aw ml bi"><span id="612b" class="jb jc hi mi b fi mm mn l mo mp">class LogisticRegression:</span><span id="8fd5" class="jb jc hi mi b fi mq mn l mo mp">    def __init__(self, eta=.01, n_iter=100000, tolerance=1e-5, random_state=42):</span><span id="8e54" class="jb jc hi mi b fi mq mn l mo mp">        self._eta = eta<br/>        self._n_iter = n_iter<br/>        self._tolerance = tolerance<br/>        self.__random_state = random_state</span></pre><p id="7a98" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">关于参数的几点说明:</p><ul class=""><li id="e739" class="lo lp hi kb b kc ky kg kz jm lq jq lr ju ls kt lt lu lv lw bi translated"><em class="mr">η</em>=η=学习率；</li><li id="d3bd" class="lo lp hi kb b kc lx kg ly jm lz jq ma ju mb kt lt lu lv lw bi translated"><em class="mr"> n_iter </em>是梯度下降停止前的迭代次数；</li><li id="8590" class="lo lp hi kb b kc lx kg ly jm lz jq ma ju mb kt lt lu lv lw bi translated"><em class="mr">公差</em>是所有<strong class="kb hj">梯度需要达到的阈值，以使算法停止；</strong></li><li id="5c0f" class="lo lp hi kb b kc lx kg ly jm lz jq ma ju mb kt lt lu lv lw bi translated"><em class="mr"> random_state </em>是随机种子，在其上生成初始权重。为再现性而规定。</li></ul><p id="c25f" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">我们添加了<em class="mr"> _sigmoid </em>实用静态方法，它采用一个特征矩阵和一个权重数组，并生成一个概率数组:</p><pre class="ku kv kw kx fd mh mi mj mk aw ml bi"><span id="7c8e" class="jb jc hi mi b fi mm mn l mo mp">    @staticmethod<br/>    def _sigmoid(X, w):<br/> <br/>        denominator = 1 + np.exp(-(X * w).sum(axis=1))<br/>        return 1 / denominator</span></pre><p id="b3df" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">我们现在完全专注于<em class="mr"> fit </em>方法，我将通过集成注释一步一步地解释它。我已经包括了<em class="mr">归一化</em>参数，该参数可选择缩放特征，这使得梯度下降收敛更加平滑和快速:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/b929be7eb3375705ffc65efcffc41a4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*VHq-Z8G6RcenMeB6Rbtzgw.png"/></div></figure><pre class="ku kv kw kx fd mh mi mj mk aw ml bi"><span id="4deb" class="jb jc hi mi b fi mm mn l mo mp">    def fit(self, X, y, normalize=False):</span><span id="e869" class="jb jc hi mi b fi mq mn l mo mp">        # Convert the input into numpy arrays<br/>        # Make <em class="mr">y</em> a 1-d array in case a matrix is supplied</span><span id="b715" class="jb jc hi mi b fi mq mn l mo mp">        self._X = np.array(X, dtype=np.dtype('float64'))<br/>        self._y = np.array(y, dtype=np.dtype('int64')).squeeze()</span><span id="9b0a" class="jb jc hi mi b fi mq mn l mo mp">        # We save <em class="mr">normalize</em> as the instance attribute;<br/>        # if set to True, test data in the <em class="mr">predict</em> method<br/>        # will also be normalized.<br/>        # We also save the data range as well as the minimum value.</span><span id="34ed" class="jb jc hi mi b fi mq mn l mo mp">        self.__normalize = normalize<br/>        if self.__normalize:<br/>            self.__Xmin = self._X.min(axis=0)<br/>            self.__Xrange = self._X.max(axis=0) - self.__Xmin<br/>            self._X = (self._X - self.__Xmin) / self.__Xrange</span><span id="b20f" class="jb jc hi mi b fi mq mn l mo mp">        # Check if the problem is multiclass:</span><span id="f560" class="jb jc hi mi b fi mq mn l mo mp">        self._classes = np.unique(self._y)<br/>        if len(self._classes) &gt; 2:</span><span id="ed0c" class="jb jc hi mi b fi mq mn l mo mp">            # If we have more than 2 classes,<br/>            # we set the corresponding boolean to True<br/>            # and prepare a container for n binary models.  </span><span id="6637" class="jb jc hi mi b fi mq mn l mo mp">            self.__multiclass = True<br/>            self.models_ = []</span><span id="03c3" class="jb jc hi mi b fi mq mn l mo mp">            for class_ in self._classes:</span><span id="5f06" class="jb jc hi mi b fi mq mn l mo mp">                # Setting 1 where an observation belongs to the <br/>                # class and 0 where it does not.</span><span id="c2d5" class="jb jc hi mi b fi mq mn l mo mp">                y = np.zeros(shape=self._y.shape)<br/>                y[self._y == class_] = 1</span><span id="116e" class="jb jc hi mi b fi mq mn l mo mp">                # Initialize and fit the model.</span><span id="357b" class="jb jc hi mi b fi mq mn l mo mp">                lr = LogisticRegression(<br/>                    eta=self._eta,<br/>                    n_iter=self._n_iter,<br/>                    tolerance=self._tolerance,<br/>                    random_state=self.__random_state<br/>                )<br/>                lr.fit(self._X, y)</span><span id="19a7" class="jb jc hi mi b fi mq mn l mo mp">                # We initialize <em class="mr">normalize </em>as False regardless<br/>                # of whether or not the main model has True<br/>                # because, if it does, self._X is already normalized</span><span id="9728" class="jb jc hi mi b fi mq mn l mo mp">                # Instead, we set the necessary attributes after<br/>                # the model is fit.</span><span id="f6e5" class="jb jc hi mi b fi mq mn l mo mp">                if self.__normalize:<br/>                    lr.__normalize = self.__normalize<br/>                    lr.__Xmin = self.__Xmin<br/>                    lr.__Xrange = self.__Xrange</span><span id="3a35" class="jb jc hi mi b fi mq mn l mo mp">                self.models_.append(lr)</span><span id="3da6" class="jb jc hi mi b fi mq mn l mo mp">            self.__fit = True</span><span id="d678" class="jb jc hi mi b fi mq mn l mo mp">            return self</span><span id="bb8f" class="jb jc hi mi b fi mq mn l mo mp">        else:</span><span id="7fdf" class="jb jc hi mi b fi mq mn l mo mp">            self.__multiclass = False</span><span id="d820" class="jb jc hi mi b fi mq mn l mo mp">            # We add the bias term to the data to fit the intercept.</span><span id="f259" class="jb jc hi mi b fi mq mn l mo mp">            self._X = np.concatenate(<br/>                [np.ones(shape=(len(X), 1)), self._X],<br/>                axis=1<br/>            )</span><span id="32cc" class="jb jc hi mi b fi mq mn l mo mp">            # Generate the initial weights.</span><span id="c6cf" class="jb jc hi mi b fi mq mn l mo mp">            rs = np.random.RandomState(seed=self.__random_state)<br/>            self.w_ = rs.normal(size=(self._X.shape[1], ))</span><span id="1375" class="jb jc hi mi b fi mq mn l mo mp">            # Gradient descent</span><span id="9afd" class="jb jc hi mi b fi mq mn l mo mp">            for _ in range(self._n_iter):<br/>                grad = ((self._sigmoid(self._X, self.w_) - self._y)[:, np.newaxis] * self._X).sum(axis=0)<br/>                self.w_ -= self._eta * grad<br/>                print(grad)<br/>                if all(np.absolute(grad) &lt; self._tolerance):<br/>                    break</span><span id="faeb" class="jb jc hi mi b fi mq mn l mo mp">            self.intercept_ = self.w_[0]<br/>            self.coef_ = self.w_[1:]</span><span id="7c5c" class="jb jc hi mi b fi mq mn l mo mp">            self.__fit = True<br/>  <br/>            return self</span></pre><p id="872f" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">让我们测试一下我们目前所拥有的:</p><pre class="ku kv kw kx fd mh mi mj mk aw ml bi"><span id="11c5" class="jb jc hi mi b fi mm mn l mo mp">from sklearn.model_selection import train_test_split<br/>from sklearn import datasets</span><span id="254a" class="jb jc hi mi b fi mq mn l mo mp">data = datasets.load_iris()<br/>X, y = data['data'][:, 3:4], data['target'] # Use one feature for simplicity (petal width).</span><span id="60a6" class="jb jc hi mi b fi mq mn l mo mp">X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)</span><span id="38f5" class="jb jc hi mi b fi mq mn l mo mp">lr = LogisticRegression(eta=.05)<br/>lr.fit(X_train, y_train, normalize=True)</span><span id="b92b" class="jb jc hi mi b fi mq mn l mo mp">print(lr.models_[0].w_)<br/>print(lr.models_[1].w_)<br/>print(lr.models_[2].w_)</span></pre><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/ccdb74169038494a241b180f035cdd2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6skwXcQptCOlxpvMDG24Lw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Setosa vs Rest. Output: [ 17.24983752 -61.98800334]</figcaption></figure><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/0a17a752175122dd03f6faff25302a52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3MCeIzJiRAvOpIMKHNuQ4A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Versicolour vs Rest. Output: [-1.04412163 0.72087761]</figcaption></figure><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/8063409f9dd2de98aef5df769d9892ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ihAICqs9nZlr0u_PoPFlSg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Virginica vs Rest. Output: [-20.88894019 32.95409127]</figcaption></figure><p id="ba4b" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">我们现在介绍接受以下参数的<em class="mr">预测</em>方法:</p><ul class=""><li id="ce41" class="lo lp hi kb b kc ky kg kz jm lq jq lr ju ls kt lt lu lv lw bi translated"><em class="mr">X _测试。</em>检测数据。如果使用训练数据的规范化版本拟合了模型，测试数据也将被规范化。</li><li id="75d0" class="lo lp hi kb b kc lx kg ly jm lz jq ma ju mb kt lt lu lv lw bi translated"><em class="mr">普罗巴</em>。是否返回概率。它们追加到预测类数组的右侧。</li><li id="7d10" class="lo lp hi kb b kc lx kg ly jm lz jq ma ju mb kt lt lu lv lw bi translated"><em class="mr">阈值</em>。判定规则:如果P ≥ <em class="mr">阈值，则</em>观察值被分类为1，否则被分类为0。应该小心操作。例如，如果患者的生命取决于发现他们是否患有疾病，那么门槛应该降低。当模型是多类时忽略。</li></ul><pre class="ku kv kw kx fd mh mi mj mk aw ml bi"><span id="1d9a" class="jb jc hi mi b fi mm mn l mo mp">    def predict(self, X_test, proba=False, threshold=.5):</span><span id="71a4" class="jb jc hi mi b fi mq mn l mo mp">        if not self.__fit:<br/>            raise TypeError('The model has not been fit.')</span><span id="03b1" class="jb jc hi mi b fi mq mn l mo mp">        if self.__multiclass:</span><span id="9bb0" class="jb jc hi mi b fi mq mn l mo mp">            # Extract the probabilities from each of the fitted <br/>            # models.</span><span id="dc5a" class="jb jc hi mi b fi mq mn l mo mp">            probas = map(lambda model: model.predict(X_test, proba=True)[:, [1]], self.models_)</span><span id="fe65" class="jb jc hi mi b fi mq mn l mo mp">            # Tie them together.</span><span id="9e55" class="jb jc hi mi b fi mq mn l mo mp">            self.proba_ = np.concatenate(list(probas), axis=1)</span><span id="220a" class="jb jc hi mi b fi mq mn l mo mp">            # Make the prediction: a class with the highest<br/>            # probability is chosen as the predicted class.</span><span id="4e28" class="jb jc hi mi b fi mq mn l mo mp">            self.y_hat_ = np.array([self._classes[idx] for idx in self._classes[np.argmax(self.proba_, axis=1)]])</span><span id="dfa3" class="jb jc hi mi b fi mq mn l mo mp">            if proba:<br/>                return np.concatenate([self.y_hat_[:, np.newaxis], self.proba_], axis=1)<br/>            else:<br/>                return self.y_hat_<br/>  <br/>        else:</span><span id="e50a" class="jb jc hi mi b fi mq mn l mo mp">            self._X_test = np.array(X_test, dtype=np.dtype('float64'))</span><span id="213c" class="jb jc hi mi b fi mq mn l mo mp">            # Normalize the testing data.</span><span id="31b5" class="jb jc hi mi b fi mq mn l mo mp">            if self.__normalize:<br/>                self._X_test = (self._X_test - self.__Xmin) / self.__Xrange</span><span id="a628" class="jb jc hi mi b fi mq mn l mo mp">            # Append the bias term.</span><span id="000c" class="jb jc hi mi b fi mq mn l mo mp">            self._X_test = np.concatenate([np.ones(shape=(self._X_test.shape[0], 1)), self._X_test], axis=1)</span><span id="2070" class="jb jc hi mi b fi mq mn l mo mp">            # Calculate the probabilities.</span><span id="4420" class="jb jc hi mi b fi mq mn l mo mp">            self.proba_ = self._sigmoid(self._X_test, self.w_)</span><span id="71c2" class="jb jc hi mi b fi mq mn l mo mp">            self.y_hat_ = np.zeros(shape=(self.proba_.shape[0], ))<br/>            self.y_hat_[self.proba_ &gt;= threshold] = 1</span><span id="0d10" class="jb jc hi mi b fi mq mn l mo mp">            if proba:<br/>                return np.concatenate([self.y_hat_[:, np.newaxis], self.proba_[:, np.newaxis]], axis=1)<br/>            else:<br/>                return self.y_hat_</span></pre><p id="8365" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">让我们来看看实际使用的方法:</p><pre class="ku kv kw kx fd mh mi mj mk aw ml bi"><span id="b3b2" class="jb jc hi mi b fi mm mn l mo mp">print(lr.predict(X_test, proba=True))</span></pre><p id="c90e" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">输出:</p><p id="f130" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">[[0.00000 1.00000 0.26618 0.00000]<br/>【2.00000 0.00000 0.35582 0.42736】<br/>【1.00000 0.00000 0.33544 0.01199】<br/>…<br/>【0.00000 0.99901 0.20]</p><p id="ae8c" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">我们看到了决策过程是什么样子的，但是这样一个模型会如何执行呢？</p><pre class="ku kv kw kx fd mh mi mj mk aw ml bi"><span id="e70e" class="jb jc hi mi b fi mm mn l mo mp">from sklearn.metrics import accuracy_score</span><span id="8fe1" class="jb jc hi mi b fi mq mn l mo mp">print(accuracy_score(lr.predict(X_test), y_test))</span></pre><p id="7249" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">输出:</p><p id="8519" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi">0.8947368421052632</p><p id="1b58" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">嗯，它确实做得很好，但是我们能做得更好吗？我不打算在本文的其余部分讨论特性选择，因为这个主题超出了它的范围，这就是为什么我们简单地插入我们所有的数据。</p><pre class="ku kv kw kx fd mh mi mj mk aw ml bi"><span id="113d" class="jb jc hi mi b fi mm mn l mo mp">X, y = data['data'], data['target']</span><span id="a71d" class="jb jc hi mi b fi mq mn l mo mp">X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)</span><span id="2e64" class="jb jc hi mi b fi mq mn l mo mp">lr.fit(X_train, y_train, normalize=True)<br/>print(accuracy_score(lr.predict(X_test), y_test))</span></pre><p id="f2fb" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">输出:</p><p id="143d" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi">0.9473684210526315</p><p id="c4cd" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">让我们将我们的结果与scikit-learn实现的结果进行比较:</p><pre class="ku kv kw kx fd mh mi mj mk aw ml bi"><span id="534f" class="jb jc hi mi b fi mm mn l mo mp">from sklearn.linear_model import LogisticRegression as LinearRegression_</span><span id="35f2" class="jb jc hi mi b fi mq mn l mo mp">lr_sklearn = LogisticRegression_(penalty='none', tol=1e-5, random_state=42, max_iter=100000) # Same hyperparams<br/>lr_sklearn.fit(X_train, y_train)</span><span id="1044" class="jb jc hi mi b fi mq mn l mo mp">print(accuracy_score(lr_sklearn.predict(X_test), y_test))</span></pre><p id="50a0" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">输出:</p><p id="3563" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi">0.8947368421052632</p><p id="9a9e" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">不打算吹嘘这个小的优势，因为它最有可能归因于一个场合。</p><p id="9754" class="pw-post-body-paragraph jz ka hi kb b kc ky ke kf kg kz ki kj jm la kl km jq lb ko kp ju lc kr ks kt hb bi translated">此外，如果我们消除“噪声”特征，我认为甚至有可能做得比94.7%更好。</p></div><div class="ab cl iu iv gp iw" role="separator"><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz"/></div><div class="hb hc hd he hf"><h2 id="8ed6" class="jb jc hi bd jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy bi translated">结论</h2><p id="7d32" class="pw-post-body-paragraph jz ka hi kb b kc kd ke kf kg kh ki kj jm kk kl km jq kn ko kp ju kq kr ks kt hb bi translated">在这篇文章中，我试图既简洁又全面。我试图尽可能清晰地传达材料，并特别关注我认为在其他资料中被肤浅或过于复杂地介绍过的东西，我不得不仔细阅读这些资料。我希望你喜欢这个指南，如果你有任何问题没有回答，请联系我，我会在我的专业知识允许的范围内回答他们。</p></div></div>    
</body>
</html>
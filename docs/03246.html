<html>
<head>
<title>Entropy and Gini Index In Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树中的熵和基尼指数</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/entropy-and-gini-index-in-decision-trees-cb99ba5d7dcc?source=collection_archive---------9-----------------------#2021-06-04">https://medium.com/geekculture/entropy-and-gini-index-in-decision-trees-cb99ba5d7dcc?source=collection_archive---------9-----------------------#2021-06-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/981cfe33ecfd58aca4995c30da43361a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rtjC1sdPhgWWkROAcx5l8A.png"/></div></div></figure><p id="5dd9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">机器学习中的决策树显示了模型用来将数据集分解为越来越小的数据子集，最终产生预测的逐步过程。决策树在监督学习下进行分类，可用于分类和回归问题。与其他监督学习模型一样，预测是基于一组特征变量和一个预定阈值进行的，当涉及决策树时，可以使用多个阈值，但是在本文中，我们将重点关注<strong class="is hj">熵</strong>和<strong class="is hj">基尼指数。</strong></p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><p id="566a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">熵</strong></p><p id="f051" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如上所述，熵是一种用于将决策树划分为更小子集的方法，通过划分树，它充当树节点的阈值。熵是对一组数据的杂质的度量，或者我们可以认为这是对一个群体的无序或不确定性的度量。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jv"><img src="../Images/4e841e404e7b4186d9dd7bd66b82032e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a8okAODlAOHgt01-G7vY4w.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">The formula used to determine entropy</figcaption></figure><p id="afdc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">直觉上，熵是某个值出现的概率乘以该概率以2为底的对数的总和。由于概率落在0-1的范围内，所以总和值将总是负的，因此需要乘以-1。</p><p id="8de0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们用一个<strong class="is hj">例子</strong>来更好地理解这一点:假设我们有来自一家家具店的关于某些产品及其销售情况的数据</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ke"><img src="../Images/cfcf67f271984e5ef291bd0d829b507b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9zJrKF9ylZPz_V8b8K8sfQ.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">Furniture Store Data</figcaption></figure><p id="87d2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">求状态变量的熵:让我们给状态变量分配虚拟变量——销售(1)和店内(0)。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kf"><img src="../Images/7d7682fc1459bcc65c9cb05f84b4ecc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8PMBtyPPs3GjcC-rGl41Wg.png"/></div></div></figure><p id="bb01" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如对状态特征的预期，我们具有高熵，这意味着高水平的不确定性。如果要增加售出商品的数量，并将店内状态商品的数量减少到1，那么我们就降低了不确定性水平，从而提高了纯度水平。</p><p id="8759" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当一组数据中的独特值被相等或几乎相等地划分时，该组数据的杂质达到最高水平。这可以想象如下:</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es kg"><img src="../Images/441e5b6c42d699898f6fb789224f2474.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*T67ZM6FMEf7QoJUwDb8-aw.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx">Relationship of Entropy and Probability</figcaption></figure><p id="2b21" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果一组数据具有均匀分布，则所有值将具有相同的出现概率，因此，对于均匀分布中的每个值，熵总是最大化。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kh"><img src="../Images/1ecfd533322dc7543f02cd072821a568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EDNkFFjjptUbtxh5U8pg4g.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx">The entropy of a Uniform Random Variable</figcaption></figure></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><p id="7d23" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">基尼指数</strong></p><p id="d853" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">基尼系数与熵略有不同，尽管它们的功能相同。基尼指数是一个变量被随机选择后不能被正确分类的概率。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es ki"><img src="../Images/83fbe7ed1bc86fc38965d9c26c8396b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*gG1-MjzNu68q6TgUPtC1EA.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx">The formula for Gini Index Calculation</figcaption></figure><p id="363d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">基尼指数倾向于偏好较大的分区，因此可能是计算密集型的。</p><p id="1ea1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一般来说，基尼系数和熵在相同数据上的表现差别不大，因此使用什么由数据分析师/科学家决定。</p><p id="b9f0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在以后的文章中，我希望展示使用python库实现上述内容。</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><p id="b939" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">图来源:<a class="ae kj" href="https://s3-ap-southeast-1.amazonaws.com/he-public-data/Entropydfcb350.jpg" rel="noopener ugc nofollow" target="_blank">https://S3-AP-southeast-1 . Amazon AWS . com/he-public-data/entropydfcb 350 . jpg</a>，<a class="ae kj" href="https://studyonline.unsw.edu.au/sites/default/files/styles/blogfeature_large/public/field/image/Decision%20Tree_1.jpg?itok=8HOGgFfc" rel="noopener ugc nofollow" target="_blank">https://study online . UNSW . edu . au/sites/default/files/styles/blog feature _ large/public/field/image/Decision % 20 tree _ 1 . jpg？itok=8HOGgFfc </a></p></div></div>    
</body>
</html>
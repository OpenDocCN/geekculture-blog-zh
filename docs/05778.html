<html>
<head>
<title>SUPPORT VECTOR MACHINES</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/support-vector-machines-bc39b26816ea?source=collection_archive---------30-----------------------#2021-07-29">https://medium.com/geekculture/support-vector-machines-bc39b26816ea?source=collection_archive---------30-----------------------#2021-07-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/25a1deeb6fe1402d70ae4be4f90b6d96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w6_ATgFUCBDY6qdx"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx">Photo by <a class="ae hv" href="https://unsplash.com/@pietrozj?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Pietro Jeng</a> on <a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="2eec" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">SVM是一种监督机器学习算法，可用于分类和回归问题。这种算法对线性和非线性可分离数据都能很好地工作，并且非常流行。在这篇博客中，我将集中讨论SVM的分类。</p><h2 id="9e1c" class="jt ju hy bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">SVM的工作</h2><p id="d817" class="pw-post-body-paragraph iv iw hy ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated"><strong class="ix hz"> <em class="kt">支持向量</em> </strong> <br/>在继续工作之前，我们先来了解一下什么是支持向量。</p><figure class="kv kw kx ky fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ku"><img src="../Images/62487ed391341404d53fda1952fdc44e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nrozIO4azcWG-w9yQSwRqw.jpeg"/></div></div></figure><p id="150b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在左侧，您可以看到由超平面π划分的两类点(粉色和蓝色或+ve和-ve)。另外还有两个超平面π+和π-与π平行。<br/>注意接触π+和π-的点。这些点称为支持向量。<br/>同样，这些点是离我们的划分超平面π最近的点。</p><blockquote class="kz"><p id="543f" class="la lb hy bd lc ld le lf lg lh li js dx translated"><em class="lj">π+和π-经过的点，或者最接近π(划分超平面)的点称为支持向量。</em></p></blockquote><blockquote class="lk ll lm"><p id="521e" class="iv iw kt ix b iy ln ja jb jc lo je jf lp lq ji jj lr ls jm jn lt lu jq jr js hb bi translated"><strong class="ix hz">SVM的意识形态。SVM的核心思想是找到一个尽可能广泛地分开两个类的超平面。</strong></p></blockquote><p id="1e29" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">看下面的图片—</p><figure class="kv kw kx ky fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lv"><img src="../Images/2ab9526e2098142b52de6ec3a3f20fdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aYoTk7yVQ0Bgo7em0l09xA.jpeg"/></div></div></figure><p id="c367" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你被要求画一个超平面来划分这两类，你可以得到许多超平面。对吗？我们取两个这样的超平面π1和π2。</p><p id="1ce9" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在π1中，我们可以看到它非常接近这两个类的点，而在π2中，这两个类都在合理的距离内。</p><p id="5caf" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，SVM试图找到这样的超平面，它最大化点之间的边缘距离，或者尽可能广泛地分隔类别。<br/> <em class="kt">边际距离是π+和π-之间的距离。<br/> </em>随着我们的边距增加，泛化精度增加。</p><h2 id="8df9" class="jt ju hy bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">逻辑回归和支持向量机</h2><p id="dbae" class="pw-post-body-paragraph iv iw hy ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated"><a class="ae hv" rel="noopener" href="/geekculture/logistic-regression-2111fa695cdd">逻辑回归</a>和SVM的基本区别在于，SVM试图找到边际最大化超平面，而逻辑回归没有这样做。上图中，LR也会给出π1作为划分超平面。但是SVM给了我们最合适的超平面，那就是π2。</p><p id="8dd0" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这就是为什么SVM不会受到异常值的影响，因为超平面的选择只取决于支持向量，不像逻辑回归。</p><h1 id="d36e" class="lw ju hy bd jv lx ly lz jz ma mb mc kd md me mf kg mg mh mi kj mj mk ml km mm bi translated"><strong class="ak"> <em class="lj">数学推导</em> </strong></h1><figure class="kv kw kx ky fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mn"><img src="../Images/2fd0c7aa683ca4a2943d408b79f33961.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gbe-QjrSmZxFR8r87PexCw.jpeg"/></div></div></figure><p id="2ce8" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">设<br/>π:w ' x+b<br/>π+:w ' x+b = 1<br/>π-:wx+b =-1<br/>w—垂直于所有超平面。</p><p id="1222" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">设d =π+和π-之间的距离。<br/> d = 2 / || w||，| | w | |<strong class="ix hz">≦</strong>1<br/>我们的目标是最大化d. <br/>因此，<br/> <em class="kt"> (w*，b*) = argmax (2/||w||) </em></p><p id="de22" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是，只有当我们的数据是线性可分的，即π-和π+之间没有数据点时，上述等式才有效。</p><p id="ca04" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此上面的等式变成了，</p><blockquote class="kz"><p id="9e5b" class="la lb hy bd lc ld le lf lg lh li js dx translated"><em class="lj"> (w*，b*) = argmax (2/||w||)，这样，yi(w'xi + b) ≥ 1对于全xi </em></p></blockquote><p id="0294" class="pw-post-body-paragraph iv iw hy ix b iy ln ja jb jc lo je jf jg lq ji jj jk ls jm jn jo lu jq jr js hb bi translated"><em class="kt">对于支持向量，yi(w'xi +b) = 1 </em> <br/>上述条件又称为<strong class="ix hz">硬裕度SVM。</strong></p><p id="4d54" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz"> <em class="kt">非线性可分数据情况下的SVM</em></strong></p><figure class="kv kw kx ky fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mo"><img src="../Images/e01e33acef96a894363b8a4560323b15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5GZqjwv30waEJfgTs570w.jpeg"/></div></div></figure><p id="296f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">考虑左边给出的数据点的分布类型，注意这里的错误分类点。<br/>对于一个误分类点，设方程为:<br/> yi(w'xi + b) = k <br/>其中k:该点到π的有符号距离。</p><p id="4a1e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们把方程重新写成<br/> yi(w'xi + b) = 1-Z <br/>这里，xi到其正确超平面(π+或π-)的距离。</p><blockquote class="lk ll lm"><p id="6449" class="iv iw kt ix b iy iz ja jb jc jd je jf lp jh ji jj lr jl jm jn lt jp jq jr js hb bi translated">可以得出结论，<br/>如果Z = 0，yi(w'xi+b) ≥ 1，这意味着正确地分类了wrt π+和π-。<br/> Z &gt; 0，分类错误，等于错误方向上离正确超平面(π+和π-)的距离。</p></blockquote><blockquote class="kz"><p id="4a26" class="la lb hy bd lc ld mp mq mr ms mt js dx translated">所以，最终方程为<br/> <strong class="ak"> (w*，b *)arg min(| | w | |/2)+c/nσZi，使得yi(w'xi+b)≥ 1-Zi且Zi ≥0 </strong></p></blockquote><p id="b001" class="pw-post-body-paragraph iv iw hy ix b iy ln ja jb jc lo je jf jg lq ji jj jk ls jm jn jo lu jq jr js hb bi translated">上述类型的SVM被称为<strong class="ix hz">软边际SVM。<br/> </strong>这个方程式也叫<strong class="ix hz">SVM的原始形式。</strong></p><h2 id="a2ac" class="jt ju hy bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">SVM的双重形式</h2><figure class="kv kw kx ky fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mu"><img src="../Images/9d654cf4378b025f0541d2345f2e0e23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fdjZfnK4zP6KQEX8OlD-Xg.jpeg"/></div></div></figure><p id="48bf" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面是被称为SVM对偶形式的方程式。</p><p id="73fe" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们来研究这个方程的细节</p><ul class=""><li id="6eb8" class="mv mw hy ix b iy iz jc jd jg mx jk my jo mz js na nb nc nd bi translated"><em class="kt"> αi &gt; 0为支持向量，αi = 0为非支持向量。</em></li><li id="e5c1" class="mv mw hy ix b iy ne jc nf jg ng jk nh jo ni js na nb nc nd bi translated">在上面的等式中，xi的xj可以用相似性矩阵(xi，xj)代替，该矩阵最终将被K(xi，xj)代替，K代表“核心技巧”。</li></ul><h2 id="f8a6" class="jt ju hy bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">内核技巧</h2><p id="4203" class="pw-post-body-paragraph iv iw hy ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">核技巧基本上是一种通过将数据点转换到不同的空间来计算它们之间的相似性的方法。内核技巧避免了将数据点显式转换到更高维度的需要。<br/>这是一个内核技巧，因为它支持向量机能够通过使用正确的内核来处理非线性数据。这种类型支持向量机也称为核支持向量机。让我们看看两个这样的内核:-</p><ol class=""><li id="5d77" class="mv mw hy ix b iy iz jc jd jg mx jk my jo mz js nj nb nc nd bi translated"><strong class="ix hz">多项式核</strong> <br/>给定两个数据点x1和x2，一个多项式核可以写成:- <br/> <strong class="ix hz"> <em class="kt"> K(x1，x2)=(x1’x2+c)^d</em></strong><em class="kt"/><br/>这里d一般是数据点将进行内部变换的维数。例如，如果d = 2，则称为二次核，依此类推。</li><li id="3cc7" class="mv mw hy ix b iy ne jc nf jg ng jk nh jo ni js nj nb nc nd bi translated"><strong class="ix hz"> RBF核</strong> <br/>给定两个数据点x1和x2，一个RBF核可以写成:- <br/> <strong class="ix hz"> <em class="kt"> K(x1，x2)= exp(-||x1-x2||/2σ)</em></strong><br/>上式中，| | x1-x2 | |实际上是两点之间的距离。<br/>随着||x1-x2||的增加，即两个数据点之间的距离增加，相似性降低，因此K(x1，x2)呈指数下降。</li></ol><h2 id="38fe" class="jt ju hy bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">训练和运行时间复杂性</h2><p id="7e25" class="pw-post-body-paragraph iv iw hy ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated"><strong class="ix hz">训练时间</strong> :- <br/> O(n)为内核SVM <br/> O(nd)，if d &lt; n(更优化)<br/> d =维数，n=数据点数。</p><p id="b270" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这就是当n较大时，避免使用支持向量机的原因。</p><p id="6771" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">运行时间:- </strong> <br/> O(kd)，其中k =支持向量个数，d =维数。<br/>k的取值范围为1≤k≤n。</p><h2 id="02e7" class="jt ju hy bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">SVM的实施</h2><p id="922f" class="pw-post-body-paragraph iv iw hy ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">我将再次使用来自Kaggle的泰坦尼克号数据集，任务是创建一个模型来预测哪些乘客在泰坦尼克号沉船中幸存。数据集可以从<a class="ae hv" href="https://www.kaggle.com/c/titanic/overview" rel="noopener ugc nofollow" target="_blank">这里</a>下载。<em class="kt"> <br/>这也是一个二进制分类问题，其中目标“幸存”对于没有在灾难中幸存的人是0，对于幸存的人是1。</em></p><p id="b974" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们已经将数据预处理成x_train和y_train，并从sklearn导入SVM。</p><figure class="kv kw kx ky fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nk"><img src="../Images/3e0dc64fc03676ede576c6d760df9252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R71TDZjkOTHwbcxbD-vH8g.png"/></div></div></figure><ul class=""><li id="aec0" class="mv mw hy ix b iy iz jc jd jg mx jk my jo mz js na nb nc nd bi translated">这里的C参数等于2。这是SVM原始形式的C。如果C增加，出错的可能性降低，从而导致过度拟合。<br/>如果C减少，会导致欠拟合。<br/>人们可以通过使用超参数调谐技术找到最佳C。</li><li id="b451" class="mv mw hy ix b iy ne jc nf jg ng jk nh jo ni js na nb nc nd bi translated">这里使用的内核是线性的</li></ul><p id="0741" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有关数据的详细实现和理解，请参考此处的<a class="ae hv" href="https://github.com/guptaa98/Kaggle-Notebooks/blob/master/Titanic%20kaggle.ipynb" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="bb39" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">编码快乐！:)</p></div></div>    
</body>
</html>
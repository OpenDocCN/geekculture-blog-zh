<html>
<head>
<title>Policy Parameterization for a Continuous Action Space</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">连续动作空间的策略参数化</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/policy-based-methods-for-a-continuous-action-space-7b5ecffac43a?source=collection_archive---------4-----------------------#2021-08-09">https://medium.com/geekculture/policy-based-methods-for-a-continuous-action-space-7b5ecffac43a?source=collection_archive---------4-----------------------#2021-08-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f6c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我实现的过去几个策略梯度和行动者批评算法中，我一直使用经典的控制环境CartPole作为我的实验基准。CartPole环境有一个简单的离散动作空间:向左或向右推小车。在这篇文章中，我将探索另一种方法来参数化我们的策略函数，以处理具有连续动作空间的环境。</p><h1 id="90f0" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">介绍</h1><p id="2b08" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">基于策略的方法的优点之一是能够处理大的甚至连续的动作空间。当我们使用Q学习方法时，代理学习一个状态中每个动作的Q值，并通过选择给出最高Q值的动作来建立一个最优策略。虽然这可能适用于某些环境，但这仅适用于离散的动作空间，因为在连续值的范围内有无限数量的动作。在基于策略的方法中，代理直接学习策略，并从动作空间的概率分布中选择动作。对于连续的动作空间，我们可以学习概率分布的参数，并从中选择一个值。首先，让我们回顾一下如何为离散操作参数化策略:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kg"><img src="../Images/fce8e82516b3a10e8d08ae922980b91a.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*mJOsjM-rZMG5-5NewI64OQ.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx">Taken from Sutton&amp;Barto 2017</figcaption></figure><p id="d80e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该函数接受一个离散动作，例如“向左推”或“向右推”，一个状态<em class="ks"> s </em>，以及一组参数θ。等式的右侧是<em class="ks"> h(s，a，θ) </em>函数，这是一个偏好函数，它为策略偏好的操作赋予更高的值。我们在这里使用指数softmax分布，因此偏好值被转换为[0，1]之间的概率值，并且一个状态中的所有动作的总和将为1。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kt"><img src="../Images/221b985dc08d57afc55601d04d32400c.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*x7RAgKKyl3kQMJ3YXCWviA.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx">Taken from Sutton&amp;Barto 2017</figcaption></figure><p id="77a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的偏好函数只是一组简单的权重θ乘以特征向量<em class="ks"> x(s，a)。</em>在实现过程中，<em class="ks"> </em>这组权重可以通过神经网络来学习，因此偏好函数可以比简单的线性函数更复杂。</p><p id="e680" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们看看如何为连续操作参数化策略。策略函数基于高斯分布的概率密度函数:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ku"><img src="../Images/6d7d4ac62d548eaf0633776a35a0654b.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*K3PPtvUi-cU7p0tFVZbrsw.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx">Taken from Sutton&amp;Barto 2017</figcaption></figure><p id="920c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，这个分布的参数是<em class="ks"> μ </em>，均值，<em class="ks"> σ </em>，标准差。请注意，<em class="ks"> p(x) </em>并没有给出<em class="ks"> x </em>的概率，而是给出了<em class="ks"> x的概率密度。</em>与离散动作的softmax分布不同，高斯密度分布给出了一个真实值，而不是一个概率。要找到一系列<em class="ks"> x </em>值的概率，您需要找到这些边界之间的定积分。使用这个概率密度函数，我们可以将我们的策略函数定义为:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kv"><img src="../Images/35eccdded0f4926e349bd0e56ce4b745.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*trg51m3QmgIAS4nBXHHBZg.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx">Taken from Sutton&amp;Barto 2017</figcaption></figure><p id="3b6e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，我们使用θ参数来近似μ和σ的值，以获得概率密度分布。μ和σ可以近似计算如下:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kw"><img src="../Images/ef87fcc83dd5f54b4cea12c832df749f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*8zyjrCuxgCJQv2APv3CrrA.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx">Taken from Sutton&amp;Barto 2017</figcaption></figure><p id="0596" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以通过线性函数来近似均值，因此转置权重和特征向量的简单矩阵乘法。对于标准差，值应该是正的，所以我们对线性函数求幂。就像离散动作的参数化一样，我们只需要使用一个网络，因为μ和σ都由相同的线性函数近似。</p><p id="e63f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">原来如此！任何使用连续动作空间的算法的底层逻辑都没有太大变化，只是策略函数的参数化方式有所改变。</p><h1 id="a7cc" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">履行</h1><p id="0bd8" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">为了实现，我将使用神经网络来参数化策略函数。然后，我将使用普通的策略梯度方法来测试我的实现。看着OpenAI健身房中的经典控制环境，我选择解决连续山地车问题。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kx"><img src="../Images/0e594eba045166f935448f861e78eff8.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*JkfAXyy63Tuw8bOO.gif"/></div><figcaption class="ko kp et er es kq kr bd b be z dx">Taken from Paperspace Blog</figcaption></figure><p id="a3cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">登山车环境的前提很简单。在每一步，代理人施加与[-1，1]之间的一个值成比例的力量，目标是把手推车开到旗子那里。当手推车到达旗子时，代理人会得到奖励，而消耗的能量会受到惩罚。然而，实现目标有点复杂。由于动力受到限制，这个想法是让手推车来回摇摆，以产生足够的动力爬坡。代理人还需要将用于获得更高分数的能量最小化。当得分达到90+时，环境被视为已解决。</p><p id="e4e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是我在实现中使用的超参数:</p><ul class=""><li id="56ff" class="ky kz hi ih b ii ij im in iq la iu lb iy lc jc ld le lf lg bi translated">γ(折扣系数):0.99</li><li id="0711" class="ky kz hi ih b ii lh im li iq lj iu lk iy ll jc ld le lf lg bi translated">α(学习率):0.001</li><li id="64e1" class="ky kz hi ih b ii lh im li iq lj iu lk iy ll jc ld le lf lg bi translated">剧集数量(要播放的剧集):1000</li><li id="27ae" class="ky kz hi ih b ii lh im li iq lj iu lk iy ll jc ld le lf lg bi translated">最大步数(每集最大步数):500</li></ul><p id="9771" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是政策网络:</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="55bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里有两个输出，μ和σ。下面是我用来选择动作的一段代码:</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="6b0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，我仍然计算分布的对数概率，因为策略的更新方程仍然没有改变，我们只是改变了参数化策略函数的方式。</p><p id="bb68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是该代理在250集强化训练中的训练历史:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lo"><img src="../Images/2f1bc9258caaa91caf6dba8ca7eddbdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*fhXXEJEM8KDxrlgzdBohmw.png"/></div></figure><p id="abd9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我第一次看到训练历史时，我对结果感到惊讶。代理人的分数迅速增加，进展在大约50集时持平。在再进行10次实验后，同样的历史模式出现了，尽管有时有一集的得分超过了0。你可能得出的结论是，代理人使用了太多的能量来产生爬山所需的动力，导致奖励抵消了能量的使用。然而，看着代理人玩着学会的策略，手推车似乎只是在山脚下轻微地来回晃动。发生了什么事？</p><p id="027a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的问题在于环境的稀疏回报。MountainCar环境根据每一步的能量使用情况对代理进行惩罚，并且只在到达旗帜后奖励代理。因此，在奖励随机到达旗帜之前，代理不会知道奖励。由于代理在每一步都受到惩罚，它的目标自然会是最小化能量消耗，从而产生一个手推车静止不动的策略。</p><p id="8f8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个问题的一个潜在解决方案可能是为环境定制奖励。然而，在尝试使用位置、速度、机械能等等之后，我仍然得到相同的结果。其原因是由于我们算法的性质。</p><p id="fe87" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们基于策略的方法依赖于直接改进策略，并从我们学习的策略中采样动作。虽然非策略方法可以使用探索方法，如epsilon-greedy，但我们的代理只有在采样行为偏离均值很远或标准偏差很大时才能进行探索。当代理试图在没有达到目标的情况下更新策略时，策略将次优地收敛。</p><p id="4d6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看起来它回到了良好的老卡特波尔环境。我将使用<a class="ae lp" href="https://gist.github.com/iandanforth" rel="noopener ugc nofollow" target="_blank">伊恩丹福斯</a>的<a class="ae lp" href="https://gist.github.com/iandanforth/e3ffb67cf3623153e968f2afdfb01dc8" rel="noopener ugc nofollow" target="_blank">连续翻筋斗</a>实现，OpenAI的解决条件是最近100集的平均195分。动作将被剪辑为[-1，1]。以下是强化培训的历史记录:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lq"><img src="../Images/3cb0282e670d243f30d8a20842e22b6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*c1h68OmZ_p4zp20_Ll_ikw.png"/></div></figure><p id="658c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的经纪人在50次试玩中取得了300.72的平均分。该代理还能够在5次试验中平均384.8次地解决侧翻问题。与使用强化的不连续动作空间的弹球的79.6分相比，这个结果要好得多。代理能够解决1000集以下的环境。这一结果是意料之中的，因为与施加+1或-1的力相比，使用连续动作将提供对推车的精确控制。</p><p id="06ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">需要注意的一点是不稳定性和频繁的灾难性遗忘。因为我们是在没有基线的情况下进行强化，所以预计会有很大的差异。然而，我们可以观察到，从第150集左右开始，代理“忘记”相当频繁。我推测这是因为我们的代理学习得太快，没有时间收敛到一个稳定的策略。除了我们的行动可能千差万别之外，几个异常的行动会导致策略崩溃。</p><p id="749e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是动作分布参数的训练历史:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es lr"><img src="../Images/b9b60158f9ad6e560f5a413998bbb417.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*crUwSmgNgF0O0fjiOfX4Kg.png"/></div></div></figure><p id="b182" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该方法的训练历史是可以预期的。当代理试图平衡极点时，策略学习在消极和积极动作之间交替均值。我们还可以观察到，除了异常值之外，大多数负均值都在-1以下，而正均值在1到2之间。因为我们将我们的动作限制在[-1，1]之间，所以积极的动作都是+1。负平均值出现的频率比正平均值高20%,以补偿较大的正平均值。我假设平均值的大小随着时间的推移而增加，因为动作被剪切，所以平均值可以继续上升而不受惩罚。</p><p id="cfaa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">标准差的历史有点出乎意料，可以解释训练分数的历史。从20，000步开始的高st. dev可以看出，我们的策略对其行动不太有信心。这大约与代理开始灾难性遗忘的时间相同，并且存在高得分差异。一旦代理解决了环境问题并收敛到最大值，st. dev再次回落。</p><p id="2ad4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来看看未剪辑动作的分数和动作分布参数的训练历史:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lq"><img src="../Images/44f43c2c3ed6a0fceded8affacb93ab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*3mbQLoaDgO_g8hfY99GnyQ.png"/></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es lw"><img src="../Images/6c618a9c363094635d9bc83681b1f0d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kawhx_1IEPb8fQsbOnIRQA.png"/></div></div></figure><p id="33d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与修剪动作不同，平均值的大小不会随着时间的推移而增加，因为对于未修剪的动作，施加额外的力会受到惩罚。标准差历史也是有意义的，因为当学习的策略向解决方案收敛时，它对其动作更有信心。</p><h1 id="d957" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">最终想法</strong></h1><p id="8f07" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">总的来说，这是一个有趣而简短的实验，不需要太长时间。我只想尝试实现一种不同的策略参数化，所以我修改了增强算法的部分代码。在未来的帖子中，我将继续讨论更复杂的处理连续动作空间的算法，如DDPG、PPO、SAC，希望这些算法能够解决更复杂的环境。</p><p id="3aa1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代码:<a class="ae lp" href="https://github.com/chengxi600/RLStuff/blob/master/Policy%20Gradients/REINFORCE-Continuous.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/cheng i600/rl stuff/blob/master/Policy % 20 gradients/REINFORCE-continuous . ipynb</a></p><p id="d2b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><ul class=""><li id="3fc3" class="ky kz hi ih b ii ij im in iq la iu lb iy lc jc ld le lf lg bi translated"><a class="ae lp" href="http://incompleteideas.net/book/bookdraft2017nov5.pdf" rel="noopener ugc nofollow" target="_blank">强化学习:导论(萨顿&amp;巴尔托2017) </a></li></ul></div></div>    
</body>
</html>
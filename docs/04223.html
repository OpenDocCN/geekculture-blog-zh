<html>
<head>
<title>Turndown Turnover with Spark ML: A Sparkify Story</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark ML的拒绝营业额:Sparkify的故事</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/turndown-turnover-with-spark-ml-a-sparkify-story-76d3419a265e?source=collection_archive---------78-----------------------#2021-06-22">https://medium.com/geekculture/turndown-turnover-with-spark-ml-a-sparkify-story-76d3419a265e?source=collection_archive---------78-----------------------#2021-06-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/ea8fee39635f70430bd65e5dfb2304d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*QwqGGVx-Js32wFK3foxfxg.jpeg"/></div><figcaption class="im in et er es io ip bd b be z dx">Image Credit: <a class="ae iq" href="https://www.artpal.com/richardwaldron?i=212438-6" rel="noopener ugc nofollow" target="_blank">Irish Street Musician — Richard Waldron</a></figcaption></figure><h1 id="2cef" class="ir is hi bd it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">介绍</h1><p id="91ec" class="pw-post-body-paragraph jp jq hi jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hb bi translated"><strong class="jr hj">概述</strong></p><p id="14e3" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">每个人都喜欢好的流媒体服务。能够随时随地毫不费力地享受几乎任何你想要的东西是一种诱人的感觉。Spotify、Pandora或网飞等流媒体企业经常按月向客户提供服务，这对用户和企业都很方便。对于企业来说，这让他们有一个可靠的，稳定的收入来源。对于用户来说，它提供了较低的前期成本和安全性，因为他们知道如果他们对服务不满意，可以随时取消。</p><p id="9dc9" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">然而，这种自由是有代价的。流媒体业务通常会经历用户的高流动率(称为流失)。能够预测哪些客户可能会流失，以及那些取消的客户有哪些共同的特征，这对企业来说是非常有价值的。</p><p id="c507" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated"><strong class="jr hj">问题陈述</strong></p><p id="acff" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">虚构的音乐流媒体服务“Sparkify”想知道他们的哪些用户可能会流失。这个项目试图预测哪些客户会流失，并了解原因。来自Sparkify的用户数据将用于查看是否可以使用分类模型来预测用户流失(针对他们虚构的业务)。将通过检查缺失值并将数据聚合为每行一个用户的格式来清理数据。然后，将使用两种不同类型的分类模型对数据进行建模:</p><ol class=""><li id="b76c" class="ks kt hi jr b js kn jw ko ka ku ke kv ki kw km kx ky kz la bi translated">逻辑回归</li><li id="a4dd" class="ks kt hi jr b js lb jw lc ka ld ke le ki lf km kx ky kz la bi translated">梯度增强树</li></ol><p id="606d" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">数据将被分成训练和验证集(或测试集)。训练集将通过分层k-fold交叉验证进行交叉验证，并网格搜索最佳超参数，然后对测试集进行最终评估。</p><p id="d9a3" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">数据访问由Udacity公开，可以在<a class="ae iq" href="https://udacity-dsnd.s3.amazonaws.com/sparkify/sparkify_event_data.json" rel="noopener ugc nofollow" target="_blank">这个S3桶</a>中找到。由于数据量很大(~12GB)，PySpark(以及Spark)将用于数据的最终分析。为了在探索阶段使事情变得更容易，将只使用数据的一个样本(如果你在笔记本中跟随，它是' mini_sparkify_event_data.json '，可以在这里找到<a class="ae iq" href="https://github.com/tomwerner5/sparkify/blob/main/Sparkify.ipynb" rel="noopener ugc nofollow" target="_blank"/>)。</p><p id="9f95" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated"><strong class="jr hj">指标</strong></p><p id="8483" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">在评估这些模型时将使用两个指标，即F1指标和准确度得分。F1指标将用于在超参数网格搜索中对模型性能进行排名，因为它提供了一个比准确性更平衡的性能视图。有关分类指标的复习，请参见此处的<a class="ae iq" href="https://en.wikipedia.org/wiki/Confusion_matrix" rel="noopener ugc nofollow" target="_blank"/>。</p></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="df2d" class="ir is hi bd it iu ln iw ix iy lo ja jb jc lp je jf jg lq ji jj jk lr jm jn jo bi translated">第一部分:早期分析和方法</h1><p id="dfc1" class="pw-post-body-paragraph jp jq hi jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hb bi translated"><strong class="jr hj">数据探索和预处理</strong></p><p id="9e72" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">加载数据后，我们发现有18列(如表2所示)。在迷你数据集中，有大约100，000行，而完整数据集有大约26，000，000百万行。表1提供了一个数据示例。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es ls"><img src="../Images/6cd8d4018ef9f6684d4987ce9e52fa26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KvT2hV_FkbVGdaQKT2-eqQ.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx">Table 1: Sample of the raw Sparkify data</figcaption></figure><p id="c5d4" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">加载数据后，采取的第一步是检查任何空值。由于每个用户的行为稍有不同，我们预计对于给定的用户会缺少一些字段。我们最感兴趣的是是否缺少sessionId或userId值。如果userId为空，那么我们无法将该数据与用户匹配，因此我们无法使用它来预测用户流失。如果用户缺少sessionId值，那么如果我们做一些假设，我们可能仍然能够使用该数据，但否则我们可能也应该删除它。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mb"><img src="../Images/7b0b36323ba3b1851d0a189fc6689cf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6_29Thz5IXACmM6m9X_XYg.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx">Table 2: Missing Values in the Mini Sparkify Dataset</figcaption></figure><p id="0877" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">根据表2，看起来有几列缺少值。这似乎也有一点规律。例如，userId、userAgent、gender和location都有相同数量的缺失值，这可能意味着这些缺失值都与缺失的userId相关联。</p><p id="66a9" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">在删除缺少userId的行之后，剩余的缺少值如表3所示:</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mc"><img src="../Images/18cbba67074df7b3ec8e680a9c0ca60e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WOUmvyA8s3cuRloVdDodMA.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx">Table 3: Missing Values After Removing Blank `userId`</figcaption></figure><p id="9a8f" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">基于剩余的缺失值，看起来艺术家、长度和歌曲字段可能仅在用户听音乐时被填充，而不是在播放器空闲或用户浏览网站的其他部分时被填充。由于该信息可能是有用的数据，因此此时不会估算或删除缺失值。</p><p id="91bb" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">除了纠正丢失的值之外，还删除了艺术家、歌曲和位置列，因为它们有很多差异(即，由于歌曲和艺术家的数量很大，几乎每一行都有唯一的值)。我们可以使用NLP来解决艺术家/歌曲/位置问题，但正如我们稍后将看到的，我们可以在不执行任何NLP的情况下获得一个相当好的分类器。下面，我试图通过使用一些正则表达式来减少歌曲中的可变性，但最终，仍然有太多的唯一值，所以我放弃了它。然而，使用下面的方法，userAgent的值被压缩了很多。</p><pre class="lt lu lv lw fd md me mf mg aw mh bi"><span id="c5d6" class="mi is hi me b fi mj mk l ml mm"><em class="mn"># artist and userAgent have a lot of unique values. The following lines</em><br/><em class="mn"># try to reduce the number of characters for each.</em><br/><br/><em class="mn"># Filter rows with regex (find rows that match expression)</em><br/>user_log = user_log.withColumn('artist', regexp_replace('artist', r'[^0-9a-zA-Z]', ' '))<br/>user_log = user_log.withColumn('userAgent', regexp_replace('userAgent', r'[^a-zA-Z]', ' '))<br/>user_log = user_log.withColumn('userAgent', trim(lower(regexp_replace('userAgent', r'\s+', ' '))))<br/>user_log = user_log.withColumn('userAgent', regexp_replace('userAgent', r'\s[a-z]\s', ' '))</span></pre><p id="92ef" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">接下来，对分类变量进行了一次性编码，我认为这有助于探索数据(也有助于建模)。这也是一个定义真相标签的好时机，这样我们就可以探索那些没有取消的人和那些取消的人之间的差异。在本分析中，客户流失被定义为完全取消其服务的客户，并在<code class="du mo mp mq me b">page==”Cancellation Confirmation"</code>时出现。</p><p id="5735" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">一键编码的变量列在下面代码片段的开头(其中“user_log”是主要的Spark数据帧)。</p><pre class="lt lu lv lw fd md me mf mg aw mh bi"><span id="da60" class="mi is hi me b fi mj mk l ml mm"><em class="mn">## One hot encode column variables</em><br/><br/><em class="mn"># Create string index for each column to be encoded</em><br/>onehot_cols = ['gender', 'level', 'method', 'userAgent', 'page', 'status']<br/>indexers = [<br/>    StringIndexer(inputCol=column, outputCol=column+"_indexed")<br/>    <strong class="me hj">for</strong> column <strong class="me hj">in</strong> onehot_cols<br/>]<br/><br/><em class="mn"># Use the indexed columns to prepare the one-hot encoder</em><br/>encoders = [<br/>    OneHotEncoder(<br/>        inputCol=indexer.getOutputCol(),<br/>        outputCol=indexer.getOutputCol()+"_encoded",<br/>        dropLast=<strong class="me hj">False</strong><br/>    ) <br/>    <strong class="me hj">for</strong> indexer <strong class="me hj">in</strong> indexers<br/>]<br/><br/><em class="mn"># Generate a vector of encoded values using VectorAssembler for</em><br/><em class="mn"># each row</em><br/>assembler = [VectorAssembler(<br/>    inputCols=[encoder.getOutputCol() <strong class="me hj">for</strong> encoder <strong class="me hj">in</strong> encoders],<br/>    outputCol="onehot_features"<br/>)]<br/><br/><em class="mn"># Run the string index, encoder, and assembler as a Pipeline,</em><br/><em class="mn"># save the output back to the spark dataframe</em><br/>onehot_pipeline = Pipeline(stages=indexers + encoders + assembler)<br/>onehot_pipeline = onehot_pipeline.fit(user_log)<br/>user_log = onehot_pipeline.transform(user_log)</span></pre><p id="6fc5" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">让一次性编码在Spark中工作有点棘手，需要大量的谷歌搜索。Spark首先必须为被编码的分类变量中的每个唯一值创建一个字符串索引。然后，它将这些字符串索引传递给一键编码器，由它来完成繁重的工作。vector assembler帮助从one-hot编码器中提取转换后的值，并将这些值放入Spark数据帧中的一列(在我的例子中称为“onehot_features”)。在该列中，每一行都是已编码的每一列的编码值列表。虽然这对于建模来说很方便(我们将在后面看到)，但是我仍然有一些处理和探索要做，所以为了提取列值(和列名)并将它们恢复到原来的位置，我必须执行下面的代码。</p><pre class="lt lu lv lw fd md me mf mg aw mh bi"><span id="bbfe" class="mi is hi me b fi mj mk l ml mm"><em class="mn"># Generate the list of column names for convenience later (using</em><br/><em class="mn"># VectorAssembler, the column names are not saved to the dataframe</em><br/><em class="mn"># and need to be recovered)</em><br/><em class="mn">#</em><br/><em class="mn"># userAgent is renamed here because each value is a really long string</em><br/><em class="mn"># and this makes the column name more convenient</em><br/>expanded_oh_names = []<br/><strong class="me hj">for</strong> i, column <strong class="me hj">in</strong> enumerate(onehot_cols):<br/>    <strong class="me hj">for</strong> j, label <strong class="me hj">in</strong> enumerate(onehot_pipeline.stages[i].labels):<br/>        <strong class="me hj">if</strong> column == 'userAgent':<br/>            expanded_oh_names.append(column + '_' + str(j))<br/>        <strong class="me hj">else</strong>:<br/>            expanded_oh_names.append(column + '_' + label)<br/><br/><em class="mn"># Generate list of columns to keep in the dataframe. For most purposes,</em><br/><em class="mn"># this would be a list of the columns that were not encoded.</em><br/><em class="mn"># If any of the columns in the list are one-hot encoded, this list</em><br/><em class="mn"># will keep the original, non-encoded column in the dataframe</em><br/>no_hot_cols = [c <strong class="me hj">for</strong> c <strong class="me hj">in</strong> original_col_list <strong class="me hj">if</strong> c <strong class="me hj">not</strong> <strong class="me hj">in</strong> onehot_cols]<br/><em class="mn">#no_hot_cols = ["artist", "gender", "itemInSession", "length",</em><br/><em class="mn">#    "level", "location", "method", "page", "registration", "sessionId",</em><br/><em class="mn">#    "song", "ts", "userAgent", "userId"]</em><br/><br/><br/><em class="mn"># create a new, expanded column list.</em><br/>all_cols = no_hot_cols + expanded_oh_names<br/><br/><strong class="me hj">def</strong> extract(row, column_list):<br/>    <em class="mn">'''</em><br/><em class="mn">    For each row, extract the one-hot values in the row vector (from</em><br/><em class="mn">    VectorAssembler), along with any columns listed in column_list.</em><br/><em class="mn">    </em><br/><em class="mn">    Returns a tuple of values.</em><br/><em class="mn">    '''</em><br/>    col_tup = tuple(row[column] <strong class="me hj">for</strong> column <strong class="me hj">in</strong> column_list)<br/>    <strong class="me hj">return</strong> col_tup + tuple(row.onehot_features.toArray().tolist())<br/><br/>user_log = user_log.rdd.map(partial(extract, column_list=no_hot_cols)).toDF(all_cols)</span></pre><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/558de8df50785f9156e07e6d42ad1f73.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*f8ixJ57Xt_YIuwdDLeQyEA.png"/></div><figcaption class="im in et er es io ip bd b be z dx">Table 3: Distribution of `auth` Variable with `page`</figcaption></figure><p id="061f" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated"><code class="du mo mp mq me b">auth</code>变量也被考虑用于编码，但是它只有两个值，并且与我们的目标响应完全相关(如表3所示)，所以它被一起删除了。</p><p id="66e7" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated"><strong class="jr hj">数据可视化</strong></p><p id="0d00" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">本节中的误差线是使用t分布生成的，数据是为使用下面的便捷函数绘图而准备的。</p><pre class="lt lu lv lw fd md me mf mg aw mh bi"><span id="7fd7" class="mi is hi me b fi mj mk l ml mm"><strong class="me hj">def</strong> group_differences(data, target_val, group_vals):<br/>    <em class="mn">'''</em><br/><em class="mn">    Groups the target variable by a grouping variable, and returns the mean values</em><br/><em class="mn">    and error for each group. </em><br/><em class="mn">    </em><br/><em class="mn">    data: dataframe</em><br/><em class="mn">        The dataframe of data</em><br/><em class="mn">    target_val : str</em><br/><em class="mn">        The target column of the dataset</em><br/><em class="mn">    group_val : str or list</em><br/><em class="mn">        The column to create the groups for comparison</em><br/><em class="mn">    '''</em><br/>    <strong class="me hj">from</strong> <strong class="me hj">scipy.stats</strong> <strong class="me hj">import</strong> t<br/>    ci_stand_error = <strong class="me hj">lambda</strong> x: (x.std(ddof=1)/(np.sqrt(x.shape[0])))*t.ppf(0.975, df=x.shape[0])<br/>    <br/>    <strong class="me hj">if</strong> isinstance(target_val, str):<br/>        target_val = [target_val]<br/>        <br/>    concats = []<br/>    <strong class="me hj">for</strong> tv <strong class="me hj">in</strong> target_val:<br/>        new_group = data.groupby(group_vals, as_index=<strong class="me hj">True</strong>).agg(<br/>                          {tv: ['mean', ci_stand_error]})<br/>        new_group.columns.set_levels([[tv], ['Mean', 'Error']],<br/>                                  level=<strong class="me hj">None</strong>, inplace=<strong class="me hj">True</strong>)<br/>        concats.append(new_group)<br/><br/>    grouped_df = pd.concat(concats, axis=1)<br/>    <strong class="me hj">return</strong> grouped_df</span></pre></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/4dd8ce7a55934a4be5a9b428d703d923.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*7dT01JlCGYZpR37NZKsz_g.png"/></div><figcaption class="im in et er es io ip bd b be z dx">Figure 1: Streaming Habits between User Groups</figcaption></figure><p id="45f4" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">可以预测用户即将取消的一件事是会话之间的平均时间。例如，我们可能预期用户会取消，因为他们没有像他们想象的那样经常或长时间使用服务。根据图1的结果，很明显，取消会话的用户和没有取消会话的用户之间的会话平均时间有显著差异。然而，这与最初的假设相反，因为取消的用户实际上似乎在会话之间有更少的时间。这可能有其他原因，例如不取消服务的用户可能不会经常使用该服务，因为他们有免费服务，并且没有使用它的义务感，而那些付费用户可能会在他们的使用速度减慢时取消服务。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/d97720bc1b2e490b9e6c212077be542c.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*Rvmi_7YiwqBTM4znuzZ_MA.png"/></div><figcaption class="im in et er es io ip bd b be z dx">Figure 2: Paid interactions compared to time between sessions</figcaption></figure><p id="6efa" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">图2调查了会话间隔时间和付费用户之间是否存在负相关关系。从图中可以看出，这两者之间可能有轻微的负相关，但如果有，也是微弱的指数关系。此外，根据用户是否是将取消的用户，似乎没有不同的趋势。</p><p id="3756" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">在任何情况下，会话之间的平均时间可能是一个很好的预测。用户的平均会话长度之间似乎没有太大的差异。</p><p id="e5e0" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">用户可能取消的另一个可能的原因是，他们在使用该服务时收到了很多错误，因此对它不感兴趣。图3和图4显示，即使取消的人出现了更多的错误，这种差异看起来并不明显。还可能有其他因素影响用户是否由于错误而取消，例如，付费用户可能不再是付费用户，但如果他们经历大量错误，则仍然是免费用户。其中一些可能的相互作用在这里没有探讨，但是可能对模型有影响。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mu"><img src="../Images/de58c8b473e5e6d29236367af2e86506.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*4VefUUG9jfpQHQZnQRIOeQ.png"/></div><figcaption class="im in et er es io ip bd b be z dx">Figure 3: How errors affect paid users</figcaption></figure><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mv"><img src="../Images/ea19cd17067c1e4ed4a0d8d92b1e092c.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*M4w2rEcGfhqsbVt1ECj1uw.png"/></div><figcaption class="im in et er es io ip bd b be z dx">Figure 4: How errors affect time between sessions</figcaption></figure><p id="6221" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated"><strong class="jr hj">特色工程</strong></p><p id="d3ac" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">为了准备用于建模的数据，使用两个连续的groupby函数将数据合并为每个用户一行的格式。下面的代码片段显示了用于生成该查询的Spark SQL函数。该查询封装在一个自定义类中，该类遵循PySpark和sklearn的fit/transform风格。</p><p id="dd94" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">我选择将查询包装在这样的转换对象中，因为userAgent(即用户正在监听的设备)的一次性编码可能是无限的，并且很难有意义地聚合所有可能的值(因为它们也有很长的名称)。以这种方式构建查询使查询更加模块化，并且我不必担心丢失其中一个userAgent值。</p><p id="cf99" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">* *下一个片段只是一瞥，而不是整个课程。不要这样跑。**</p><pre class="lt lu lv lw fd md me mf mg aw mh bi"><span id="2bbf" class="mi is hi me b fi mj mk l ml mm"><strong class="me hj">class</strong> <strong class="me hj">SqlFeatureEngineer</strong>(Transformer):<br/>    <em class="mn">"""</em><br/><em class="mn">    Custom Transform to feature engineer using a SQL query</em><br/><em class="mn">    """</em><br/><br/>    <strong class="me hj">def</strong> __init__(self, table_name: str, agents: Iterable[str]):<br/>        super(SqlFeatureEngineer, self).__init__()<br/>        self.table_name = table_name<br/>        <br/>        sum_UA = "".join("<strong class="me hj">\n</strong>        ,SUM(<strong class="me hj">%s</strong>) as num_<strong class="me hj">%s</strong>_interactions" % (agent, agent) <strong class="me hj">for</strong> agent <strong class="me hj">in</strong> agents)<br/>        avg_UA = "".join("<strong class="me hj">\n</strong>    ,AVG(sess.num_<strong class="me hj">%s</strong>_interactions) as avg_<strong class="me hj">%s</strong>_interactions" % (agent, agent) <strong class="me hj">for</strong> agent <strong class="me hj">in</strong> agents)<br/><br/>        session_query_select = """SELECT <br/>            sess.userId<br/>            ,AVG(sess.num_items_in_session) as avg_num_items_in_session<br/>            ,MAX(sess.longest_song_in_session) as longest_song<br/>            ,AVG(sess.longest_song_in_session) as longest_song_per_session<br/>            ,COUNT(sess.sessionId) as total_number_of_sessions<br/>            ,SUM(sess.session_listening_time)/COUNT(sess.sessionId) as listening_time_per_session<br/>            ,SUM(sess.number_of_songs)/COUNT(sess.sessionId) as avg_number_of_songs_per_session<br/>            ,SUM(sess.session_listening_time)/SUM(sess.number_of_songs) as avg_song_length<br/>            ,AVG(sess.session_length) as avg_session_length<br/>            ,MAX(sess.session_end)-MIN(sess.registration) as time_since_joined<br/>            ,MIN(sess.session_begin)-MIN(sess.registration) as time_to_first_session <br/>            ,CASE<br/>                WHEN COUNT(sess.sessionId) &lt;= 1 THEN 0<br/>                ELSE ((MAX(sess.session_end)-MIN(sess.session_begin)) - SUM(sess.session_length))/(COUNT(sess.sessionId)-1)<br/>            END as avg_time_between_sessions<br/>            ,AVG(sess.gender_F) as avg_gender_F<br/>            <br/>               ...                    ...</span><span id="737a" class="mi is hi me b fi mw mk l ml mm">            ,AVG(sess.num_downgrades) as avg_num_downgrades<br/>            --,AVG(sess.num_cancel_visits) as avg_num_cancel_visits<br/>            ,MAX(sess.cancelled) as cancelled"""<br/><br/>        session_query_from = """<br/>        FROM (<br/>            SELECT <br/>                userId<br/>                ,sessionId<br/>                ,MAX(itemInSession) as num_items_in_session<br/>                ,MAX(length) as longest_song_in_session<br/>                ,SUM(length) as session_listening_time<br/>                ,COUNT(song) as number_of_songs<br/>                ,min(ts) as session_begin<br/>                ,max(ts) as session_end<br/>                ,max(ts) - min(ts) as session_length<br/>                ,MIN(registration) as registration<br/>                ,MAX(cancelled) as cancelled<br/><br/>              ...                    ...</span><span id="c2a5" class="mi is hi me b fi mw mk l ml mm">                ,SUM(`page_Submit Downgrade`) as num_downgrades<br/>                """ <br/><br/>        session_query = session_query_select + avg_UA + session_query_from + sum_UA + """<br/>            FROM """ + self.table_name + """<br/>            GROUP BY<br/>                userId,<br/>                sessionId<br/>        ) as sess<br/>        GROUP BY<br/>            userId<br/>        """<br/>        <br/>        self.query = session_query<br/><br/>    <strong class="me hj">def</strong> _transform(self, df: DataFrame) -&gt; DataFrame:<br/>        df.createOrReplaceTempView(self.table_name)<br/>        summary = spark.sql(self.query)<br/>        <strong class="me hj">return</strong> summary</span></pre><p id="23f6" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">该对象用两个值实例化，临时表名(spark用来识别/构建查询)和用户代理列表(填充到查询中)。第一个groupby压缩了每个用户的会话数据，因此每个用户都记录了每个会话的汇总统计信息。然后，将每个会话分组在一起，以便每个用户都有关于其总会话历史的汇总统计信息。总的来说，小数据集中大约有120个分组用户，完整数据集中有22277个用户，总共有61列。</p><p id="c22e" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">后来，我发现有一种更python/sparkish的方法可以做到这一点(我用于初始聚合以生成图)，下面我分享一个例子:</p><pre class="lt lu lv lw fd md me mf mg aw mh bi"><span id="c882" class="mi is hi me b fi mj mk l ml mm"><em class="mn"># Aggregate using spark, and convert to pandas for easy plotting/exploring</em><br/>id_group = user_log.groupBy(["userId", "sessionId"]) \<br/>                    .agg(<br/>                        Fmax('itemInSession').alias('num_items_in_session'),<br/>                        Fmax('length').alias('longest_song_in_session'),<br/>                        Fsum('length').alias('session_listening_time'),<br/>                        count('song').alias('number_of_songs'),<br/>                        Fmin('ts').alias('session_begin'),<br/>                        Fmax('ts').alias('session_end'),<br/>                        (Fmax('ts')-Fmin('ts')).alias('session_length'),<br/>                        Fmax('gender_F').alias('gender_F'),<br/>                        Fsum('page_Error').alias('num_errors'),<br/>                        Fsum('level_paid').alias('num_paid_interactions'),<br/>                        Fmax('cancelled').alias('cancelled')<br/>                    ) \<br/>                    .groupBy('userId') \<br/>                    .agg(<br/>                        when(count('sessionId') &lt;= 1, 0)<br/>                        .otherwise(((Fmax('session_end')-Fmin('session_begin'))<br/>                                    -Fsum('session_length'))/(count('sessionId')-1))<br/>                        .alias('avg_time_between_sessions'),<br/>                        avg('session_length').alias('avg_session_length'),<br/>                        avg('gender_F').alias('avg_gender_F'),<br/>                        (Fsum('session_listening_time')/count('sessionId')).alias('listening_time_per_session'),<br/>                        count('sessionId').alias('total_number_of_sessions'),<br/>                        avg('num_errors').alias('avg_num_errors'),<br/>                        avg('num_paid_interactions').alias('avg_num_paid_interactions'),<br/>                        Fmax('cancelled').alias('cancelled')<br/>                    ).toPandas()</span></pre><p id="d42e" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">压缩数据后，使用最小-最大缩放器进行归一化。下面的代码片段在Spark中执行最小-最大缩放。</p><pre class="lt lu lv lw fd md me mf mg aw mh bi"><span id="b3d7" class="mi is hi me b fi mj mk l ml mm"><em class="mn"># create a list of columns to min-max scale. Make sure to exclude</em><br/><em class="mn"># the userId column and the target variable</em><br/>columns_to_scale = [column <strong class="me hj">for</strong> column <strong class="me hj">in</strong> user_summary.columns <strong class="me hj">if</strong> column != 'userId' <strong class="me hj">and</strong> column != 'cancelled']<br/><br/><em class="mn"># min-max scaler</em><br/>assembler = VectorAssembler(inputCols=columns_to_scale, outputCol="features")<br/>transformed = assembler.transform(user_summary)<br/>scaler = MinMaxScaler(inputCol="features", outputCol="scaledFeatures")<br/>scalerModel =  scaler.fit(transformed.select("features"))<br/>user_scaled = scalerModel.transform(transformed)</span></pre><h1 id="61f1" class="ir is hi bd it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">第二部分:建模和实现</h1><p id="ee50" class="pw-post-body-paragraph jp jq hi jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hb bi translated">幸运的是，让建模在Spark中工作并不太困难，因为这部分的API与sklearn非常相似。唯一有点陌生的部分是创建用于评估分类指标的对象。可能有一种更简单的方法，但是我可以通过将multiclassclarticationevaluator对象传递给cross validator来实现，如下例所示。</p><pre class="lt lu lv lw fd md me mf mg aw mh bi"><span id="a967" class="mi is hi me b fi mj mk l ml mm"><em class="mn"># Split into training and test data<br/></em>(trainingData, testData) = user_scaled.randomSplit([0.8, 0.2], seed=42)</span><span id="529d" class="mi is hi me b fi mw mk l ml mm">logreg = LogisticRegression(labelCol="cancelled", featuresCol="scaledFeatures",<br/>                            maxIter=100, regParam=0.01, elasticNetParam=0.5)<br/>evaluator = MulticlassClassificationEvaluator(<br/>    labelCol="cancelled", predictionCol="prediction",<br/>    metricName="f1")<br/><br/>pipeline_lr = Pipeline(stages=[logreg])<br/><br/>paramGrid_lr = ParamGridBuilder() \<br/>                .addGrid(logreg.elasticNetParam, [0]) \<br/>                .addGrid(logreg.regParam, [0.01, 0.1]) \<br/>                .build()<br/>    <br/>crossval_lr = CrossValidator(estimator=logreg,<br/>                             estimatorParamMaps=paramGrid_lr,<br/>                             evaluator=evaluator,<br/>                             numFolds=2)</span></pre><p id="3bc9" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">建模分三个阶段进行。在第一阶段，我使用Spark创建了一个端到端的模型管道，但只针对小样本数据，我执行了一个不太详尽的模型搜索。在第二步中，我处理了一个大得多的数据集，并执行了一个稍微大一点的模型搜索，但是由于AWS成本的快速上升，我离线完成了建模。第三步(离线)是下载数据，分块处理，并在我的本地机器上执行快速、更彻底的搜索。</p><p id="f86b" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated"><strong class="jr hj">第一步:在小数据集上用Spark建模</strong></p><p id="0d3f" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">通过首先将数据分成训练集和测试集(80/20%分割)来执行建模。然后，在逻辑回归模型上使用2重交叉验证网格搜索对训练集建模(具有弹性净惩罚)。该模型考虑的两个参数(以及搜索到的相关值)是:</p><ol class=""><li id="d90f" class="ks kt hi jr b js kn jw ko ka ku ke kv ki kw km kx ky kz la bi translated">elasticNetParam: [0]</li><li id="0c6e" class="ks kt hi jr b js lb jw lc ka ld ke le ki lf km kx ky kz la bi translated">regParam: [0.01，0.1]</li></ol><p id="1432" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">建模前从数据中删除<code class="du mo mp mq me b">userId</code>变量。模型中使用最小-最大化要素，而不是原始比例的变量。假设这是一个分类问题，基于最高F1分数选择最佳模型。总的取消率约为20%，所以这种不平衡不足以担心过采样或欠采样，但鉴于这种轻微的不平衡，像F1这样的指标可能是比准确性更好的衡量标准。</p><p id="970d" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">一旦选择了最佳拟合模型，就在整个训练集上训练该模型，并在测试集上执行最终评估。基于使用小数据集的初步运行，测试集上的模型性能具有<strong class="jr hj"> F1分数</strong>为<strong class="jr hj"> 0.65 </strong>和<strong class="jr hj">准确度分数</strong>为<strong class="jr hj"> 0.74 </strong>。</p><p id="4389" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated"><strong class="jr hj">Spark大数据集建模(云部署)</strong></p><p id="5cfe" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">在AWS上运行模型涉及的过程与步骤1中概述的过程相同，只是实际的建模工作更加详尽。数据被分成75/25%的训练/测试集，交叉验证是3倍而不是2倍。在弹性网逻辑回归中搜索的参数是:</p><ol class=""><li id="2bdb" class="ks kt hi jr b js kn jw ko ka ku ke kv ki kw km kx ky kz la bi translated">elasticNetParam: [0，0.33，0.66，1]</li><li id="e843" class="ks kt hi jr b js lb jw lc ka ld ke le ki lf km kx ky kz la bi translated">regParam: [0.001，0.01，0.1]</li></ol><p id="bc1b" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">为了在更大的数据集上有效地运行模型，我使用了一个AWS EMR/EC2实例。我按照这里<a class="ae iq" href="https://towardsdatascience.com/how-to-set-up-a-cost-effective-aws-emr-cluster-and-jupyter-notebooks-for-sparksql-552360ffd4bc" rel="noopener" target="_blank">列出的说明</a>启动并运行它。</p><p id="1bd1" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">一旦我运行了实例并加载了数据，剩下的分析就相当顺利了，我不需要对之前的分析做任何大的修改(除了本节中概述的内容)。</p><p id="8a35" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">然而，由于较大的(以前未探索的)数据集，出现了一些小错误。例如，以前没有缺失值的一些列现在被填充了<code class="du mo mp mq me b">null</code>，这干扰了模型的构建。为了解决这些问题，我在Spark SQL查询中添加了case语句，用0替换了<code class="du mo mp mq me b">null</code>值。还有一个糟糕的用户Id，其中一个用户可能听了0首歌，但每天也有大约2500个唯一的会话Id(除非你是机器人，这似乎不太可能)。但是，删除这个坏用户并没有提高模型的性能。</p><p id="07b0" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">端到端的过程花费了大约2.5小时，使用了大约96个核心(3个主节点、1个核心节点和3个不同大小、功率等的任务节点)。).关于云实现的更多细节可以在<a class="ae iq" href="https://github.com/tomwerner5/sparkify" rel="noopener ugc nofollow" target="_blank"> github库</a>中找到。对流程进行调试和质量检查大约需要20个小时的计算时间。总的来说，运行EMR/EC2实例的成本约为30美元。</p><p id="66dd" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">该步骤中模型的性能等同于步骤3中评估的模型(在相同的超参数条件下，将在下一步中呈现)。</p><p id="0b2c" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated"><strong class="jr hj">第三步:在sklearn中离线建模</strong></p><p id="8d73" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">因为30美元对我来说开始变得有点贵，我下载了完整的数据集，并离线进行了一些最终的建模工作。离线代码以及相关数据在<a class="ae iq" href="https://github.com/tomwerner5/sparkify" rel="noopener ugc nofollow" target="_blank"> github库</a>上提供。通过使用Python中的urllib从一个<a class="ae iq" href="https://udacity-dsnd.s3.amazonaws.com/sparkify/sparkify_event_data.json" rel="noopener ugc nofollow" target="_blank"> S3桶</a>中下载大型数据集。我不会在这里详细说明下载和提取数据的具体步骤，但是可以在run_sparkify_offline.py文件中找到并复制它们。</p><p id="e62a" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">一旦数据被下载并以. json文件的形式保存在我的硬盘上，下一步就是尝试加载数据。正如所料，我的32 GB内存无法处理12GB的。json数据(很明显，Pandas增加的内存是未压缩文件大小的5-10倍)。为了解决这个问题，我知道我必须把数据分成块。然而，我不能盲目地分离数据，因为我需要将单个用户的完整数据放在单个文件中，这样当我将它加载到Pandas时，我可以更容易地聚合该用户的数据。因此，在加载任何数据之前，我搜索了整个文本，提取了每个唯一的userId，并将它们放入一个python列表中(这相当快)。接下来，我将唯一用户列表分成10个部分，并扫描了。又是json文件。在第二遍中，我检查当前数据行属于哪个组(基于userId ),并将该行数据写出到一个较小的文件中。这种情况一直重复到文件的末尾，我只剩下10个文件~ 1GB大，每个文件都有完整的用户配置文件。</p><p id="76b9" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">之后，脚本循环遍历10个文件，并对每组数据执行聚合步骤(使用spark概述的所有相同方法，但使用Pandas/sklearn)。</p><p id="8a08" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">最后，对模型进行拟合、网格搜索和评估。首先，我想尽可能地模仿第2步中的AWS Spark模型，因此这一步中的配置是75/25%的训练/测试分割、双重验证和以下参数:</p><ol class=""><li id="30f9" class="ks kt hi jr b js kn jw ko ka ku ke kv ki kw km kx ky kz la bi translated">l1_ratio: [0，0.33，0.66，1] #与Spark中的elasticNetParam相同</li><li id="c701" class="ks kt hi jr b js lb jw lc ka ld ke le ki lf km kx ky kz la bi translated">C: [1/0.01，1/0.1，1] <em class="mn"> #与Spark </em>中值为[0.001，0.01，0.1]的regParam相同</li></ol><p id="ce18" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">由于Spark和sklearn对于逻辑回归具有不同的成本函数，正则化的罚值计算如下:</p><pre class="lt lu lv lw fd md me mf mg aw mh bi"><span id="032d" class="mi is hi me b fi mj mk l ml mm">sklearn_reg_param = 1/(spark_reg_param * 10)</span></pre><p id="77b5" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">测试集上的模型性能的<strong class="jr hj"> F1分数</strong>为<strong class="jr hj"> 0.86 </strong>，而<strong class="jr hj">准确度分数</strong>为<strong class="jr hj"> 0.94。根据这些结果，我扩大了网格搜索，看看是否可以做出更多改进。下一节将介绍最终结果。</strong></p><h1 id="6b5b" class="ir is hi bd it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">第三部分:最终结果</h1><p id="9cf3" class="pw-post-body-paragraph jp jq hi jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hb bi translated"><strong class="jr hj">模型评估</strong></p><p id="36aa" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">因为我现在有能力扩展建模工作，所以我对逻辑回归模型进行了更大的网格搜索。在75/25%分割的训练集上使用5重交叉验证，最佳模型在维持集上产生了<strong class="jr hj"> 0.86 </strong>的<strong class="jr hj"> F1分数</strong>和<strong class="jr hj">0.94</strong>的准确度分数。相对于较小的数据集，这是一个很大的改进。搜索的参数如下:</p><ol class=""><li id="6de0" class="ks kt hi jr b js kn jw ko ka ku ke kv ki kw km kx ky kz la bi translated">l1_ratio: [0，0.25，0.4，0.5，0.6，0.75，1]</li><li id="558b" class="ks kt hi jr b js lb jw lc ka ld ke le ki lf km kx ky kz la bi translated">C: [0.000001，0.00001，0.0001，0.001，0.01，0.1，1，10，100]</li></ol><p id="6558" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">图5描述了模型的系数排名。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mx"><img src="../Images/f5250817bf4fcad66ebc6ad6f0c4289e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ehwc3ZYM6yS8E0zBVEuXKQ.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx">Figure 5: Coefficient Rankings (LogReg Model)</figcaption></figure><p id="30fa" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">虽然这不是一个完美的排名(因为最小-最大缩放器没有标准化为等于均值/方差)，但图5确实呈现了哪些特征对于用户取消可能是重要的一般概念，其中蓝色表示(数学上)与取消的正关联，红色表示负关联。根据排名，似乎最重要的预测因素是http状态代码307，这是一个临时重定向。据我所知，这是一个错误的排序，所以它是有意义的，这将与取消有积极的联系。加好友和喜欢的歌(竖起大拇指)也是以负面联想高居榜首，这也是符合逻辑的。</p><p id="e822" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">我发现令人惊讶的一件事是，广告与取消有负面联系。我本以为看到更多的添加会让一些人想要取消(除非添加是如此之好，他们会让人们注册)。然而，也可能有其他影响因素，例如付费用户可能看不到那么多的添加，因此添加仅与特定类型的用户群相关联。</p><p id="7148" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">为流式传输选择的设备(用户代理)似乎没有影响，大多数系数的值都很低。这似乎合乎逻辑，因为许多人有多台设备，一台设备的问题可能不足以让某人取消。</p><p id="b20a" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">为了查看不同的模型是否可能产生更好的结果，我还测试了梯度增强树(GBT)模型，该模型使用以下参数在维持集上实现了<strong class="jr hj"> F1得分</strong>为<strong class="jr hj"> 0.75 </strong>，以及<strong class="jr hj">准确度得分</strong>为<strong class="jr hj"> 0.90 </strong>:</p><ol class=""><li id="db9a" class="ks kt hi jr b js kn jw ko ka ku ke kv ki kw km kx ky kz la bi translated">最大深度:[2，5，10，50]</li><li id="1731" class="ks kt hi jr b js lb jw lc ka ld ke le ki lf km kx ky kz la bi translated">min_samples_split: [2，12，32]</li></ol><p id="62f5" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">在这种情况下，GBT模型的表现不如逻辑回归模型。通常情况下，GBT模型的总体性能更好，但它们也有几个更高的超参数，可能很难调整，因此可能没有进行足够的搜索。未来的研究应该更多地探索这一模式。</p><p id="8aa5" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">需要注意的是，GBT模型只在离线脚本中的完整数据集上进行了测试(使用sklearn)。</p><h1 id="2fa7" class="ir is hi bd it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">结论</h1><p id="6be8" class="pw-post-body-paragraph jp jq hi jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hb bi translated"><strong class="jr hj">反思与改进</strong></p><p id="80f1" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">预测用户流失是当今机器学习问题中的一个相关主题，能够建立一个提供可靠预测的模型是一项有价值的技能。由于大多数流媒体网站都有非常庞大的用户群，建模中使用的数据量对于一台简单的笔记本电脑来说可能会迅速增长。在这些情况下，像Spark这样的工具可以克服处理障碍，帮助人们分析大型数据集。该项目试图在虚构的Sparkify数据集上预测用户流失，并利用Spark进行分布式数据处理。</p><p id="418a" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">我发现Spark比用熊猫更有挑战性。我觉得有很多繁琐的方法需要大量的试错(或互联网的帮助)才能正常工作。然而，尽管有这些挑战，我了解到使用Spark确实使处理更大的数据集变得容易得多。说到数据清理和预处理，Spark非常高效。所有的数据清理步骤都很快。Spark让我失望的地方是造型。一旦我有了聚集的数据，有22，277行和60列，这是一个相对较小的数据集，但Spark花了一个小时来交叉验证和网格搜索一些参数。相比之下，在相同的数据集上，我的本地机器只花了几分钟。我怀疑这种差异与Spark需要更多开销有关，当数据集很小时(聚合后)，Spark不会带来好处。</p><p id="99bf" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">虽然这可以被认为是一种端到端的分析，但是有几种方法可以改进这种分析。例如，没有使用NLP，但是可以改善结果。有可能听某种流派音乐的用户比其他人更容易取消，或者可能听某些艺术家的音乐反映了某些性格特征。在任一情况下，从艺术家、歌曲和位置信息中提取NLP特征可以产生更高的模型性能。此外，由于我不是Sparkify平台或所有网络流量细微差别的专家，有可能对数据进行进一步改进，以便提取更多有用的特征。例如，更谨慎的做法可能是按时间向后聚合变量(用户A在过去3、6、12个月中是否发生过事件B？等等。).</p></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><p id="c03e" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">[1]:<a class="ae iq" href="https://udacity-dsnd.s3.amazonaws.com/sparkify/sparkify_event_data.json" rel="noopener ugc nofollow" target="_blank">https://uda city-dsnd . S3 . Amazon AWS . com/spark ify/spark ify _ event _ data . JSON</a></p><p id="d9b2" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">[2]:我在文章中提供了一些代码示例，但是请参考我的github以获得完整的工作示例(【https://github.com/tomwerner5/sparkify/】T2)。</p><p id="7732" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">[3]: <a class="ae iq" href="https://towardsdatascience.com/how-to-set-up-a-cost-effective-aws-emr-cluster-and-jupyter-notebooks-for-sparksql-552360ffd4bc" rel="noopener" target="_blank">如何为SparkSQL建立经济高效的AWS EMR集群和Jupyter笔记本电脑(2020年11月更新)|作者安德鲁·杨|走向数据科学</a></p></div></div>    
</body>
</html>
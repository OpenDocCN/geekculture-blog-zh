<html>
<head>
<title>Web Scraping 101</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">网页抓取101</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/web-scraping-101-95d67aa57d55?source=collection_archive---------56-----------------------#2021-06-07">https://medium.com/geekculture/web-scraping-101-95d67aa57d55?source=collection_archive---------56-----------------------#2021-06-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/ed415542f15c9caeed9441fabfa0cb21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lGmtE_11gjj10uPztGsZHQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Image by <a class="ae iu" href="https://pixabay.com/users/shafin_protic-16278454/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5471975" rel="noopener ugc nofollow" target="_blank">Shafin Al Asad Protic</a> from <a class="ae iu" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5471975" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><p id="18be" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据科学需要大量的数据，所以每个优秀的数据科学家都应该知道如何从互联网上免费获取数据。无论你是从维基百科上抓取表格，还是从零售网站上抓取价格，都有一些简单的方法可以做到。浏览互联网时用眼睛看到的任何东西都可以刮下来。</p><p id="f3a1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">流程相当简单。我用s <em class="jt"> elenium </em>或者<em class="jt"> requests </em>作为包来刮。</p><p id="b3f4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里我们将使用这个网站:【https://publicholidays.us/school-holidays/ T4】</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ju"><img src="../Images/5074d87bddf189ff47ceeed6e5c19425.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0SERlu6vrSicoABACS2jBg.png"/></div></div></figure><p id="ff7f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这个网站的第一页是有校历的州。每个状态都是一个可点击的链接。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jz"><img src="../Images/d3f6851b45381e2a1c44c9472818ae2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cpxyUiGgAjDAJsHVh_Kf6Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">First layer</figcaption></figure><p id="13fb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">每个州点击进入一个学区。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ka"><img src="../Images/02f87d7c853d2de3204c2a780d44ca25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KqOBrKokLH8C6BEYcPskjA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Second Layer</figcaption></figure><p id="3ae6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，在学区页面上是我们想要的日历。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kb"><img src="../Images/afcf6a7c0d7674b744f6d76c181a6003.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-_r887Kawo-XyEvTsLOFzw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Third and final layer</figcaption></figure><p id="b2d6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们最终想要提取最终的HTML，并将其绑定到州和校区。</p><p id="1c5a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这些是我的进口货:</p><pre class="jv jw jx jy fd kc kd ke kf aw kg bi"><span id="b801" class="kh ki hi kd b fi kj kk l kl km">from bs4 import BeautifulSoup<br/>import time<br/>from urllib.parse import urlencode<br/>from selenium import webdriver<br/>from selenium.webdriver.firefox.options import Options<br/>import pandas as pd<br/>import random<br/>import json<br/>import requests</span></pre><p id="3ce1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后我构建几个函数。</p><pre class="jv jw jx jy fd kc kd ke kf aw kg bi"><span id="1a40" class="kh ki hi kd b fi kj kk l kl km">def soup_builder(url):<br/>    page = requests.get(url)<br/>    html = page.content<br/>    soup = BeautifulSoup(html, 'html.parser')<br/>    return soup</span></pre><p id="ae1a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">或者</p><pre class="jv jw jx jy fd kc kd ke kf aw kg bi"><span id="0535" class="kh ki hi kd b fi kj kk l kl km">def soup_builder(url):<br/>    options = Options()<br/>    options.headless = True<br/>    browser = webdriver.Firefox(options=options)<br/>    browser.get(url)<br/>    # add rand time to not get kick off<br/>    delay = random.randint(2, 6)<br/>    time.sleep(delay)<br/>    html = browser.page_source<br/>    browser.quit()<br/>    soup = BeautifulSoup(html, 'html.parser')<br/>    return soup</span></pre><p id="55d2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第一个功能要快得多，但某些网站会在将你标记为机器人后让你关机。第二个函数非常慢，但是你的脚本模拟了一个浏览器，随机延迟模拟了你点击的时间。这可以让你的脚本从那些不让你抓取的网站中抓取。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kn"><img src="../Images/71b451bf51bd4a0338806fdcc7ecac62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L538HL4Mi77lNpe3IAAqBA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">I use command+shift c to look at the HTML as I explore it on the browser</figcaption></figure><p id="0708" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦你很好地理解了HTML布局，你就可以很容易地编写一些简单的代码来从中提取表格信息。</p><p id="c677" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在获得“汤”即HTML之后，您可以开始对HTML字符串进行排序，以提取您所关心的信息。在这个阶段，你需要经常查看HTML来知道如何解析它。</p><pre class="jv jw jx jy fd kc kd ke kf aw kg bi"><span id="0b3d" class="kh ki hi kd b fi kj kk l kl km">school_district_frames = []<br/>for url, state in zip(states_df['href'], states_df['state']):<br/>    soup = soup_builder(url)<br/>    # grab table<br/>    table_tag = soup.select('table')<br/>    d = find_table_urls(table_tag)<br/>    school_district_df = pd.DataFrame(d)<br/>    school_district_df.columns = ['href', 'school district']<br/>    school_district_df['state'] = state<br/>    school_district_frames.append(school_district_df)</span></pre><p id="ce1a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这将产生:</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ko"><img src="../Images/6a1694d4349ed99e3acbfeea2d65fbb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JYVQgojYYfP8mS8Xfz9jcw.png"/></div></div></figure><pre class="jv jw jx jy fd kc kd ke kf aw kg bi"><span id="7e14" class="kh ki hi kd b fi kj kk l kl km">def find_holiday_table_urls(table_tag):<br/>    d = {'holiday': [], <br/>         'start_date': [],<br/>         'finish_date': [] <br/>        }<br/>    '''<br/>     Pulls urls from table<br/>    '''<br/>    for t in table_tag:<br/>        for row in t.findAll('tr')[1:]:<br/>            try:<br/>                text = row.findAll('td')[0].text<br/>                start_date = row.findAll('td')[1].text<br/>                finish_date = row.findAll('td')[2].text<br/>                d['holiday'].append(text)<br/>                d['start_date'].append(start_date)<br/>                d['finish_date'].append(finish_date)<br/>            except:<br/>                continue<br/>    return d</span><span id="eeb6" class="kh ki hi kd b fi kp kk l kl km">holiday_frames = []<br/>data = zip(school_district_df['href'], school_district_df['state'], school_district_df['school district'])<br/>for url, state, school_district in data:<br/>    if school_district in school_districts:<br/>        continue<br/>    soup = soup_builder(url)<br/>    # grab table<br/>    table_tag = soup.select('table')<br/>    d = find_holiday_table_urls(table_tag)<br/>    holiday_df = pd.DataFrame(d)<br/>    holiday_df.columns = ['holiday', 'start_date', 'finish_date']<br/>    holiday_df['state'] = state<br/>    holiday_df['school district'] = school_district<br/>    holiday_frames.append(holiday_df)</span></pre><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kq"><img src="../Images/57fa8ff3e215deeab7dc7d0e4a0b9ec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gku44BQQSMTLVvbVhvOXSA.png"/></div></div></figure><p id="4943" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">嘣！现在，您有了一个包含美国所有学校假期的熊猫数据框架。这么多免费数据！然后你可以把它存储到红移或者其他地方。</p><p id="e4f0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">无论你是用抓取来建立你的创业公司数据仓库还是一次性项目，它对任何数据科学家来说都是一个强大的工具。</p><p id="73b5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">参考文献:</strong></p><div class="kr ks ez fb kt ku"><a href="https://selenium-python.readthedocs.io/" rel="noopener  ugc nofollow" target="_blank"><div class="kv ab dw"><div class="kw ab kx cl cj ky"><h2 class="bd hj fi z dy kz ea eb la ed ef hh bi translated">Selenium与Python - Selenium Python绑定2文档</h2><div class="lb l"><h3 class="bd b fi z dy kz ea eb la ed ef dx translated">请注意，这不是正式文档。如果你想对这个文档有所贡献，你可以叉这个…</h3></div><div class="lc l"><p class="bd b fp z dy kz ea eb la ed ef dx translated">selenium-python.readthedocs.io</p></div></div></div></a></div><div class="kr ks ez fb kt ku"><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener  ugc nofollow" target="_blank"><div class="kv ab dw"><div class="kw ab kx cl cj ky"><h2 class="bd hj fi z dy kz ea eb la ed ef hh bi translated">美丽的汤文档-美丽的汤4.9.0文档</h2><div class="lb l"><h3 class="bd b fi z dy kz ea eb la ed ef dx translated">Beautiful Soup是一个Python库，用于从HTML和XML文件中提取数据。它与您最喜欢的解析器一起工作…</h3></div><div class="lc l"><p class="bd b fp z dy kz ea eb la ed ef dx translated">www.crummy.com</p></div></div><div class="ld l"><div class="le l lf lg lh ld li io ku"/></div></div></a></div><div class="kr ks ez fb kt ku"><a href="https://pypi.org/project/beautifulsoup4/" rel="noopener  ugc nofollow" target="_blank"><div class="kv ab dw"><div class="kw ab kx cl cj ky"><h2 class="bd hj fi z dy kz ea eb la ed ef hh bi translated">beautifulsoup4</h2><div class="lb l"><h3 class="bd b fi z dy kz ea eb la ed ef dx translated">美汤是一个库，可以很容易的从网页上抓取信息。它位于HTML或XML之上…</h3></div><div class="lc l"><p class="bd b fp z dy kz ea eb la ed ef dx translated">pypi.org</p></div></div><div class="ld l"><div class="lj l lf lg lh ld li io ku"/></div></div></a></div><div class="kr ks ez fb kt ku"><a href="https://docs.python-requests.org/en/master/" rel="noopener  ugc nofollow" target="_blank"><div class="kv ab dw"><div class="kw ab kx cl cj ky"><h2 class="bd hj fi z dy kz ea eb la ed ef hh bi translated">请求:HTTP for Humans请求2.25.1文档</h2><div class="lb l"><h3 class="bd b fi z dy kz ea eb la ed ef dx translated">版本2.25.1 .(安装)Requests是一个优雅而简单的Python HTTP库，是为人类而构建的…</h3></div><div class="lc l"><p class="bd b fp z dy kz ea eb la ed ef dx translated">docs.python-requests.org</p></div></div></div></a></div><div class="kr ks ez fb kt ku"><a href="https://docs.python.org/3/library/urllib.parse.html" rel="noopener  ugc nofollow" target="_blank"><div class="kv ab dw"><div class="kw ab kx cl cj ky"><h2 class="bd hj fi z dy kz ea eb la ed ef hh bi translated">urllib.parse -将URL解析成组件- Python 3.9.5文档</h2><div class="lb l"><h3 class="bd b fi z dy kz ea eb la ed ef dx translated">源代码:Lib/urllib/parse.py这个模块定义了一个标准接口来中断统一资源定位符(URL)…</h3></div><div class="lc l"><p class="bd b fp z dy kz ea eb la ed ef dx translated">docs.python.org</p></div></div></div></a></div><p id="29be" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">无论你是用抓取来建立你的创业公司数据仓库还是一次性项目，它对任何数据科学家来说都是一个强大的工具。</p></div></div>    
</body>
</html>
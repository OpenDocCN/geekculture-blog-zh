<html>
<head>
<title>Word Embeddings in AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能中的词嵌入</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/word-embeddings-in-ai-10a9e430cb59?source=collection_archive---------16-----------------------#2021-07-10">https://medium.com/geekculture/word-embeddings-in-ai-10a9e430cb59?source=collection_archive---------16-----------------------#2021-07-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="745b" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">用简单的英语解释。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/a590aa8a3af9613f2abe5dd78c061a9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U5dvsp1QMkXzJXR_JTNj6A.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Photo by <a class="ae jn" href="https://unsplash.com/@fotowei" rel="noopener ugc nofollow" target="_blank">Wei Zeng</a> on Unsplash</figcaption></figure><p id="f3de" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">机器学习模型将数字向量作为输入。当处理文本时，我们必须将字符串转换成数字。长期以来，单词嵌入一直是人工智能中事实上的文本表示。</p><h1 id="07b0" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">什么是单词嵌入？</h1><p id="17ae" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">我们可以将每个唯一的单词与向量中的索引相关联。用这样长的向量来表示单词或文档实际上是没有效率的，因为它们通常是稀疏的，许多位置被零值填充。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lh"><img src="../Images/1ea69472e088519a7d26821dcb64af60.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/0*Px2EeEgVDlq4dX8n.png"/></div></figure><p id="f7ff" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">与稀疏向量相反，密集向量可以更成功地用作机器学习系统中的特征。密集向量也意味着要估计的参数更少。</p><p id="8897" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如果我们在二元预测任务中训练一个分类器，“单词<em class="li"> w </em>有可能出现在<em class="li"> v </em>附近吗？”，那么我们可以使用学习到的分类器权重作为单词嵌入。基于意义是上下文相关的假设，这些成为单词意义的连续矢量表示。</p><h1 id="f024" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">如何计算单词嵌入？</h1><p id="9d75" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">对于给定的语料库，我们如何确定这些嵌入呢？这正是Word2vec语言模型的设计目的。嵌入向量通常至少大约100维长才有效。训练一个分类器来估计这些向量。</p><p id="14ec" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">Word2vec <strong class="jq hj"> </strong>有两种版本:CBOW模型和skip-gram模型，由<a class="ae jn" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank"> Mikolov等人提出。艾尔。，2013，</a>在谷歌。在skip-gram变体中，使用目标单词来预测上下文单词。使用周围单词预测目标单词的变体是CBOW模型。通常人们会在大型语料库上使用W <a class="ae jn" href="https://code.google.com/archive/p/word2vec/" rel="noopener ugc nofollow" target="_blank"> ord2vec实现</a>和预训练嵌入。</p><p id="8aa1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">下面的例子描述了Word2vec模型的skip-gram变体背后的直觉。让我们假设下面的句子，其中“杏子”是目标单词，“汤匙”、“of”、“果酱”和“a”是考虑双单词窗口时的上下文单词。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lj"><img src="../Images/042f229cb0368e62a5d8452718776538.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/0*wJVIlfkdtLx3La_b.png"/></div></figure><p id="3c92" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们可以构建一个二元分类器，它将任意一对单词<em class="li"> (t，c) </em>作为输入，如果<em class="li"> c </em>是其他地方的<em class="li"> t </em>和<em class="li"> False </em>的真实上下文单词，则预测<em class="li"> True </em>。例如，分类器将为<em class="li">(杏，汤匙)</em>返回<em class="li">真</em>，为<em class="li">(杏，捏)返回<em class="li">假</em>。下面我们会看到更多的正面和负面分类的例子，当我们把杏作为目标时。</em></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lk"><img src="../Images/29fbde010b64757e8800d29b97fdfcb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/0*qYNxznMBTiuVT1T7.png"/></div></figure><p id="b34b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">更进一步，我们必须将每个(上下文窗口，目标)对分成(输入，输出)示例，其中输入是目标，输出是上下文中的一个单词。出于说明的目的，对<em class="li">(【of，杏子，a，pinch】，果酱)</em>将产生正例<em class="li">(果酱，杏子)，</em>，因为<em class="li">杏子</em>是<em class="li">果酱</em>的真实上下文词。<em class="li">(果酱、柠檬)</em>会是一个反面例子。通过对每个(上下文窗口、目标)对应用这种操作，我们获得了完整的数据集。最后，我们用词汇表中的唯一索引替换每个单词，以便处理独热向量表示。</p><p id="1cc6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">给定一组正的和负的训练样本以及一组初始的用于目标的<br/>嵌入<em class="li"> W </em>和用于上下文单词的<em class="li"> C </em>，分类器的目标是调整这些嵌入，使得我们最大化正样本嵌入的相似性(点积),并且最小化负样本嵌入的相似性。典型地，逻辑回归将解决这个任务。或者，使用带有softmax的神经网络，如下所示。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ll"><img src="../Images/4cc2e501f9fd95562ce9e54383699248.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/0*5IWgXl8Ma-nEyAY5.png"/></div></figure><h1 id="ac48" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">带单词编码的算法</h1><p id="429a" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">令人惊讶的是，Word2vec嵌入允许探索单词之间有趣的数学关系。例如，如果我们从单词“king”的向量中减去单词“man”的向量，然后加上单词“woman”的向量，我们就获得了单词“queen”的向量。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lm"><img src="../Images/a3aedea631aed1d87ff7c131b04caebb.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*Sl1pBb0UfVkoyKs66kBD3g.png"/></div></figure><p id="c424" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们还可以使用嵌入向量元素的元素式加法来询问诸如“德国+航空公司”之类的问题。通过查看与合成向量最接近的单词，我们得出了令人印象深刻的答案，如下所示。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ln"><img src="../Images/69c206666cd52eb4279f9648b8ccde2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*87AatM8VU-ggZZ_p8IFbuA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by <a class="ae jn" href="https://arxiv.org/abs/1310.4546" rel="noopener ugc nofollow" target="_blank">Mikolov et. al.</a></figcaption></figure><p id="f3ea" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">当在不同语言的可比语料库上训练时，Word2vec向量往往具有相似的结构，这允许我们执行机器翻译。下面的例子展示了英语和西班牙语单词相似的空间分布。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lo"><img src="../Images/bd1d7f2553d641906961398a4c0f04fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/0*omtogM9VTf1QtCnM.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by <a class="ae jn" href="https://link.springer.com/article/10.1007/s13347-020-00393-9" rel="noopener ugc nofollow" target="_blank">Juan Luis Gastaldi</a></figcaption></figure><p id="3937" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">嵌入有助于搜索引擎。他们可以通过比较向量并查看向量之间的角度来找到与给定查询最接近的单词。他们还可以检测单词或同义词之间的性别和复数-单数关系。单词向量也可以用于从庞大的数据集中导出单词类别，例如通过在单词向量的顶部执行K-means聚类。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lp"><img src="../Images/72de3c6948d7635b0835d169a1370ce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*B-GgNU4QGiurRCS6.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by <a class="ae jn" href="https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space" rel="noopener ugc nofollow" target="_blank">developers.google.com</a></figcaption></figure><h1 id="bc34" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">结论</h1><p id="ccc0" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">在本文中，我们讨论了Word2vec，它属于本地上下文窗口方法。它们可以很好地处理少量的训练数据，甚至可以表示被认为是罕见的单词。然而，它们没有充分利用关于单词共现的全局统计信息。</p><p id="4e58" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><a class="ae jn" href="https://github.com/facebookresearch/fastText" rel="noopener ugc nofollow" target="_blank"> FastText </a>是Word2vec的扩展，包括字符<a class="ae jn" href="https://en.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank"> n-grams </a>。<a class="ae jn" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank"> GloVe (Global Vectors) </a>通过采用不同于Word2vec的方法提供了一些改进。上下文相关的方法包括<a class="ae jn" href="https://allennlp.org/elmo" rel="noopener ugc nofollow" target="_blank"> ELMo </a>。变形金刚构建了当前最先进的方法，由<a class="ae jn" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>、<a class="ae jn" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">伯特</a>、<a class="ae jn" href="https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/" rel="noopener ugc nofollow" target="_blank"> CTRL </a>使用。</p></div></div>    
</body>
</html>
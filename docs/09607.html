<html>
<head>
<title>PCA Implementation in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python实现PCA</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/ca-pca-implementation-in-python-151fe466a4b4?source=collection_archive---------1-----------------------#2021-12-19">https://medium.com/geekculture/ca-pca-implementation-in-python-151fe466a4b4?source=collection_archive---------1-----------------------#2021-12-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/daab67387928cf026094a9336ebd57c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*etnIn95VRFKuJ4FeAXPhkA.png"/></div></div></figure><p id="e446" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">PCA或<em class="jo">主成分分析</em>是一种古老的机器学习算法，它的主要用途是用于<em class="jo">维度缩减。</em> PCA是一种数学技术，允许您从给定的数据集中设计新的要素，使新的要素在尺寸上更小，但能够表示原始要素，因此这些减少的要素(新的要素)可以传递给机器学习模型，并仍然可以获得合理的结果，同时大大降低复杂性。</p><p id="f2d2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在第一段中有很多大的术语，如果你现在不太明白，不要担心，因为我们将一步一步地看到PCA的作用。</p><h2 id="9d69" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated">什么是PCA？</h2><p id="f2ad" class="pw-post-body-paragraph iq ir hi is b it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj ko jl jm jn hb bi translated">让我们来分解术语本身；</p><p id="708b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">本金</em> </strong> <em class="jo">:反映重要性</em></p><p id="2228" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">成分</em> </strong> <em class="jo">:某物</em>的一部分</p><p id="a9cb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">分析</em> </strong> <em class="jo">:分析某事</em></p><p id="bb85" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以合起来就是<strong class="is hj">寻找或分析某个实体最重要的部分</strong>。在机器学习中，实体是数据，PCA的工作是从数据中提取最重要的特征。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/6d9e22c63ba6589bf08a547328220012.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MXM57jKbcVosAu7GC2lcPA.png"/></div></div></figure><p id="2c41" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里需要澄清一点，PCA并没有像大多数人错误认为的那样丢弃任何数据，它创建了给定数据的线性组合，使得结果数据非常接近(<em class="jo">如果不是精确的</em>)原始数据的表示。</p><h2 id="c174" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated">机器学习为什么需要PCA？</h2><p id="40a1" class="pw-post-body-paragraph iq ir hi is b it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj ko jl jm jn hb bi translated">PCA用于对抗高维数据出现的问题——也称为<a class="ae kt" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank"> <em class="jo">高维诅咒</em> </a> <em class="jo">。</em> <strong class="is hj">维度</strong>指数据集中每个示例中的要素数量。以下面的虚拟数据为例。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b334f3a7c3b09807d5e8f32f1aad1c8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ekTF4E3JR-Kdf97pclyhOg.png"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx">3-dimensional data</figcaption></figure><p id="59af" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">列<strong class="is hj">年龄、体重、性别</strong>是数据的输入特征。这意味着我们的数据有3个输入要素。现在假设我们有100个人的数据，那么我们的总输入数据点就变成了<em class="jo"> 100 </em> x <em class="jo"> 3 = 300。</em>现在，假设我们向数据集添加了另一个特征，<strong class="is hj">练习</strong>。现在我们的总数据点增加到<em class="jo"> 100 </em> x <em class="jo"> 4 = 400。</em>我们刚刚通过添加一个功能将数据集增加了100个点。当数据集增加到数千个时，这就成了一个真正的问题。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d18ad3c23734bb8ba230d53864ab5b02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qXfgUG_rE9VmAMkGTGBSqw.png"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx">4-dimensional data</figcaption></figure><p id="3fc6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这不仅增加了对该数据的训练时间，而且降低了数据覆盖真实世界的所有可能组合的概率。</p><p id="fc06" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用主成分分析，我们可以降低数据的维数，同时保留完整数据集描述的信息。</p><h2 id="16d0" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated"><strong class="ak"> PCA实施(一步一步)</strong></h2><p id="c775" class="pw-post-body-paragraph iq ir hi is b it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj ko jl jm jn hb bi translated">为了从给定的数据集中找到主成分，执行以下步骤:</p><ul class=""><li id="448f" class="ky kz hi is b it iu ix iy jb la jf lb jj lc jn ld le lf lg bi translated"><strong class="is hj">归一化</strong>原始数据集</li><li id="404c" class="ky kz hi is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">计算<strong class="is hj">归一化数据点</strong>之间的<strong class="is hj">协方差矩阵</strong>。</li><li id="f5c5" class="ky kz hi is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">计算<strong class="is hj">协方差矩阵的<strong class="is hj">特征值</strong>和<strong class="is hj">特征向量</strong>。</strong></li><li id="2a4f" class="ky kz hi is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">根据<strong class="is hj">特征值选择<strong class="is hj">前N个特征向量</strong>作为你的<strong class="is hj">主成分</strong>。</strong></li><li id="5b2d" class="ky kz hi is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">使用<strong class="is hj">特征向量</strong>将原始数据转换到一个新的(l <strong class="is hj">低维</strong>空间。</li></ul><p id="1656" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们在真实数据集上进行Python实现。</p><p id="497e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用的数据集可以在<a class="ae kt" href="https://www.kaggle.com/kukuroo3/body-performance-data" rel="noopener ugc nofollow" target="_blank"> Kaggle这里</a>找到。</p><p id="a1c4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为数据分析加载最重要的库。</p><pre class="kp kq kr ks fd lm ln lo lp aw lq bi"><span id="5485" class="jp jq hi ln b fi lr ls l lt lu">import pandas as pd<br/>import numpy as np</span></pre><p id="6c20" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">…现在加载数据集。</p><pre class="kp kq kr ks fd lm ln lo lp aw lq bi"><span id="5dcf" class="jp jq hi ln b fi lr ls l lt lu">data = pd.read_csv("archive/bodyPerformance.csv")<br/>data.head()</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lv"><img src="../Images/5cddf7e9718bdc8588e0de61f4d87252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Pm4GtzhAr8weUQe05g6Ew.png"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx">Complete Dataset to be use for PCA calculation</figcaption></figure><p id="3c3a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们只需要输入变量和数值变量，因此我们将删除输出'<strong class="is hj"> class </strong>和分类变量'<strong class="is hj"> Age </strong>'。</p><pre class="kp kq kr ks fd lm ln lo lp aw lq bi"><span id="3b38" class="jp jq hi ln b fi lr ls l lt lu">data.drop(columns = ["gender", "class"], inplace = True)<br/>data.head()</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/9dc2f5e1e4f7fb6e32bde624ce480a1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5o3OGFX-CAMR_YJNBCtRpA.png"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx">dataset with only numerical variables</figcaption></figure><p id="0d86" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在进行第一步<strong class="is hj"><em class="jo"/></strong></p><h2 id="b618" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated">数据标准化</h2><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/0fcdb9a3af0e8d206706b80ad6a9eb75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xIzd5VM0GMMRujv8NRJ1Qg.png"/></div></div></figure><p id="269c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">数据标准化是将数据的平均值降低到0并将标准偏差降低到1的过程。将对数据中的每个特征(列)执行这两个步骤。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/fdb9b663815c6388c370f8373b89ed7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*C2wHKi5aPqBtlX28-L0Kfg.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx">Data Normalization formula</figcaption></figure><pre class="kp kq kr ks fd lm ln lo lp aw lq bi"><span id="0fe0" class="jp jq hi ln b fi lr ls l lt lu">def data_normalisation(scaled_data):<br/>    for col in scaled_data.columns: #iterate over each column<br/>        scaled_data[col] = (scaled_data[col]-scaled_data[col].mean())/scaled_data[col].std() #data normalisation <br/>    <br/>    return scaled_data</span></pre><p id="7b1f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这将返回一个缩放数据集，如下所示。</p><pre class="kp kq kr ks fd lm ln lo lp aw lq bi"><span id="0725" class="jp jq hi ln b fi lr ls l lt lu">scaled_data = data_normalisation(data)<br/>print(scaled_data)</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ly"><img src="../Images/48f3e4e3995759400b693d1b7f66d88f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hWo0qxxxDTiU1POcmF4Lbw.png"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx">Normalized data</figcaption></figure><h2 id="ff3a" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated">协方差计算</h2><p id="8474" class="pw-post-body-paragraph iq ir hi is b it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj ko jl jm jn hb bi translated">协方差是一个变量相对于另一个变量的变化的度量。例如，如果我们有两个变量X和Y，那么这两个变量之间的协方差会告诉我们一个变量相对于另一个变量如何变化。协方差是一个可以取任何值(负的或正的)的整数。重要的不是整数，而是符号。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d4a04621b7bb7c6f41e3d27aa9d557d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TvtJM4jeLyQATsKwoqKG2Q.png"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx">Data correlation</figcaption></figure><ul class=""><li id="0b4c" class="ky kz hi is b it iu ix iy jb la jf lb jj lc jn ld le lf lg bi translated"><strong class="is hj">负协方差</strong>意味着两个变量的移动方向相反，即它们成反比。</li><li id="87e5" class="ky kz hi is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated"><strong class="is hj">正协方差</strong>意味着它们向同一方向移动，即它们彼此成正比</li></ul><p id="6f7a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们有下面的公式来计算协方差。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/b500a0bc1174b140a51243fbc0f5cb74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m9f9Z8b7Is5ddbSUcbsQPw.png"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx">Covariance formula for 2 variables X and Y</figcaption></figure><p id="69aa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是我们用python编写的代码。</p><pre class="kp kq kr ks fd lm ln lo lp aw lq bi"><span id="9710" class="jp jq hi ln b fi lr ls l lt lu">def covariance_calculation(mean_subtracted):<br/>    #calcualte covariance amongst scaled values<br/>    for col in mean_subtracted.columns:<br/>        mean_subtracted[col] = mean_subtracted[col]-mean_subtracted[col].mean()<br/>        <br/>    return np.dot(mean_subtracted.T,mean_subtracted)/(len(mean_subtracted) - 1)</span><span id="b707" class="jp jq hi ln b fi ma ls l lt lu">cov_calc <strong class="ln hj">=</strong> covariance_calculation(scaled_data) #calling the above function</span></pre><blockquote class="mb mc md"><p id="61d5" class="iq ir jo is b it iu iv iw ix iy iz ja me jc jd je mf jg jh ji mg jk jl jm jn hb bi translated">注意，我们已经使用<strong class="is hj">矢量化</strong>来计算上面返回语句中的协方差矩阵。这是python pro的一个技巧，当你必须对大量数字执行数学运算时，总是分析你是否可以通过<strong class="is hj">向量/矩阵运算</strong>来完成，这比使用循环要快得多，也更节省内存。</p></blockquote><h2 id="7719" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated">特征向量计算</h2><p id="1da4" class="pw-post-body-paragraph iq ir hi is b it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj ko jl jm jn hb bi translated">特征值和特征向量是微积分中非常重要的一部分，但是解释它们是什么以及它们是如何工作的远远超出了本文的范围。如果你想深入了解它们，你可以在网上找到一些有用的资源。</p><div class="mh mi ez fb mj mk"><a href="https://writersbyte.com/programming/data-visualization-in-python-using-matplotlib/?swcfpc=1" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab dw"><div class="mm ab mn cl cj mo"><h2 class="bd hj fi z dy mp ea eb mq ed ef hh bi translated">使用MatPlotLib - WritersByte在Python中进行数据可视化和分析</h2><div class="mr l"><h3 class="bd b fi z dy mp ea eb mq ed ef dx translated">人们经常提出的一个问题是“数据科学vs数据分析”。我们已经在许多其他方面讨论了数据科学…</h3></div><div class="ms l"><p class="bd b fp z dy mp ea eb mq ed ef dx translated">writersbyte.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my io mk"/></div></div></a></div><p id="a105" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对本征向量最简单的解释是，它是N维空间中的一个向量，使得一个特定的矩阵变换(比如说<strong class="is hj"><em class="jo"/></strong>)<strong class="is hj"><em class="jo">不会引起这个特定向量的任何旋转。</em> </strong>这个<strong class="is hj">特征向量</strong>是，特别是对于变换<strong class="is hj"> <em class="jo"> A，</em> </strong>如果还有另一个变换<strong class="is hj"> <em class="jo"> B </em> </strong>它在同一个N维空间中会有不同的特征向量与之对应。</p><p id="5693" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">即使没有发生旋转，该特征向量也会按某个因子进行缩放，该因子被称为该向量的<strong class="is hj"> <em class="jo">特征值</em> </strong>。</p><p id="ef19" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如我之前提到的，特征向量和特征值背后的深入概念超出了本文的范围，因此我们将使用<strong class="is hj"> <em class="jo"> eig </em> </strong>模块来计算这两个实体。</p><pre class="kp kq kr ks fd lm ln lo lp aw lq bi"><span id="c08f" class="jp jq hi ln b fi lr ls l lt lu">from numpy.linalg import eig</span><span id="2487" class="jp jq hi ln b fi ma ls l lt lu">w,v=eig(cov_calc)</span></pre><p id="30a8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo"> w </em> </strong>包含特征值<strong class="is hj"> <em class="jo"> v </em> </strong>包含相应的特征向量。</p><p id="0d72" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">每个特征向量的重要性由其对应的特征值来描述。更高的特征值意味着由特征向量描述的数据分布更广，这正是我们想要的，所以让我们绘制并查看特征值的趋势。</p><pre class="kp kq kr ks fd lm ln lo lp aw lq bi"><span id="ee67" class="jp jq hi ln b fi lr ls l lt lu">import matplotlib.pyplot as plt</span><span id="50f2" class="jp jq hi ln b fi ma ls l lt lu">plt.bar(["e" + str(i+1) for i in range(len(w))], w)<br/>plt.title("Eigenvalues")<br/>plt.xlabel("Magnitude of the eigenvalue")</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/946d15a8675d2cb28b0dcb2ad6d438ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*EgOrQGTejjuliP-7IrCGmQ.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx">Plot of the eigenvalues calculated</figcaption></figure><p id="ea8b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">看起来前4或5个特征向量(e1到e5)应该足够给我们一个数据的表示了。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/3196448a12b4f415f828b6639bd9e3a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b-euUhtv-LiAx4ZQ4bvr1g.png"/></div></div></figure><p id="f2fe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们可以用它把我们的原始数据转换到一个5维的空间(基本上降低到5维)。我们可以简单地通过计算数据集和这些特征向量之间的点积来实现。</p><pre class="kp kq kr ks fd lm ln lo lp aw lq bi"><span id="e6e6" class="jp jq hi ln b fi lr ls l lt lu">useful_pc = v[:,:5] #keeping only the first 5 eigenvectors</span><span id="6ffc" class="jp jq hi ln b fi ma ls l lt lu">principle_data = np.dot(data.values, useful_pc)#dot product of the two terms</span><span id="10eb" class="jp jq hi ln b fi ma ls l lt lu">print(priciple_data)<br/>print("Shape of the New data is:", principle_data.shape)</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/8ee7017a84d42decdab199eba1a4c2d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*z13l36UBdK1_fHDjJmLjYw.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx">Original data transformed into a 5-dimensional space</figcaption></figure><p id="f18f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">搞定了。！</p><p id="6b0f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这就是我们如何将原始数据集(包含10个要素)转换为更小的数据集，但仍然表示原始数据。</p></div></div>    
</body>
</html>
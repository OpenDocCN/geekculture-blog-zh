<html>
<head>
<title>Meta’s new model can turn text prompt into videos</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Meta的新模型可以将文本提示转化为视频</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/metas-new-model-can-turn-text-prompt-into-videos-b6dadb94ab3b?source=collection_archive---------7-----------------------#2022-10-04">https://medium.com/geekculture/metas-new-model-can-turn-text-prompt-into-videos-b6dadb94ab3b?source=collection_archive---------7-----------------------#2022-10-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="91b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">制作视频——生成艺术的新突破</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/644145250dc590a1dbd07d159efccb12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KQQZ6vkcjDllcP62ZjQe4w.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">image generated by the author using OpenAI’s <a class="ae jt" href="https://arxiv.org/pdf/2204.06125.pdf" rel="noopener ugc nofollow" target="_blank">DALL-E 2</a></figcaption></figure><p id="0209" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">几个月前，DALL-E2震惊了世界，它能够在几秒钟内将文本提示变成图像。上周，Meta发布了其最新的视频制作模型，该模型可以将文本提示转化为几秒钟的短视频。</p><p id="cbe2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">生成艺术正在迅速发展</strong></p><p id="acdf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">DALL-E 2是OpenAI在4月份公布的。新模型似乎能够以令人难以置信的精确度从文本中生成图像。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ju"><img src="../Images/a917372bfe5204bd393e1bd6de558d12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qsxYqaslZ3i1dj4zuTsOFA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Image source: <a class="ae jt" href="https://cdn.openai.com/papers/dall-e-2.pdf" rel="noopener ugc nofollow" target="_blank">DALL-E2 original article</a></figcaption></figure><p id="53fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">结果令世界震惊，因为似乎还需要一段时间才能克服它。相反，此后不久，谷歌发布了两款能够超越DALLE2的机型。<a class="ae jt" href="https://imagen.research.google/" rel="noopener ugc nofollow" target="_blank">谷歌的Imagen </a>和<a class="ae jt" href="https://parti.research.google/" rel="noopener ugc nofollow" target="_blank"> Parti </a>在短时间内发布，它们也没有保持最先进的状态很长时间。事实上，几个月前发布了稳定扩散。</p><p id="5b16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在接下来的几个月里，生成领域发展迅速，许多模型都以开源方式开放或发布。最近<a class="ae jt" href="https://techcrunch.com/2022/09/28/openai-removes-the-waitlist-for-dall-e-2-allowing-anyone-to-sign-up/" rel="noopener ugc nofollow" target="_blank"> DALL-E2向公众开放</a>(不再有等待名单)并宣布了呼出能力。DALL-E mini已经被纳入HuggingFace，甚至Photoshop现在也纳入了稳定扩散。</p><p id="1f94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有这些模型都将一个简短的文本描述作为输入，并生成一个图像作为输出。另一方面，Meta宣布了一种可以将文本提示转化为短视频的模式</p><h1 id="a1ef" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">制作视频</strong></h1><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="kt ku l"/></div></figure><p id="683a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">制作视频于上周公布。Meta研究人员还发表了一篇科学论文(<a class="ae jt" href="https://arxiv.org/pdf/2209.14792.pdf" rel="noopener ugc nofollow" target="_blank"> arXiv链接</a>)，详细描述了这个模型。Meta最近一直在研究生成模型，事实上，它已经在今年发布了<a class="ae jt" href="https://ai.facebook.com/blog/greater-creative-control-for-ai-image-generation/" rel="noopener ugc nofollow" target="_blank"> Make-A-scene </a>，这是一个能够使用单词、文本行和自由草图来创建照片般逼真的插图的模型。</p><p id="201c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">简要技术描述</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kv"><img src="../Images/c46521899d8120ba9e682ab3d7adb17a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J1W7IXlX3pnUuNEubS-A4A.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Image source: <a class="ae jt" href="https://arxiv.org/pdf/2209.14792.pdf" rel="noopener ugc nofollow" target="_blank">Meta’s original article</a></figcaption></figure><p id="66f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该模型由三个主要部分构成:</p><ul class=""><li id="21b2" class="kw kx hi ih b ii ij im in iq ky iu kz iy la jc lb lc ld le bi translated">一个基本的文本到图像模型，在文本-图像对上跟踪。模型的这一部分与我们目前在其他文本到图像模型中看到的非常相似(正如他们在文章中所写的，它与用于DALL-E2的模型非常相似)</li><li id="27f1" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">时空卷积和注意力层允许我们切换到时间维度。事实上，作者修改了卷积和注意力层，以便能够在不增加太多计算成本的情况下，从图像切换到时间维度。</li><li id="e71d" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">允许生成高帧速率的时空网络(这部分网络用于增加视频中的帧数，以获得更平滑的视频)</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lk"><img src="../Images/2f01a5913685ed5ab1222961a8cc9092.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tgyI7JBae6FdTvmPeGI6OQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">from the article: The architecture and initialization scheme of the Pseudo-3D convolutional and attention layers, enabling the seamless transition of a pre-trained Text-to-Image model to the temporal dimension.</figcaption></figure><p id="4458" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为一个数据集，他们使用了23亿张图像(Laion 5b数据集的子集，这是一个包含图像示例和相关文本的庞大数据集)。作者描述，他们过滤了带有有毒文字的图像或水印概率大于0.5的r图像。他们还使用了WebVid-10M和HD-VILA-100M或训练他们的视频生成模型。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ll"><img src="../Images/dc352fe19c474bbce0089d7e2d1c3739.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UG-bgrlfseS09OvwKF1z2g.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">T2V generation examples. image source: original article</figcaption></figure><p id="0e0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作者解释说，使用标记图像的数据集有助于向模型解释对象的名称和外观。此外，视频的使用使模型能够理解这些物体在世界上是如何运动的。这种训练模型的新方法被证明在教导模型如何从文本生成视频方面是有效的。</p><blockquote class="lm ln lo"><p id="c699" class="if ig lp ih b ii ij ik il im in io ip lq ir is it lr iv iw ix ls iz ja jb jc hb bi translated">使用无监督学习从数量级更多的视频中学习世界动态有助于研究人员摆脱对标记数据的依赖。目前的工作已经展示了标记图像如何与未标记的视频片段有效结合来实现这一点。—原始文章</p></blockquote><p id="975f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">模型能做什么？</strong></p><blockquote class="lt"><p id="fdf5" class="lu lv hi bd lw lx ly lz ma mb mc jc dx translated">制作视频可以让你用几个单词或几行文字制作出异想天开、独一无二的视频，从而让你的想象力发挥到极致。— Meta制作视频<a class="ae jt" href="https://makeavideo.studio/?fbclid=IwAR2qmh03xaed6H3VsOtnp3pBrjAh2F5VHdQhdt77wY2B_grZQOkj5z7XonA" rel="noopener ugc nofollow" target="_blank">网站</a></p></blockquote><p id="9aa2" class="pw-post-body-paragraph if ig hi ih b ii md ik il im me io ip iq mf is it iu mg iw ix iy mh ja jb jc hb bi translated">Meta简要描述了其新模型的功能:</p><ul class=""><li id="0ce5" class="kw kx hi ih b ii ij im in iq ky iu kz iy la jc lb lc ld le bi translated">从文本生成视频，提供一个简短的文本模型能够生成一个几秒钟的小视频</li><li id="6d3f" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">给图像添加运动，提供一个输入图像，模型返回一个短视频</li><li id="b40a" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">创建视频的变体，用户可以提供一个小视频并生成变体</li></ul><p id="61c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">文章的作者还描述了未来的步骤:</p><blockquote class="lm ln lo"><p id="6a49" class="if ig lp ih b ii ij ik il im in io ip lq ir is it lr iv iw ix ls iz ja jb jc hb bi translated">下一步，我们计划解决几个技术限制。如前所述，我们的方法无法学习文本和现象之间的关联，这些关联只能在视频中推断。如何结合这些(例如，生成一个人从左到右或从右到左挥手的视频)，以及生成具有多个场景和事件的更长的视频，描绘更详细的故事，留待未来的工作来完成。——<a class="ae jt" href="https://arxiv.org/pdf/2209.14792.pdf" rel="noopener ugc nofollow" target="_blank">原创文章</a></p></blockquote><p id="e6d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">事实上，有些视频相当奇怪，无论是动作还是细节都远非完美。运动不是很流畅，有时物体以一种超现实的方式相互作用(好像模型不能理解物体的边界和它们应该如何相互作用)</p><p id="7a45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，研究人员试图从训练数据集中消除带有相关有毒单词的图像。决定也使用公共数据集的动机是增加训练过程的透明度，以便任何人都可以检查所使用的数据。然而，提交人指出:</p><blockquote class="lm ln lo"><p id="b9ae" class="if ig lp ih b ii ij ik il im in io ip lq ir is it lr iv iw ix ls iz ja jb jc hb bi translated">正如所有基于网络数据训练的大规模模型一样，我们的模型已经学会并可能夸大了社会偏见，包括有害的偏见</p></blockquote><p id="2ec8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">离别的思念</strong></p><p id="eb1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">制作视频只是一个开始，正如在其他案例中看到的那样，对<a class="ae jt" href="https://towardsdatascience.com/googles-pali-language-image-learning-in-100-languages-31d32f9b74fe" rel="noopener" target="_blank">多模态模型的研究是未来几年的趋势</a>。几个能够从文本生成高质量图像的模型已经在一年内问世，因此生成短视频可能只是时间问题。</p><p id="967a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然这是意料之中的，但该模型提出了几个技术上有趣的解决方案，也旨在降低每个提示生成几个帧的计算成本。</p><p id="ac6e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如前面提到的模型一样，这些生成模型仍然有几个未解决的伦理问题。第一个问题是这些数据集是使用爬虫组装的。事实上，它们包含了许多艺术家的作品，没有经过许可，也没有提供鸣谢。第二个问题是，它们通常包含偏见和带有有毒含义的图像。OpenAI Dall-E的第一个版本包含了一些作者难以纠正的偏见。</p><p id="2269" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些模型在深度伪造视频生成中可能是危险的，这就是为什么Meta在所有生成的视频中添加了水印。</p><p id="2392" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梅塔说，这种模式为创作者和艺术家开辟了新的机会。技术层面的结果令人鼓舞，尽管目前第一个模型仅限于短片。目前，在潜在风险减轻之前，Meta没有计划将其公之于众。</p><h1 id="bcb0" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">如果你觉得有趣:</h1><p id="3ed9" class="pw-post-body-paragraph if ig hi ih b ii mi ik il im mj io ip iq mk is it iu ml iw ix iy mm ja jb jc hb bi translated">你可以寻找我的其他文章，你也可以<a class="ae jt" href="https://salvatore-raieli.medium.com/subscribe" rel="noopener"> <strong class="ih hj">订阅</strong> </a>在我发表文章时获得通知，你也可以在<strong class="ih hj"/><a class="ae jt" href="https://www.linkedin.com/in/salvatore-raieli/" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">LinkedIn</strong></a><strong class="ih hj">上连接或联系我。</strong>感谢您的支持！</p><p id="d11f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是我的GitHub知识库的链接，我计划在这里收集代码和许多与机器学习、人工智能等相关的资源。</p><div class="mn mo ez fb mp mq"><a href="https://github.com/SalvatoreRa/tutorial" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab dw"><div class="ms ab mt cl cj mu"><h2 class="bd hj fi z dy mv ea eb mw ed ef hh bi translated">GitHub - SalvatoreRa/tutorial:关于机器学习、人工智能、数据科学的教程…</h2><div class="mx l"><h3 class="bd b fi z dy mv ea eb mw ed ef dx translated">关于机器学习、人工智能、数据科学的教程，包括数学解释和可重复使用的代码(python…</h3></div><div class="my l"><p class="bd b fp z dy mv ea eb mw ed ef dx translated">github.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne jn mq"/></div></div></a></div><p id="996d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">或者随意查看我在Medium上的其他文章:</p><div class="mn mo ez fb mp mq"><a href="https://towardsdatascience.com/alphafold2-year-1-did-it-change-the-world-499a5a38130a" rel="noopener follow" target="_blank"><div class="mr ab dw"><div class="ms ab mt cl cj mu"><h2 class="bd hj fi z dy mv ea eb mw ed ef hh bi translated">AlphaFold2第一年:它改变了世界吗？</h2><div class="mx l"><h3 class="bd b fi z dy mv ea eb mw ed ef dx translated">DeepMind向我们承诺了一场革命，它发生了吗？</h3></div><div class="my l"><p class="bd b fp z dy mv ea eb mw ed ef dx translated">towardsdatascience.com</p></div></div><div class="mz l"><div class="nf l nb nc nd mz ne jn mq"/></div></div></a></div><div class="mn mo ez fb mp mq"><a href="https://towardsdatascience.com/a-critical-analysis-of-your-dataset-2b388e7ca01e" rel="noopener follow" target="_blank"><div class="mr ab dw"><div class="ms ab mt cl cj mu"><h2 class="bd hj fi z dy mv ea eb mw ed ef hh bi translated">对数据集的批判性分析</h2><div class="mx l"><h3 class="bd b fi z dy mv ea eb mw ed ef dx translated">停止微调你的模型:你的模型已经很好了，但不是你的数据</h3></div><div class="my l"><p class="bd b fp z dy mv ea eb mw ed ef dx translated">towardsdatascience.com</p></div></div><div class="mz l"><div class="ng l nb nc nd mz ne jn mq"/></div></div></a></div><div class="mn mo ez fb mp mq"><a href="https://towardsdatascience.com/speaking-the-language-of-life-how-alphafold2-and-co-are-changing-biology-97cff7496221" rel="noopener follow" target="_blank"><div class="mr ab dw"><div class="ms ab mt cl cj mu"><h2 class="bd hj fi z dy mv ea eb mw ed ef hh bi translated">说生命的语言:AlphaFold2和公司如何改变生物学</h2><div class="mx l"><h3 class="bd b fi z dy mv ea eb mw ed ef dx translated">人工智能正在重塑生物学研究，并开辟治疗的新领域</h3></div><div class="my l"><p class="bd b fp z dy mv ea eb mw ed ef dx translated">towardsdatascience.com</p></div></div><div class="mz l"><div class="nh l nb nc nd mz ne jn mq"/></div></div></a></div><div class="mn mo ez fb mp mq"><a href="https://towardsdatascience.com/blending-the-power-of-ai-with-the-delicacy-of-poetry-3671f82d2e1" rel="noopener follow" target="_blank"><div class="mr ab dw"><div class="ms ab mt cl cj mu"><h2 class="bd hj fi z dy mv ea eb mw ed ef hh bi translated">融合人工智能的力量和诗歌的细腻</h2><div class="mx l"><h3 class="bd b fi z dy mv ea eb mw ed ef dx translated">人工智能现在能够从文本中生成图像，如果我们给它们提供伟大诗人的话语会怎么样？梦幻之旅…</h3></div><div class="my l"><p class="bd b fp z dy mv ea eb mw ed ef dx translated">towardsdatascience.com</p></div></div><div class="mz l"><div class="ni l nb nc nd mz ne jn mq"/></div></div></a></div></div></div>    
</body>
</html>
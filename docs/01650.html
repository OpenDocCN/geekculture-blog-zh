<html>
<head>
<title>Decision Tree with CART Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带CART算法的决策树</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/decision-trees-with-cart-algorithm-7e179acee8ff?source=collection_archive---------0-----------------------#2021-04-19">https://medium.com/geekculture/decision-trees-with-cart-algorithm-7e179acee8ff?source=collection_archive---------0-----------------------#2021-04-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9e29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树是最基本的机器学习算法之一，具有广泛的用例，易于解释和实现。我们可以在回归和分类任务中使用决策树。在本文中，我们将尝试理解决策树算法的基础。然后，如何使用CART算法从训练数据集中生成决策树。</p><h2 id="cd57" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">关于<a class="ae jy" href="https://en.wikipedia.org/wiki/Decision_tree#Overview" rel="noopener ugc nofollow" target="_blank">决策树</a>T2:</h2><p id="a15b" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">决策树是一种<a class="ae jy" href="https://www.geeksforgeeks.org/difference-between-parametric-and-non-parametric-methods/" rel="noopener ugc nofollow" target="_blank">非参数</a>监督学习技术，它是一个由多个决策规则组成的树，所有这些规则都将从数据特征中导出。这是最容易理解的可解释的机器学习算法之一。这种ML算法是随机森林最基本的组成部分，也是最流行的&amp;强大的ML算法。</p><ol class=""><li id="b1a1" class="ke kf hi ih b ii ij im in iq kg iu kh iy ki jc kj kk kl km bi translated"><strong class="ih hj">决策树的结构</strong>:</li></ol><ul class=""><li id="77e6" class="ke kf hi ih b ii ij im in iq kg iu kh iy ki jc kn kk kl km bi translated">在下图中，我试着展示了决策树的样子。每个内部节点代表一个段或区域。关于树的类比，片段或区域是树的节点或叶子。</li></ul><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es ko"><img src="../Images/a3d71b1423f25123b57eeebd06a17410.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*BIZC6GTML_XBohD1VvpLRA.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx">Image 1 : Decision tree structure</figcaption></figure><ul class=""><li id="7d9d" class="ke kf hi ih b ii ij im in iq kg iu kh iy ki jc kn kk kl km bi translated"><strong class="ih hj">根节点</strong>:这是我们训练数据集的第一个节点。</li><li id="c579" class="ke kf hi ih b ii la im lb iq lc iu ld iy le jc kn kk kl km bi translated"><strong class="ih hj">内部节点</strong>:这是子组被分割成新的子组或叶节点的点。我们也可以称之为决策节点，因为这是基于子组的最佳属性进一步分裂的节点。</li><li id="f149" class="ke kf hi ih b ii la im lb iq lc iu ld iy le jc kn kk kl km bi translated"><strong class="ih hj">叶节点</strong>:任意内部节点的最后一个节点，保存决策。</li></ul><p id="cb0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。关于决策树中的节点？</strong></p><ul class=""><li id="a4f8" class="ke kf hi ih b ii ij im in iq kg iu kh iy ki jc kn kk kl km bi translated">如前所述，决策树是一种具有嵌套节点的树状结构，根据属性的阈值将一个节点拆分为另一个节点，我们将很快对此进行详细讨论。</li><li id="8699" class="ke kf hi ih b ii la im lb iq lc iu ld iy le jc kn kk kl km bi translated">决策树算法将训练集(根节点)分成子组- <strong class="ih hj">内部节点</strong> &amp;任何具有最终子组的内部节点都将是<strong class="ih hj">叶节点。</strong>我们可以称之为<a class="ae jy" href="https://en.wikipedia.org/wiki/Recursive_partitioning" rel="noopener ugc nofollow" target="_blank">递归分割</a>。</li></ul><blockquote class="lf lg lh"><p id="8d14" class="if ig li ih b ii ij ik il im in io ip lj ir is it lk iv iw ix ll iz ja jb jc hb bi translated">现在我们将更深入地理解“节点的分割是如何基于属性的阈值发生的？”</p></blockquote><p id="362d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于纯度将节点(根节点)分裂成子节点，决策树算法在为子节点找到最佳同质性的地方分裂节点。如果一个子节点拥有它的所有类成员，那么<a class="ae jy" href="https://discuss.analyticsvidhya.com/t/decision-tree-gini-impurity-purity/37650/3" rel="noopener ugc nofollow" target="_blank">同质性</a>将会更高。如果你的子节点具有5/5的类成员分布，那么当它是8/2或9/1时，同质性将是最低和最高的。</p><p id="fdbe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分割一个节点的决策树算法需要最佳的<strong class="ih hj">属性&amp;阈值</strong>。最佳属性&amp;阈值对(<strong class="ih hj"> f，t </strong>)的选择基于以下算法进行，这些算法将为您提供最纯净的节点。以下算法有助于找到最佳属性的测量值:</p><ul class=""><li id="16a3" class="ke kf hi ih b ii ij im in iq kg iu kh iy ki jc kn kk kl km bi translated">CART算法:<a class="ae jy" href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity" rel="noopener ugc nofollow" target="_blank">基尼指数</a></li><li id="f475" class="ke kf hi ih b ii la im lb iq lc iu ld iy le jc kn kk kl km bi translated">ID3算法:信息增益</li><li id="bda5" class="ke kf hi ih b ii la im lb iq lc iu ld iy le jc kn kk kl km bi translated">C4.5算法:增益比率</li></ul><p id="03ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将使用CART算法来创建决策树。</p><p id="372d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">购物车算法:</strong></p><p id="ef75" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该算法可用于分类和回归。CART算法使用基尼指数标准将一个节点拆分为一个子节点。它从作为根节点的训练集开始，在成功地将根节点一分为二之后，它使用相同的逻辑来分裂子集&amp;再次递归地分裂子子集，直到它发现进一步的分裂不会给出任何纯子节点或生长树中的最大数量的叶子，或者称之为树修剪。</p><p id="19d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">如何计算基尼指数？</strong></p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es lm"><img src="../Images/40940d860e20a39ac1e21f80f5ccbaee.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*raxh-80AQzObilrTOgLIjw.jpeg"/></div><figcaption class="kw kx et er es ky kz bd b be z dx">Image 2: Formula of Gini Index</figcaption></figure><p id="f096" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在基尼系数中，P是阶级<strong class="ih hj">I</strong>T22】存在总<strong class="ih hj"> c </strong>阶级的概率。</p><p id="514a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑到你只有两个预测因子/属性:湿度和风</p><p id="82c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">班级:雨天和晴天</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es ln"><img src="../Images/a057a67c9cacabc964d6a02bce8a9d0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RjBlF6QTHV42RMfOWn6mAg.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx">Image 3: Data &amp; it’s distribution by class</figcaption></figure><p id="7338" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GI = 1 —((来自特征1的观察值数量/总观察值)+(来自特征2的观察值数量/总观察值) )</p><p id="afc9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GI = 1-((6/10)+(4/10))= &gt; 1-(0.36+0.16)= &gt; 1–0.52 = &gt; 0.48</p><p id="ad31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，第一组/初始组的基尼指数为0.48</p><p id="d630" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">关于节点分割如何发生的基本想法:</strong></p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es ls"><img src="../Images/b1c7b9deeb25cda79f678cec4403d7a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xyUoQbse50Gho56jiPqRXQ.jpeg"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx">Image 4: Node splitting on Gini Index</figcaption></figure><p id="4bfe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于属性“风”(f)和阈值“3.55”(t)，CART算法创建了节点/子集，这些节点/子集将给出上述流右侧的纯子集(参考:图4)。</p><blockquote class="lf lg lh"><p id="307e" class="if ig li ih b ii ij ik il im in io ip lj ir is it lk iv iw ix ll iz ja jb jc hb bi translated">让我们理解我们如何选择最佳对(<strong class="ih hj"> f，t </strong>)来分割根节点:</p></blockquote><h2 id="b185" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">步骤1:从初始集合中找到最佳基尼指数/分数</h2><p id="ec55" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">我写了一小段代码来更好地理解它:</p><figure class="kp kq kr ks fd kt"><div class="bz dy l di"><div class="lt lu l"/></div></figure><p id="05ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，在加载数据之后，我们将找到初始集合或根节点的基尼系数，这将是best_gini系数:</p><figure class="kp kq kr ks fd kt"><div class="bz dy l di"><div class="lt lu l"/></div><figcaption class="kw kx et er es ky kz bd b be z dx">Utility function to calculate gini score</figcaption></figure><figure class="kp kq kr ks fd kt"><div class="bz dy l di"><div class="lt lu l"/></div></figure><p id="7806" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的代码中，我们得到了要执行的下一组指令的输入:</p><ul class=""><li id="87db" class="ke kf hi ih b ii ij im in iq kg iu kh iy ki jc kn kk kl km bi translated">行号2:特征数= 2</li><li id="959b" class="ke kf hi ih b ii la im lb iq lc iu ld iy le jc kn kk kl km bi translated">行号9:最佳属性(最佳属性)和阈值(最佳阈值)的占位符</li><li id="6179" class="ke kf hi ih b ii la im lb iq lc iu ld iy le jc kn kk kl km bi translated">行号11:初始设置的best_gini得分= 0.48(参考图4)</li></ul><p id="4873" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤2:从初始/训练集中找到最佳分割</strong></p><p id="6ee4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我将解释最重要的部分，我们将如何从初始集合中搜索最佳属性(f)和阈值(t)？</p><figure class="kp kq kr ks fd kt"><div class="bz dy l di"><div class="lt lu l"/></div></figure><ul class=""><li id="5bd3" class="ke kf hi ih b ii ij im in iq kg iu kh iy ki jc kn kk kl km bi translated">行号1:8 -该算法将循环“属性”的次数，即2次&amp;创建两个左右桶。左边一个没有分配任何值，而右边一个将所有排序的行值命名为阈值</li><li id="0cda" class="ke kf hi ih b ii la im lb iq lc iu ld iy le jc kn kk kl km bi translated">行号14:26 -两个初始化的桶(左和右)将进入下一个循环，该循环将迭代行数次-10，在每次迭代中，算法将从右到左分配每个类观察值，并每次计算基尼加权平均值new_gini</li><li id="1945" class="ke kf hi ih b ii la im lb iq lc iu ld iy le jc kn kk kl km bi translated">行号35:38 -如果new_gini低于best_gini，那么接下来我们将找到最佳属性&amp; threshold (f，t)</li></ul><p id="865c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代码流:算法选取每个属性，对其值列表(我们称之为阈值列表)进行排序，将每个观察值顺序移动到左侧存储桶，计算new_gini ( <strong class="ih hj"> Gini加权平均值</strong>)并将其与初始Best_Gini进行比较，如果new_gini得分较小，则将其作为最佳属性(f) &amp;阈值(t) &amp;再次对所有属性重复该过程。然后使用最佳属性(f) &amp;阈值(t)将节点分裂为子节点/子集。</p><blockquote class="lf lg lh"><p id="d5d8" class="if ig li ih b ii ij ik il im in io ip lj ir is it lk iv iw ix ll iz ja jb jc hb bi translated">下面的图片试图总结流程，这里绿色圆圈代表班级成员:晴天，红色三角形代表班级成员:雨天。</p></blockquote><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es lv"><img src="../Images/d21363ff0216960425dbad2203bc3c9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xor5PYt0QTGTdX5R0TOOjQ.jpeg"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx">Image 6: Flow to calculate new_gini</figcaption></figure><p id="cddc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">最佳分割方式示例</strong></p><p id="e893" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更好地解释最佳分割是如何发生的，考虑第7次迭代中的属性“湿度”,其分布如下:</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es lw"><img src="../Images/78ed09efe0448d35775a7a556f3ea519.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xi5Bp56oJ_ErYOXHN07r4g.jpeg"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx">Image 7: Flow for 7th iteration for “Humidity” attribute</figcaption></figure><p id="aa48" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分配步骤后的等式是CART成本函数或<strong class="ih hj">基尼加权平均值</strong>，它将给出新的基尼系数(new_gini):</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es lx"><img src="../Images/b38f8b9baaf8de4ad73fcafa79eab564.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*O8mxvMEROkiyiWTP8pljng.jpeg"/></div><figcaption class="kw kx et er es ky kz bd b be z dx">Image 5: Gini weighted average</figcaption></figure><p id="d69b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">new_gini分数将基于左右桶的等级分布:</p><ul class=""><li id="23f3" class="ke kf hi ih b ii ij im in iq kg iu kh iy ki jc kn kk kl km bi translated">左侧桶:2晴天和5雨天</li></ul><p id="7633" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">G_left = 1 —((晴天观测数/总观测数)+(雨天观测数/总观测数) )</p><p id="ddf9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">G_left = 1-((2/7) +(5/7) )) =&gt; 0.40</p><ul class=""><li id="f677" class="ke kf hi ih b ii ij im in iq kg iu kh iy ki jc kn kk kl km bi translated">右侧铲斗:2晴天和1雨天:</li></ul><p id="e35c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">G_right = 1 —((晴天的观测数/总观测数)+(雨天的观测数/总观测数) )</p><p id="5a34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">G_right = 1- ((2/3) +(1/3) )) =&gt; 0.44</p><p id="cc1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">左= 7，右= 3</p><p id="48e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">new _ Gini =(7 * 0.40)+(3 * 0.44)/10 = &gt; 0.41</p><p id="86f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其小于best_gini (0.48)，因此，我们将其视为最佳阈值&amp;属性:</p><p id="2ae3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">阈值列表= [1.5，1.5，1.5，1.6，3.4，3.9，4.6，4.7，5.0，5.1]</p><p id="7acb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们记得我们在第7次迭代中，best_threshold值将是= (4.7+4.6)/2=&gt;4.66</p><p id="5551" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">目前，最佳特征和阈值(f，t)为(0，4.66)，如果这是最小的基尼系数，则算法将根据属性“湿度”和阈值“4.66”来分割初始节点，如下图所示:</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es ly"><img src="../Images/c96575d7ee9908561d40663d461ffc5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yq8fPN6t_Qdh9rgCHgAHWg.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx">Image 8: Node splitting assumption</figcaption></figure><p id="1f5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但它<strong class="ih hj">不是最佳对</strong> (f，t)。对于所有可用的属性，上述过程将继续&amp;将继续搜索新的最低基尼系数，如果它发现它将保持其属性的阈值&amp;，稍后它将基于最佳属性阈值&amp;来分割节点。根据我们的数据集，“风”属性的最佳基尼系数为“0.40”(<strong class="ih hj">f</strong>)&amp;，“3.55”为最佳阈值(t)。由DecisionTreeClassifier使用scikit-learn生成的以下树显示了基于相同阈值&amp;属性发生的节点拆分:</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es lz"><img src="../Images/dd3922df550050b569d025fd472f8332.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*1hOmCP5cmJVtgJK7E8dWqg.jpeg"/></div><figcaption class="kw kx et er es ky kz bd b be z dx">Image 9: Tree Generated by DecisionTreeClassifier</figcaption></figure><p id="4bd2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们回顾一下:</p><ul class=""><li id="d57b" class="ke kf hi ih b ii ij im in iq kg iu kh iy ki jc kn kk kl km bi translated">我们学习了什么是决策树</li><li id="b015" class="ke kf hi ih b ii la im lb iq lc iu ld iy le jc kn kk kl km bi translated">创建新节点/子集的不同标准度量</li><li id="f80e" class="ke kf hi ih b ii la im lb iq lc iu ld iy le jc kn kk kl km bi translated">基尼指数</li><li id="d743" class="ke kf hi ih b ii la im lb iq lc iu ld iy le jc kn kk kl km bi translated">如何计算基尼指数</li><li id="103f" class="ke kf hi ih b ii la im lb iq lc iu ld iy le jc kn kk kl km bi translated">如何基于属性(<strong class="ih hj"> f </strong> ) &amp;阈值(<strong class="ih hj"> t </strong>)创建节点的示例</li><li id="4062" class="ke kf hi ih b ii la im lb iq lc iu ld iy le jc kn kk kl km bi translated">我们如何搜索最佳属性，阈值对(<strong class="ih hj"> f，t </strong>)</li><li id="4891" class="ke kf hi ih b ii la im lb iq lc iu ld iy le jc kn kk kl km bi translated">(f，t)对值如何帮助创建纯节点</li><li id="f0f9" class="ke kf hi ih b ii la im lb iq lc iu ld iy le jc kn kk kl km bi translated">决策树的工作代码</li></ul><p id="7fd9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">谢谢&amp;阅读愉快…</p><p id="8012" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><ol class=""><li id="028b" class="ke kf hi ih b ii ij im in iq kg iu kh iy ki jc kj kk kl km bi translated">图2 - <a class="ae jy" href="https://www.springer.com/in/book/9781461471370" rel="noopener ugc nofollow" target="_blank">统计学习介绍及在R中的应用</a></li><li id="3516" class="ke kf hi ih b ii la im lb iq lc iu ld iy le jc kj kk kl km bi translated">图5- <a class="ae jy" href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/" rel="noopener ugc nofollow" target="_blank">用Scikit-Learn进行动手机器学习&amp; Tensorflow </a></li><li id="b1c5" class="ke kf hi ih b ii la im lb iq lc iu ld iy le jc kj kk kl km bi translated">一些<a class="ae jy" href="https://gist.github.com/joachimvalente/9c221f1937a0e5b1726701467abbbbba#file-cart-py" rel="noopener ugc nofollow" target="_blank">代码引用</a></li></ol></div></div>    
</body>
</html>
<html>
<head>
<title>Deploying a TensorFlow Model to Kubernetes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将张量流模型部署到Kubernetes</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/deploying-a-tensorflow-model-to-kubernetes-444b36672207?source=collection_archive---------6-----------------------#2021-05-01">https://medium.com/geekculture/deploying-a-tensorflow-model-to-kubernetes-444b36672207?source=collection_archive---------6-----------------------#2021-05-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="823a" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">在微服务架构中使用人工智能的通用方法</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/ee2e63dd566eb2394a501cfcceb2f711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eard3lg28AAKKaP9slJ70A.png"/></div></div></figure><p id="8bf9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">假设您刚刚完成新TensorFlow模型的训练，并希望开始在您的应用程序中使用它。一个显而易见的方法是将它导入到使用它的每个应用程序的源代码中。然而，将您的模型保持在一个独立的地方，并简单地让应用程序通过HTTP调用与它交换数据，这可能更通用。本文将介绍构建这样一个系统的步骤，并将结果部署到Kubernetes以获得最大的可用性。</p><h1 id="64c6" class="kf kg hi bd kh ki kj kk kl km kn ko kp io kq ip kr ir ks is kt iu ku iv kv kw bi translated">张量流模型</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kx"><img src="../Images/4ece6cf14ee695552ef7d9702c89f449.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*oN7Mz9M-t-Fp3SE6PIbgxg.png"/></div></figure><p id="3100" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">首先，我们需要一个工作模型。这里，我们将使用<a class="ae ky" href="https://www.tensorflow.org/tutorials/quickstart/beginner" rel="noopener ugc nofollow" target="_blank"> TensorFlow入门指南</a>中的MNIST分类器:</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="f8d8" class="le kg hi la b fi lf lg l lh li"># Importing TensorFlow<br/>import tensorflow as tf</span><span id="e13b" class="le kg hi la b fi lj lg l lh li"># Loading the data<br/>mnist = tf.keras.datasets.mnist<br/>(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><span id="dee2" class="le kg hi la b fi lj lg l lh li"># Data preprocessing (here, normalization)<br/>x_train, x_test = x_train / 255.0, x_test / 255.0</span><span id="1d66" class="le kg hi la b fi lj lg l lh li"># Building the model<br/>model = tf.keras.models.Sequential([<br/>  tf.keras.layers.Flatten(input_shape=(28, 28)),<br/>  tf.keras.layers.Dense(128, activation='relu'),<br/>  tf.keras.layers.Dropout(0.2),<br/>  tf.keras.layers.Dense(10)<br/>])</span><span id="b982" class="le kg hi la b fi lj lg l lh li"># Loss function declaration<br/>loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)</span><span id="dab3" class="le kg hi la b fi lj lg l lh li"># Model compilation<br/>model.compile(optimizer='adam',<br/>              loss=loss_fn,<br/>              metrics=['accuracy'])</span><span id="778c" class="le kg hi la b fi lj lg l lh li"># Training<br/>model.fit(x_train, y_train, epochs=5)</span></pre><p id="c7ef" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">一旦经过训练，该模型可用于进行预测。以下是将前两个测试图像输入模型的示例:</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="36f2" class="le kg hi la b fi lf lg l lh li">model(x_test[:2])</span></pre><p id="073d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在某些情况下，人们可能希望将AI模型的开发与使用它的应用程序的开发完全分开。一个典型的例子是微服务架构。</p><p id="ff94" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">对于这种情况，可以使用REST API来公开模型，这可以使用像<a class="ae ky" href="https://flask.palletsprojects.com/en/1.1.x/" rel="noopener ugc nofollow" target="_blank"> Flask </a>这样的模块来完成。然而，TensorFlow为此提供了一个预制选项:<a class="ae ky" href="https://www.tensorflow.org/tfx/guide/serving" rel="noopener ugc nofollow" target="_blank"> TensorFlow Serving </a>。TensorFlow服务由一个Docker容器组成，该容器包含向HTTP请求公开导出的TensorFlow模型所需的所有逻辑。</p><p id="5003" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">作为使用TensorFlow服务的第一步，我们将从使用Keras的<em class="lk"> save </em>函数导出模型开始:</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="b3e1" class="le kg hi la b fi lf lg l lh li">model.save('./mymodel/1/')</span></pre><p id="b49f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">导出的模型现在位于文件夹<em class="lk">中。/mymodel/1/ </em>。请注意，此处的<em class="lk"> mymodel </em>是型号的名称，而<em class="lk"> 1 </em>成为其版本。</p><h1 id="3308" class="kf kg hi bd kh ki kj kk kl km kn ko kp io kq ip kr ir ks is kt iu ku iv kv kw bi translated">TensorFlow服务码头集装箱</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kx"><img src="../Images/745ab005e69112e2ee8ae853ed811836.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*nBnv35Cr17xKNpB9HxkfRA.png"/></div></figure><p id="157e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">模型现在已经导出，可以在TensorFlow服务容器中复制。为此，我们首先从Docker Hub中拉出一个空容器，并在本地运行它:</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="8db1" class="le kg hi la b fi lf lg l lh li">docker run -d --name serving_base tensorflow/serving</span></pre><p id="3a28" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">随着容器的运行，可以复制导出的模型:</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="9351" class="le kg hi la b fi lf lg l lh li">docker cp ./mymodel serving_base:/models/mymodel</span></pre><p id="4eb1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">容器现在包含了我们的模型，并且可以保存为新的图像。这可以通过使用docker commit命令来完成:</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="f65b" class="le kg hi la b fi lf lg l lh li">docker commit --change "ENV MODEL_NAME mymodel" serving_base my-registry/mymodel-serving</span></pre><p id="b2e8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">该命令还将<em class="lk"> MODEL_NAME </em>环境变量设置为我们的模型名称。请注意，my-registry是将图像推送到的docker注册表的URL。</p><p id="d2cd" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">一旦完成，我们可以摆脱原来的TensorFlow服务形象</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="aa3a" class="le kg hi la b fi lf lg l lh li">docker kill serving_base<br/>docker rm serving_base</span></pre><p id="ebc2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在这里，通过运行它来检查容器是否真的在工作可能是个好主意:</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="b11a" class="le kg hi la b fi lf lg l lh li">docker run -d -p 8501:8501 my-registry/mymodel-serving</span></pre><p id="fed5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">请注意，8501是TensorFlow服务用于其REST API的端口。</p><p id="f8c1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">对<code class="du ll lm ln la b">http://&lt;docker host IP&gt;:8501/v1/models/mymodel</code>的Get请求应该返回以下JSON:</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="e9ef" class="le kg hi la b fi lf lg l lh li">{<br/> "model_version_status": [<br/>  {<br/>   "version": "1",<br/>   "state": "AVAILABLE",<br/>   "status": {<br/>    "error_code": "OK",<br/>    "error_message": ""<br/>   }<br/>  }<br/> ]<br/>}</span></pre><p id="35c6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如果到目前为止一切顺利，那么可以将容器推送到注册中心，这将使它可供Kubernetes拉取:</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="ce5a" class="le kg hi la b fi lf lg l lh li">docker push my-registry/mymodel-serving</span></pre><h1 id="2b56" class="kf kg hi bd kh ki kj kk kl km kn ko kp io kq ip kr ir ks is kt iu ku iv kv kw bi translated">将容器部署到Kubernetes</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kx"><img src="../Images/8e7bb700b4a3f2327dd749082f73f11e.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*CXGD3oJFoK_OungF7eE66g.png"/></div></figure><p id="42a8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在容器已经被推送到注册中心，可以部署到我们的Kubernetes集群了。这是通过在集群中创建两个资源来实现的:一个部署和一个服务。部署基本上是应用程序本身，而服务则允许用户从集群外部访问部署。这里，我们将使用一个节点端口服务，这样我们的TensorFlow服务容器就可以通过使用一个专用端口从集群外部进行访问。这件事我们会选30111。</p><p id="de7a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">创建这些资源只需用kubectl命令应用YAML清单文件的内容。在我们的例子中，这里是我们的<em class="lk"> kubernetes_manifest.yml </em>文件的内容:</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="4622" class="le kg hi la b fi lf lg l lh li">apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: mymodel-serving<br/>spec:<br/>  replicas: 1<br/>  selector:<br/>    matchLabels:<br/>      app: mymodel-serving<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: mymodel-serving<br/>    spec:<br/>      containers:<br/>      - name: mymodel-serving<br/>        image: my-registry/mymodel-serving<br/>        ports:<br/>        - containerPort: 8501<br/>---<br/>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: mymodel-serving<br/>spec:<br/>  ports:<br/>  - port: 8501<br/>    nodePort: 30111<br/>  selector:<br/>    app: mymodel-serving<br/>  type: NodePort</span></pre><p id="e7dd" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">然后可以通过执行以下命令来创建资源</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="2453" class="le kg hi la b fi lf lg l lh li">kubectl apply -f kubernetes_manifest.yml</span></pre><p id="63b2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">容器现在应该部署在Kubernetes集群中了</p><h1 id="999a" class="kf kg hi bd kh ki kj kk kl km kn ko kp io kq ip kr ir ks is kt iu ku iv kv kw bi translated">使用TensorFlow服务API</h1><p id="cba5" class="pw-post-body-paragraph jj jk hi jl b jm lo ij jo jp lp im jr js lq ju jv jw lr jy jz ka ls kc kd ke hb bi translated">部署在Kubernetes中的人工智能模型现在可以用于预测。这可以通过向TensorFlow服务容器的预测API发送POST请求来完成。请求的主体由一个JSON组成，JSON包含要提供给模型的输入数据。然后，模型将回复其预测，也是JSON格式。这里有一个例子说明如何用Python实现这一点，使用的是<em class="lk">请求</em>模块</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="6623" class="le kg hi la b fi lf lg l lh li"># Import the necessary modules<br/>import requests<br/>import numpy as np<br/>import json<br/>import tensorflow as tf</span><span id="8a99" class="le kg hi la b fi lj lg l lh li"># Loading data<br/>mnist = tf.keras.datasets.mnist<br/>(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><span id="eb5e" class="le kg hi la b fi lj lg l lh li"># Data preprocessing (here, normalization)<br/>x_train, x_test = x_train / 255.0, x_test / 255.0</span><span id="d342" class="le kg hi la b fi lj lg l lh li"># Format the image data so as to be sent as JSON<br/>payload = json.dumps( { 'instances': x_test[:2].tolist() } )</span><span id="bdb6" class="le kg hi la b fi lj lg l lh li"># URL of the TensorFlow Serving container's API<br/>url = 'http://&lt;cluster IP&gt;:30111/v1/models/mymodel:predict'</span><span id="6f31" class="le kg hi la b fi lj lg l lh li"># Send the request<br/>response = requests.post(url, data=payload)</span><span id="c455" class="le kg hi la b fi lj lg l lh li"># Parse the response<br/>prediction =  response.json()["predictions"]</span><span id="1443" class="le kg hi la b fi lj lg l lh li"># Print the result<br/>print(prediction)</span></pre><h1 id="87af" class="kf kg hi bd kh ki kj kk kl km kn ko kp io kq ip kr ir ks is kt iu ku iv kv kw bi translated">结论</h1><p id="c135" class="pw-post-body-paragraph jj jk hi jl b jm lo ij jo jp lp im jr js lq ju jv jw lr jy jz ka ls kc kd ke hb bi translated">由于Tensorflow服务，人工智能模型可以很容易地容器化，将它们变成自己的独立应用程序，可以通过HTTP调用进行交互。这增加了关注点的分离和模块化，特别是与将模型直接嵌入到依赖它的应用程序的源代码中相比。</p></div></div>    
</body>
</html>
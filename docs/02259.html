<html>
<head>
<title>Machine Learning Basics with the Support Vector Machine Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机算法的机器学习基础</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/machine-learning-basics-with-the-support-vector-machine-algorithm-caf296b38542?source=collection_archive---------14-----------------------#2021-05-08">https://medium.com/geekculture/machine-learning-basics-with-the-support-vector-machine-algorithm-caf296b38542?source=collection_archive---------14-----------------------#2021-05-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e9838838d1ab7de1f7c5636e7c4b0188.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fuy7_WQaJKBsGb-MSRu17Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Image: <a class="ae iu" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.kdnuggets.com%2F2021%2F02%2Fmachine-learning-assumptions.html&amp;psig=AOvVaw0BDc4o_zm_saxuIAGGiAt8&amp;ust=1620485807151000&amp;source=images&amp;cd=vfe&amp;ved=0CAMQjB1qFwoTCIil7_Xqt_ACFQAAAAAdAAAAABAD" rel="noopener ugc nofollow" target="_blank">KDnuggets</a></figcaption></figure><p id="0384" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://www.sas.com/en_us/insights/analytics/machine-learning.html#:~:text=Machine%20learning%20is%20a%20method,decisions%20with%20minimal%20human%20intervention." rel="noopener ugc nofollow" target="_blank">机器学习</a>是人工智能的一个分支，基于系统可以从数据中学习、识别模式并在最少人工干预的情况下做出决策的想法。</p><p id="419a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">学习的过程始于观察或数据，如例子、直接经验或指导，以寻找数据中的模式，并根据我们提供的例子在未来做出更好的决策。主要目的是让计算机在没有人类干预或帮助的情况下自动学习<strong class="ix hj"> </strong>，并相应地调整动作。</p><p id="3632" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们把机器学习分成两个子类别；</p><p id="6757" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">监督学习</strong>是一种，你可以认为学习是由老师指导的。我们有一个充当教师的数据集，它的作用是训练计算机/机器。一旦机器得到训练，它就可以在获得全新数据时开始做出预测或决策。</p><p id="9b49" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">例如，想象一台计算机是一名学生，我们是他/她的老师，我们希望学生(计算机)学习芒果的样子。我们将向学生展示一些不同水果的图片，包括一些芒果和其他水果的图片，如香蕉、苹果等。</p><p id="c44e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当我们看到一个芒果，我们认为它是一个“芒果！”而当它不是芒果时，我们就认定它是“不，不是芒果！”在对学生做了几次之后，我们给学生看了一张图片，并问“这是芒果吗？”这个学生大部分时候会说“芒果！”或者“不，不是芒果！”取决于图片是什么。这种学习的艺术就是监督机器学习。</p><p id="9d90" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">监督机器学习算法用于解决分类或回归问题。</p><p id="cf43" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一个<strong class="ix hj">分类问题</strong>有一个离散值作为它的输出。例如，上面教学生识别芒果的类比。它的产出要么是“芒果”，要么是“不是芒果”，没有中间地带。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/0ff98246bcd42b87f2b52eec8df23013.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*uB0eXAhHpTcRp1JtL9-zLQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Image showing randomly generated number</figcaption></figure><p id="3fc4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上图显示了分类数据的一个示例。我们有一个预测器(或一组预测器)和一个标签。在图像中，我们试图根据学生对水果的识别(预测器)来预测“芒果”(1)或“非芒果”(0)。</p><p id="61e5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一个<strong class="ix hj">回归问题</strong>的输出是一个实数(带小数点的数)。例如，回归预测一个连续的<strong class="ix hj"> </strong>目标变量y。它允许您根据输入数据x估计一个值，如房价或体重。</p><p id="b93e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">给定房间数量，我们可以使用下表中的数据来估计房价。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/43059c0acee9c2aefa26066382bac8a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*3W3ATtoVr7LCsKFh5g474Q.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Image showing randomly generated number</figcaption></figure><p id="0de9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">无监督学习</strong>只对输入数据进行操作，没有输出或任何标签。与监督学习不同，它没有老师来纠正机器。模型通过观察学习，并在数据中找到结构。一旦为模型提供了数据集，它就会通过在数据集中创建分类来自动查找数据集中的模式和关系。</p><p id="8c21" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">假设我们向模型呈现苹果、香蕉和芒果的图像，那么它会根据一些模式和关系创建聚类，并将数据集划分到这些聚类中。现在，如果一个新数据被提供给模型，它会将它添加到一个已创建的分类中。</p><h1 id="e329" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak">支持向量机</strong></h1><p id="d596" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">支持向量机(SVM)是用于分类和回归问题的监督学习模型，如支持向量分类(SVC)和支持向量回归(SVR)。它用于较小的数据集，因为处理时间太长。他们可以解决线性和非线性问题，并使用边际的概念来分类数据点。</p><h2 id="4355" class="lb jz hi bd ka lc ld le ke lf lg lh ki jg li lj km jk lk ll kq jo lm ln ku lo bi translated"><strong class="ak">支持向量机是如何工作的？</strong></h2><p id="6b12" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">SVM算法是基于寻找一个超平面的想法，该超平面清楚地将数据点分成不同的组。在SVM，我们在N维空间(N-要素的数量)中绘制数据集中的每个数据点。为了将数据点分成两组，可以选择许多可能的超平面。我们的目标是找到一个具有最大余量的平面，即两组数据点之间的最大距离。最大化边缘距离提供了一些支持，以便可以更有把握地对未来的数据点进行分类。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lp"><img src="../Images/3919e598bee3908bc0098c524894f574.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xZpU5Oa87QUfZknWqvDNhQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Image: <a class="ae iu" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fsubscription.packtpub.com%2Fbook%2Fbig_data_and_business_intelligence%2F9781789345070%2F3%2Fch03lvl1sec30%2Fsvm-for-churn-prediction&amp;psig=AOvVaw0KPciUO9qWLNIchHVHXphb&amp;ust=1620469904861000&amp;source=images&amp;cd=vfe&amp;ved=0CAMQjB1qFwoTCNDrj4qvt_ACFQAAAAAdAAAAABAs" rel="noopener ugc nofollow" target="_blank">Packt Subscription</a></figcaption></figure><p id="dfa4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们探索一些SVM的术语</p><h2 id="c0f5" class="lb jz hi bd ka lc ld le ke lf lg lh ki jg li lj km jk lk ll kq jo lm ln ku lo bi translated"><strong class="ak">超平面</strong></h2><p id="a08b" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">超平面是帮助分类数据点的决策边界。落在超平面任一侧的数据点可以归属于不同的组。此外，超平面的维数取决于特征的数量。如果输入特征的数量是2，那么超平面只是一条线。如果输入特征的数量是3，则超平面变成二维平面。当特征的数量超过3时，就很难想象超平面了。见下图理解这个概念。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lq"><img src="../Images/de75b4508f93dd7bb193b9ea0c3e786b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A5Bh1b4EutMXndQLHzlHQg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Image: <a class="ae iu" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Ftowardsdatascience.com%2Fsupport-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47&amp;psig=AOvVaw0QOlIALHzR1L9XGiC8CSeI&amp;ust=1620484579284000&amp;source=images&amp;cd=vfe&amp;ved=0CAMQjB1qFwoTCKjBjNLlt_ACFQAAAAAdAAAAABAD" rel="noopener ugc nofollow" target="_blank">Towards Data Science</a></figcaption></figure><p id="0b57" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">什么是支持向量？</strong></p><p id="9350" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://www.marktechpost.com/2021/03/25/introduction-to-support-vector-machines-svms/" rel="noopener ugc nofollow" target="_blank">支持向量</a>是在超平面上或最接近超平面的数据点，并确定超平面的位置。使用这些支持向量，我们最大限度地提高利润率。删除支持向量将改变超平面的位置。支持向量与超平面等距，有助于构建SVM。它们被称为支持向量，因为它们支持超平面，如果它们的位置移动，超平面也移动。</p><p id="ac39" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">更进一步，让我们通过一个例子来理解SVM思想:</p><p id="cd18" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">例如，你的老板在他的工作邮件中收到大量的信息，并要求你区分工作和非工作信息。现在，您希望设计一个函数(超平面)来区分这两种情况，这样，每当您收到一封非工作邮件时，它将被分类为垃圾邮件，而当收到一封工作邮件时，它将被分类为非垃圾邮件。</p><p id="8b01" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们将找到一些将数据分为垃圾邮件和非垃圾邮件的行。这将是一条线，使得两组中的每一组离最近点的距离最远。查看下面的图表，找出区分这两个类别的最佳线条(假设蓝色表示垃圾邮件，红色表示非垃圾邮件)。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/9c1cbb6d904078fca2da76d6ac1d2b71.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*0T-8rV2-nL0VtuKf6ihPYQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Image: <a class="ae iu" href="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/" rel="noopener ugc nofollow" target="_blank">AnalyticsVidhya.com</a></figcaption></figure><p id="e186" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你选择C线，你的直觉是正确的。从上面可以看出，与A和b相比，超平面C的边缘具有两个类的数据点之间的最大距离。因此，我们将右超平面命名为C。</p><p id="71c4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上图是一个<strong class="ix hj">线性可分的例子</strong>，在这个例子中，SVM正试图寻找一个使裕度最大化的超平面，条件是两个类都被正确分类。但在现实中，数据集可能永远不会线性分离，所以超平面100%正确分类的条件永远不会满足。</p><p id="8c5f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们考虑一下<strong class="ix hj">在你老板的电子邮件中垃圾邮件和非垃圾邮件的非线性可分情况</strong>。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/813bd939b8b6c63271f001dacfacadd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*2J50f3FiR_AO6AVREzmgjg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Image: <a class="ae iu" href="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/" rel="noopener ugc nofollow" target="_blank">AnalyticsVidhya.com</a></figcaption></figure><p id="3299" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上面的图中，我无法使用直线将两个类别分为垃圾邮件和非垃圾邮件，因为其中一个蓝点(“垃圾邮件”)位于“非垃圾邮件”(红色)类别的范围内，属于异常值。SVM算法具有忽略异常值并找到具有最大余量的超平面的特征。因此，我们可以说，SVM分类对异常值是稳健的。</p><p id="da77" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了找到<strong class="ix hj">的超平面，像上面的<strong class="ix hj">、</strong>这样的非线性可分情况</strong>，SVM算法有一种技术叫做<a class="ae iu" href="https://en.wikipedia.org/wiki/Kernel_method" rel="noopener ugc nofollow" target="_blank">、<strong class="ix hj">内核</strong>、</a>、<strong class="ix hj">技巧</strong>。SVM核是采用低维输入空间并将其修改到高维空间的函数，即，它将不可分离问题转换为可分离问题。它主要用于非线性分离问题。很明显，它会进行一些复杂的数据修改，然后根据您设置的标签或输出找出区分数据的过程。</p><p id="8a06" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当我们在原始输入空间中观察超平面时，它看起来像一个圆:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/4c40520f02920f4734d8bb74f79cbec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*k78jP0JbhKq65GZRHYx4FQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Image: <a class="ae iu" href="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/" rel="noopener ugc nofollow" target="_blank">AnalyticsVidhya.com</a></figcaption></figure><h2 id="140c" class="lb jz hi bd ka lc ld le ke lf lg lh ki jg li lj km jk lk ll kq jo lm ln ku lo bi translated"><strong class="ak">使用python实现SVM</strong></h2><p id="f6f4" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">使用下面的<a class="ae iu" href="https://github.com/Ifeoluwa96/SCA-MP-C4-Assessment/blob/main/Implementing%20SVM%20code.ipynb" rel="noopener ugc nofollow" target="_blank">代码</a>使用<strong class="ix hj"> </strong> <a class="ae iu" href="https://scikit-learn.org/stable/modules/svm.html" rel="noopener ugc nofollow" target="_blank"> sci-kit learn </a>库实现SVM:</p><pre class="ju jv jw jx fd lu lv lw lx aw ly bi"><span id="7f9c" class="lb jz hi lv b fi lz ma l mb mc">#importing sci-kit learn and other important libraries</span><span id="ab24" class="lb jz hi lv b fi md ma l mb mc">from sklearn.datasets import make_circles</span><span id="9ce0" class="lb jz hi lv b fi md ma l mb mc">from sklearn import svm</span><span id="cfb3" class="lb jz hi lv b fi md ma l mb mc">import matplotlib.pyplot as plt</span><span id="ec70" class="lb jz hi lv b fi md ma l mb mc">%matplotlib inline</span><span id="2dce" class="lb jz hi lv b fi md ma l mb mc">from mpl_toolkits.mplot3d import Axes3D</span><span id="33b7" class="lb jz hi lv b fi md ma l mb mc">import numpy as np<br/></span><span id="df2c" class="lb jz hi lv b fi md ma l mb mc">X,Y = make_circles(n_samples=500,noise=0.02)</span><span id="7bd5" class="lb jz hi lv b fi md ma l mb mc">plt.scatter(X[:,0],X[:,1],c=Y)</span><span id="4d42" class="lb jz hi lv b fi md ma l mb mc">plt.show()</span><span id="8285" class="lb jz hi lv b fi md ma l mb mc">def phi(X):</span><span id="d14b" class="lb jz hi lv b fi md ma l mb mc">""""Non Linear Transformation"""</span><span id="295a" class="lb jz hi lv b fi md ma l mb mc">X1 = X[:,0]</span><span id="2f13" class="lb jz hi lv b fi md ma l mb mc">X2 = X[:,1]</span><span id="0dd0" class="lb jz hi lv b fi md ma l mb mc">X3 = X1**2 + X2**2</span><span id="9f60" class="lb jz hi lv b fi md ma l mb mc">X_ = np.zeros((X.shape[0],3))</span><span id="f005" class="lb jz hi lv b fi md ma l mb mc">print(X_.shape)</span><span id="6212" class="lb jz hi lv b fi md ma l mb mc">X_[:,:-1] = X</span><span id="e7e8" class="lb jz hi lv b fi md ma l mb mc">X_[:,-1] = X3</span><span id="c781" class="lb jz hi lv b fi md ma l mb mc">return X_</span><span id="a315" class="lb jz hi lv b fi md ma l mb mc">def plot3d(X,show=True):</span><span id="c3c5" class="lb jz hi lv b fi md ma l mb mc">fig = plt.figure(figsize=(10,10))</span><span id="fc82" class="lb jz hi lv b fi md ma l mb mc">ax = fig.add_subplot(111,projection='3d')</span><span id="69eb" class="lb jz hi lv b fi md ma l mb mc">X1 = X[:,0]</span><span id="1fc4" class="lb jz hi lv b fi md ma l mb mc">X2 = X[:,1]</span><span id="4714" class="lb jz hi lv b fi md ma l mb mc">X3 = X[:,2]</span><span id="c571" class="lb jz hi lv b fi md ma l mb mc">ax.scatter(X1,X2,X3,zdir='z',s=20,c=Y,depthshade=True)</span><span id="e1a0" class="lb jz hi lv b fi md ma l mb mc">if(show==True):</span><span id="9c78" class="lb jz hi lv b fi md ma l mb mc">plt.show()</span><span id="8847" class="lb jz hi lv b fi md ma l mb mc">return ax</span><span id="4b8f" class="lb jz hi lv b fi md ma l mb mc">ax = plot3d(X_)<br/></span><span id="72d2" class="lb jz hi lv b fi md ma l mb mc"># using the rbf kernel function to use the kernel trick</span><span id="d4c2" class="lb jz hi lv b fi md ma l mb mc">svc = svm.SVC(kernel="rbf")</span><span id="36c8" class="lb jz hi lv b fi md ma l mb mc">svc.fit(X,Y)</span><span id="f02a" class="lb jz hi lv b fi md ma l mb mc">svc.score(X,Y)</span></pre><h2 id="38dc" class="lb jz hi bd ka lc ld le ke lf lg lh ki jg li lj km jk lk ll kq jo lm ln ku lo bi translated">SVM的优势和劣势</h2><h2 id="40bc" class="lb jz hi bd ka lc ld le ke lf lg lh ki jg li lj km jk lk ll kq jo lm ln ku lo bi translated"><strong class="ak">优点:</strong></h2><ul class=""><li id="02d7" class="me mf hi ix b iy kw jc kx jg mg jk mh jo mi js mj mk ml mm bi translated">它工作得很好，有一个清晰的分离边界</li><li id="fea4" class="me mf hi ix b iy mn jc mo jg mp jk mq jo mr js mj mk ml mm bi translated">在高维空间是有效的。</li><li id="475b" class="me mf hi ix b iy mn jc mo jg mp jk mq jo mr js mj mk ml mm bi translated">它在维数大于样本数的情况下是有效的。</li><li id="ce92" class="me mf hi ix b iy mn jc mo jg mp jk mq jo mr js mj mk ml mm bi translated">它在决策函数中使用训练点的子集(称为支持向量)，因此它也是内存高效的。</li></ul><h2 id="85fe" class="lb jz hi bd ka lc ld le ke lf lg lh ki jg li lj km jk lk ll kq jo lm ln ku lo bi translated"><strong class="ak">缺点:</strong></h2><ul class=""><li id="c441" class="me mf hi ix b iy kw jc kx jg mg jk mh jo mi js mj mk ml mm bi translated">当我们有大的数据集时，它表现不好，因为所需的训练时间更长</li><li id="89b8" class="me mf hi ix b iy mn jc mo jg mp jk mq jo mr js mj mk ml mm bi translated">当数据集有更多噪声时，即目标类重叠时，它的性能也不是很好</li><li id="ea32" class="me mf hi ix b iy mn jc mo jg mp jk mq jo mr js mj mk ml mm bi translated">SVM没有直接提供概率估计，这些是通过昂贵的五重交叉验证计算出来的。它包含在Python scikit-learn库的相关SVC方法中。</li></ul><p id="5954" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我们已经熟悉了机器学习和SVM算法的基础知识。我希望这能对你有所帮助。感谢阅读！！</p><p id="341d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请通过<a class="ae iu" href="http://linkedin.com/in/ifeoluwa-wuraola-72a750204" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae iu" href="https://twitter.com/Ifeooluwa" rel="noopener ugc nofollow" target="_blank"> Twitter </a>与我联系。</p><h2 id="9d3c" class="lb jz hi bd ka lc ld le ke lf lg lh ki jg li lj km jk lk ll kq jo lm ln ku lo bi translated">资源</h2><p id="eebd" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated"><a class="ae iu" href="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2017/09/understanding-support-vector-machine-example-code/</a></p><p id="fd89" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://www.edureka.co/blog/what-is-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://www.edureka.co/blog/what-is-machine-learning/</a></p><p id="80e5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://www.expert.ai/blog/machine-learning-definition/" rel="noopener ugc nofollow" target="_blank">https://www.expert.ai/blog/machine-learning-definition/</a></p></div></div>    
</body>
</html>
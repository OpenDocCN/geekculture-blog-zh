# 通俗易懂的人工智能—第二部分

> 原文：<https://medium.com/geekculture/ai-in-laymans-terms-part-2-2-6ef1ac8b0504?source=collection_archive---------39----------------------->

![](img/718bc50445e27d2bbf0bf5d15c44b82b.png)

Quote by Abeba Birhane and Vinay Uday Prabhu from their paper — Large datasets: a pyrrhic win for computer vision?

在第一部分中，我介绍了计算机是如何被引入和进化来帮助我们跨越集体计算的限制，并被设计来帮助我们追求按照我们的形象制造它们——一个有思想、有行为的实体。人工智能的实现是相当新的，它在半个世纪前就经过了深思熟虑，但由于电子技术的发展和数据的增加，我们生活在一个有趣的时代，可以看到它发展到实用。

在这篇文章中，我强调了人工智能不那么漂亮和被过度宣传的方面。你会经历数据偏差、人工智能技术的滥用、伦理难题、技术优势、行业专家的建议、纪录片和书籍推荐等等，以更好地了解这个领域。

虽然这篇文章可以独立阅读，但我强烈建议你也阅读第一部分，以便在现有的编码空间中对人工智能有一个整体的了解——概念、类型、行话及其各种应用。链接:[https://medium . com/geek culture/ai-in-laymans-terms-part-1-423 C5 c 91901 a](/geekculture/ai-in-laymans-terms-part-1-423c5c91901a)

下面是人工智能的第二部分:

# 头号数据偏差、误用和伦理难题

人工智能系统是数据密集型的，它们建立在人类提供给它们的数据之上。如第一部分所述，这些训练数据的大小可能跨越数百 GB。GPT-3 或生成式预训练转换器 3 是一种语言模式(LM ),用于从一种语言到另一种语言的机器翻译，具有 570 GB 的数据集大小、96 层和 1750 亿个参数(信息束或节点之间的关系)。

为了将这种海量放入视角，我重新插入了机器学习内部实际看起来像什么的非常简化的图表。

![](img/0eca9649037341754c38fa1bedb57a2b.png)

输入这些系统的数据是人类产生的，由于人类有偏见，输入人工智能系统的数据也带有我们内在的无意识偏见。

## #1.1 语言模型中的偏差(LMs)

第 1 部分列出的人工智能的应用之一是 NMT 或神经机器翻译。机器对人类语言的理解是一个难以解决的问题，因此在全球范围内，有许多公司和研究人员寻求通过从在线来源摄取内容来解决这个问题。

然而，机器不是复杂的有意识的生物，无法辨别对错。在一篇关于 LMs 的论文中— [关于随机鹦鹉的危险:语言模型会不会太大？](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)作者(以及其他问题)坚定地指出,“规模并不能保证多样性”,输入人工智能系统的数据并非没有性别歧视、肤色歧视、年龄歧视等偏见，因为互联网本身在性别、种族、年龄和更多因素之间的分布并不均匀，从而使在线数据来源固有地不足和有偏见。该论文深入探讨了数据偏差、编码偏差、不断变化的社会观点与静态数据、对更干净数据的建议以及要求问责制。如果你还没有读过这篇文章，我建议你读一读。

这不是人类数据使有偏见的 AI 系统出现的唯一情况。2016 年，微软发布了一个名为 Tay 的 Twitter 机器人，供 Twitteratis 与之聊天，以便人工智能可以学习。[在 24 小时内，Tay 学会了变得令人讨厌——从厌恶女人、种族主义的推特变成了纳粹的同情者。](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist)

这不是人工智能的问题，这是人的问题。Tay 学会了人们所教的东西。

## #1.2 图像数据中的偏差

数据偏差的另一个例子可以在一种不同的应用中看到——面部识别。如果你还没有看过网飞的[编码偏见](https://www.codedbias.com/)，请一定要看。Shalini Kantayya 的这部纪录片以图像格式的数据偏见开始，这是由麻省理工学院媒体实验室的博士生 Joy Buolamwini 发现的，她认为除非她在脸上戴上白色面具，否则算法对她不起作用。她发现，在 IT 行业的大公司中，面部识别系统对白皮肤的男性很有效，但对女性不太有效，对有色人种则很糟糕。

为了正确看待这种过度依赖人工智能缺陷的风险，请查看这篇推文。一名 14 岁的黑人女孩被一个在溜冰场实施的面部识别系统错误地识别，并被指控在她根本不在那里的时候打架。在所有的问题中，一个更好的问题是，正如在提到的 Twitter 帖子中所问的那样——为什么首先要在溜冰场安装面部识别系统？

有了这些信息，想知道当我们允许机器在稍微严重一点的事情上超越人类的判断会发生什么。ProPublica 在 2016 年发表的一篇文章显示了[罪犯风险评估](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)——一种预测一个人未来犯罪可能性的评分系统——是如何对黑人产生偏见的。

## #1.3 数据误用示例:Deepfake

既然是 AI 空间的图像格式，那就查看这个网站——【https://thispersondoesnotexist.com/】[。正如这个网站的名字所说，每次你访问这个网站，你都会看到一张并不存在的脸。它们都是使用 GAN(生成式对抗网络，一种机器学习框架)创建的 AI。还有什么？人们可以在 https://generated.photos/](https://thispersondoesnotexist.com/)[网站上购买生成的图像，这些图像可以用来提供产品评论或任何类似的在线功能，在这些功能中，我们需要另一个人的信任来决定一些事情。纽约时报的这篇互动文章说得很好。](https://generated.photos/)

正如第 1 部分所概述的，AI 应用程序可以根据所使用的数据格式而有所不同。制造假货的一个更高级的版本是音频和视频格式，称为 deepfake。这可能被滥用来创建人们的语音或行为，而没有他们实际参与的音频/视频格式，这听起来/看起来非常有说服力。美国有线电视新闻网[的这一页以巴拉克·奥巴马的 deepfake 视频](https://edition.cnn.com/interactive/2019/01/business/pentagons-race-against-deepfakes/)为例，更深入地探讨了 deepfake。

人工智能确实是我们拥有的一项令人惊叹的技术，但仅仅因为我们可以创造这些系统，这是否意味着我们应该把它们放在任何有明显潜在危害的地方？

# 人工智能专家的#2 建议

[梅雷迪思·布鲁萨德](https://twitter.com/merbroussard)在她的书— [人工智能](https://mitpress.mit.edu/books/artificial-unintelligence)中，不仅为外行人理解计算机和人工智能创造了一个公平的竞争环境，而且(在许多其他见解中)创造了一个术语——技术沙文主义，她写道，我引用她的原话——“技术沙文主义是相信技术总是解决方案。”这本书是值得推荐的读物。

她和许多人工智能专家，如[吴恩达](https://twitter.com/AndrewYNg)、[蒂姆尼特·格布鲁](https://twitter.com/timnitGebru)，以及更多的人已经通过课程和发表论文公开提出建议，说明如何解决数据偏见，或者如何首先正确和合乎道德地获取数据。

## #2.1 对数据策略和监管的需求

人工智能的方法通常是获取所有可以在网上找到的数据并训练机器，越多越好，而人工智能专家通常建议根据预期用途战略性地获取数据。

## #2.2 将想法放在数据中，而不是无意识的偏见中

人类容易受到自己偏见的影响，但当我们认识到自己的这一特点时，我们有责任削弱我们为人工智能或其他方面建立的系统中的偏见。作为上述建议的延伸，其中一个建议是— [数据集数据表](https://arxiv.org/abs/1803.09010)，以鼓励人工智能系统数据收集和使用的透明度和问责制。

## #2.3 避免人工智能新闻中的耸人听闻

注意力经济不放过任何人，包括技术出版商，他们可能经常把人工智能描绘成一个活生生的实体，而事实并非如此。吴恩达是他的入门课程——[人工智能为每个人服务](https://www.deeplearning.ai/program/ai-for-everyone/)指出，在人工智能领域有很多失败之处，在应该给出人工智能是什么以及它能做什么和不能做什么的更现实的画面时，它们经常没有成为新闻。

## #2.4 对人工智能问责制的需求

人工智能系统是强大的，拥有这些系统的人拥有对数据和人的巨大权力。因此，要求问责以确保人工智能领域的道德实践是合乎逻辑的。

## #2.5 监控与隐私

在一部由两部分组成的 DW 纪录片中，该纪录片列出了人工智能的利弊，还近距离展示了中国实施的人工智能系统，以及隐私交换如何以安全监控的名义几乎成为一种规范。作为一种观点，这是一个典型的技术沙文主义案例，因此我引用旁白——“也许信任比智能控制更好。”

# 人工智能的量子边缘

量子计算本身是传统计算机迄今为止的一次飞跃。虽然全球运行的系统仍在处理 0 和 1，其中比特值在给定时间为 0 或 1，但量子计算正在通过实现纠缠和叠加的量子物理现象来改变这一点。

简而言之，一个量子位或量子位在给定时间可以同时为 0 和 1，从而以指数方式增加计算时间。量子人工智能已经是谷歌旗下的一个项目，其他公司如 IBM、霍尼韦尔和微软也已经在投资量子计算。

请注意，这些公司也拥有大量数据，拥有自己的人工智能系统，随着量子计算的进一步发展，将拥有更多处理和创造的能力。这两种强大技术的结合是前所未有的，因此需要谨慎。

# 结论

总之，人工智能系统可以被视为我们人类的一面镜子。如果技术有偏见或被滥用，那是因为我们作为人类有偏见并倾向于滥用，虽然我们在现实生活中很好地指出了种族主义、性别歧视、肤色歧视、年龄歧视和类似的偏见，但同样重要的是，在我们赋予无知觉的人工智能为有知觉的公众做出现实生活中的决定时，密切关注我们提供给人工智能系统的数据，要求问责和起草法律。

关于这一点，我想引用 Abeba Birhane 和 Vinay Uday Prabhu 在他们的论文[——大数据集:计算机视觉得不偿失的胜利？](https://arxiv.org/abs/2006.16923)他简洁而优美地总结了整个人工智能:“给人工智能系统灌输世界的美、丑和残酷，但期望它只反映美是一种幻想。”

我希望我们会记住这一点。

【https://www.linkedin.com】最初发表于[](https://www.linkedin.com/pulse/ai-laymans-terms-part-2-sangita-ekka/)**。**

**如果你喜欢我的工作，想支持它，* [*请我喝杯啤酒*](https://www.buymeacoffee.com/SangitaEkka) *！你可以在这里查看我的其他作品:*[【https://linktr.ee/SangitaEkka】T21](https://linktr.ee/SangitaEkka)*
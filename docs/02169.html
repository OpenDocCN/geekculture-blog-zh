<html>
<head>
<title>Real-Time Automated Number Plate Recognition System</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实时自动车牌识别系统</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/real-time-automated-number-plate-recognition-system-1820b5e42bae?source=collection_archive---------9-----------------------#2021-05-06">https://medium.com/geekculture/real-time-automated-number-plate-recognition-system-1820b5e42bae?source=collection_archive---------9-----------------------#2021-05-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/15355cf0842cfecdcbf8a0d96a86eb6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*93jzFYJew9L-FCIH"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Photo by <a class="ae iu" href="https://unsplash.com/@sam0076?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Sahil Patel</a> on <a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="4dc9" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">介绍</h1><p id="d45a" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在当今时代，安全性已经成为任何组织最关心的问题之一，这种安全性的自动化是必不可少的。可以提高系统安全性的一个领域是通过实时自动车牌识别，这可以帮助组织更好地了解和收集简单的细节，如通过大门、车道或路口的交通性质，否则这将变得乏味。虽然视频安全在世界范围内被采用，但是仍然缺乏将这种非结构化数据转换成可用且易于存储的数据的系统。通过深度学习和神经网络，这样一个系统的开发是可能的。这些系统使用各种可用的算法来预测最准确的结果，这些系统的能力在于所使用的各种各样的训练集、所使用的各种对象检测和识别库以及所使用的大量训练。</p><h1 id="ab80" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">内容:</h1><ol class=""><li id="e0dd" class="kr ks hi jv b jw jx ka kb ke kt ki ku km kv kq kw kx ky kz bi translated">关于YOLO和暗网的介绍。</li><li id="9d30" class="kr ks hi jv b jw la ka lb ke lc ki ld km le kq kw kx ky kz bi translated">训练集的图像注释。</li><li id="d261" class="kr ks hi jv b jw la ka lb ke lc ki ld km le kq kw kx ky kz bi translated">下载所有需要的资源。</li><li id="a8a1" class="kr ks hi jv b jw la ka lb ke lc ki ld km le kq kw kx ky kz bi translated">实施准则。</li><li id="4e40" class="kr ks hi jv b jw la ka lb ke lc ki ld km le kq kw kx ky kz bi translated">结论和参考文献。</li></ol><h1 id="5c4c" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">1.什么是暗网，什么是YOLO？</h1><p id="4458" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><strong class="jv hj"> Darknet </strong>是一个用C和CUDA编写的开源神经网络框架。它速度快，易于安装，支持CPU和GPU计算。你可以在开发者Joseph Redmon的<a class="ae iu" href="https://github.com/pjreddie/darknet" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到源代码。</p><p id="eeff" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated"><strong class="jv hj">你只看一次(YOLO) </strong>是建立在Darknet之上的最先进的实时物体检测系统。你可以在官网<a class="ae iu" href="https://pjreddie.com/darknet/yolo/" rel="noopener ugc nofollow" target="_blank">这里</a>下载所有资源。YOLO是最有趣的实时检测算法之一，因为它只有一个前向传播步骤来进行预测。非最大值后抑制然后输出带有边界框的已识别对象。</p><p id="c23a" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated"><strong class="jv hj">架构:</strong></p><ul class=""><li id="5e59" class="kr ks hi jv b jw lf ka lg ke lk ki ll km lm kq ln kx ky kz bi translated"><strong class="jv hj">输入</strong>是一批图像，每个都被整形为(608，608，3)。</li><li id="b9c3" class="kr ks hi jv b jw la ka lb ke lc ki ld km le kq ln kx ky kz bi translated">图像然后被分成19乘19的部分，然后这些部分的每一个被输入到一个深度CNN，我们在这里使用5个锚盒。</li><li id="7d86" class="kr ks hi jv b jw la ka lb ke lc ki ld km le kq ln kx ky kz bi translated">我们需要5个参数来检测框中的对象并检测边界框(p，bx，by，bw，bh)，我们需要80个不同的类来决定它是哪个对象。</li><li id="70c3" class="kr ks hi jv b jw la ka lb ke lc ki ld km le kq ln kx ky kz bi translated">所需的<strong class="jv hj">输出</strong>是形状为(19，19，5，85)的Numpy数组</li></ul><figure class="lp lq lr ls fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lo"><img src="../Images/c1b99cba664a81f7bfcb7ccb459a2213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7KJQVGIyiRpTFPyqURrZRA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Figure 1: Working of YOLO</figcaption></figure><p id="3950" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">一旦我们得到(19，19，5，85)形式的输出。最后两个维度被展平，因此最终输出将具有(19，19，425)的形状。</p><h1 id="ac37" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">2.训练图像的注释</h1><p id="db0d" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在注释训练集之前，您需要先收集它。在这里，我从一些Kaggle数据集，一些自己的图片和谷歌收集到了它。这种标注数据集的相同技术可以用于检测任何类型的定制对象(尽管大多数已经在Yolo中预先分类，这里车牌检测是一个非常特殊的情况)。</p><p id="52a3" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">一旦收集到数据，我们就使用由GitHub用户tzutalin开发的名为<a class="ae iu" href="https://github.com/tzutalin/labelImg" rel="noopener ugc nofollow" target="_blank"> LabelImg </a>的定制python程序。GitHub页面上面的链接给出了如何使用它的清晰说明。这个软件可以帮助你注释，绘制边界框，将注释保存在XML文件中，这可以通过我们的YOLO模型来处理，它甚至有一个名为YOLO的配置来使事情变得更容易。该软件自动创建一个名为classes.txt的配置文件来表示类的数量。请确保删除除了您为号码牌创建的类别之外的所有内容。</p><p id="08ca" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">这个过程很繁琐，而且必须对每张图像进行T2。让大多数人退出这个项目已经够无聊的了，但是我向你保证，当最终代码成功时，这一切都是值得的。</p><figure class="lp lq lr ls fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/720f98681d0f27d84a98b9a9d679413b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rdXFX_yjsEztU2O7cMer-g.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Figure 2: labeling program(Source:<a class="ae iu" href="https://github.com/tzutalin/labelImg" rel="noopener ugc nofollow" target="_blank">https://github.com/tzutalin/labelImg</a>)</figcaption></figure><h1 id="4bc5" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">3.下载所有需要的资源</h1><p id="d512" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">有很多关于在自定义对象上训练Yolo模型的资源，一些最好的资源是<a class="ae iu" href="https://towardsdatascience.com/how-to-detect-license-plates-with-python-and-yolo-8842aa6d25f7" rel="noopener" target="_blank">这个</a>和<a class="ae iu" href="https://towardsdatascience.com/how-to-train-a-custom-object-detection-model-with-yolo-v5-917e9ce13208" rel="noopener" target="_blank">这个</a>，这里的基本要点是从pjreddie的GitHub安装darknet和YOLO，构建它，然后在配置和路径上进行更改，以将模型指向自定义数据集并训练它。我发现这个研究过程非常乏味，但是感谢GitHub用户thtrieu创建的<a class="ae iu" href="https://github.com/thtrieu/darkflow" rel="noopener ugc nofollow" target="_blank"> dark flow </a>，它变成了实现darknet和Yolo的一个命令。尽管之后还需要做一些配置来运行车牌检测。</p><p id="f221" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">为了简化实现这个项目的一切，最简单的方法是下载或克隆我的GitHub <a class="ae iu" href="https://github.com/saky9596/ALPR" rel="noopener ugc nofollow" target="_blank">库</a>。</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="0aa6" class="lz iw hi lv b fi ma mb l mc md">git clone <a class="ae iu" href="https://github.com/saky9596/ALPR.git" rel="noopener ugc nofollow" target="_blank">https://github.com/saky9596/ALPR.git</a></span></pre><h1 id="615c" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">4.实施准则</h1><h2 id="b55c" class="lz iw hi bd ix me mf mg jb mh mi mj jf ke mk ml jj ki mm mn jn km mo mp jr mq bi translated">入门指南</h2><p id="d12d" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">训练数据集的最好方法是在Google colab上。为此，您需要首先将下载的存储库上传到您的google drive，并在google colab中打开文件名<a class="ae iu" href="https://github.com/saky9596/ALPR/blob/master/License_plate_recognition-colab.ipynb" rel="noopener ugc nofollow" target="_blank">License _ plate _ recognition-colab . ipynb</a>。通过在运行时&gt;管理会话中更改GPU内核来使用它。我们需要使用TensorFlow 1.x，因为它最适合暗流。然后我们需要用下面的方法安装你的谷歌硬盘，如果你觉得有困难，请参考这个<a class="ae iu" href="https://colab.research.google.com/notebooks/io.ipynb" rel="noopener ugc nofollow" target="_blank">参考</a>。然后转到项目文件夹，使用下面的代码构建cython_utils。</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="09a7" class="lz iw hi lv b fi ma mb l mc md">%tensorflow_version 1.x //this will force colab to use TF 1.x<br/>from google.colab import drive<br/>drive.mount('/content/drive')<br/>import os<br/>os.getcwd()<br/>os.chdir('/content/drive/MyDrive/ALPR')// changes working directory<br/>!python3 setup.py build_ext --inplace // build cython extensions</span></pre><p id="3263" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">导入所需的库</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="aff6" class="lz iw hi lv b fi ma mb l mc md">import cv2</span><span id="ce53" class="lz iw hi lv b fi mr mb l mc md">from darkflow.net.build import TFNet</span><span id="7a90" class="lz iw hi lv b fi mr mb l mc md">import matplotlib.pyplot as plt</span><span id="ca11" class="lz iw hi lv b fi mr mb l mc md">%config InlineBackend.figure_format = ‘svg’</span></pre><h2 id="2955" class="lz iw hi bd ix me mf mg jb mh mi mj jf ke mk ml jj ki mm mn jn km mo mp jr mq bi translated">培养</h2><p id="923f" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">以下是配置选项，我拍了一批4个图像，每批300个图像，确保将GPU参数添加为1.0:</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="dd96" class="lz iw hi lv b fi ma mb l mc md">%%time</span><span id="e6eb" class="lz iw hi lv b fi mr mb l mc md">options = {"model": "cfg/yolo-voc-1c.cfg",</span><span id="a822" class="lz iw hi lv b fi mr mb l mc md">"load": "weights/tiny-yolo-voc.weights",</span><span id="a459" class="lz iw hi lv b fi mr mb l mc md">"batch": 4,</span><span id="4f97" class="lz iw hi lv b fi mr mb l mc md">"gpu": 1.0,</span><span id="ce4d" class="lz iw hi lv b fi mr mb l mc md">"epoch": 300,</span><span id="06d7" class="lz iw hi lv b fi mr mb l mc md">"train": True,</span><span id="d494" class="lz iw hi lv b fi mr mb l mc md">"annotation": "new_data/labels",</span><span id="2b7b" class="lz iw hi lv b fi mr mb l mc md">"dataset": "new_data/images"}</span></pre><p id="17b7" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">使用以下代码构建模型:</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="ddfc" class="lz iw hi lv b fi ma mb l mc md">%%time</span><span id="5f93" class="lz iw hi lv b fi mr mb l mc md">from darkflow.net.build import TFNet</span><span id="3df6" class="lz iw hi lv b fi mr mb l mc md">tfnet = TFNet(options)</span></pre><p id="fa73" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">训练数据:</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="bb94" class="lz iw hi lv b fi ma mb l mc md">%%time</span><span id="5aa8" class="lz iw hi lv b fi mr mb l mc md">tfnet.train()</span></pre><p id="6148" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">这将花费很多时间(在我的情况下至少2到3个小时)，权重将每2000步保存到您的项目文件夹中一次。我们应该确保进行足够的训练，大约22000次。一旦训练了权重，在google drive的项目文件夹中应该有一个名为ckpt的文件夹，其中应该有训练过的数据。您可以使用下面的代码使用这些权重来构建您的模型。负载参数应该是训练到最后一个检查点的步数。</p><p id="6f1a" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated"><a class="ae iu" href="https://drive.google.com/drive/folders/1k52gpFxyz2RY-UiZ9koheHmHU_pCEKPP?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里的</a>是我的google drive项目文件夹的链接，这样你就可以使用我之前训练的权重(权重在ckpt文件夹中，将其复制到你pc上的项目文件夹中)，最好使用笔记本<a class="ae iu" href="https://github.com/saky9596/ALPR/blob/master/License_plate_recognition-pc.ipynb" rel="noopener ugc nofollow" target="_blank">License _ plate _ recognition-pc . ipynb</a>在你的PC上运行以下内容，因为google colab无法创建单独的窗口来显示视频结果。(像以前一样使用项目文件夹下cmd中的“python 3 setup . py Build _ ext—in place”构建cython_utils，一定要使用TensorFlow 1.x.x和python 3.7)。在您的电脑上再次导入所有需要的库，然后运行下面的代码来检索您之前训练的权重并建立一个模型。</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="fd9b" class="lz iw hi lv b fi ma mb l mc md">%%time</span><span id="cfd7" class="lz iw hi lv b fi mr mb l mc md">options = {</span><span id="e0a0" class="lz iw hi lv b fi mr mb l mc md">‘model’: ‘cfg/yolo-voc-1c.cfg’,</span><span id="6836" class="lz iw hi lv b fi mr mb l mc md">‘load’: 24000,</span><span id="9411" class="lz iw hi lv b fi mr mb l mc md">‘gpu’:1.0,</span><span id="ecb5" class="lz iw hi lv b fi mr mb l mc md">‘train’:True,</span><span id="bc39" class="lz iw hi lv b fi mr mb l mc md">‘threshold’: 0.1,</span><span id="18dd" class="lz iw hi lv b fi mr mb l mc md">‘backup’:’ckpt/’,</span><span id="bf07" class="lz iw hi lv b fi mr mb l mc md">“annotation”: “new_data/labels”,</span><span id="52bd" class="lz iw hi lv b fi mr mb l mc md">“dataset”: “new_data/images”,</span><span id="ae82" class="lz iw hi lv b fi mr mb l mc md">“batch”:4,</span><span id="fa86" class="lz iw hi lv b fi mr mb l mc md">“epoch”:100</span><span id="2f4d" class="lz iw hi lv b fi mr mb l mc md">}</span><span id="c233" class="lz iw hi lv b fi mr mb l mc md">tfnet2 = TFNet(options)<br/>tfnet2.load_from_ckpt()</span></pre><p id="dc3d" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">使用Yolo模型的结果:</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="4b92" class="lz iw hi lv b fi ma mb l mc md"># read the color image and covert to RGB<br/>import cv2</span><span id="17df" class="lz iw hi lv b fi mr mb l mc md">img = cv2.imread(‘new_data/images/licensed_car168.jpeg’,cv2.IMREAD_COLOR)<br/>img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><span id="c61f" class="lz iw hi lv b fi mr mb l mc md"># use YOLO to predict the image<br/>result = tfnet2.return_predict(img)</span><span id="106f" class="lz iw hi lv b fi mr mb l mc md">result</span></pre><figure class="lp lq lr ls fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/49da16ae2da2b154d9628faa73a94f37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*pzmGcGhhL8JUeKjhVk4NqQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Figure 3: Result of YOLO model</figcaption></figure><h2 id="5b64" class="lz iw hi bd ix me mf mg jb mh mi mj jf ke mk ml jj ki mm mn jn km mo mp jr mq bi translated">使用宇宙魔方的字符识别；</h2><p id="79db" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">由于噪声、背景干扰、颜色变化、旋转和倾斜，字符识别对于模型来说总是困难的。因此，我们将使用一个辅助函数，将每个号码牌转换成相似的格式:</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="08d5" class="lz iw hi lv b fi ma mb l mc md">def my_function(image):<br/>    <br/>    # import the necessary packages<br/>    import numpy as np<br/>    #import argparse<br/>    import cv2<br/>    <br/>    # convert the image to grayscale and flip the foreground<br/>    # and background to ensure foreground is now "white" and<br/>    # the background is "black"<br/>    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)<br/>    gray = cv2.bitwise_not(gray)<br/>    <br/>    # threshold the image, setting all foreground pixels to<br/>    # 255 and all background pixels to 0<br/>    thresh = cv2.threshold(gray, 0, 255,<br/>        cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]<br/>    <br/>    # grab the (x, y) coordinates of all pixel values that<br/>    # are greater than zero, then use these coordinates to<br/>    # compute a rotated bounding box that contains all<br/>    # coordinates<br/>    coords = np.column_stack(np.where(thresh &gt; 0))<br/>    angle = cv2.minAreaRect(coords)[-1]<br/>    <br/>    # the `cv2.minAreaRect` function returns values in the<br/>    # range [-90, 0); as the rectangle rotates clockwise the<br/>    # returned angle trends to 0 -- in this special case we<br/>    # need to add 90 degrees to the angle<br/>    if angle &lt; -45:<br/>        angle = -(90 + angle)<br/>    <br/>    # otherwise, just take the inverse of the angle to make<br/>    # it positive<br/>    else:<br/>        angle = -angle<br/>    <br/>    # rotate the image to deskew it<br/>    (h, w) = image.shape[:2]<br/>    center = (w // 2, h // 2)<br/>    M = cv2.getRotationMatrix2D(center, angle, 1.0)<br/>    rotated = cv2.warpAffine(image, M, (w, h),<br/>        flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)<br/>    <br/>    # draw the correction angle on the image so we can validate it<br/>    #cv2.putText(rotated, "Angle: {:.2f} degrees".format(angle),<br/>    #    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)<br/>    <br/>    # show the output image<br/>    print("[INFO] angle: {:.3f}".format(angle))<br/>    cv2.imshow("Input", image)<br/>    cv2.imshow("Rotated", rotated)<br/>    return rotated<br/>    cv2.waitKey(0)</span></pre><p id="a5ad" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">现在让我们测试OCR并可视化结果。</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="6b11" class="lz iw hi lv b fi ma mb l mc md">img1=img.copy()<br/># pull out some info from the results<br/>for i in range(0, len(result)):<br/>tl = (result[i]['topleft']['x'], result[i]['topleft']['y'])<br/>br = (result[i]['bottomright']['x'], result[i]['bottomright']['y'])<br/>label = result[i]['label']<br/># add the box and label and display it<br/>img = cv2.rectangle(img, tl, br, (0, 255, 0), 1)<br/>img = cv2.putText(img, label, tl, cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 0), 2)<br/>plt.imshow(img)<br/>#image[1:(y2 + 1), 1:(x2 + 1)]<br/>plt.show()<br/>for i in range(0,len(result)):<br/>tl = (result[i]['topleft']['x'], result[i]['topleft']['y'])<br/>br = (result[i]['bottomright']['x'], result[i]['bottomright']['y'])<br/>label = result[i]['label']<br/>cropped=img1[tl[1]:(br[1] + 1), tl[0]:(br[0] + 1)]<br/>cropped=my_function(cropped)<br/>image = cropped<br/>plt.imshow(image)<br/>plt.show()<br/>gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)<br/>plt.imshow(gray)<br/>plt.show()<br/>gray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]<br/>plt.imshow(gray)<br/>plt.show()<br/>out_below = pytesseract.image_to_string(gray)<br/>print("OUTPUT:", out_below)<br/>print("Number of Characters:", len(out_below))<br/>plt.imshow(cropped)<br/>plt.show()</span></pre><p id="7d07" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">这将输出:</p><figure class="lp lq lr ls fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mt"><img src="../Images/5901cee62dbf3edc67c3df3e62f87073.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*20ZXJDk8tlPj_Xp29TbRMQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Figure 4: Output of Character Recognition</figcaption></figure><h2 id="3b8b" class="lz iw hi bd ix me mf mg jb mh mi mj jf ke mk ml jj ki mm mn jn km mo mp jr mq bi translated">视频实现模型</h2><p id="6665" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">首先，我们需要将视频转换成较小的尺寸，以便我们的GPU进行处理，但是，如果您有足够的能力，就不需要执行这一步</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="449a" class="lz iw hi lv b fi ma mb l mc md">import moviepy.editor as mp<br/>import time</span><span id="7c76" class="lz iw hi lv b fi mr mb l mc md">clip = mp.VideoFileClip(“1.mp4”)</span><span id="b230" class="lz iw hi lv b fi mr mb l mc md">clip_resized = clip.resize(height=360) # make the height 360px ( According to moviePy documenation The width is then computed so that the width/height ratio is conserved.)</span><span id="5a61" class="lz iw hi lv b fi mr mb l mc md">clip_resized.write_videofile(“1_resized.mp4”)</span></pre><p id="a512" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">我们将视频捕获到OpenCV中，并实现我们上面定义的模型。</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="e8b7" class="lz iw hi lv b fi ma mb l mc md">capture = cv2.VideoCapture('1_resized.mp4')<br/>colors = [tuple(255 * np.random.rand(3)) for i in range(10)]<br/>while (capture.isOpened()):<br/>    stime = time.time()<br/>    ret, frame = capture.read()# ret is a boolean. True when the video is playing.<br/>    frame1=frame.copy()<br/>    if ret:<br/>        results = tfnet2.return_predict(frame)<br/>        <br/>        for color, result in zip(colors, results):<br/>            tl = (result['topleft']['x'], result['topleft']['y'])<br/>            br = (result['bottomright']['x'], result['bottomright']['y'])<br/>            label = result['label']<br/>            cropped=frame1[tl[1]:(br[1] + 1), tl[0]:(br[0] + 1)]<br/>            image = cropped<br/>            plt.imshow(image)<br/>            plt.show()<br/>            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)<br/>            plt.imshow(gray)<br/>            plt.show()<br/>            gray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]<br/>            plt.imshow(gray)<br/>            plt.show()<br/>            plt.show()<br/>            out_below = pytesseract.image_to_string(gray)<br/>            print("OUTPUT:", out_below,len(out_below))<br/>            plt.imshow(cropped)<br/>            plt.show()<br/>        for color, result in zip(colors, results):<br/>            tl = (result['topleft']['x'], result['topleft']['y'])<br/>            br = (result['bottomright']['x'], result['bottomright']['y'])<br/>            label = result['label']<br/>            frame = cv2.rectangle(frame, tl, br, color, 7)<br/>            frame = cv2.putText(frame, label+out_below, tl, cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 0), 2)<br/>        cv2.imshow('frame', frame)<br/>        #cv2.imshow('frame', frame)<br/>        print('FPS {:.1f}'.format(1 / (time.time() - stime)))<br/>        if cv2.waitKey(1) &amp; 0xFF == ord('q'):<br/>            break<br/>    else:<br/>        capture.release()<br/>        cv2.destroyAllWindows()<br/>        break</span></pre><p id="3f3f" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">要关闭窗口，请中断内核或运行以下代码:</p><pre class="lp lq lr ls fd lu lv lw lx aw ly bi"><span id="4668" class="lz iw hi lv b fi ma mb l mc md">capture.release()<br/>cv2.destroyAllWindows()</span></pre><p id="eae8" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">这将需要更多的训练和优化，特别是关于IOU阈值，以消除重复的对象检测。</p><h1 id="4e0f" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">5.结论和参考文献</h1><p id="cdf1" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">恭喜你读到这篇长文的结尾。使用这种方法，基本的对象检测和识别将变得轻而易举，这种特定的应用程序可用于简化任务，如允许车辆进入受限区域。随时训练，并进一步优化它。</p><p id="eb77" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">驱动器和Github链接中的我的项目文件夹:</p><div class="mu mv ez fb mw mx"><a href="https://drive.google.com/drive/folders/1k52gpFxyz2RY-UiZ9koheHmHU_pCEKPP?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hj fi z dy nc ea eb nd ed ef hh bi translated">ALPR -谷歌车道</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">编辑描述</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">drive.google.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl io mx"/></div></div></a></div><p id="bcb1" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated"><a class="ae iu" href="https://github.com/saky9596/ALPR" rel="noopener ugc nofollow" target="_blank">https://github.com/saky9596/ALPR</a></p><h2 id="c981" class="lz iw hi bd ix me mf mg jb mh mi mj jf ke mk ml jj ki mm mn jn km mo mp jr mq bi translated">参考资料:</h2><ol class=""><li id="d235" class="kr ks hi jv b jw jx ka kb ke kt ki ku km kv kq kw kx ky kz bi translated">雷德蒙，j .，迪夫瓦拉，s .，吉尔希克，r .和法尔哈迪，a .，2016年。你只看一次:统一的，实时的物体检测。在<em class="nm">IEEE计算机视觉和模式识别会议论文集</em>(第779-788页)。</li><li id="6ec8" class="kr ks hi jv b jw la ka lb ke lc ki ld km le kq kw kx ky kz bi translated">【https://pjreddie.com/darknet/yolo/ T4】</li><li id="d44d" class="kr ks hi jv b jw la ka lb ke lc ki ld km le kq kw kx ky kz bi translated"><a class="ae iu" href="https://github.com/tzutalin/labelImg" rel="noopener ugc nofollow" target="_blank">https://github.com/tzutalin/labelImg</a></li><li id="8bd0" class="kr ks hi jv b jw la ka lb ke lc ki ld km le kq kw kx ky kz bi translated"><a class="ae iu" href="https://towardsdatascience.com/guide-to-car-detection-using-yolo-48caac8e4ded" rel="noopener" target="_blank">https://towards data science . com/guide-to-car-detection-using-yolo-48 cac8 E4 ded</a></li></ol></div></div>    
</body>
</html>
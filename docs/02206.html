<html>
<head>
<title>Machine Learning Logistic Regression in Python From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的机器学习逻辑回归从零开始</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/machine-learning-logistic-regression-in-python-from-scratch-3d9e2875f27a?source=collection_archive---------3-----------------------#2021-05-07">https://medium.com/geekculture/machine-learning-logistic-regression-in-python-from-scratch-3d9e2875f27a?source=collection_archive---------3-----------------------#2021-05-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/fada3dccc6eb6eb8d87932eef42d1612.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qL73SxBZMyUKlPKvuH3uyw.jpeg"/></div></div></figure><p id="4a4b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">分类是监督学习的两个分支之一。顾名思义，它根据对象的特性将对象分成组或类。</p><p id="9932" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">逻辑回归是最常用的分类算法之一。在本文中，我将展示该算法的Python实现。它将一步一步地继续，我们将从头开始构建算法。</p><h2 id="634e" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">现在让我们深入研究代码！</h2><h1 id="dc9d" class="kj jp hi bd jq kk kl km ju kn ko kp jy kq kr ks kb kt ku kv ke kw kx ky kh kz bi translated">1.加载和数据准备。</h1><p id="8eda" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">最初，将导入pandas模块，使用“read_csv”读取包含数据集的csv文件，并使用“head”函数打印出前10行:</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="043b" class="jo jp hi lk b fi lo lp l lq lr">import pandas as pd</span><span id="f9ed" class="jo jp hi lk b fi ls lp l lq lr">df = pd.read_csv('Social_Network_Ads.csv')<br/>print(df.head(10))</span></pre><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/b3c5e844892956e89182286faa631e01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*JUBLz6HujpK89kWCnYT3AA.png"/></div></figure><p id="b0d5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">查看数据集，算法的目标是<em class="lu">消费者是否已经购买了产品</em>。这就是为什么它会是<strong class="is hj"> y值。</strong></p><p id="ae68" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在给定的特性中，“用户ID”不会有任何影响，因为它对消费者购买产品没有任何影响。在这种情况下，我们只剩下3个特征:性别、年龄和估计工资。这三个特征将成为<strong class="is hj">的X值。</strong></p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="dca5" class="jo jp hi lk b fi lo lp l lq lr">X = df[['Gender', 'Age', 'EstimatedSalary']]<br/>y = df['Purchased']</span></pre><p id="d98c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，X和y数据集将如下所示:</p><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/9cd47d8dc67f2ebdb3b0b23d88c985ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*z75Xn11HTMkvnXx2Z8Y8-g.png"/></div></figure><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/0f2a11e5fdaff1c5b7160f395c86e64f.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*9QZtnE3Xd7CZwYPIio4kkQ.png"/></div></figure></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><p id="374c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">与线性回归一样，逻辑回归中也有偏差项。以这种方式，<strong class="is hj">一列1</strong>将被添加到x .的开头，因为它在矩阵乘法中非常方便。</p><p id="2970" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里，“m”是数据集中的行数。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="3c40" class="jo jp hi lk b fi lo lp l lq lr">import numpy as np</span><span id="294a" class="jo jp hi lk b fi ls lp l lq lr">m = X.shape[0]<br/>a = np.ones((m, 1))<br/>X.insert(loc = 0, column = 'Ones', value = a)</span></pre></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><p id="f81e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于我们在连续特征中有一个分类数据(性别)，我们需要用<strong class="is hj">虚拟变量来处理它。</strong>在使用虚拟变量时，如果有(n)个类别，则(n-1)个变量就足以将其转换为连续数据。在我们的例子中，将生成“性别_男性”列，如果值为“1”，则表示男性，反之亦然，如果值为“0”，则表示女性。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="61dc" class="jo jp hi lk b fi lo lp l lq lr">X.loc[X['Gender'] == 'Male', 'Gender_Male'] = 1         #1 if male<br/>X.loc[X['Gender'] == 'Female', 'Gender_Male'] = 0       #0 if female</span><span id="73c6" class="jo jp hi lk b fi ls lp l lq lr">del X['Gender']               #delete intial gender column</span></pre></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><p id="274b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">此外，工资和年龄列的比例并不接近，这将对模型的准确性造成严重问题。我们将使用一种称为标准化的<strong class="is hj">特征缩放</strong>技术:</p><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es me"><img src="../Images/845dbeff3dff85eadf14883a207c35fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*wopYk4-3pEuhv8Edx7qMaw.png"/></div></figure><p id="f89e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里，</p><ul class=""><li id="1283" class="mf mg hi is b it iu ix iy jb mh jf mi jj mj jn mk ml mm mn bi translated">x̅是中庸之道；</li><li id="537f" class="mf mg hi is b it mo ix mp jb mq jf mr jj ms jn mk ml mm mn bi translated">σ是标准差。</li></ul><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="69a6" class="jo jp hi lk b fi lo lp l lq lr">age_std = X['Age'].std()<br/>age_ave = X['Age'].mean()</span><span id="96af" class="jo jp hi lk b fi ls lp l lq lr">sala_std = X['EstimatedSalary'].std()<br/>sala_ave = X['EstimatedSalary'].mean()</span><span id="c840" class="jo jp hi lk b fi ls lp l lq lr">X['Age'] = (X['Age'].subtract(age_ave)).divide(age_std)<br/>X['EstimatedSalary'] =(X['EstimatedSalary'].subtract(sala_ave)).divide(sala_std)</span></pre></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><p id="d632" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">像往常一样，我们将数据集分为测试集和训练集:</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="c3ed" class="jo jp hi lk b fi lo lp l lq lr">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)</span></pre></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><p id="408d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">到目前为止，这项工作是在熊猫数据帧上完成的，因为我们只需要修改数据集。但是现在，当我们开始对数据集进行<strong class="is hj">数学运算</strong>时，我们将熊猫数据帧转换成<strong class="is hj"> numpy数组</strong>。由于这个原因，numpy阵列在计算上有更好的速度，并且它们提供了矩阵运算的很大可变性。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="a165" class="jo jp hi lk b fi lo lp l lq lr">X_train, X_test, y_train, y_test = X_train.to_numpy(), X_test.to_numpy(), y_train.to_numpy(), y_test.to_numpy()</span></pre><h1 id="a572" class="kj jp hi bd jq kk kl km ju kn ko kp jy kq kr ks kb kt ku kv ke kw kx ky kh kz bi translated">2.功能</h1><p id="0a7b" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated"><strong class="is hj"> Sigmoid函数</strong>接受一个输入，仅返回0和1之间的输出<em class="lu"/>。在逻辑回归中，你计算样本在一个类别中的概率，概率用0到1之间的数字<em class="lu"/>表示。因此，Sigmoid函数是逻辑回归中的关键函数之一。</p><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/d7025d160f64e8637f516c7cb6687423.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*BwZmaMBaVW8f8lBAQUoytw.png"/></div></figure><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="deb9" class="jo jp hi lk b fi lo lp l lq lr">def sigmoid(z):<br/> return (1/(1+np.exp(-z)))</span></pre></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><p id="8459" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">逻辑回归中的假设</strong>与线性回归相同，但有一点不同。在逻辑回归中，假设的输出只需要在0和1之间。因为它显示了一个物体在某一类中的<em class="lu">概率</em>，概率不能小于0也不能大于1。为此，使用Sigmoid函数，这是与线性回归中假设的区别。</p><p id="c0a2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">“matmul”是numpy模块的一个功能，用于矩阵乘法。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="1ddd" class="jo jp hi lk b fi lo lp l lq lr">def h(theta, X):<br/> return sigmoid(np.matmul(X, theta))</span></pre></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><p id="bf7c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">成本函数</strong>决定了模型与数据集的拟合程度。如果它的量级很高，则意味着模型不适合数据集，如果它的量级很低，则意味着模型可以使用。因此，优化算法试图最小化成本函数值，换句话说，试图使模型更好地适应数据集。</p><p id="4ff5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们回到线性回归，它的成本函数是:</p><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mu"><img src="../Images/cc02f2474d6702491ae0035f53011229.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*tFVOa1t1o0Q8aNj98SuguA.png"/></div></figure><p id="846c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果在逻辑回归中使用相同的成本函数，我们将有一个非凸函数，梯度下降将无法优化它，因为将有多个最小值。非凸性的原因是，用于计算假设的sigmoid函数是非线性函数。因此，我们将使用<strong class="is hj">一个不同的成本函数:</strong></p><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mv"><img src="../Images/0dda61902c740950276ee5f5fdc57e52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uhB5TyfzvikNAnhJ-q3l0w.png"/></div></div></figure><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="3560" class="jo jp hi lk b fi lo lp l lq lr">def cost_function(X, y, theta, m):</span><span id="2d67" class="jo jp hi lk b fi ls lp l lq lr"> y = y.reshape(y.shape[0], 1)<br/> H = h(theta, X)</span><span id="671b" class="jo jp hi lk b fi ls lp l lq lr">return (sum((y)*np.log(H) + (1-y)*np.log(1-H))) / (-m)</span></pre></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><p id="488e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">梯度下降</strong>的作用是优化θ参数。它的方程是由成本函数的求导得到的。它类似于线性回归中的梯度下降函数，但由于假设函数不同，这些梯度下降函数也不相同。</p><p id="03ad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了简单起见，代码是用分离的操作一步一步编写的。你可以很容易地遵循这个等式:</p><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/f8586a9c279a9ddb077a2d286b22a591.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*KrM3VzmYtI48ur-tr1kKpQ.png"/></div></figure><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="9ceb" class="jo jp hi lk b fi lo lp l lq lr">def gradient_descent(theta, X, y, alfa, m):</span><span id="677b" class="jo jp hi lk b fi ls lp l lq lr"> H = h(theta, X)<br/> H = H.reshape((H.shape[0],))</span><span id="b10e" class="jo jp hi lk b fi ls lp l lq lr"> diff = np.subtract(H, y)<br/> a = np.matmul(np.transpose(X), diff).reshape((theta.shape[0],1))<br/> <br/> theta = theta - (alfa/m) * a</span><span id="844d" class="jo jp hi lk b fi ls lp l lq lr"> return theta</span></pre></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><p id="67e0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在算法期间，梯度下降运行多次，准确地说，是在迭代次数上。在一些迭代之后，成本函数的值降低，并且看到成本函数的值是好的实践。因为在某一点之后，成本函数值不变或变化极小。在这种情况下，在该点之后反复运行梯度下降是没有用的，并且在下一次尝试中减少迭代次数。</p><p id="d4f8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，我们将所有这些操作组合在一起——定义迭代次数，选择在多少次迭代后您希望看到成本函数的返回，称为梯度下降函数，成为一个函数，这个函数称为<strong class="is hj">训练函数。</strong></p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="af38" class="jo jp hi lk b fi lo lp l lq lr">def train(X, y, theta, alfa, m, num_iter):</span><span id="00ed" class="jo jp hi lk b fi ls lp l lq lr">  for i in range(num_iter):<br/>    theta = gradient_descent(theta, X, y, alfa, m)</span><span id="a4ad" class="jo jp hi lk b fi ls lp l lq lr">    if i % 200== 0:<br/>      print("Cost: ", cost_function(X, y, theta, m))</span><span id="c4e3" class="jo jp hi lk b fi ls lp l lq lr">return theta</span></pre></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><p id="542c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">预测函数</strong>以θ和X为输入，通过比较假设(h)的答案和阈值，返回“0”或“1”。通常，阈值选择为0.5，但可以在函数中设置为任何所需的值:</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="ed91" class="jo jp hi lk b fi lo lp l lq lr">def predict(X, theta, threshold = 0.5):<br/> <br/> a = h(theta, X)<br/> a [a &gt;= threshold] = 1<br/> a [a &lt; threshold]  = 0</span><span id="ce96" class="jo jp hi lk b fi ls lp l lq lr">return a</span></pre></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><p id="428f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">评分函数</strong>用于计算模型的准确度。“y1”是数据集中给定的答案，“y2”是模型计算的答案。分数函数比较它们，并找出正确预测的答案的百分比。</p><p id="fc5b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我使用了xnor门，如果值相同，它返回1，如果值不相等，它返回0:</p><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/4545bcfb633f07be0f673cdce4f4eeaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*tR0UBkPG40rc6djw7kPgJQ.png"/></div></figure><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="10ad" class="jo jp hi lk b fi lo lp l lq lr">def score(y1, y2):</span><span id="f803" class="jo jp hi lk b fi ls lp l lq lr"> #y1 is the correct answers<br/> #y2 is calculated by the model</span><span id="b025" class="jo jp hi lk b fi ls lp l lq lr"> y1 = y1.reshape(y1.shape[0], 1)<br/> y2 = y2.reshape(y2.shape[0], 1)</span><span id="1c5a" class="jo jp hi lk b fi ls lp l lq lr"> y1_not = (1 - y1).reshape(y1.shape[0], 1)<br/> y2_not = (1 - y2).reshape(y1.shape[0], 1)</span><span id="c201" class="jo jp hi lk b fi ls lp l lq lr"> a = np.multiply(y1_not, y2_not) + np.multiply(y1, y2)   <br/> #1 means  correct prediction, 0 means wrong prediction<br/> <br/> ones_ = np.count_nonzero(a == 1)  #count ones to get the percentage</span><span id="d692" class="jo jp hi lk b fi ls lp l lq lr"> return (ones_ / y1.shape[0]) * 100</span></pre><h1 id="4975" class="kj jp hi bd jq kk kl km ju kn ko kp jy kq kr ks kb kt ku kv ke kw kx ky kh kz bi translated">3.参数和调用函数的初始化</h1><p id="9670" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">为了初始化theta，我们需要知道与数据集中的列数相同的特征数。另一方面，在梯度下降中需要样本数。所以我们用shape函数找到这两个值，它返回numpy数组中的行数和列数:</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="8f16" class="jo jp hi lk b fi lo lp l lq lr">m = X_train.shape[0]  #number of rows<br/>n = X_train.shape[1]  #number of columns</span></pre></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><p id="878f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">θ向量包含将被算法优化的参数。元素的数量应该与特征的数量相同，这就是为什么我们用“n”行零来初始化它。</p><p id="0900" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">迭代次数最初定义为3000左右的值，通过查看成本函数的值，您可以在以后减少或增加它:如果成本函数不再减少，就没有必要一遍又一遍地运行算法，因此我们设置较少的迭代次数。另一方面，如果成本函数仍然变小，您可以增加迭代次数，并观察它是否提高了模型性能。</p><p id="2f32" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">α是算法的学习速率。其值在0.001和10之间变化。同样，没有一个确切的数字对每个模型都是最佳的。您需要观察哪个值适合您的模型。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="ca35" class="jo jp hi lk b fi lo lp l lq lr">theta = np.zeros((n, 1))<br/>num_iter = 2000<br/>alfa = 0.3</span></pre></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><p id="0956" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在所有需要的值都设置好了，我们终于可以<strong class="is hj">运行这个节目了</strong>！</p><p id="cf69" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里，训练函数使用<strong class="is hj">训练集</strong>返回优化的theta向量，theta向量用于预测<strong class="is hj">测试集</strong>中的答案。之后，我们返回score来查看我们的模型表现如何。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="4ee7" class="jo jp hi lk b fi lo lp l lq lr">opt_theta = train(X_train, y_train, theta, alfa, m, num_iter)<br/>y_ = predict(X_test, opt_theta)<br/>print("Accuracy: ", score(y_test, y_))</span></pre></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><p id="8e38" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">数据集及全部代码:<a class="ae my" href="https://github.com/anarabiyev/Logistic-Regression-Python-implementation-from-scratch" rel="noopener ugc nofollow" target="_blank">https://github . com/anarabiyev/Logistic-Regression-Python-implementation-from scratch</a></p><h1 id="ac4e" class="kj jp hi bd jq kk kl km ju kn ko kp jy kq kr ks kb kt ku kv ke kw kx ky kh kz bi translated">参考</h1><p id="e75a" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated"><a class="ae my" href="https://en.wikipedia.org/wiki/Feature_scaling#Standardization_(Z-score_Normalization)" rel="noopener ugc nofollow" target="_blank">特征缩放标准化</a></p><p id="afdc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">理论背景:<a class="ae my" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">https://www.coursera.org/learn/machine-learning</a></p></div></div>    
</body>
</html>
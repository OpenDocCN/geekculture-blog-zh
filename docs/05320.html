<html>
<head>
<title>Data Science👨‍💻: Scrapping Data Using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学👨‍💻:使用Python废弃数据</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/1-data-science-scrapping-data-e244669e74e2?source=collection_archive---------12-----------------------#2021-07-18">https://medium.com/geekculture/1-data-science-scrapping-data-e244669e74e2?source=collection_archive---------12-----------------------#2021-07-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="89e5" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">这是关于数据科学概念的整个系列。这是第一个关于数据废弃的博客，或者我们可以说是数据科学中的数据收集过程。请务必关注我，这样您就可以更新以下系列。</p><blockquote class="je"><p id="3e35" class="jf jg hi bd jh ji jj jk jl jm jn jd dx translated">当某件事足够重要时，即使胜算对你不利，你也要去做。</p><p id="2cfb" class="jf jg hi bd jh ji jj jk jl jm jn jd dx translated">~埃隆·马斯克</p></blockquote><blockquote class="jo jp jq"><p id="2281" class="ig ih jr ii b ij js il im in jt ip iq ju jv it iu jw jx ix iy jy jz jb jc jd hb bi translated"><strong class="ii hj">简介</strong></p></blockquote><p id="ae9e" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">这个博客主要是从网站数据中搜集基本数据。数据科学就是与数据打交道。因此，数据科学最重要的部分之一就是数据收集。所以这里我们从网页上抓取数据。有许多收集数据的方法。对于数据抓取，我使用了以下库:</p><p id="192b" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">(1) <strong class="ii hj">硒</strong>:铬驱动</p><p id="21ca" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">(2) <strong class="ii hj">美汤</strong>:网刮</p><p id="d49c" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">(3) <strong class="ii hj">熊猫</strong>:数据操作</p><p id="8118" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">为此，我们从<strong class="ii hj"> Covid 19印度仪表板数据</strong>中抓取数据。</p><blockquote class="je"><p id="937b" class="jf jg hi bd jh ji jj jk jl jm jn jd dx translated">链接:<a class="ae ka" href="https://www.covid19india.org/" rel="noopener ugc nofollow" target="_blank">https://www.covid19india.org/</a></p><p id="cbd3" class="jf jg hi bd jh ji jj jk jl jm jn jd dx translated">关于<a class="ae ka" href="https://www.questionpro.com/blog/data-collection/" rel="noopener ugc nofollow" target="_blank">数据收集的更多信息。</a></p></blockquote><blockquote class="jo jp jq"><p id="745c" class="ig ih jr ii b ij js il im in jt ip iq ju jv it iu jw jx ix iy jy jz jb jc jd hb bi translated"><strong class="ii hj">概述:</strong></p></blockquote><p id="a805" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">我通过三个主要步骤完成了这一过程:</p><p id="d481" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">(1)使用selenium chrome web drive访问网页。</p><p id="b9cb" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">(2)用美汤刮数据。</p><p id="341d" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">(3)将数据转换为pandas数据框，并将数据保存为CSV文件。</p><blockquote class="jo jp jq"><p id="a4ab" class="ig ih jr ii b ij ik il im in io ip iq ju is it iu jw iw ix iy jy ja jb jc jd hb bi translated"><strong class="ii hj">编码实现:</strong></p></blockquote><p id="1636" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">如需完整代码，请查看本博客末尾的实现链接。</p><h2 id="302b" class="kb kc hi bd kd ke kf kg kh ki kj kk kl ir km kn ko iv kp kq kr iz ks kt ku kv bi translated"><strong class="ak">安装依赖关系:</strong></h2><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="517e" class="kb kc hi lb b fi lf lg l lh li">pip install selenium pandas beautifulsoup4</span></pre><h2 id="19fa" class="kb kc hi bd kd ke kf kg kh ki kj kk kl ir km kn ko iv kp kq kr iz ks kt ku kv bi translated"><strong class="ak">导入依赖关系:</strong></h2><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="81a0" class="kb kc hi lb b fi lf lg l lh li"><strong class="lb hj">from</strong> <strong class="lb hj">selenium</strong> <strong class="lb hj">import</strong> webdriver<br/><strong class="lb hj">from</strong> <strong class="lb hj">bs4</strong> <strong class="lb hj">import</strong> BeautifulSoup<br/><strong class="lb hj">import</strong> <strong class="lb hj">pandas</strong> <strong class="lb hj">as</strong> <strong class="lb hj">pd</strong></span></pre><h2 id="7588" class="kb kc hi bd kd ke kf kg kh ki kj kk kl ir km kn ko iv kp kq kr iz ks kt ku kv bi translated"><strong class="ak">设置Chrome驱动:</strong></h2><p id="6ba0" class="pw-post-body-paragraph ig ih hi ii b ij lj il im in lk ip iq ir ll it iu iv lm ix iy iz ln jb jc jd hb bi translated">从这个<a class="ae ka" href="https://chromedriver.chromium.org/downloads" rel="noopener ugc nofollow" target="_blank">链接</a>下载Chrome驱动。</p><p id="2463" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">解压文件，在你喜欢的地方设置文件，然后<strong class="ii hj"> <em class="jr">在环境变量中添加PATH变量。</em> </strong></p><p id="1be8" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">我在<strong class="ii hj">系统变量</strong>中添加了路径。</p><p id="63b5" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">我将chrome驱动程序添加到我的工作目录中。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="ea9e" class="kb kc hi lb b fi lf lg l lh li">driver = webdriver.Chrome("chromedriver")</span></pre><p id="29ad" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">你必须添加你的chromedriver.exe路径。</p><h2 id="9c04" class="kb kc hi bd kd ke kf kg kh ki kj kk kl ir km kn ko iv kp kq kr iz ks kt ku kv bi translated"><strong class="ak">网页信息:</strong></h2><p id="915b" class="pw-post-body-paragraph ig ih hi ii b ij lj il im in lk ip iq ir ll it iu iv lm ix iy iz ln jb jc jd hb bi translated">我们有很多关于印度covid情况的信息。我们获取表格，特别是关于covid病例总数、完全康复病例数和每个州的疫苗剂量的列。</p><p id="6fc1" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">列出州名、covid病例总数、治愈病例总数和疫苗剂量的空列表。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="6317" class="kb kc hi lb b fi lf lg l lh li">states = []<br/>total_covid_cases = []<br/>total_recoverd_cases = []<br/>vaccinated_people = []</span></pre><p id="ca49" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">将网页的Url添加到驱动程序中，这样我们就可以获得特定网页的源代码。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="5d57" class="kb kc hi lb b fi lf lg l lh li">driver.get("https://www.covid19india.org/")</span></pre><h2 id="cfc4" class="kb kc hi bd kd ke kf kg kh ki kj kk kl ir km kn ko iv kp kq kr iz ks kt ku kv bi translated"><strong class="ak">刮取数据:</strong></h2><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="9b18" class="kb kc hi lb b fi lf lg l lh li">content = driver.page_source<br/>soup = BeautifulSoup(content)</span></pre><p id="1586" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">首先，我们获得页面的源代码，然后创建一个漂亮的Soup对象，以便我们可以对它执行一些操作。</p><p id="ceb7" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">现在我们必须知道我们要获取什么，类名是什么，什么特定的标签包含我们想要的信息。所有这些问答我们都可以通过<strong class="ii hj">查看网页</strong>得到。为此，你必须有一点点关于HTML和CSS的<strong class="ii hj">知识。</strong></p><p id="ce17" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">因此，对于我们的Covid数据，我们抓取包含在<em class="jr">‘表格容器’</em>类之间的整个表格。Soup有一个内置的函数find，它将查找我们的模式的第一次出现。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="8297" class="kb kc hi lb b fi lf lg l lh li">table = soup.find('div',attrs={'class':'table-container'})</span></pre><p id="0475" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">在这条线的上面，意味着在特定的网页中查找div标签，这个标签有一个类名<em class="jr">‘table container’</em>。</p><p id="f923" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">现在我们有了一个表，但是我们只想获取一些列，所以我们必须再次过滤这个表。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="f0d2" class="kb kc hi lb b fi lf lg l lh li">data = []<br/>row_data = []<br/><strong class="lb hj">for</strong> index , item <strong class="lb hj">in</strong> enumerate(table.find_all('div' , attrs={'class' : 'total'})):<br/>    <strong class="lb hj">if</strong> index%6 == 0 <strong class="lb hj">and</strong> index != 0:<br/>        data.append(row_data)<br/>        row_data = []<br/>    row_data.append(item.text)</span></pre><p id="ed28" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">在这里，我们首先从表中取出所有数据，并制作一个列表结构 的<strong class="ii hj"> <em class="jr">列表。因此，data[0]表示covid19 dashboard India网页上表格的第一列。</em></strong></p><figure class="kw kx ky kz fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es lo"><img src="../Images/5dfac5ba8634ca1ec06d63ee1c81af51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vkLDV-jatrqbXzxHe9ISOg.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx">The highlighted area we get by using data[0]</figcaption></figure><p id="fcb1" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">现在，在获得列表结构列表后，我们使用python中的特殊循环，将值添加到空列表中。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="0f61" class="kb kc hi lb b fi lf lg l lh li"><strong class="lb hj">for</strong> d <strong class="lb hj">in</strong> data:<br/>    total_covid_cases.append(d[0])<br/>    total_recoverd_cases.append(d[2])<br/>    vaccinated_people.append(d[-1])</span></pre><p id="faf2" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">只要关注代码，你就会明白。很简单…👍</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="c94e" class="kb kc hi lb b fi lf lg l lh li"><strong class="lb hj">for</strong> item <strong class="lb hj">in</strong> table.find_all('div',attrs={'class':'state-name'}):<br/>    states.append(item.text)</span></pre><p id="fcee" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">之后，我们将州名添加到州列表中。对于state name，我们必须获取所有的div标签，并将类作为“state-name”。</p><h2 id="0432" class="kb kc hi bd kd ke kf kg kh ki kj kk kl ir km kn ko iv kp kq kr iz ks kt ku kv bi translated"><strong class="ak">将数据转换为熊猫数据帧:</strong></h2><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="4122" class="kb kc hi lb b fi lf lg l lh li">data = pd.DataFrame({'States':states , <br/>'Total Covid Cases':total_covid_cases,<br/>'Total Recovered Cases':total_recoverd_cases,<br/>'Vaccine Doses Administered':vaccinated_people})</span></pre><p id="a437" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">我们得到这样的结果。</p><figure class="kw kx ky kz fd lp er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es ma"><img src="../Images/0b4e40ed39c75f03dcb879d4d5b35bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sEV1inPrCOx38eca2MrnVA.png"/></div></div></figure><h2 id="f2fc" class="kb kc hi bd kd ke kf kg kh ki kj kk kl ir km kn ko iv kp kq kr iz ks kt ku kv bi translated"><strong class="ak">将数据帧保存为CSV: </strong></h2><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="0e7a" class="kb kc hi lb b fi lf lg l lh li">data.to_csv('covid-report-statewise.csv')</span></pre><p id="dcf1" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">通过使用<strong class="ii hj"> <em class="jr"> to_csv </em> </strong>功能我们可以将数据帧转换成csv文件。</p><blockquote class="jo jp jq"><p id="e8e9" class="ig ih jr ii b ij ik il im in io ip iq ju is it iu jw iw ix iy jy ja jb jc jd hb bi translated"><strong class="ii hj">代码:</strong></p></blockquote><div class="mb mc ez fb md me"><a href="https://github.com/manthan89-py/Data-Science/tree/master/Practical%201%20Scrapping%20the%20Data" rel="noopener  ugc nofollow" target="_blank"><div class="mf ab dw"><div class="mg ab mh cl cj mi"><h2 class="bd hj fi z dy mj ea eb mk ed ef hh bi translated">man than 89-py/数据科学</h2><div class="ml l"><h3 class="bd b fi z dy mj ea eb mk ed ef dx translated">这个知识库包含了我们大学数据科学学科的实践。-man than 89-py/数据科学</h3></div><div class="mm l"><p class="bd b fp z dy mj ea eb mk ed ef dx translated">github.com</p></div></div><div class="mn l"><div class="mo l mp mq mr mn ms lu me"/></div></div></a></div><blockquote class="jo jp jq"><p id="7e4e" class="ig ih jr ii b ij ik il im in io ip iq ju is it iu jw iw ix iy jy ja jb jc jd hb bi translated"><strong class="ii hj">结论:</strong></p></blockquote><p id="0771" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">我们可以通过使用web scrapping库(如beautiful soup、scrappy等)从网页中获取任何数据。在转换成pandas后，我们可以对该数据应用所有Pandas函数。</p><p id="73a4" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">更多关于<strong class="ii hj">熊猫</strong>功能<a class="ae ka" href="https://pandas.pydata.org/pandas-docs/stable/reference/general_functions.html" rel="noopener ugc nofollow" target="_blank">在这里。</a></p><p id="fe71" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">更多关于<strong class="ii hj">美汤</strong> <a class="ae ka" href="https://beautiful-soup-4.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">在这里。</a></p><blockquote class="jo jp jq"><p id="90ae" class="ig ih jr ii b ij ik il im in io ip iq ju is it iu jw iw ix iy jy ja jb jc jd hb bi translated"><strong class="ii hj">领英:</strong></p></blockquote><div class="mb mc ez fb md me"><a href="https://www.linkedin.com/in/manthanbhikadiya/" rel="noopener  ugc nofollow" target="_blank"><div class="mf ab dw"><div class="mg ab mh cl cj mi"><h2 class="bd hj fi z dy mj ea eb mk ed ef hh bi translated">印度古吉拉特邦苏拉特曼丹·比卡第亚-查罗特科技大学|…</h2><div class="ml l"><h3 class="bd b fi z dy mj ea eb mk ed ef dx translated">查看Manthan Bhikadiya在世界上最大的职业社区LinkedIn上的个人资料。Manthan有2个工作列在…</h3></div><div class="mm l"><p class="bd b fp z dy mj ea eb mk ed ef dx translated">www.linkedin.com</p></div></div></div></a></div><blockquote class="jo jp jq"><p id="e9ec" class="ig ih jr ii b ij ik il im in io ip iq ju is it iu jw iw ix iy jy ja jb jc jd hb bi translated"><strong class="ii hj">更多项目和博客:</strong></p></blockquote><p id="de6e" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="jr">项目:</em> </strong></p><div class="mb mc ez fb md me"><a href="https://github.com/manthan89-py" rel="noopener  ugc nofollow" target="_blank"><div class="mf ab dw"><div class="mg ab mh cl cj mi"><h2 class="bd hj fi z dy mj ea eb mk ed ef hh bi translated">manthan89-py -概述</h2><div class="ml l"><h3 class="bd b fi z dy mj ea eb mk ed ef dx translated">对AI、深度学习、机器学习、计算机视觉、区块链、Flutter感兴趣😇。做一些竞争性的…</h3></div><div class="mm l"><p class="bd b fp z dy mj ea eb mk ed ef dx translated">github.com</p></div></div><div class="mn l"><div class="mt l mp mq mr mn ms lu me"/></div></div></a></div><p id="25f0" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="jr">博客:</em> </strong></p><div class="mb mc ez fb md me"><a href="https://manthan-bhikadiya.medium.com/" rel="noopener follow" target="_blank"><div class="mf ab dw"><div class="mg ab mh cl cj mi"><h2 class="bd hj fi z dy mj ea eb mk ed ef hh bi translated">曼丹·比卡第亚·🖋-中等</h2><div class="ml l"><h3 class="bd b fi z dy mj ea eb mk ed ef dx translated">阅读曼丹·比丘迪亚·🖋在媒介上的作品。对人工智能、深度学习、机器学习、计算机视觉感兴趣…</h3></div><div class="mm l"><p class="bd b fp z dy mj ea eb mk ed ef dx translated">manthan-bhikadiya.medium.com</p></div></div><div class="mn l"><div class="mu l mp mq mr mn ms lu me"/></div></div></a></div><blockquote class="jo jp jq"><p id="482a" class="ig ih jr ii b ij ik il im in io ip iq ju is it iu jw iw ix iy jy ja jb jc jd hb bi translated"><strong class="ii hj">最后备注:</strong></p></blockquote><p id="0e1a" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj">感谢阅读！如果你喜欢这篇文章，请点击</strong>👏<strong class="ii hj">尽可能多的按按钮。这将意味着很多，并鼓励我继续分享我的知识。如果你喜欢我的内容，请在medium上关注我，我会尽可能多地发布博客。</strong></p></div></div>    
</body>
</html>
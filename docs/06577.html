<html>
<head>
<title>How to Execute a REST API call on Apache Spark the Right Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何正确地在Apache Spark上执行REST API调用</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/how-to-execute-a-rest-api-call-on-apache-spark-the-right-way-in-python-4367f2740e78?source=collection_archive---------3-----------------------#2021-08-24">https://medium.com/geekculture/how-to-execute-a-rest-api-call-on-apache-spark-the-right-way-in-python-4367f2740e78?source=collection_archive---------3-----------------------#2021-08-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/518781cce0bd203b594e0b8fd3d2bff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dRh9pO09kE78756X"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Photo by <a class="ae iu" href="https://unsplash.com/@jeztimms?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jez Timms</a> on <a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="756c" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">介绍</h1><p id="5025" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">本文使用Python作为例子。对于那些正在寻找Scala解决方案的人来说，这些理论和方法是完全适用的，请查看我的Github repo中的Scala源代码【https://github.com/jamesshocking/Spark-REST-API-UDF-Scala】<a class="ae iu" href="https://github.com/jamesshocking/Spark-REST-API-UDF-Scala" rel="noopener ugc nofollow" target="_blank"><em class="kr"/></a><em class="kr">。</em></p><p id="fcfd" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">注:2022年10月，REST API终点(<a class="ae iu" href="https://vpic.nhtsa.dot.gov/api/vehicles/getallmakes?format=json" rel="noopener ugc nofollow" target="_blank">https://vpic.nhtsa.dot.gov/api/vehicles/getallmakes?演示代码中使用的format=json </a>)在调用者(Spark代码)没有识别出是什么浏览器类型(即Chrome、Edge)时不再接受请求。当向远程服务发出请求导致没有数据返回时，会引发异常。当执行演示代码时，请为您的实验和测试尝试不同的端点。</p><p id="61bb" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">Apache Spark是一项了不起的发明，可以解决许多问题。它的灵活性和适应性给了我们巨大的力量，但也给了我们犯大错误的机会。其中一个错误是在驱动程序上执行代码，您认为它会以分布式方式在工作程序上运行。一个这样的例子是当您在Dataframe的上下文之外执行Python代码时。</p><p id="b3fb" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">例如，当您执行类似于以下代码时:</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="a7b0" class="lg iw hi lc b fi lh li l lj lk">s = "Python syntax highlighting"<br/>print s</span></pre><p id="5b0f" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">Apache Spark将在驱动程序上执行代码，而不是在工作程序上。对于这样一个简单的命令来说这不是问题，但是当您需要通过REST API服务下载大量数据时会发生什么呢？</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="4194" class="lg iw hi lc b fi lh li l lj lk">import requests<br/>import json</span><span id="dbfa" class="lg iw hi lc b fi ll li l lj lk">res = None</span><span id="8778" class="lg iw hi lc b fi ll li l lj lk">try:<br/>  res = requests.get(url, data=body, headers=headers)<br/>    <br/>except Exception as e:<br/>  print(e)</span><span id="eaa6" class="lg iw hi lc b fi ll li l lj lk">if res != None and res.status_code == 200:<br/> print(json.loads(res.text))</span></pre><p id="dd1f" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">如果我们执行上面的代码，它将在驱动程序上执行。如果我要创建一个包含多个API请求的循环，那么就没有并行性，没有伸缩性，对驱动程序有很大的依赖性。这种方法扼杀了Apache Spark，使它比单线程Python程序好不了多少。为了利用Apache Spark的伸缩性和分布性，必须寻找一种替代解决方案。</p><p id="f6a8" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">解决方案是使用与withColumn语句耦合的UDF。这个例子演示了如何创建一个数据帧，每一行代表一个对REST服务的请求。使用UDF(用户定义的函数)封装HTTP请求，返回一个表示REST API响应的结构化列，然后可以使用explode和其他内置的DataFrame函数(或折叠，参见<a class="ae iu" href="https://github.com/jamesshocking/collapse-spark-dataframe" rel="noopener ugc nofollow" target="_blank">https://github.com/jamesshocking/collapse-spark-dataframe</a>)对其进行分割。</p><h1 id="c3ee" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">解决方案</h1><p id="2645" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">为了简洁起见，我假设已经创建了一个SparkSession，并将其分配给一个名为spark的变量。此外，对于这个例子，我将使用Python请求HTTP库。</p><p id="cd73" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">该解决方案假设您需要使用REST API中的数据，您将多次调用该API来获取您需要的数据。为了利用Apache Spark提供的并行性，每个REST API调用都将由一个UDF封装，该封装绑定到一个数据帧。数据帧中的每一行都代表对REST API服务的一次调用。一旦在数据帧上执行了一个动作，每个单独的REST API调用的结果将作为结构化数据类型附加到每一行。</p><p id="88fe" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">为了演示这个机制，我将使用一个免费的美国政府REST API服务，该服务返回美国汽车的品牌和型号<a class="ae iu" href="https://vpic.nhtsa.dot.gov/api/vehicles/getallmakes?format=json" rel="noopener ugc nofollow" target="_blank">https://vpic.nhtsa.dot.gov/api/vehicles/getallmakes?format=json </a>。</p><h1 id="914c" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">从申报进口开始:</h1><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="b1f4" class="lg iw hi lc b fi lh li l lj lk">import requests<br/>import json<br/>from pyspark.sql.functions import udf, col, explode<br/>from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType<br/>from pyspark.sql import Row</span></pre><h1 id="207b" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">现在声明一个将执行REST API调用的函数</h1><p id="a1da" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">使用请求库执行HTTP get或post。这个函数没有什么特别的，除了REST服务响应将作为一个JSON对象传递回来。</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="adbd" class="lg iw hi lc b fi lh li l lj lk">def executeRestApi(verb, url, headers, body):<br/>  #<br/>  headers = {<br/>      'content-type': "application/json"<br/>  }</span><span id="7a4d" class="lg iw hi lc b fi ll li l lj lk">  res = None<br/>  # Make API request, get response object back, create dataframe from above schema.<br/>  try:<br/>    if verb == "get":<br/>      res = requests.get(url, data=body, headers=headers)<br/>    else:<br/>      res = requests.post(url, data=body, headers=headers)<br/>  except Exception as e:<br/>    return e</span><span id="9445" class="lg iw hi lc b fi ll li l lj lk">  if res != None and res.status_code == 200:<br/>    return json.loads(res.text)</span><span id="145b" class="lg iw hi lc b fi ll li l lj lk">  return None</span></pre><h1 id="4f26" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">定义响应模式和UDF</h1><p id="f64f" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">这是Apache Spark中我非常喜欢的部分之一。我可以从REST API调用返回的JSON中挑选我想要的值。我所要做的就是在声明模式时，我只需要确定我想要JSON的哪些部分。</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="d356" class="lg iw hi lc b fi lh li l lj lk">schema = StructType([<br/>  StructField("Count", IntegerType(), True),<br/>  StructField("Message", StringType(), True),<br/>  StructField("SearchCriteria", StringType(), True),<br/>  StructField("Results", ArrayType(<br/>    StructType([<br/>      StructField("Make_ID", IntegerType()),<br/>      StructField("Make_Name", StringType())<br/>    ])<br/>  ))<br/>])</span></pre><p id="822f" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">接下来，我声明UDF，确保将返回类型设置为我声明的模式。这将确保用于执行UDF的新列最终包含结构化对象形式的数据，而不是普通的JSON格式的文本。该操作类似于使用from_json函数，该函数将模式作为其第二个参数。</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="d89d" class="lg iw hi lc b fi lh li l lj lk">udf_executeRestApi = udf(executeRestApi, schema)</span></pre><h1 id="1367" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">创建请求数据帧并执行</h1><p id="47ca" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">最后一部分是创建一个数据帧，其中每一行代表一个REST API调用。Dataframe中的列数由您决定，但您至少需要一列，它将存放执行REST API调用所需的URL和/或参数。我将使用四个来反映REST API调用函数需要的单个参数的数量。</p><p id="bdd7" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">使用美国政府的免费访问车辆make REST服务，我们将创建如下数据框架:</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="bd3a" class="lg iw hi lc b fi lh li l lj lk">from pyspark.sql import Row</span><span id="7d8e" class="lg iw hi lc b fi ll li l lj lk">headers = {<br/>    'content-type': "application/json"<br/>}</span><span id="4002" class="lg iw hi lc b fi ll li l lj lk">body = json.dumps({<br/>})</span><span id="891f" class="lg iw hi lc b fi ll li l lj lk">RestApiRequestRow = Row("verb", "url", "headers", "body")<br/>request_df = spark.createDataFrame([<br/>            RestApiRequestRow("get", "https://vpic.nhtsa.dot.gov/api/vehicles/getallmakes?format=json", headers, body)<br/>          ])</span></pre><p id="3948" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">Row类用于定义Dataframe的列，并使用spark对象的createDataFrame方法，为我们要进行的每个API调用声明一个RestApiRequestRow实例。</p><p id="2094" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">如果一切顺利，数据帧看起来会像这样:</p><p id="9dba" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">动词、url、标题、正文【https://vpic.nhtsa.dot.gov/api/vehicles/getallmakes?】get、<a class="ae iu" href="https://vpic.nhtsa.dot.gov/api/vehicles/getallmakes?format=json" rel="noopener ugc nofollow" target="_blank">T2format=json </a>，{ ' content-type ':" application/JSON " }，{}</p><p id="6125" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">最后，我们可以在数据帧上使用withColumn来执行UDF和REST API。</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="ad82" class="lg iw hi lc b fi lh li l lj lk">result_df = request_df \<br/>             .withColumn("result", udf_executeRestApi(col("verb"), col("url"), col("headers"), col("body")))</span></pre><p id="4412" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">由于Spark比较懒惰，一旦对数据帧执行了count()或show()之类的操作，UDF就会执行。Spark将在返回结果之前，将API调用分配给所有的工人，例如:</p><p id="b0d0" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">动词，网址，标题，正文，结果<br/>获取，<a class="ae iu" href="https://vpic.nhtsa.dot.gov/api/vehicles/getallmakes?format=json" rel="noopener ugc nofollow" target="_blank">https://vpic.nhtsa.dot.gov/api/vehicles/getallmakes?format=json </a> {'content-type ':"应用程序/json"}，{}，[9773，响应r…]</p><p id="547a" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">REST服务返回许多属性，我们只对标识为Results的属性感兴趣(即result。结果)。如果我们使用我的collapse_columns函数(<a class="ae iu" href="https://github.com/jamesshocking/collapse-spark-dataframe" rel="noopener ugc nofollow" target="_blank">https://github.com/jamesshocking/collapse-spark-dataframe</a>):</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="72ed" class="lg iw hi lc b fi lh li l lj lk">df = result_df.select(explode(col("result.Results")).alias("results"))<br/>df.select(collapse_columns(df.schema)).show()</span></pre><p id="46cf" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">你会看到:</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="f7ac" class="lg iw hi lc b fi lh li l lj lk">results_Make_ID, results_Make_Name <br/>440, ASTON MARTIN <br/>441, TESLA <br/>442, JAGUAR <br/>443, MASERATI <br/>444, LAND ROVER <br/>445, ROLLS ROYCE<br/>...</span></pre><p id="e566" class="pw-post-body-paragraph jt ju hi jv b jw ks jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kw ko kp kq hb bi translated">本文的源代码可以在我位于https://github.com/jamesshocking/Spark-REST-API-UDF<a class="ae iu" href="https://github.com/jamesshocking/Spark-REST-API-UDF" rel="noopener ugc nofollow" target="_blank">的Github上找到。</a></p></div></div>    
</body>
</html>
<html>
<head>
<title>This 1 Explainable AI(XAI) package can solve your business problems</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这个可解释的人工智能(XAI)包可以解决你的业务问题</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/interpreting-black-box-models-using-shapley-additive-explanations-85a9842b5cbd?source=collection_archive---------8-----------------------#2022-10-02">https://medium.com/geekculture/interpreting-black-box-models-using-shapley-additive-explanations-85a9842b5cbd?source=collection_archive---------8-----------------------#2022-10-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="7a78" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">Shap</h2><div class=""/><div class=""><h2 id="62f7" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">解释你的黑盒模型的算法</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/01b5faa440db6f5215c23908606e51cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bRiLJTqqgZGdMTVq"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Photo by <a class="ae jw" href="https://unsplash.com/@herfrenchness?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Clarisse Croset</a> on <a class="ae jw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="6079" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">开发机器学习应用需要很多步骤。收集数据，进行数据清理，通过探索性数据分析、特征工程、特征选择、建模、评估和解释来探索数据。<strong class="jz hs">解释</strong>是开发机器学习应用的关键部分之一。由于业务流程的原因，这个过程需要很多关注。机器学习工程师需要向非技术专家/利益相关者解释机器学习模型如何对数据做出特定预测。</p><p id="d2e6" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">想象一下，如果你使用一个具有复杂特征的神经网络模型来开发你的机器学习应用程序，<strong class="jz hs">非技术专家会相信你的模型预测吗？绝对没有</strong>。对于技术专家和利益相关者来说，由于对模型的误解，使用机器学习来加速业务流程将是一个大问题。因此，在对数据进行特定预测时，解释模型的能力是让非技术专家信任我们的机器学习模型的必要知识。</p><p id="20e7" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">Shap(Shapley Additive exPlanations)</strong>是最具可解释性的人工智能(XAI)工具之一，用来解释机器学习模型为什么会对数据实例做出特定的预测。Shapley Additive exPlanations使用shap值来解释机器学习模型所做的预测。Shap值是基于博弈论的方法做出的预测。</p><p id="beb7" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">想象一下，你和你的团队(由4名成员组成)正在参加一场机器学习黑客马拉松。为了解决黑客马拉松中给出的问题，每个人都有自己独特的能力。如果你的团队赢得了黑客马拉松，你如何决定一个特定成员的贡献的公平支付。</p><p id="a5e1" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">为某个特定成员分配一部分奖金绝对是明智之举。这与shap如何根据我们在机器学习模型上训练的数据来解释我们的模型是一样的。从数学上讲，这是一件很难解释的事情。你可以查看<a class="ae jw" href="https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jz hs">研究论文</strong> </a>中对这个可解释工具的数学解释，以及他们的<a class="ae jw" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"> <strong class="jz hs"> GitHub库</strong> </a>中的更新特性。</p><blockquote class="kt ku kv"><p id="03ef" class="jx jy kw jz b ka kb is kc kd ke iv kf kx kh ki kj ky kl km kn kz kp kq kr ks hb bi translated">我们<!-- -->将通过一个例子来实现电信客户流失数据集的shap，我们可以在这里下载</p></blockquote><div class="la lb ez fb lc ld"><a href="https://www.kaggle.com/datasets/blastchar/telco-customer-churn" rel="noopener  ugc nofollow" target="_blank"><div class="le ab dw"><div class="lf ab lg cl cj lh"><h2 class="bd hs fi z dy li ea eb lj ed ef hr bi translated">电信客户流失</h2><div class="lk l"><h3 class="bd b fi z dy li ea eb lj ed ef dx translated">重点客户保留计划</h3></div><div class="ll l"><p class="bd b fp z dy li ea eb lj ed ef dx translated">www.kaggle.com</p></div></div><div class="lm l"><div class="ln l lo lp lq lm lr jq ld"/></div></div></a></div><p id="3528" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">该数据集包含7043个客户和21个基于数据实例预测客户是否会流失的特征。在这里，我们不会更深入地研究完整的机器学习过程。我只是做一个简单的探索性数据分析和数据预处理。</p><p id="5b10" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">建议进行适当的探索性数据分析、数据清理、特性工程、特性选择、交叉验证和评估步骤，以便更好地预测数据实例。在本教程中，我们主要关注Shap的实现来解释模型。</p><p id="a6c4" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">你可以看看这个笔记本里的实现，供你参考。</p><div class="la lb ez fb lc ld"><a href="https://deepnote.com/workspace/josua-naiborhu-6357636c-7a32-47a2-9932-f929b2448e11/project/Interpreting-black-box-models-1bc2c841-e6bf-4fd7-9ba8-536abd8d71ce" rel="noopener  ugc nofollow" target="_blank"><div class="le ab dw"><div class="lf ab lg cl cj lh"><h2 class="bd hs fi z dy li ea eb lj ed ef hr bi translated">深度笔记</h2><div class="lk l"><h3 class="bd b fi z dy li ea eb lj ed ef dx translated">面向数据科学家和研究人员的托管笔记本电脑。</h3></div><div class="ll l"><p class="bd b fp z dy li ea eb lj ed ef dx translated">deepnote.com</p></div></div><div class="lm l"><div class="ls l lo lp lq lm lr jq ld"/></div></div></a></div><h1 id="e413" class="lt lu hi bd lv lw lx ly lz ma mb mc md ix me iy mf ja mg jb mh jd mi je mj mk bi translated"><strong class="ak">翻译结果</strong></h1><blockquote class="kt ku kv"><p id="3390" class="jx jy kw jz b ka kb is kc kd ke iv kf kx kh ki kj ky kl km kn kz kp kq kr ks hb bi translated">在解释结果之前，我们需要做的第一件事是在训练数据上训练我们的机器学习模型。在这个数据集上，我们使用XGBoostClassifier来解决这个分类问题，以如下方式研究流失或未流失的客户。</p></blockquote><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="2b9d" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">Shap作为一个代理模型，使用shap值来解释我们的机器学习模型预测。我们必须使用下面的代码来获得这个值。我们实例化shap对象，并用我们的测试数据测试shap算法。</p><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mn mm l"/></div></figure><p id="3a37" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">Shap框架能够基于两种方法解释数据，即局部可解释性和全局可解释性。局部可解释性是shap根据特定数据实例解释数据的方式，而全局可解释性则根据我们拥有的整体特征和数据来解释数据。通常，由于不同的解释和结果，基于特定的数据实例对所有数据进行预测会更好。我们将为我们自己的用例可视化全局可解释性和本地可解释性。</p><h1 id="51cf" class="lt lu hi bd lv lw lx ly lz ma mb mc md ix me iy mf ja mg jb mh jd mi je mj mk bi translated">具有特征重要性的全局可解释性</h1><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mo mm l"/></div></figure><p id="3f19" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">前面的条形图显示了每个特征如何影响我们根据我们使用的机器学习模型对训练数据进行的预测。我们可以看到，合同特征对预测目标结果的影响最大，其次是月费用和任期。</p><h1 id="ddae" class="lt lu hi bd lv lw lx ly lz ma mb mc md ix me iy mf ja mg jb mh jd mi je mj mk bi translated">热图的全球可解释性</h1><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mp mm l"/></div></figure><p id="52f5" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">热图显示了所有功能对模型的影响。我们可以看到，增加合同功能也将增加红色区域的任期和总费用，因为这些功能的shap值更高。顶部的f(x)曲线显示了随着数据实例的增加，预测的流失情况。我们可以看到客户流失预测如何影响数据实例的增长。</p><h1 id="294b" class="lt lu hi bd lv lw lx ly lz ma mb mc md ix me iy mf ja mg jb mh jd mi je mj mk bi translated">分层的全局可解释性</h1><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mq mm l"/></div></figure><p id="0354" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">该图显示了功能使用权和总费用形成一组，在线安全性和乘数形成另一组。这个过程称为层次聚类。此图还显示了一些特性如何根据子组进行交互，这可能是我们可以在EDA中检测到的特性，以便进行进一步分析。</p><h1 id="c146" class="lt lu hi bd lv lw lx ly lz ma mb mc md ix me iy mf ja mg jb mh jd mi je mj mk bi translated">SHAP汇总图的全球可解释性</h1><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mr mm l"/></div></figure><p id="112e" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">violin图显示了该特性对于某些数据点的积极或消极影响。由于红色渐变，TotalCharges与目标结果呈正相关。然而，它与合同、任期和总费用不同，后者的低值表示流失。</p><h1 id="ff00" class="lt lu hi bd lv lw lx ly lz ma mb mc md ix me iy mf ja mg jb mh jd mi je mj mk bi translated">具有特征依赖图的全局可解释性</h1><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="ms mm l"/></div></figure><p id="6490" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">特征独立性图就像一个散点图，我们可以从中看到两个变量的相关性。可视化显示总费用的增加也将增加合同。</p><h1 id="f31f" class="lt lu hi bd lv lw lx ly lz ma mb mc md ix me iy mf ja mg jb mh jd mi je mj mk bi translated">使用瀑布图的局部可解释性</h1><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mt mm l"/></div></figure><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mu mm l"/></div></figure><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mv mm l"/></div></figure><p id="f5c3" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">我们可以从数据实例对第一和第二观察上的特征的推断中看出，示出了不同的结果，其中在第一观察上，合同特征对模型预测有负贡献，而在第二观察上，合同特征对模型预测有正贡献。</p><h1 id="43d8" class="lt lu hi bd lv lw lx ly lz ma mb mc md ix me iy mf ja mg jb mh jd mi je mj mk bi translated">决策图的局部可解释性</h1><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mw mm l"/></div></figure><p id="aa06" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">决策图有助于解释处理数据集中的许多要素。我们可以从前面的图中看到每个当前局部预测如何高于或低于平均预测结果，以及特征值如何影响模型结果。在所示的两个示例中，合同给技术支持的案例功能对模型产生负面影响，而第二次观察的大多数功能对模型预测产生正面影响。</p><p id="4ec6" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">你也可以阅读另一篇关于可解释人工智能(XAI)的文章</p><div class="la lb ez fb lc ld"><a href="https://www.freecodecamp.org/news/interpret-black-box-model-using-lime/" rel="noopener  ugc nofollow" target="_blank"><div class="le ab dw"><div class="lf ab lg cl cj lh"><h2 class="bd hs fi z dy li ea eb lj ed ef hr bi translated">如何使用LIME解释黑盒模型(本地可解释的模型不可知解释)</h2><div class="lk l"><h3 class="bd b fi z dy li ea eb lj ed ef dx translated">机器学习模型是黑盒模型。通过给这些模型输入，我们可以根据特定的…</h3></div><div class="ll l"><p class="bd b fp z dy li ea eb lj ed ef dx translated">www.freecodecamp.org</p></div></div><div class="lm l"><div class="mx l lo lp lq lm lr jq ld"/></div></div></a></div><p id="9573" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">参考资料:</p><p id="c6f6" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">[1].斯科特·伦德伯格。“<a class="ae jw" href="https://arxiv.org/abs/1705.07874v2" rel="noopener ugc nofollow" target="_blank">解释模型预测的统一方法</a>”。(2017).</p><p id="673a" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">[2].斯科特·伦德伯格。”<a class="ae jw" href="https://paperswithcode.com/method/shap" rel="noopener ugc nofollow" target="_blank">沙普利加解释</a>。(2017).</p></div><div class="ab cl my mz gp na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="hb hc hd he hf"><h1 id="8d95" class="lt lu hi bd lv lw nf ly lz ma ng mc md ix nh iy mf ja ni jb mh jd nj je mj mk bi translated">感谢您的阅读！</h1><p id="0ec4" class="pw-post-body-paragraph jx jy hi jz b ka nk is kc kd nl iv kf kg nm ki kj kk nn km kn ko no kq kr ks hb bi translated"><em class="kw">我真的很感激！</em>🤗<em class="kw">如果你喜欢这个帖子并想看更多，可以考虑</em> <a class="ae jw" href="https://naiborhujosua.medium.com/" rel="noopener"> <strong class="jz hs"> <em class="kw">关注我</em> </strong> </a> <em class="kw">。我发布与机器学习和深度学习相关的主题。我尽量让我的帖子简单而精确，总是提供可视化和模拟。</em></p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es np"><img src="../Images/6945da5ee6c6b62bca14b0e84906ad30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/0*1mifsTEBjIiHgFah.png"/></div></figure><p id="7d22" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs"> Josua Naiborhu </strong>是一名业务发展分析师，后来成为一名自学成才的机器学习工程师。他的兴趣包括<strong class="jz hs">统计学习、预测建模和可解释机器学习</strong>。他喜欢跑步，这教会他不要放弃做任何事情，即使是在实施<strong class="jz hs">机器学习生命周期(MLOps) </strong>的时候。除了追求他对机器学习的热情，他还热衷于投资印度尼西亚证券交易所和加密货币。他一直在跑2015年<strong class="jz hs">雅加达马拉松和2019年</strong>大阪马拉松的全程马拉松。他的下一个梦想是参加波士顿马拉松、TCS纽约市马拉松和Virgin Money伦敦马拉松。</p><p id="5eb5" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><em class="kw">你可以在</em><strong class="jz hs"><em class="kw">LinkedIn</em></strong><em class="kw">，</em><strong class="jz hs"><em class="kw">Twitter</em></strong><em class="kw">，G</em><strong class="jz hs"><em class="kw">ithub</em></strong><em class="kw">，</em><strong class="jz hs"><em class="kw"/></strong><em class="kw">上和他联系或者直接在他的</em> <strong class="jz hs"> <em class="kw">个人网站上联系他。</em> </strong></p></div></div>    
</body>
</html>
<html>
<head>
<title>Policy Optimizations: TRPO/PPO</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">政策优化:TRPO/PPO</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/policy-optimizations-trpo-ppo-cf9479407df1?source=collection_archive---------12-----------------------#2021-09-17">https://medium.com/geekculture/policy-optimizations-trpo-ppo-cf9479407df1?source=collection_archive---------12-----------------------#2021-09-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="066e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将从论文<a class="ae jd" href="https://arxiv.org/pdf/1502.05477.pdf" rel="noopener ugc nofollow" target="_blank">信任区域策略优化(舒尔曼等人，2015) </a>和<a class="ae jd" href="https://arxiv.org/pdf/1707.06347.pdf" rel="noopener ugc nofollow" target="_blank">邻近策略优化算法(舒尔曼等人，2017) </a>中讨论策略优化方法。然后，我将简要介绍信赖域策略优化方法，并实现两种类型的近似策略优化方法:对代理目标的自适应KL (Kullback-Leibler)惩罚和裁剪代理目标。</p><h1 id="8c8c" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated"><strong class="ak">简介</strong></h1><p id="a4a6" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">在传统的策略梯度方法中，我们采样状态、动作和奖励的轨迹，然后使用采样轨迹更新策略。虽然这种方法很好，并且解决了基本的控制问题，但是该算法往往不稳定，并且在解决环境问题时不一致。一个问题是，当我们更新策略时，近似策略分布的输入和输出的分布将改变，导致不稳定性。</p><p id="3c2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一个问题是，当执行梯度上升时，我们不能保证策略在正确的方向上更新。优势函数是随机初始化的，因此根据优势更新策略会导致性能下降，而不是提高。对此的解决方案将是在旧策略和新策略之间创建某种安全距离，使得更新策略将保证单调改进而不发散。</p><h1 id="bfd6" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">信任区域策略优化</h1><p id="6127" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">让我们从TRPO论文中引入信任区域的概念开始。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kh"><img src="../Images/4ed8b0c91e41c08926e1db639fd6779d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*J9LTPt_41KzLcIXsq3idow.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx">Trust Region Policy Optimization (Schulman et al. 2015)</figcaption></figure><p id="b94a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，我们有<em class="kt"> η，</em>遵循策略<em class="kt"> π的预期贴现收益。</em>我们可以在以下等式中使用该预期贴现回报，该等式根据相对于另一项政策的优势来表达一项政策的预期贴现回报<a class="ae jd" href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf" rel="noopener ugc nofollow" target="_blank">(Kakade&amp;Langford 2002)</a>:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ku"><img src="../Images/799c1f7edcb7345cebf5d2b487076263.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*7fS4TsSjAX7AaXY62B1gqw.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx">Trust Region Policy Optimization (Schulman et al. 2015)</figcaption></figure><p id="231e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用优势函数<em class="kt"> A </em>的标准定义，其中<em class="kt"> A(s，a)= Q(s，a)- V(s)，</em>其中<em class="kt"> Q </em>是在状态<em class="kt"> s </em>中采取行动A的值，<em class="kt"> V </em>是状态s的值。该恒等式通过计算π分布下的优势，同时从π'策略中采样，来计算选择策略π'相对于π的优势。然后，我们将这个身份重写为状态和动作的总和，而不是时间步长:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es kv"><img src="../Images/46c73a4a15607308895e64a4fb234874.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EV5zi816UaRKVEIeTPP9HQ.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Trust Region Policy Optimization (Schulman et al. 2015)</figcaption></figure><p id="49de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在最后一行中，我们将<em class="kt"> ρ_π(s) </em>定义为在策略<em class="kt"> π </em>下状态s的贴现访问频率，因此我们可以去掉时间步长。从这个等式可以看出，只要优势对于所有状态<em class="kt"> s </em>都是非负的，那么从π→π’更新策略将保证非负的改善。然后，该论文介绍了该恒等式的局部近似:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es la"><img src="../Images/6b20b7090fe3b7932e29119d61dcb959.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*eiyAu549TTjiym9VCAqAPQ.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx">Trust Region Policy Optimization (Schulman et al. 2015)</figcaption></figure><p id="2058" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用旧策略的分布，而不是从新策略中采样状态转换。这是因为计算新政策的折扣访问频率太复杂，因为您需要在新政策上再次模拟收集的轨迹。通过这种近似，我们可以在更新策略之前简单地使用收集的轨迹。</p><p id="b8ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">既然这是局部近似，这并不意味着<em class="kt"> L </em>可以全局近似<em class="kt"> η </em>。本文推导出一个近似精确的下界或信赖域，这样我们就知道在近似可行的情况下，我们可以向新政策迈出多大的一步。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lb"><img src="../Images/7f05e4b3fdeea24a68ce622d411b0d07.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*yBYH1Q76ZiVgmTB1P23oWw.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx">Taken from UWaterloo CS885 Lecture 15a</figcaption></figure><p id="cf26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Kakade和Langford推导出下限如下:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lc"><img src="../Images/41e95bb5be23173f0ec835fa5df1b3c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*IrTiCG0L5Ftj3usegu-5rQ.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx">Trust Region Policy Optimization (Schulman et al. 2015)</figcaption></figure><p id="529c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">罚函数使用函数D_KL，它是π和π'之间的Kullback-Leibler散度，用于测量任意变量<em class="kt"> x </em>的两个分布之间的距离。这是局部近似值的视觉效果:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es ld"><img src="../Images/a8d9d522f72bce2dba31cad15feca6cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AFknXIrrwiHMorkAQWVrmg.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Taken from UWaterloo CS885 Lecture 15a</figcaption></figure><p id="7a9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，利用我们的代理函数和信赖域下界，我们可以提出一个保证非递减预期收益η的算法:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es le"><img src="../Images/8483e1b20d05ad05f986c85dc81c966b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xBjmZjR2sTcVct5f2NN8Tg.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Trust Region Policy Optimization (Schulman et al. 2015)</figcaption></figure><p id="b4fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们知道，这个算法通过查看具有导出的下限的代理目标来保证改进。如果我们改进右边的代理函数，这意味着我们改进了预期收益η。在这里，我们在每次迭代中最大化代理目标，所以我们知道新政策的代理目标至少是一样好的，所以预期的贴现回报将有非负的改善。</p><p id="f8b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">TRPO论文随后提出了一种在我们参数化策略π时实现该算法的更实用的方法。如果我们使用算法1中提出的惩罚常数C，更新步骤将非常小。相反，我们将最大化没有惩罚的代理目标，这是补偿局部近似的导出的下限，并且在新旧策略之间的KL差异上使用信任区域约束。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lf"><img src="../Images/9e75be21172e73348a407022b4ce9796.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*Zo-9EmRZQOwY8cXomK9Y4A.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx">Trust Region Policy Optimization (Schulman et al. 2015)</figcaption></figure><p id="e833" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在实践中，保证KL散度在每个可能的状态下都遵循约束是不切实际的，因此，我们使用一种近似，只要平均KL散度在约束内，它就是好的。接下来，让我们尝试重写代理目标函数。回想一下，这是我们当前试图解决的优化问题:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lg"><img src="../Images/64edbb1505490cbe7c6927934a7cfa2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*oWNjwGQ92MEttkQ9CKF96w.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx">Trust Region Policy Optimization (Schulman et al. 2015)</figcaption></figure><p id="50e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们可以通过使用一个无穷几何序列(1/1-γ)从状态分布和状态访问频率中采样的期望来消除状态的求和。然后，我们用不改变目标的Q函数代替优势函数，因为唯一的差别是常数。最后，我们可以通过从动作分布中抽样并使用重要性抽样来消除动作的总和。现在我们有以下目标:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lh"><img src="../Images/34309f40af21ce0cc30e54a0d6cb5735.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*ET1t1wxKOub8JpefYCNz2w.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Trust Region Policy Optimization (Schulman et al. 2015)</figcaption></figure><p id="a546" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">TRPO论文使用这个目标和一个Q函数来代替，并提出了一个实用的算法来解决每次迭代中的约束问题。我不会详细介绍TRPO算法，因为与它的继任者PPO相比，它的计算量太大，太复杂。</p><h1 id="3f0e" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">最近策略优化</h1><p id="ecc2" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">现在，有了TRPO的背景知识，我们可以讨论PPO论文中提出的修改，这些修改简化了TRPO中的约束优化问题，该问题解决起来可能很复杂且计算量很大。回顾TRPO试图最大化的目标:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es li"><img src="../Images/c2b96d530d61c3bb7fda115f6520399e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*fgKQ7qpt75KdCpW4bkU0Mw.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx">Proximal Policy Optimization Algorithms (Schulman et al. 2017)</figcaption></figure><p id="9377" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，我们只是将<em class="kt"> rₜ(θ) </em>定义为新旧政策之间的重要抽样比。PPO论文提出了一种新的目标:截断替代目标。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lj"><img src="../Images/bb7108ad0cceb6a090b74ca4414f8e37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*i5pKzQZgqdWxKkrUHgKrPg.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx">Proximal Policy Optimization Algorithms (Schulman et al. 2017)</figcaption></figure><p id="6424" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果没有约束，代理目标可能会被夸大。然而，我们没有对替代目标使用约束，而是将新旧政策之间的比率限制在[1 - ϵ，1 + ϵ]的范围内，ϵ作为超参数常数，指示比率可以偏离1多远。然后，我们取削波和未削波目标之间的最小值。直观地说，我们可以这样看待这个剪辑:</p><p id="2f39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果优势为正，我们希望比率大于1，因为我们希望新策略更频繁地出现。然后，剪辑会将比率限制在1 + ϵ以下，以便新政策不会偏离旧政策太远。如果优势是负的，我们希望比率小于1，因为我们希望新策略发生的频率更低。该剪辑将确保该比率超过1 - ϵ，因此新政策不会偏离旧政策太远。</p><p id="5be3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文提出的另一种方法是采用自适应KL罚函数。回想一下本地近似值与真实预期贴现回报的偏差的导出下限。像在TRPO中一样，我们将对代理目标使用惩罚。然而，我们使用超参数常数β，而不是使用导致小更新的计算常数C。这是带有惩罚的替代目标:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lk"><img src="../Images/aa0b966489d1a958e1760292bea2d0ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1ODfKgDT5WRit0QwJW0Vgg.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Proximal Policy Optimization Algorithms (Schulman et al. 2017)</figcaption></figure><p id="86e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于很难选择一个包含所有情况的β，我们可以使用一个自适应惩罚来代替。在几个时期的策略更新之后，我们在以下条件下改变β常数(假设d =新旧策略之间的KL散度):</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ll"><img src="../Images/880ad200027e8bd69284faac095a95b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*weveqViPf_EBqrxgXbmKMw.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx">Proximal Policy Optimization Algorithms (Schulman et al. 2017)</figcaption></figure><p id="563f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们选择了新老政策的一个目标。如果KL偏离低于我们的目标，我们可以减少对替代目标的惩罚。如果KL偏离超过我们的目标，我们将增加对替代目标的惩罚。</p><p id="61b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用截取的替代目标或具有自适应KL惩罚的目标，我们可以在实践中对目标进行更多的修改。如果我们使用在演员和评论家之间共享其参数的神经网络结构，因为我们需要评论家来估计优势，我们可以向目标函数添加两个以上的项。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lm"><img src="../Images/a9481a4957b962782a178dd5f7ea20e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F8PYfD61laabCZYWn2NSPQ.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Proximal Policy Optimization Algorithms (Schulman et al. 2017)</figcaption></figure><p id="f8c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，替代目标使用剪切目标，尽管它也与自适应KL惩罚一起工作。我们为价值函数损失(通常为均方误差损失)设定了权重c1，为熵红利s设定了权重c2。将所有因素放在一起，我们就有了采用行动者-批评家结构的PPO算法:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es ln"><img src="../Images/de3657077462c2bc8b9ac5a1416df102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oZ2urnTSBEpGjcxhyYalTw.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Proximal Policy Optimization Algorithms (Schulman et al. 2017)</figcaption></figure><h1 id="c665" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">履行</h1><p id="693c" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">对于我的实现，我在训练循环中使用了具有共享参数的Actor-Critic结构，用价值网络计算了我的优势估计，并实现了两种优化方法，clipped和KL自适应惩罚。然后，我在CartPole-v1和LunarLander-v2环境中使用这两种优化方法来训练我的PPO代理。这里快速回顾一下这两种环境。</p><p id="3901" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">CartPole是一个经典的控制问题，代理人试图平衡推车上的杆子。该环境有一个具有4个变量的连续状态空间，手推车速度/加速度和角速度/加速度，以及一个具有向左或向右推动动作的离散动作空间。代理人每平衡一步都会得到奖励。</p><p id="b035" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LunarLander-v2是一个火箭轨迹优化问题，其中一个代理人试图将火箭降落在一个表面上。该环境具有8个变量的连续状态空间，x位置、y位置、水平速度、垂直速度、着陆器方向角、角速度、左腿触地(布尔型)和右腿触地(布尔型)，以及具有动作点火主引擎、左引擎、右引擎和无动作的离散动作空间。代理人因正确着陆在表面上和着陆垫内而受到奖励，因燃料使用而受到惩罚。</p><p id="65f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于我的演员-评论家网络，我使用了与PPO文件相同的层，具有64个单元和tanh激活函数的两个隐藏层。下面是我用来训练代理的超参数和实验设置:</p><ul class=""><li id="138d" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc lt lu lv lw bi translated">γ(折扣系数):0.99</li><li id="cc6d" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">ϵ(代理剪辑):0.2</li><li id="157f" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">β(初始KL惩罚):1</li><li id="e054" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">δ(目标KL偏差):0.01</li><li id="5626" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">c1(价值损失重量):0.5</li><li id="bfd9" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">c2(熵权):0.01</li><li id="1801" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">k_epoch(更新中训练时期的数量):40</li><li id="708a" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">α_θ(演员学习率):0.0003</li><li id="8c35" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">αv(评论家学习率):0.001</li><li id="0189" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">总训练步骤:300，000</li><li id="c821" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">最大每集步数:400</li><li id="b59b" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">批量:1600个</li></ul><p id="5688" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是有着被削减的替代目标的扁担的训练历史:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mc"><img src="../Images/47577095de8a5191d30774835f0eddcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*RwO6tPz_p6MhCOfNSI0sLg.png"/></div></figure><p id="a0f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该代理能够在30，000个步骤中解决环境问题，在50次测试运行中的平均回报为242.26。训练历史看起来有点不稳定，但我怀疑这是因为我没有进行任何类型的超参数搜索。另一个原因可能是因为我计算自己优势的方式。PPO论文使用广义优势估计(Schulman et al. 2015)来计算优势，但我只是从累积贴现回报中减去了一个学习价值基线。在没有提前停止的情况下再次训练代理后，代理能够通过100，000步的训练达到最高平均分400。</p><p id="dc95" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我曾经试图用适应性KL惩罚来训练代理，但结果是不一致和不稳定的，尽管它仍然能够解决横竿环境。</p><p id="a365" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，这是LunarLander的训练历史和视频，其中包含了替代目标:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es md"><img src="../Images/0b5ed93ec11e066248084027f3aa80b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*TA3gMA0rOY20SX39Ffxzdg.png"/></div></figure><p id="71e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代理人被训练了300，000步，在50次测试中平均奖励142.4。与横竿环境一样，由于上述因素，训练历史有点不稳定。虽然142.4的测试分数不被认为是解决了问题，但LunarLander能够平稳地在表面着陆，比随机策略好得多。通过超参数扫描，PPO试剂应该能够实现超过200的求解分数。这是一段特工行动的视频:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es me"><img src="../Images/d49fb08533edee4603b75bf4f9bb41d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/1*vE6GuE5A-XUwYLCSD4-AMQ.gif"/></div><figcaption class="kp kq et er es kr ks bd b be z dx">LunarLander-v2 trained with PPO</figcaption></figure><p id="f5e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">code:<a class="ae jd" href="https://github.com/chengxi600/RLStuff/blob/master/Policy%20Optimization%20Algorithms/PPO_Discrete.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/cheng i600/rl stuff/blob/master/Policy % 20 optimization % 20 algorithms/PPO _ discrete . ipynb</a></p><p id="40b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><ul class=""><li id="ea5a" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc lt lu lv lw bi translated"><a class="ae jd" href="https://arxiv.org/pdf/1502.05477.pdf" rel="noopener ugc nofollow" target="_blank">信任区域政策优化(舒尔曼等，2015) </a></li><li id="c53f" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated"><a class="ae jd" href="https://arxiv.org/pdf/1707.06347.pdf" rel="noopener ugc nofollow" target="_blank">近似策略优化算法(舒尔曼等人，2017) </a></li><li id="a462" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated"><a class="ae jd" href="https://cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring18/slides/cs885-lecture15b.pdf" rel="noopener ugc nofollow" target="_blank"> UWaterloo CS885讲座15b </a></li></ul></div></div>    
</body>
</html>
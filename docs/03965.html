<html>
<head>
<title>Understanding Machine Learning Fundamentals by Implementing Linear Regression from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过从零开始实现线性回归来理解机器学习基础</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/understanding-linear-regression-by-implementing-from-scratch-761e2a48fce4?source=collection_archive---------14-----------------------#2021-06-18">https://medium.com/geekculture/understanding-linear-regression-by-implementing-from-scratch-761e2a48fce4?source=collection_archive---------14-----------------------#2021-06-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="9513" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">学习训练大多数机器学习模型的关键概念</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/ee0d05c26d7e57db9bc225abe63a26ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mTtQAgn-lZJP_5ZBUWyLhQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Drawn in excalidraw.com.</figcaption></figure><p id="a94f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi kj translated">线性回归可能是最简单的机器学习模型，但在进入更复杂的模型之前，充分理解它的基本原理至关重要。通过学习线性回归的关键概念，您将能够构建理解一般机器学习所需的基础知识。在我看来，有一个坚实的基础会让你在学习的大部分事情上变得容易得多，我相信这也适用于其他人。</p><p id="5d20" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在本文中，将涵盖以下关于线性回归模型的信息:</p><ul class=""><li id="6d55" class="ks kt hi jp b jq jr jt ju jw ku ka kv ke kw ki kx ky kz la bi translated">什么是线性回归模型？</li><li id="1509" class="ks kt hi jp b jq lb jt lc jw ld ka le ke lf ki kx ky kz la bi translated">为什么使用线性回归模型？</li><li id="e233" class="ks kt hi jp b jq lb jt lc jw ld ka le ke lf ki kx ky kz la bi translated">训练线性回归模型或一般大多数机器学习模型的关键概念</li><li id="4252" class="ks kt hi jp b jq lb jt lc jw ld ka le ke lf ki kx ky kz la bi translated">从头开始实施，并解释衍生工具</li><li id="c4ab" class="ks kt hi jp b jq lb jt lc jw ld ka le ke lf ki kx ky kz la bi translated">用tf实现。GradientTape(为我们执行导数的计算)</li></ul><h1 id="7951" class="lg lh hi bd li lj lk ll lm ln lo lp lq io lr ip ls ir lt is lu iu lv iv lw lx bi translated">那么到底什么是线性回归模型呢？</h1><p id="9906" class="pw-post-body-paragraph jn jo hi jp b jq ly ij js jt lz im jv jw ma jy jz ka mb kc kd ke mc kg kh ki hb bi translated">线性回归是为<strong class="jp hj">回归任务</strong>建立的<strong class="jp hj">监督机器学习模型</strong>，这些任务涉及预测连续(或实值)变量，而不是离散(分类)变量。监督学习意味着它需要输入数据和正确的输出，以便被训练来做出正确的预测。</p><p id="1b51" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">基本上有<strong class="jp hj">两种类型的线性回归模型</strong>:一元线性回归和多元(或多元)线性回归。单变量只是一个“花哨”的词，表示模型只使用一个特征(或变量)来进行预测，而多元线性回归使用多个特征。多元线性回归可能听起来很复杂，但实际上，它只是简单线性回归模型的一个非常简单的“升级”，由于矩阵乘法的强大功能，它的实现也非常简单，基本相同。这将在后面的实现中显示。</p><p id="970d" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">线性回归模型采用了定义机器学习模型的最基本方面。包括线性回归在内的大多数机器学习模型都利用了以下<strong class="jp hj">关键原则</strong>:</p><ol class=""><li id="9407" class="ks kt hi jp b jq jr jt ju jw ku ka kv ke kw ki md ky kz la bi translated">定义用于预测的<strong class="jp hj">方程式</strong></li><li id="fb9e" class="ks kt hi jp b jq lb jt lc jw ld ka le ke lf ki md ky kz la bi translated">定义<strong class="jp hj">参数</strong>以学习进行预测</li><li id="98f3" class="ks kt hi jp b jq lb jt lc jw ld ka le ke lf ki md ky kz la bi translated">定义训练模型所需的<strong class="jp hj">成本函数</strong>(或损失函数)</li><li id="6163" class="ks kt hi jp b jq lb jt lc jw ld ka le ke lf ki md ky kz la bi translated"><strong class="jp hj">使用<strong class="jp hj">梯度下降</strong>训练</strong>模型，以最小化成本函数</li><li id="2d24" class="ks kt hi jp b jq lb jt lc jw ld ka le ke lf ki md ky kz la bi translated">使用训练好的参数进行<strong class="jp hj">预测</strong></li></ol><h1 id="22ea" class="lg lh hi bd li lj lk ll lm ln lo lp lq io lr ip ls ir lt is lu iu lv iv lw lx bi translated">定义用于训练线性回归模型的关键概念</h1><p id="c1c7" class="pw-post-body-paragraph jn jo hi jp b jq ly ij js jt lz im jv jw ma jy jz ka mb kc kd ke mc kg kh ki hb bi translated">这些概念对于从零开始构建实现模型所需的知识非常重要，虽然有点长，但是花时间去理解它是非常值得的，所以请耐心等待。</p><p id="a769" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在线性回归模型的情况下，要定义的内容是:</p><h2 id="1c2a" class="me lh hi bd li mf mg mh lm mi mj mk lq jw ml mm ls ka mn mo lu ke mp mq lw mr bi translated"><strong class="ak">方程式</strong></h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ms"><img src="../Images/3f2dd083cb9c19a4de967f7439d256a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*Btk0tNiMuJi8Kp8NifE0GQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image from Investopedia. (<a class="ae mt" href="https://www.investopedia.com/terms/m/mlr.asp" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="3132" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">由于矩阵乘法的强大功能，对于任意数量的样本和任意数量的特征，上面的长等式实际上可以简化为下面的等式，以实现更有效的训练过程:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mu"><img src="../Images/d5b065cecb4cc3302b129b37ff30000f.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*cyRUhzBq0d1usRuy7aMSUA.png"/></div></figure><p id="2991" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">其中:<br/> <strong class="jp hj"> Y </strong> =矩阵形式<strong class="jp hj">中的因变量(目标变量)</strong>，其中<code class="du mv mw mx my b">(number_of_samples, 1)</code>的<strong class="jp hj">形状</strong>—用(行数，列数)<br/> <strong class="jp hj"> b </strong> = y截距(类似于神经网络中的偏差，因此有<code class="du mv mw mx my b">b</code>的表示法)<br/> <strong class="jp hj"> W </strong> =矩阵形式<strong class="jp hj">中的斜率系数</strong>(类似于权重在<br/> <strong class="jp hj"> X </strong> =矩阵形式的自变量(特征)，具有<code class="du mv mw mx my b">(number_of_samples, number_of_features)</code>的<strong class="jp hj">形状</strong>。</p><p id="e269" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在<strong class="jp hj"> W </strong>和<strong class="jp hj"> X </strong>之间的<code class="du mv mw mx my b">dot</code>代表它们的点积。这个点积将输出与上面所示的长等式中y轴截距(<strong class="jp hj"><em class="mz">【β₀</em></strong>)的后的<em class="mz">项完全相同的结果。这将在下一节中得到证明，其中将解释矩阵乘法(与点积相同)。</em></p><p id="f1aa" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">注</strong>:用<strong class="jp hj">大写字母</strong>表示<strong class="jp hj">矩阵形式</strong>。这里使用的符号是主观的，这里使用的符号是基于我的个人喜好。</p><h2 id="8ab0" class="me lh hi bd li mf mg mh lm mi mj mk lq jw ml mm ls ka mn mo lu ke mp mq lw mr bi translated">矩阵乘法(点积)</h2><p id="3fe0" class="pw-post-body-paragraph jn jo hi jp b jq ly ij js jt lz im jv jw ma jy jz ka mb kc kd ke mc kg kh ki hb bi translated">下图显示了在只有<strong class="jp hj"> 1个样本</strong>和<strong class="jp hj"> 3个特征</strong>的情况下矩阵乘法的示例，它将输出一个形状为<code class="du mv mw mx my b">(n_samples, 1)</code>的矩阵，输出与之前显示的图像中的等式完全相同。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es na"><img src="../Images/0b066b1533ec00d7c1df65f50d315b3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U-j4wPRBiNCxd5sOPKrWPg.png"/></div></div></figure><p id="1b0d" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我一直喜欢使用的一个很好的技巧是通过<strong class="jp hj">记住矩阵乘法过程中形状是如何变化的</strong>。例如，在上图中，第一个矩阵的形状为<code class="du mv mw mx my b">(1, 3)</code>，第二个矩阵为<code class="du mv mw mx my b">(3, 1)</code>，第三个矩阵为<code class="du mv mw mx my b">(1, 1)</code>。形状的变化如下图所示，其中中间部分必须具有相同的形状(本例中为3)，它们将被切掉，最后只留下侧边:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nb"><img src="../Images/823308b33ddcb2fb8bcf90537e0cb4a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*jPYZQGRQMn4DJh7H94bLGw.png"/></div></figure><p id="6520" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">为了进一步实验矩阵乘法以了解它是如何工作的，我推荐这个有趣的网站，它可以让你尝试任何形状的矩阵乘法。</p><h2 id="3dbb" class="me lh hi bd li mf mg mh lm mi mj mk lq jw ml mm ls ka mn mo lu ke mp mq lw mr bi translated"><strong class="ak">参数</strong></h2><p id="68c8" class="pw-post-body-paragraph jn jo hi jp b jq ly ij js jt lz im jv jw ma jy jz ka mb kc kd ke mc kg kh ki hb bi translated">这些参数将被随机初始化，并且它们将在训练过程中保持更新(通过使用梯度下降)，以便更好地进行精确预测。这些参数如下:</p><ol class=""><li id="774e" class="ks kt hi jp b jq jr jt ju jw ku ka kv ke kw ki md ky kz la bi translated"><code class="du mv mw mx my b">W</code>:每个解释变量的斜率系数(与“特征”含义相同)，其中每个特征都有自己的斜率系数。</li><li id="1341" class="ks kt hi jp b jq lb jt lc jw ld ka le ke lf ki md ky kz la bi translated"><code class="du mv mw mx my b">b</code>:y轴截距。</li></ol><h2 id="9663" class="me lh hi bd li mf mg mh lm mi mj mk lq jw ml mm ls ka mn mo lu ke mp mq lw mr bi translated"><strong class="ak">成本函数</strong></h2><p id="97cb" class="pw-post-body-paragraph jn jo hi jp b jq ly ij js jt lz im jv jw ma jy jz ka mb kc kd ke mc kg kh ki hb bi translated">通常，均方误差(MSE)用于回归任务。简单来说，MSE就是<strong class="jp hj">残差</strong>(预测值-真值)的平方的平均值，如下式所示。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nc"><img src="../Images/1911b8c5e59642eddfeb074de46eee0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*pc9B24G1Ag1BULbWDpP1ig.png"/></div></figure><p id="1d01" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">下图显示了如何计算每个点的<strong class="jp hj">残差(虚线)</strong>。紫色线由线性回归模型生成。所以，<code class="du mv mw mx my b">y_pred</code>(预测值)是紫线上的点，而<code class="du mv mw mx my b">y</code>(真实值)是蓝点。<strong class="jp hj">注</strong>:该图仅基于单个特征绘制，因此是二维的，如果有多个特征，则需要更高的维度来绘制。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nd"><img src="../Images/8b336ac69815b411193541693c4a00a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4tbrAFzyAfXz9Ot4u-4xuA.png"/></div></div></figure><h2 id="bf09" class="me lh hi bd li mf mg mh lm mi mj mk lq jw ml mm ls ka mn mo lu ke mp mq lw mr bi translated">梯度下降</h2><p id="eb18" class="pw-post-body-paragraph jn jo hi jp b jq ly ij js jt lz im jv jw ma jy jz ka mb kc kd ke mc kg kh ki hb bi translated">这里只涉及梯度下降的基础知识，更深入的解释，你可以参考这个令人惊叹的<a class="ae mt" href="https://youtu.be/IHZwWFHWa-w" rel="noopener ugc nofollow" target="_blank"> YouTube视频</a>。现在，我们只知道梯度下降有如下图所示的公式。</p><p id="80fa" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">设<strong class="jp hj"> <em class="mz"> p </em> </strong> =任意可训练参数，<strong class="jp hj"> <em class="mz"> lr </em> </strong> =学习率和<strong class="jp hj"><em class="mz">DP</em>/<em class="mz">dx</em></strong><em class="mz"/>= p<em class="mz"/>相对于<strong class="jp hj">代价函数的参数</strong> <strong class="jp hj"> <em class="mz"> x </em> </strong>:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ne"><img src="../Images/6d960fb6644f6678a4a5023c25708782.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*iNWn2zmlCiGWAmRUf3NAwA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">General gradient descent formula.</figcaption></figure><p id="ce50" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">梯度下降与机器学习模型的成本函数密切相关。因此，理解在解释梯度下降时使用的成本函数非常重要。对于这种情况下的MSE的成本函数，用于训练单个参数的<strong class="jp hj">的图形将类似于下图中的<strong class="jp hj">紫色曲线</strong>:</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nf"><img src="../Images/75f84f800193bce030261074a603ee7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NbsNn4kf1-8J12F-Odzchw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Example of MSE loss function = (x - 100)² / 5000, where true value = 100, n_samples = 5000.</figcaption></figure><p id="ba36" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj"> x轴</strong>代表预测值，而<strong class="jp hj"> y轴</strong>代表MSE值。橙色和绿色线条表示曲线在相应点的切线，以便计算斜率。最初，由于高损耗(不良预测)，预测值将开始于点A附近的某个较高位置，斜率(或梯度)为-1.674，如图所示，由<code class="du mv mw mx my b">m</code>表示。然后，在梯度下降的每个<em class="mz">迭代</em>之后，理想地，预测值和斜率将逐渐沿着曲线下降，直到它最终收敛到<em class="mz">全局最小值</em>——图中的点C，斜率为零，这基本上与零损失相同，即已经拟合了最佳线性回归线，使得最精确的预测成为可能。</p><p id="b592" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">用简单的话来解释梯度下降:在每次迭代中，计算的梯度用于最小化成本函数，以减少残差。当梯度达到零时，即残差平方和的平均值达到零时，模型将已经找到当前数据集的最佳拟合线。</p><p id="3db5" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">如果<strong class="jp hj">学习率</strong>设置得太低，将需要多次迭代才能收敛到全局最小值。相反，如果<strong class="jp hj">学习率</strong>设置得太高，将导致梯度爆炸，并且永远无法收敛到全局最小值。</p><p id="ebec" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在这种情况下，MSE损失函数的<strong class="jp hj">导数</strong>(基本上是斜率的数学符号)如下:</p><p id="a4a3" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">成本函数:</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ng"><img src="../Images/be244417f0bc5af59cb8b57e2395035a.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*mI1aIUZeGxXakKyfKhlRpg.png"/></div></figure><p id="56d0" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">衍生产品:</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nh"><img src="../Images/8770d161610d15d9d1ef6bd7236b2225.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*ligoacE8y1TVnpJLqKsN2Q.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">from StackOverflow (<a class="ae mt" href="https://math.stackexchange.com/questions/3152235/partial-derivative-of-mse-cost-function-in-linear-regression?newreg=d8f2602266cc42de80d9c1a37974a781" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="0692" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">对我来说，通过直接将数学符号“翻译”成代码，它总是更具可读性和可理解性，所以下面是推导导数的“翻译”代码:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ni"><img src="../Images/f5d2494210d07b9ebe00d8d0ea104442.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*mAtOYEF6P24C3ZQe2WvpyA.png"/></div></figure></div><div class="ab cl nj nk gp nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="hb hc hd he hf"><h1 id="8bf1" class="lg lh hi bd li lj nq ll lm ln nr lp lq io ns ip ls ir nt is lu iu nu iv lw lx bi translated">履行</h1><p id="c3fe" class="pw-post-body-paragraph jn jo hi jp b jq ly ij js jt lz im jv jw ma jy jz ka mb kc kd ke mc kg kh ki hb bi translated">这些步骤可以细分如下:</p><ol class=""><li id="dfb4" class="ks kt hi jp b jq jr jt ju jw ku ka kv ke kw ki md ky kz la bi translated">随机初始化参数</li><li id="b1d6" class="ks kt hi jp b jq lb jt lc jw ld ka le ke lf ki md ky kz la bi translated">使用当前状态的模型进行预测</li><li id="e094" class="ks kt hi jp b jq lb jt lc jw ld ka le ke lf ki md ky kz la bi translated">计算成本函数的导数</li><li id="7f53" class="ks kt hi jp b jq lb jt lc jw ld ka le ke lf ki md ky kz la bi translated">使用梯度下降来更新参数</li><li id="4477" class="ks kt hi jp b jq lb jt lc jw ld ka le ke lf ki md ky kz la bi translated">根据给定的迭代次数，循环重复步骤2-4</li></ol><p id="97bc" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在开始实施之前，我想先谈一谈一个重要的技巧。在机器学习的情况下，导致调试灾难的一个常见错误是当变量具有用于计算的<strong class="jp hj">不正确形状</strong>时。导致我浪费大量时间的是形状为<code class="du mv mw mx my b">(nrows,)</code>的变量，因此，我总是会对这样的变量计算一个<code class="du mv mw mx my b">reshape(-1, 1)</code>来将它们变成<code class="du mv mw mx my b">(nrows, 1)</code>的形状，如后面的实现所示。</p><p id="5d86" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">好了，废话少说，让我们从创建一个虚拟数据集开始。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nv nw l"/></div></figure><p id="171e" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">可以看到，在<strong class="jp hj">线4 </strong>处，y的形状是<code class="du mv mw mx my b">(500,)</code>。正如我之前提到的，我立即重塑成<code class="du mv mw mx my b">(500, 1)</code>以避免以后的灾难。</p><p id="27bc" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">第9行</strong>:分成20%大小的训练组和测试组。</p><p id="4c40" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">数据的散点图<code class="du mv mw mx my b">plt.scatter(X, y)</code>如下:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nx"><img src="../Images/1b4f0411c8b39437075c6a01939b28a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*55vB9gLaitaXE_94yVtQKw.png"/></div></div></figure><p id="e04a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">然后，用正态分布产生的随机数初始化参数(因为机器学习模型一般都爱正态分布的东西)。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nv nw l"/></div></figure><p id="7cd1" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">然后进行预测，计算导数，执行梯度下降。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nv nw l"/></div></figure><p id="4ab7" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">然后将其与Scikit-learn <code class="du mv mw mx my b">sklearn</code>实现中的黄金标准进行比较，注意我总是跟踪形状，因为它们对于调试非常重要:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nv nw l"/></div></figure><p id="0af5" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">万岁！该实现与<code class="du mv mw mx my b">sklearn</code>实现具有完全相同的R2分数。</p><p id="96a1" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">但是让我们将它们重构为一个<code class="du mv mw mx my b">class</code>对象，因为Python应该总是利用其强大的Python面向对象编程(OOP)能力。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nv nw l"/></div></figure><p id="19c8" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">让我们运行一个训练循环来确认它的工作。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nv nw l"/></div></figure><p id="cb8f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">很好，它对只有一个特征变量的数据集有效。现在让我们尝试运行一个包含<strong class="jp hj">多个特征</strong>的数据集。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nv nw l"/></div></figure><p id="d674" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">好的，一切都很好。但每次手动推导导数可能不是最实用的方法，这就是为什么更复杂模型(如神经网络)的梯度通常会使用TensorFlow或PyTorch提供的函数来计算导数和损失。接下来将展示一个使用Tensorflow的<code class="du mv mw mx my b">tf.GradientTape</code>为我们计算导数的例子。</p><h2 id="9251" class="me lh hi bd li mf mg mh lm mi mj mk lq jw ml mm ls ka mn mo lu ke mp mq lw mr bi translated">使用tf。梯度胶带</h2><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nv nw l"/></div></figure><p id="ce3a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">正如您在<strong class="jp hj">第29–38行</strong>中看到的，损失是在<code class="du mv mw mx my b">tf.GradientTape</code>上下文管理器中计算的，然后<strong class="jp hj">第41行</strong>，<code class="du mv mw mx my b">tape.gradient</code>将计算导数，供我们用于执行梯度下降。</p><p id="93ad" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">但是实际上有一个更短的方法，如上面脚本中的<code class="du mv mw mx my b">fit2</code>方法所示，它使用一个<code class="du mv mw mx my b">optimizer</code>来<code class="du mv mw mx my b">minimize()</code>直接为我们使用<code class="du mv mw mx my b">optimizer</code>的参数，跳过手动编码<code class="du mv mw mx my b">GradientTape</code>和更新参数的整个步骤，代价是训练时间略有增加。考虑到<strong class="jp hj"> <em class="mz">随机梯度下降优化器</em> </strong>与正常梯度下降几乎相同，数据集相对简单，这种情况下最终结果完全相同。</p><h1 id="2921" class="lg lh hi bd li lj lk ll lm ln lo lp lq io lr ip ls ir lt is lu iu lv iv lw lx bi translated">可视化最佳拟合线，并制作动画！</h1><p id="cdd3" class="pw-post-body-paragraph jn jo hi jp b jq ly ij js jt lz im jv jw ma jy jz ka mb kc kd ke mc kg kh ki hb bi translated">让我们尝试将单变量线性回归模型的结果可视化:</p><pre class="iy iz ja jb fd ny my nz oa aw ob bi"><span id="5946" class="me lh hi my b fi oc od l oe of">if X.shape == y.shape:<br/>    plt.scatter(X, y, color='orange')<br/>    plt.plot(X, my_linear_reg.predict(X.reshape(-1, 1)), linewidth=3);</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es og"><img src="../Images/5fc6ba8495fe606237d2a5528c093954.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z2Zyk8eiL3603tPnz8964Q.png"/></div></div></figure><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nv nw l"/></div></figure><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="oh nw l"/></div></figure><h1 id="6fba" class="lg lh hi bd li lj lk ll lm ln lo lp lq io lr ip ls ir lt is lu iu lv iv lw lx bi translated">结论</h1><p id="4e59" class="pw-post-body-paragraph jn jo hi jp b jq ly ij js jt lz im jv jw ma jy jz ka mb kc kd ke mc kg kh ki hb bi translated">总之，我们已经讨论了一般情况下训练机器学习模型的关键概念，并特别关注在Python中实现线性回归模型。这些是非常重要的概念，可以应用于更复杂的模型。</p><p id="3571" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">实现也适用于多元线性回归，对导数进行硬编码的步骤也可以转化为TensorFlow的<code class="du mv mw mx my b">tf.GradientTape</code>来为我们简化过程。</p><p id="c500" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">尾注</strong>:希望你从这篇文章中学到了一些有用的东西。这是我在Medium上发表的第一篇文章，如果我的文字或视觉效果有一些瑕疵，请原谅我。我非常欢迎任何形式的反馈。非常感谢你读到这里！</p></div></div>    
</body>
</html>
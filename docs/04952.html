<html>
<head>
<title>Glow: Graph Lowering Compiler Techniques for Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Glow:用于神经网络的图形降低编译技术</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/glow-graph-lowering-compiler-techniques-for-neural-network-fb1eacbd0508?source=collection_archive---------23-----------------------#2021-07-06">https://medium.com/geekculture/glow-graph-lowering-compiler-techniques-for-neural-network-fb1eacbd0508?source=collection_archive---------23-----------------------#2021-07-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="55c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Glow是一个机器学习编译器，旨在用作Pytorch、Tensorflow等高级机器学习框架的后端。它还充当硬件加速器的执行引擎。该编译器旨在创建一流的编译器优化和神经网络图的代码生成。在这篇文章中，我将讨论GLOW论文的摘要，以及我在这篇论文中发现的优势和弱点，以及为什么Glow、TVM等深度学习编译器是人工智能的未来。</p><p id="fd2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">概要:</strong></p><p id="6ab1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Glow从Tensorflow、Pytorch等高级框架获得传统神经网络数据流图输入，并将其降低为两阶段强类型中间表示。不同级别的IR执行不同的操作，就像高级IR允许优化器执行特定于域的优化。基于低级指令的仅地址IR允许编译器执行与内存相关的优化，例如指令调度、静态内存分配和复制消除。在最底层，优化器借助专门的硬件特性生成特定于机器的代码。Glow通过支持降低阶段，使编译器能够支持大量的硬件目标和输入操作符，从而消除了在所有目标上实现所有操作符的需要。降低阶段减少了输入空间，并允许新的硬件后端专注于少量的线性代数运算。Glow将基于浮点的网络转换为带符号的8位整数网络，用于量化值。Glow使用轮廓导向量化，在推断过程中观察执行情况，以估计神经网络每个阶段的可能数值范围。Glow运行时组件包含第一分区，这些分区组成一个或多个子网。置备程序编译每个子网，并将它们分配给一个或多个设备。设备管理器充当物理设备的抽象。它处理网络负载、内存传输、设备上的执行，并跟踪硬件状态。执行器处理网络的执行。它跟踪每个子网的执行状态，并传播子网的输入和输出。</p><p id="4b48" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">动机:</strong></p><p id="1827" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看一个例子来理解为什么神经网络图的优化需要高级IR。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/79993386335159198db251852eb067c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/0*txhnPfZnjXJSxHwg"/></div></figure><p id="781b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们可以看到两个for循环写入某个内存区域，return语句从数组的第一个元素中读取。GCC和LLVM编译器都无法删除多余的第一个循环，也无法用常量值“4”替换加载操作。原因是分析循环和内存很难。编译器检查循环是否没有溢出和索引是否正确，以及计算结果是否准确并符合C编程语言的规范。</p><p id="2ec2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是像GLOW这样的编译器看到了对这种代码进行优化的需要，高级中间表示允许编译器推理和优化高级结构，如张量和运算。深度神经网络处理时间、内存和计算敏感，并将其部署在低内存设备中，图形需要基于低级设备特定IR和高级IR进行优化，这是Tensorflow和Pytorch框架等框架无法做到的。我们也不能依赖传统的C++或JVM编译器，为此我们需要一些智能框架来处理内存优化，并基于程序执行创建编译器级别的代码设备特定代码，而无需针对每个不同的硬件进行手动调整。</p><p id="1e59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">论文优势:</strong></p><ol class=""><li id="5fdb" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">Glow在高水平IR下进行图形级优化，这对于所有设备都是通用的，也可以进行特定设备的低水平优化，并且可以处理许多不同的设备。这减少了工程师在每个设备上优化代码的工作量，并将这项任务留给了编译器。</li><li id="c084" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">Glow支持两种类型的编译技术:即时编译(在执行模型之前执行编译)和提前编译(在离线状态下执行编译以生成目标文件(称为Glow bundle ),该文件稍后将与用户的应用程序代码相链接。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es jz"><img src="../Images/239190b12c737f6d2f0413c2ea597869.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7E2XVXDPEGYIpNmd6HD8DA.png"/></div></div><figcaption class="ke kf et er es kg kh bd b be z dx">A simple example showing a graph partitioned into multiple sub-graphs, themselves making up a directed graph, and then converted into a schedule by the executor.</figcaption></figure><p id="2d97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Glow仅支持ONNX和cafe 2模型作为输入格式，但从Tensorflow制作的其他模型可以很容易地导出为ONNX格式，以便在Glow编译器中使用。</p><p id="a371" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">纸的弱点:</strong></p><ol class=""><li id="4fd4" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">首先，我们必须使用Tensorflow、caffe或pytorch等高级框架库创建一个神经网络图，然后该图将被输入到Glow编译器，该编译器将优化高级图，然后进行特定于设备的优化，因此与普通C++编译相比，这将是一个耗时、繁琐且缓慢的过程，并且模型还需要转换为ONNX或caffe 2模型格式才能输入到Glow编译器中，不接受其他格式。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ki"><img src="../Images/91f0526abc5434627dc73f1a7bb450ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*CxkS8iZE0d0iRRf8_GGcng.png"/></div><figcaption class="ke kf et er es kg kh bd b be z dx">Glow vs. TensorFlow-1.7 and TVM on an IntelR Core i7–7600U; frames per second on a single thread.</figcaption></figure><p id="c85a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.与TVM或内部编译器相比，没有任何高级优化，例如内存不足时的激活/权重分区、重用激活以避免内存移动、计算数据移动并行。</p><p id="57de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.当Tensorflow在启用XLA的情况下编译时，Glow的性能比Tensorflow高2.7倍，但只有在TVM中禁用自动调整和计划时，glow的性能才能优于TVM。</p><p id="d5d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献</strong></p><ol class=""><li id="d473" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated"><a class="ae kj" href="https://www.opensourceforu.com/2019/06/the-capabilities-of-tensor-virtual-machine-an-open-deep-learning-compiler-stack/" rel="noopener ugc nofollow" target="_blank">https://www . opensourceforu . com/2019/06/the-capabilities-of-tensor-virtual-machine-an-open-deep-learning-compiler-stack/</a></li><li id="7825" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated"><a class="ae kj" href="https://ucbrise.github.io/cs294-ai-sys-sp19/assets/lectures/lec12/dl-compilers.pdf" rel="noopener ugc nofollow" target="_blank">https://ucbrise . github . io/cs 294-ai-sys-sp19/assets/lec12/dl-compilers . pdf</a></li><li id="8104" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated"><a class="ae kj" href="https://www.groundai.com/project/the-deep-learning-compiler-a-comprehensive-survey/1" rel="noopener ugc nofollow" target="_blank">https://www . ground ai . com/project/the-deep-learning-compiler-a-comprehensive-survey/1</a></li><li id="2a95" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated"><a class="ae kj" href="https://arxiv.org/pdf/1805.00907.pdf" rel="noopener ugc nofollow" target="_blank">神经网络的图形降低编译技术</a></li></ol></div></div>    
</body>
</html>
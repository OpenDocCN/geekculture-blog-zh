<html>
<head>
<title>Evaluating Metrics for Multi-class Classification and Implementations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">评估多类分类和实现的度量</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/evaluating-metrics-for-multi-class-classification-and-implementations-be389cb17e65?source=collection_archive---------5-----------------------#2022-02-20">https://medium.com/geekculture/evaluating-metrics-for-multi-class-classification-and-implementations-be389cb17e65?source=collection_archive---------5-----------------------#2022-02-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/c994c0a676a07b34d6689741a018196d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gCxPWQCSIp90cGyH.jpg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Image from Google</figcaption></figure><p id="4680" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这篇博客是这篇文章的延续。</p><p id="d27c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">到目前为止，您已经了解了二进制分类指标。从现在开始，我们将学习多类分类的度量，在下一篇文章中，您将学习多标签分类及其度量。</p><p id="03ec" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">多类分类无非是解决一个分类问题，其中我们在目标列中有多个类(类的数量&gt; 2)。例如，将水果图像分类为苹果、桔子和香蕉。</p><p id="1576" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我在这里附上一个示例数据集<a class="ae js" href="https://github.com/VishnuVardhanVarapalli/Classification-Metrics/blob/main/Development%20Index.csv" rel="noopener ugc nofollow" target="_blank"/>。在该数据集中，发展指数列是具有多个类的目标(类的数量&gt; 2)。</p><p id="c90d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">正如我们在二进制分类度量学过的那样，有了它们，我们就可以制定多分类度量。</p><p id="2dc0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在多分类指标中，每个二元分类指标有3种实现方式。我写的时候你可能会明白…</p><p id="d0fe" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于精确二进制分类度量，在多分类度量中，我们有三个版本，</p><ol class=""><li id="9034" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr jy jz ka kb bi translated">微平均精度</li><li id="d722" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr jy jz ka kb bi translated">宏观平均精度</li><li id="34d0" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr jy jz ka kb bi translated">加权精度</li></ol><p id="acc8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">作为精度，我们在多类分类中有3个版本的召回二进制分类度量。</p><ol class=""><li id="bb53" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr jy jz ka kb bi translated">微观平均回忆</li><li id="b0ba" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr jy jz ka kb bi translated">宏观平均回忆</li><li id="d71f" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr jy jz ka kb bi translated">加权回忆</li></ol><p id="bf37" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如上所述，我们可以将每个二进制分类度量分为3个版本，并将其命名为多分类度量。</p><p id="7c84" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，我将讨论精度，并添加召回代码，所有剩余的指标将以相同的方式构建(下面的精度遵循相同的过程)。在未来的帖子中，我将向您展示如何在多类分类中形成ROC-AUC。</p><p id="cb04" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">注意:-不要跳进任何东西，除非你知道基本知识。因为它可能会吞噬你的信心和信念。</p><p id="92c6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们讨论一下precision的3个版本。</p><ol class=""><li id="fc50" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr jy jz ka kb bi translated"><strong class="iw hj">微平均精度:- </strong>在这里，首先，我们将找到所有类的所有真阳性，并将它们相加，以获得所有类的总真阳性。之后，我们将找到所有类的所有误报，并将它们相加，得到所有类的总误报。现在，计算真阳性和假阳性总数的精度。</li></ol><p id="2cec" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">2.<strong class="iw hj">宏平均精度</strong> :-这里，我们将找到目标列中不同类的精度，然后对它们进行平均以获得最终精度。</p><p id="ce94" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">3.<strong class="iw hj">加权精度</strong> :-这里，加权精度与宏平均精度相同，但区别就在这里，它取决于每类中存在的样本数并根据样本数进行加权。</p><p id="c512" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我知道，我已经为每一个给出了小的定义，但这就是定义，代码解释了一切。所以，检查定义，看看定义所需的代码，了解代码中发生了什么，你就会明白事情。</p><p id="76da" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">请记住，当数据平衡时，我们可以使用宏观版本，但当数据不平衡时，微观和加权版本更可取。</p><p id="fbd7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">请注意，下面的代码由precision的所有3个版本组成。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="9593" class="kq kr hi km b fi ks kt l ku kv">import pandas<br/>import argparse<br/>import numpy<br/>from collections import Counter</span><span id="073c" class="kq kr hi km b fi kw kt l ku kv">from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import train_test_split<br/>from sklearn import metrics</span><span id="5eb5" class="kq kr hi km b fi kw kt l ku kv">#there are 3 Types of precision in case of Multi-class classification. <br/>#1. Macro averaged precision<br/>#2. Micro averaged precision<br/>#3. Weighted precision</span><span id="6a94" class="kq kr hi km b fi kw kt l ku kv">def true_positive(y_true, y_pred):<br/>    tp = 0<br/>    for yt, yp in zip(y_true, y_pred):<br/>        if yt == 1 and yp == 1:<br/>            tp += 1<br/>    return tp<br/>    <br/>def true_negative(y_true, y_pred):<br/>    tn = 0<br/>    for yt, yp in zip(y_true, y_pred):<br/>        if yt == 0 and yp == 0:<br/>            tn += 1<br/>    return tn<br/>    <br/>def false_positive(y_true, y_pred):<br/>    fp = 0<br/>    for yt, yp in zip(y_true, y_pred):<br/>        if yt == 0 and yp == 1:<br/>            fp += 1<br/>    return fp<br/>    <br/>def false_negative(y_true, y_pred):<br/>    fn = 0<br/>    for yt, yp in zip(y_true, y_pred):<br/>        if yt == 1 and yp == 0:<br/>            fn += 1<br/>    return fn</span><span id="6b4d" class="kq kr hi km b fi kw kt l ku kv">def precision(y_test, y_pred):<br/>    tp =true_positive(y_test, y_pred)<br/>    fp = false_positive(y_test, y_pred)<br/>    try:<br/>        return(tp/(tp+fp))<br/>    except ZeroDivisionError:<br/>        return 0</span><span id="c1d5" class="kq kr hi km b fi kw kt l ku kv">def Macro_averaged_precision(y_test, predictions):<br/>    precisions = []<br/>    for i in range(1,5):<br/>        temp_ytest = [1 if x == i else 0 for x in y_test]<br/>        temp_ypred = [1 if x == i else 0 for x in predictions]<br/>        print(temp_ypred)<br/>        print(temp_ytest)<br/>        prec = precision(temp_ytest, temp_ypred)<br/>        precisions.append(prec)<br/>    <br/>    return (sum(precisions)/len(precisions))<br/>         <br/>def Micro_averaged_precision(y_test, predictions):<br/>    tp = 0<br/>    fp = 0<br/>    for i in range(1,5):<br/>        temp_ytest = [1 if x == i else 0 for x in y_test]<br/>        temp_ypred = [1 if x == i else 0 for x in predictions]</span><span id="d852" class="kq kr hi km b fi kw kt l ku kv">        tp += true_positive(temp_ytest, temp_ypred)<br/>        fp += false_positive(temp_ytest, temp_ypred)</span><span id="67b0" class="kq kr hi km b fi kw kt l ku kv">    precisions = tp / (tp + fp)</span><span id="f36a" class="kq kr hi km b fi kw kt l ku kv">    return precisions</span><span id="1667" class="kq kr hi km b fi kw kt l ku kv">def weighted_precision(y_test, predictions):<br/>    num_classes = len(numpy.unique(y_test))<br/>    #coutns for every class<br/>    precision = 0<br/>    for i in range(1, num_classes):<br/>        temp_ytest = [1 if x == i else 0 for x in y_test]<br/>        temp_ypred = [1 if x == i else 0 for x in predictions]</span><span id="0ff5" class="kq kr hi km b fi kw kt l ku kv">        tp = true_positive(temp_ytest, temp_ypred)<br/>        fp = false_positive(temp_ytest, temp_ypred)<br/>        <br/>        try:<br/>            preai = tp / (tp+fp)<br/>        except ZeroDivisionError:<br/>            preai = 0</span><span id="5d3e" class="kq kr hi km b fi kw kt l ku kv">        weighted = preai*sum(temp_ytest)</span><span id="7fe7" class="kq kr hi km b fi kw kt l ku kv">        precision += weighted</span><span id="882a" class="kq kr hi km b fi kw kt l ku kv">    precision = precision/len(y_test)<br/>    return precision</span><span id="643e" class="kq kr hi km b fi kw kt l ku kv">if __name__ == "__main__":<br/>    <br/>    data = pandas.read_csv("C:\\Users\\iamvi\\OneDrive\\Desktop\\Metrics_in_Machine_Learning\\development-index\\Development Index.csv")<br/>    <br/>    train = data.drop(['Development Index'], axis = 1).values<br/>    test = data["Development Index"].values</span><span id="a5c4" class="kq kr hi km b fi kw kt l ku kv">model = LogisticRegression()</span><span id="b28c" class="kq kr hi km b fi kw kt l ku kv">X_train, X_test, y_train, y_test = train_test_split(train, test, stratify = test)</span><span id="c0d0" class="kq kr hi km b fi kw kt l ku kv">model.fit(X_train, y_train)<br/>    predictions = model.predict(X_test)</span><span id="51f7" class="kq kr hi km b fi kw kt l ku kv">print("Macro precision is:", Macro_averaged_precision(y_test, predictions))<br/>    print("Micro precision is:", Micro_averaged_precision(y_test, predictions))<br/>    print("Weighted precision is:", weighted_precision(y_test, predictions))<br/>    print("sklearn Macro", metrics.precision_score(y_test, predictions, average = "macro"))<br/>    print("sklearn Micro", metrics.precision_score(y_test, predictions, average = "micro"))<br/>    print("sklearn weighted", metrics.precision_score(y_test, predictions, average = "weighted"))</span></pre><p id="8faa" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">你可能会发现由于介质接口导致的不一致的缩进，很难看到代码，点击<a class="ae js" href="https://github.com/VishnuVardhanVarapalli/Classification-Metrics/blob/main/Multi_class_classification_precision.py" rel="noopener ugc nofollow" target="_blank">此处</a>查看Github中的代码。</p><p id="0847" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这就是所有多类分类度量是如何从二进制分类度量产生的。</p><p id="e9e3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">正如我所说的，我会给你的代码召回类似精度，以及所有其余的指标。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="b569" class="kq kr hi km b fi ks kt l ku kv">import pandas<br/>import argparse<br/>import numpy<br/>from collections import Counter</span><span id="ced1" class="kq kr hi km b fi kw kt l ku kv">from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import train_test_split<br/>from sklearn import metrics</span><span id="375d" class="kq kr hi km b fi kw kt l ku kv">#there are 3 Types of recall in case of Multi-class classification. <br/>#1. Macro averaged recall<br/>#2. Micro averaged recall<br/>#3. Weighted recall</span><span id="5ec1" class="kq kr hi km b fi kw kt l ku kv">def true_positive(y_true, y_pred):<br/>    tp = 0<br/>    for yt, yp in zip(y_true, y_pred):<br/>        if yt == 1 and yp == 1:<br/>            tp += 1<br/>    return tp<br/>    <br/>def true_negative(y_true, y_pred):<br/>    tn = 0<br/>    for yt, yp in zip(y_true, y_pred):<br/>        if yt == 0 and yp == 0:<br/>            tn += 1<br/>    return tn<br/>    <br/>def false_positive(y_true, y_pred):<br/>    fp = 0<br/>    for yt, yp in zip(y_true, y_pred):<br/>        if yt == 0 and yp == 1:<br/>            fp += 1<br/>    return fp<br/>    <br/>def false_negative(y_true, y_pred):<br/>    fn = 0<br/>    for yt, yp in zip(y_true, y_pred):<br/>        if yt == 1 and yp == 0:<br/>            fn += 1<br/>    return fn</span><span id="efed" class="kq kr hi km b fi kw kt l ku kv">def recall(y_test, y_pred):<br/>    tp = true_positive(y_test, y_pred)<br/>    fn = false_negative(y_test, y_pred)<br/>    return(tp/(tp+fn))</span><span id="b353" class="kq kr hi km b fi kw kt l ku kv">def Macro_averaged_recall(y_test, predictions):<br/>    recalls = []<br/>    for i in range(1,5):<br/>        temp_ytest = [1 if x == i else 0 for x in y_test]<br/>        temp_ypred = [1 if x == i else 0 for x in predictions]<br/>        print(temp_ypred)<br/>        print(temp_ytest)<br/>        rec = recall(temp_ytest, temp_ypred)<br/>        recalls.append(rec)<br/>    <br/>    return (sum(recalls)/len(recalls))<br/>         <br/>def Micro_averaged_recall(y_test, predictions):<br/>    tp = 0<br/>    tn = 0<br/>    for i in range(1,5):<br/>        temp_ytest = [1 if x == i else 0 for x in y_test]<br/>        temp_ypred = [1 if x == i else 0 for x in predictions]</span><span id="0702" class="kq kr hi km b fi kw kt l ku kv">        tp += true_positive(temp_ytest, temp_ypred)<br/>        tn += true_negative(temp_ytest, temp_ypred)</span><span id="0b53" class="kq kr hi km b fi kw kt l ku kv">    recall = tp / (tp + tn)</span><span id="8258" class="kq kr hi km b fi kw kt l ku kv">    return recall</span><span id="f3a7" class="kq kr hi km b fi kw kt l ku kv">def weighted_recall(y_test, predictions):<br/>    num_classes = len(numpy.unique(y_test))<br/>    #counts for every class<br/>    recall = 0<br/>    for i in range(1, num_classes):<br/>        temp_ytest = [1 if x == i else 0 for x in y_test]<br/>        temp_ypred = [1 if x == i else 0 for x in predictions]</span><span id="dc19" class="kq kr hi km b fi kw kt l ku kv">tp = true_positive(temp_ytest, temp_ypred)<br/>        tn = true_negative(temp_ytest, temp_ypred)<br/>        <br/>        try:<br/>            rec = tp / (tp+tn)<br/>        except ZeroDivisionError:<br/>            rec = 0</span><span id="4820" class="kq kr hi km b fi kw kt l ku kv">weighted = rec*sum(temp_ytest)</span><span id="5513" class="kq kr hi km b fi kw kt l ku kv">recall += weighted</span><span id="35c1" class="kq kr hi km b fi kw kt l ku kv">recall = recall/len(y_test)<br/>    return recall</span><span id="614a" class="kq kr hi km b fi kw kt l ku kv">if __name__ == "__main__":<br/>    <br/>    data = pandas.read_csv("C:\\Users\\iamvi\\OneDrive\\Desktop\\Metrics_in_Machine_Learning\\development-index\\Development Index.csv")<br/>    <br/>    train = data.drop(['Development Index'], axis = 1).values<br/>    test = data["Development Index"].values</span><span id="ce93" class="kq kr hi km b fi kw kt l ku kv">model = LogisticRegression()</span><span id="82ef" class="kq kr hi km b fi kw kt l ku kv">X_train, X_test, y_train, y_test = train_test_split(train, test, stratify = test)</span><span id="8230" class="kq kr hi km b fi kw kt l ku kv">model.fit(X_train, y_train)<br/>    predictions = model.predict(X_test)</span><span id="63ee" class="kq kr hi km b fi kw kt l ku kv">print("Macro recall is:", Macro_averaged_recall(y_test, predictions))<br/>    print("Micro recall is:", Micro_averaged_recall(y_test, predictions))<br/>    print("Weighted recall is:", weighted_recall(y_test, predictions))<br/>    print("sklearn Macro", metrics.recall_score(y_test, predictions, average = "macro"))<br/>    print("sklearn Micro", metrics.recall_score(y_test, predictions, average = "micro"))<br/>    print("sklearn weighted", metrics.recall_score(y_test, predictions, average = "weighted"))</span></pre><p id="4fe2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">precision的3个版本的定义也将应用于召回。</p><p id="c464" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">当你在3个版本中实现了我上面所说的所有多类分类度量标准之后，你可以自信地说“我知道多类分类”。😉</p><p id="d99d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">检查<a class="ae js" href="https://github.com/VishnuVardhanVarapalli/Classification-Metrics" rel="noopener ugc nofollow" target="_blank">分类-度量库</a>以获得我解释的所有概念的代码。当我在这里写文章时，GitHub将会更新代码。所以，把叉子放在上面。</p><p id="dfa3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果你想在这里补充什么，或者想让我解释这里遗漏的概念，让我从<a class="ae js" href="https://iamvishnuvarapally.wixsite.com/port-folio" rel="noopener ugc nofollow" target="_blank">这里</a>知道。</p><p id="ad8c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">可以在不同平台<a class="ae js" href="https://www.linkedin.com/in/vishnu-vardhan-varapalli-b6b454150/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae js" href="https://github.com/VishnuVardhanVarapalli" rel="noopener ugc nofollow" target="_blank"> Github </a>、<a class="ae js" href="https://iamvishnu-varapally.medium.com/" rel="noopener"> medium </a>关注我。</p><p id="6a2d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">快乐的Learning✌.</p></div></div>    
</body>
</html>
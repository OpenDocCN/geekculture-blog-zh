<html>
<head>
<title>Brief Overview of Natural Language Processing with tensorflow2.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">tensorflow2.0自然语言处理概述</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/natural-language-processing-ed48334f80be?source=collection_archive---------44-----------------------#2021-05-31">https://medium.com/geekculture/natural-language-processing-ed48334f80be?source=collection_archive---------44-----------------------#2021-05-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/262dd71f2f70a52c733741b4bc65c0b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UsFIEny149WPIRqNaR2MaQ.jpeg"/></div></div></figure><p id="5483" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">自然语言处理(简称NLP)是一门处理自然(人类)语言和计算机语言之间交流的计算学科。NLP的一个常见例子是拼写检查或自动完成。本质上，NLP是关注计算机如何理解和/或处理自然/人类语言的领域。</p><h1 id="1a53" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">递归神经网络</h1><p id="e4e7" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">在本教程中，我们将介绍一种新的神经网络，它能够更好地处理序列数据，如文本或字符，称为<strong class="is hj">递归神经网络</strong>(简称RNN)。</p><p id="5880" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将学习如何使用当前的神经网络来完成以下任务:</p><ul class=""><li id="182a" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated">情感分析</li><li id="2ef9" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">字符生成</li></ul><p id="a1e3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">RNN很复杂，有许多不同的形式，所以在本教程中，我们将重点介绍它们是如何工作的，以及它们最适合解决什么样的问题。</p><h1 id="66a7" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">序列数据</h1><p id="2472" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">在前一篇博客中，我们关注的是可以表示为一个静态数据点的数据，其中时间或步骤的概念是不相关的。以我们的图像数据为例，它只是一个形状张量(宽度、高度、通道)。这些数据不会改变也不关心时间的概念。</p><p id="dda1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这个博客中，我们将看看文本序列，并学习如何以一种有意义的方式对它们进行编码。与图像不同，序列数据，如长链文本、天气模式、视频以及任何与步骤或时间相关的东西，都需要以特殊的方式进行处理。</p><p id="9080" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是我说的序列是什么意思，为什么文本数据是序列？这是个好问题。由于文本数据包含许多单词，这些单词以非常特定和有意义的顺序出现，我们需要能够跟踪每个单词以及它何时出现在数据中。简单地将一整段文字编码成一个数据点并不能给我们一个非常有意义的数据图像，而且很难做任何事情。这就是为什么我们将文本视为一个序列，一次处理一个单词。我们将跟踪这些单词出现的位置，并使用这些信息来理解文本的意思。</p><h1 id="5146" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">编码文本</h1><p id="c31f" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">正如我们所知，机器学习模型和神经网络不会将原始文本数据作为输入。这意味着我们必须以某种方式将文本数据编码成模型能够理解的数值。有许多不同的方法可以做到这一点，我们将在下面看几个例子。</p><p id="26ae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们进入不同的编码/预处理方法之前，让我们通过查看下面两个电影评论来理解我们可以从文本数据中获得的信息。</p><p id="1f63" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du lf lg lh li b">I thought the movie was going to be bad, but it was actually amazing!</code></p><p id="a1e4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du lf lg lh li b">I thought the movie was going to be amazing, but it was actually bad!</code></p><p id="ee0e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">虽然这两个词非常相似，但我们知道它们有非常不同的含义。这是因为单词的排序是文本数据的一个非常重要的属性。</p><p id="4622" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，在我们考虑一些不同的文本数据编码方式时，请记住这一点。</p><h1 id="0056" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">一袋单词</h1><p id="962b" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">对我们的数据进行编码的第一个也是最简单的方法是使用一种叫做<strong class="is hj">的单词包</strong>。这是一个非常简单的技术，句子中的每个单词都用一个整数编码，并被放入一个集合中，该集合不保持单词的顺序，但会跟踪频率。看看下面的python函数，它将一串文本编码成单词包。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="3e2c" class="lr jp hi li b fi ls lt l lu lv">vocab = {} # maps word to integer representing it</span><span id="202b" class="lr jp hi li b fi lw lt l lu lv">word_encoding = 1</span><span id="f0b0" class="lr jp hi li b fi lw lt l lu lv">def bag_of_words(text):</span><span id="d721" class="lr jp hi li b fi lw lt l lu lv">global word_encoding</span><span id="5946" class="lr jp hi li b fi lw lt l lu lv">words = text.lower().split(“ “) # create a list of all of the words in the text, well assume there is no grammar in our text for this example</span><span id="0431" class="lr jp hi li b fi lw lt l lu lv">bag = {} # stores all of the encodings and their frequency</span><span id="4563" class="lr jp hi li b fi lw lt l lu lv">for word in words:</span><span id="d406" class="lr jp hi li b fi lw lt l lu lv">if word in vocab:</span><span id="ec4d" class="lr jp hi li b fi lw lt l lu lv">encoding = vocab[word] # get encoding from vocab</span><span id="3fb9" class="lr jp hi li b fi lw lt l lu lv">else:</span><span id="54a3" class="lr jp hi li b fi lw lt l lu lv">vocab[word] = word_encoding</span><span id="719c" class="lr jp hi li b fi lw lt l lu lv">encoding = word_encoding</span><span id="e9f9" class="lr jp hi li b fi lw lt l lu lv">word_encoding += 1</span><span id="cf4e" class="lr jp hi li b fi lw lt l lu lv">if encoding in bag:</span><span id="a038" class="lr jp hi li b fi lw lt l lu lv">bag[encoding] += 1</span><span id="f36f" class="lr jp hi li b fi lw lt l lu lv">else:</span><span id="00f6" class="lr jp hi li b fi lw lt l lu lv">bag[encoding] = 1</span><span id="eac7" class="lr jp hi li b fi lw lt l lu lv">return bag</span><span id="7c02" class="lr jp hi li b fi lw lt l lu lv">text = “this is a test to see if this test will work is is test a a”</span><span id="b7b6" class="lr jp hi li b fi lw lt l lu lv">bag = bag_of_words(text)</span><span id="b6aa" class="lr jp hi li b fi lw lt l lu lv">print(bag)</span><span id="9ee9" class="lr jp hi li b fi lw lt l lu lv">print(vocab)</span></pre><p id="2b52" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这不是我们在实践中的做法，但我希望它能让你了解单词袋是如何工作的。请注意，我们已经忘记了单词出现的顺序。事实上，让我们来看看这种编码对于我们上面展示的两个句子是如何工作的。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="12e3" class="lr jp hi li b fi ls lt l lu lv">positive_review = “I thought the movie was going to be bad but it was actually amazing”</span><span id="f86d" class="lr jp hi li b fi lw lt l lu lv">negative_review = “I thought the movie was going to be amazing but it was actually bad”</span><span id="4d56" class="lr jp hi li b fi lw lt l lu lv">pos_bag = bag_of_words(positive_review)</span><span id="3d61" class="lr jp hi li b fi lw lt l lu lv">neg_bag = bag_of_words(negative_review)</span><span id="05df" class="lr jp hi li b fi lw lt l lu lv">print(“Positive:”, pos_bag)</span><span id="fe41" class="lr jp hi li b fi lw lt l lu lv">print(“Negative:”, neg_bag)</span></pre><p id="35a8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以看到，即使这些句子有着非常不同的含义，它们的编码方式却完全相同。显然，这是行不通的。再来看看其他一些方法。</p><h1 id="3a35" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">整数编码</h1><p id="e7f0" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">我们要看的下一个技术叫做<strong class="is hj">整数编码</strong>。这包括将句子中的每个单词或字符表示为唯一的整数，并保持这些单词的顺序。这有望解决我们之前看到的单词顺序混乱的问题</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="0616" class="lr jp hi li b fi ls lt l lu lv">word_encoding = 1</span><span id="937f" class="lr jp hi li b fi lw lt l lu lv">def one_hot_encoding(text):</span><span id="bebe" class="lr jp hi li b fi lw lt l lu lv">global word_encoding</span><span id="dc13" class="lr jp hi li b fi lw lt l lu lv">words = text.lower().split(“ “)</span><span id="71ab" class="lr jp hi li b fi lw lt l lu lv">encoding = []</span><span id="260e" class="lr jp hi li b fi lw lt l lu lv">for word in words:</span><span id="3588" class="lr jp hi li b fi lw lt l lu lv">if word in vocab:</span><span id="f9b5" class="lr jp hi li b fi lw lt l lu lv">code = vocab[word]</span><span id="da81" class="lr jp hi li b fi lw lt l lu lv">encoding.append(code)</span><span id="4127" class="lr jp hi li b fi lw lt l lu lv">else:</span><span id="f6c4" class="lr jp hi li b fi lw lt l lu lv">vocab[word] = word_encoding</span><span id="0c4d" class="lr jp hi li b fi lw lt l lu lv">encoding.append(word_encoding)</span><span id="d8ec" class="lr jp hi li b fi lw lt l lu lv">word_encoding += 1</span><span id="add1" class="lr jp hi li b fi lw lt l lu lv">return encoding</span><span id="f2d7" class="lr jp hi li b fi lw lt l lu lv">text = “this is a test to see if this test will work is is test a a”</span><span id="f0ab" class="lr jp hi li b fi lw lt l lu lv">encoding = one_hot_encoding(text)</span><span id="78f1" class="lr jp hi li b fi lw lt l lu lv">print(encoding)</span><span id="84a0" class="lr jp hi li b fi lw lt l lu lv">print(vocab)</span><span id="4460" class="lr jp hi li b fi lw lt l lu lv">And now let’s have a look at one hot encoding on our movie reviews.</span><span id="5620" class="lr jp hi li b fi lw lt l lu lv">positive_review = “I thought the movie was going to be bad but it was actually amazing”</span><span id="4dee" class="lr jp hi li b fi lw lt l lu lv">negative_review = “I thought the movie was going to be amazing but it was actually bad”</span><span id="2f4f" class="lr jp hi li b fi lw lt l lu lv">pos_encode = one_hot_encoding(positive_review)</span><span id="e8c0" class="lr jp hi li b fi lw lt l lu lv">neg_encode = one_hot_encoding(negative_review)</span><span id="71ed" class="lr jp hi li b fi lw lt l lu lv">print(“Positive:”, pos_encode)</span><span id="c137" class="lr jp hi li b fi lw lt l lu lv">print(“Negative:”, neg_encode)</span></pre><p id="fcb1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">更好的是，现在我们跟踪单词的顺序，我们可以知道每个单词出现在哪里。但是这仍然有一些问题。理想情况下，当我们对单词进行编码时，我们希望相似的单词有相似的标签，而不同的单词有非常不同的标签。例如，快乐和喜悦这两个词应该有非常相似的标签，这样我们就可以确定它们是相似的。而像“可怕的”和“惊人的”这样的词应该有非常不同的标签。我们上面看到的方法不能为我们做这样的事情。这可能意味着模型将很难确定两个单词是否相似，这可能会导致一些非常严重的性能影响。</p><h1 id="de65" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">单词嵌入</h1><p id="af85" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">幸运的是，有第三种方法要好得多，<strong class="is hj">单词嵌入</strong>。这种方法保持单词的顺序不变，并且用非常相似的标签对相似的单词进行编码。它不仅试图对单词的频率和顺序进行编码，还试图对这些单词在句子中的含义进行编码。它将每个单词编码成一个密集的向量，代表它在句子中的上下文。</p><p id="0617" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">与之前的技术不同，单词嵌入是通过查看许多不同的训练示例来学习的。你可以将所谓的<em class="lx">嵌入层</em>添加到你的模型的开始，当你的模型训练你的嵌入层时，它将学习单词的正确嵌入。您也可以使用预训练的嵌入层。</p><p id="7f04" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是我们将在示例中使用的技术，它的实现将在后面展示。</p><h1 id="d93e" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">递归神经网络(RNN氏)</h1><p id="e547" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">既然我们已经了解了一些如何对文本进行编码的知识，那么是时候深入研究递归神经网络了。到目前为止，我们一直在使用一种叫做<strong class="is hj">前馈</strong>神经网络的东西。这仅仅意味着我们所有的数据通过网络从左到右向前(一次全部)传送。这对于我们之前考虑的问题来说很好，但是对于处理文本来说就不太好了。毕竟，即使是我们(人类)也不会一次性处理文本。我们从左到右一个单词一个单词地读，并跟踪句子的当前意思，这样我们就能理解下一个单词的意思。这正是递归神经网络的设计目的。当我们说递归神经网络时，我们真正的意思是一个包含循环的网络。RNN将一次处理一个单词，同时保持已经看到的内容的内部记忆。这将允许它根据单词在句子中的顺序来区别对待单词，并慢慢地建立对整个输入的理解，一次一个单词。</p><p id="aceb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这就是为什么我们将文本数据视为一个序列！这样我们就可以一个单词一个单词地传递给RNN。</p><p id="2535" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们来看看循环层可能是什么样子。</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ly"><img src="../Images/441067dfefb7b290d94e5bddda5a78be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*E4AGtlEFD2GSCKjV.png"/></div></div></figure><p id="49b7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">【https://colah.github.io/posts/2015-08-Understanding-LSTMs/】来源:<a class="ae lz" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"><em class="lx"/></a></p><p id="79a1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们开始解释之前，让我们先定义一下所有这些变量代表什么。</p><p id="8f7b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> ht </strong>在时间t的输出</p><p id="56de" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> xt </strong>在时间t输入</p><p id="86bb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">一个</strong>循环层(loop)</p><p id="629b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该图试图说明的是，递归层一次处理一个单词或输入，并与前一次迭代的输出相结合。因此，随着我们在输入序列中的进一步发展，我们对整个文本建立了更复杂的理解。</p><p id="8317" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们刚刚看到的是一个简单的RNN层。对于简单的问题，它可以有效地处理较短的文本序列，但也有许多缺点。其中之一是，随着文本序列变得越来越长，网络越来越难以正确理解文本。</p><h1 id="0939" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">LSTM</h1><p id="fbcd" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">我们上面深入讨论的层被称为简单层。然而，确实存在一些比简单的RNN层工作得更好的其他循环层(包含循环的层)。我们将在这里谈论的一个叫做LSTM(长短期记忆)。这一层的工作方式与simpleRNN层非常相似，但是增加了一种访问过去任何时间步长的输入的方式。而在我们简单的RNN层中，随着输入的深入，来自先前时间戳的输入逐渐消失。有了LSTM，我们就有了一个长期的记忆数据结构来存储所有之前看到的输入以及我们看到它们的时间。这允许我们在任何时间点访问我们想要的任何先前的值。这增加了我们网络的复杂性，并允许它发现输入之间以及输入何时出现的更有用的关系。</p><p id="4ae4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">出于本课程的目的，我们将不再深入这些层如何工作的数学或细节。</p><h1 id="bf1b" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">LSTM</h1><p id="5d1e" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">现在是时候看看一个循环神经网络的运行了。对于这个例子，我们要做一个叫做情感分析的东西。</p><p id="c073" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">维基百科对该术语的正式定义如下:</p><p id="d8d2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="lx">对一篇文章中表达的观点进行计算机识别和分类的过程，尤其是为了确定作者对特定主题、产品等的态度。是正的、负的或中性的。</em></p><p id="e545" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们在这里使用的例子是将电影评论分为正面、负面或中性。</p><p id="a9a3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="lx">本指南基于以下tensorflow教程:</em><a class="ae lz" href="https://www.tensorflow.org/tutorials/text/text_classification_rnn" rel="noopener ugc nofollow" target="_blank"><em class="lx">https://www . tensor flow . org/tutorials/text/text _ classification _ rnn</em></a></p><h1 id="3406" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">情感分析</h1><p id="db91" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">我们从从keras加载IMDB电影评论数据集开始。该数据集包含来自IMDB的25，000条评论，其中每条评论都已经过预处理，并具有正面或负面的标签。每个评论都由整数编码，代表一个词在整个数据集中的常见程度。例如，由整数3编码的单词意味着它是数据集中第三个最常见的单词。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="5491" class="lr jp hi li b fi ls lt l lu lv">%tensorflow_version 2.x # this line is not required unless you are in a notebook</span><span id="2dc4" class="lr jp hi li b fi lw lt l lu lv">from keras.datasets import imdb</span><span id="e9df" class="lr jp hi li b fi lw lt l lu lv">from keras.preprocessing import sequence</span><span id="cc47" class="lr jp hi li b fi lw lt l lu lv">import keras</span><span id="8c2b" class="lr jp hi li b fi lw lt l lu lv">import tensorflow as tf</span><span id="e21b" class="lr jp hi li b fi lw lt l lu lv">import os</span><span id="c82b" class="lr jp hi li b fi lw lt l lu lv">import numpy as np</span><span id="f665" class="lr jp hi li b fi lw lt l lu lv">VOCAB_SIZE = 88584</span><span id="717d" class="lr jp hi li b fi lw lt l lu lv">MAXLEN = 250</span><span id="9d82" class="lr jp hi li b fi lw lt l lu lv">BATCH_SIZE = 64</span><span id="537b" class="lr jp hi li b fi lw lt l lu lv">(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)</span><span id="c3d9" class="lr jp hi li b fi lw lt l lu lv">[ ]</span><span id="bb64" class="lr jp hi li b fi lw lt l lu lv"># Lets look at one review</span><span id="8224" class="lr jp hi li b fi lw lt l lu lv">train_data[1]</span></pre><h1 id="01db" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">更多预处理</h1><p id="593f" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">如果我们看一下我们的一些评论，我们会注意到它们的长度不同。这是一个问题。我们不能将不同长度的数据传递到我们的神经网络中。所以一定要让每次复习的长度一样。为此，我们将遵循以下程序:</p><ul class=""><li id="3089" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated">如果评论超过250字，那么删掉多余的字</li><li id="809c" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">如果评论少于250字，则添加必要数量的0，使其等于250字。</li></ul><p id="f5a9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">幸运的是，keras有一个功能可以帮我们做到这一点:</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="2847" class="lr jp hi li b fi ls lt l lu lv">train_data = sequence.pad_sequences(train_data, MAXLEN)</span><span id="be91" class="lr jp hi li b fi lw lt l lu lv">test_data = sequence.pad_sequences(test_data, MAXLEN)</span></pre><h1 id="2f4b" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">创建模型</h1><p id="cd55" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">现在是创建模型的时候了。我们将使用一个单词嵌入层作为我们模型的第一层，然后添加一个LSTM层，该层进入一个密集的节点，以获得我们预测的情绪。</p><p id="5b1b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">32代表嵌入层生成的矢量的输出维数。如果我们愿意，我们可以改变这个值！</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="ab87" class="lr jp hi li b fi ls lt l lu lv">model = tf.keras.Sequential([</span><span id="55e8" class="lr jp hi li b fi lw lt l lu lv">tf.keras.layers.Embedding(VOCAB_SIZE, 32),</span><span id="8fef" class="lr jp hi li b fi lw lt l lu lv">tf.keras.layers.LSTM(32),</span><span id="4245" class="lr jp hi li b fi lw lt l lu lv">tf.keras.layers.Dense(1, activation=”sigmoid”)</span><span id="577f" class="lr jp hi li b fi lw lt l lu lv">])</span><span id="7f6d" class="lr jp hi li b fi lw lt l lu lv">model.summary()</span></pre><h1 id="0e78" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">培养</h1><p id="5c10" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">现在是编译和训练模型的时候了。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="e862" class="lr jp hi li b fi ls lt l lu lv">model.compile(loss=”binary_crossentropy”,optimizer=”rmsprop”,metrics=[‘acc’])</span><span id="e3c1" class="lr jp hi li b fi lw lt l lu lv">history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)</span></pre><p id="59d9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将根据我们的训练数据评估该模型，看看它的表现如何。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="ab31" class="lr jp hi li b fi ls lt l lu lv">results = model.evaluate(test_data, test_labels)</span><span id="c893" class="lr jp hi li b fi lw lt l lu lv">print(results)</span></pre><p id="9ba6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，我们的得分在80%左右。对于一个简单的循环网络来说，这已经不错了。</p><h1 id="5700" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">做预测</h1><p id="8d0a" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">现在让我们利用我们的网络对我们自己的评论进行预测。</p><p id="bf67" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因为我们的评论是编码好的，所以需要将我们写的任何评论转换成那种形式，以便网络能够理解它。为此，从数据集中加载编码，并使用它们来编码我们自己的数据。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="885b" class="lr jp hi li b fi ls lt l lu lv">word_index = imdb.get_word_index()</span><span id="3756" class="lr jp hi li b fi lw lt l lu lv">def encode_text(text):</span><span id="3ab9" class="lr jp hi li b fi lw lt l lu lv">tokens = keras.preprocessing.text.text_to_word_sequence(text)</span><span id="afcd" class="lr jp hi li b fi lw lt l lu lv">tokens = [word_index[word] if word in word_index else 0 for word in tokens]</span><span id="2abb" class="lr jp hi li b fi lw lt l lu lv">return sequence.pad_sequences([tokens], MAXLEN)[0]</span><span id="b646" class="lr jp hi li b fi lw lt l lu lv">text = “that movie was just amazing, so amazing”</span><span id="b22f" class="lr jp hi li b fi lw lt l lu lv">encoded = encode_text(text)</span><span id="c428" class="lr jp hi li b fi lw lt l lu lv">print(encoded)</span></pre><p id="879b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当我们在这里的时候，让我们做一个解码函数</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="8817" class="lr jp hi li b fi ls lt l lu lv">reverse_word_index = {value: key for (key, value) in word_index.items()}</span><span id="9097" class="lr jp hi li b fi lw lt l lu lv">def decode_integers(integers):</span><span id="65f7" class="lr jp hi li b fi lw lt l lu lv">PAD = 0</span><span id="e4a1" class="lr jp hi li b fi lw lt l lu lv">text = “”</span><span id="8f86" class="lr jp hi li b fi lw lt l lu lv">for num in integers:</span><span id="f78c" class="lr jp hi li b fi lw lt l lu lv">if num != PAD:</span><span id="03c2" class="lr jp hi li b fi lw lt l lu lv">text += reverse_word_index[num] + “ “</span><span id="a662" class="lr jp hi li b fi lw lt l lu lv">return text[:-1]</span><span id="7904" class="lr jp hi li b fi lw lt l lu lv">print(decode_integers(encoded))</span></pre><p id="42a1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在是做预测的时候了</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="6808" class="lr jp hi li b fi ls lt l lu lv">def predict(text):</span><span id="5b7d" class="lr jp hi li b fi lw lt l lu lv">encoded_text = encode_text(text)</span><span id="aab2" class="lr jp hi li b fi lw lt l lu lv">pred = np.zeros((1,250))</span><span id="7252" class="lr jp hi li b fi lw lt l lu lv">pred[0] = encoded_text</span><span id="83c6" class="lr jp hi li b fi lw lt l lu lv">result = model.predict(pred)</span><span id="43c2" class="lr jp hi li b fi lw lt l lu lv">print(result[0])</span><span id="d3dd" class="lr jp hi li b fi lw lt l lu lv">positive_review = “That movie was! really loved it and would great watch it again because it was amazingly great”</span><span id="7ec6" class="lr jp hi li b fi lw lt l lu lv">predict(positive_review)</span><span id="c6c5" class="lr jp hi li b fi lw lt l lu lv">negative_review = “that movie really sucked. I hated it and wouldn’t watch it again. Was one of the worst things I’ve ever watched”</span><span id="2754" class="lr jp hi li b fi lw lt l lu lv">predict(negative_review)</span></pre><h1 id="5235" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">RNN游戏发生器</h1><p id="422b" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">现在我们来看一个迄今为止最酷的例子。我们将使用RNN生成一个剧本。我们将简单地向RNN展示我们希望它重新创建的东西的一个例子，它将学习如何自己编写它的一个版本。我们将使用字符预测模型来实现这一点，该模型将可变长度序列作为输入，并预测下一个字符。我们可以连续多次使用该模型，将上一次预测的输出作为下一次调用的输入来生成序列。</p><p id="ebad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="lx">本指南基于以下:</em><a class="ae lz" href="https://www.tensorflow.org/tutorials/text/text_generation" rel="noopener ugc nofollow" target="_blank"><em class="lx"/></a></p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="61be" class="lr jp hi li b fi ls lt l lu lv">%tensorflow_version 2.x # this line is not required unless you are in a notebook</span><span id="0b69" class="lr jp hi li b fi lw lt l lu lv">from keras.preprocessing import sequence</span><span id="8b3b" class="lr jp hi li b fi lw lt l lu lv">import keras</span><span id="8986" class="lr jp hi li b fi lw lt l lu lv">import tensorflow as tf</span><span id="215d" class="lr jp hi li b fi lw lt l lu lv">import os</span><span id="d4bb" class="lr jp hi li b fi lw lt l lu lv">import numpy as np</span></pre><h1 id="82a8" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">资料组</h1><p id="863b" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">对于这个例子，我们只需要一组训练数据。事实上，如果我们愿意，我们可以写我们自己的诗或剧本，并把它传给网络进行训练。然而，为了简单起见，我们将使用莎士比亚戏剧中的一段摘录。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="77ba" class="lr jp hi li b fi ls lt l lu lv">path_to_file = tf.keras.utils.get_file(‘shakespeare.txt’, ‘https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')</span></pre><h1 id="d8c6" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">加载您自己的数据</h1><p id="5800" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">要加载您自己的数据，您需要在google colab中上传一个文件。然后你需要按照上面的步骤，但是加载这个新文件。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="8903" class="lr jp hi li b fi ls lt l lu lv">from google.colab import files</span><span id="1f2a" class="lr jp hi li b fi lw lt l lu lv">path_to_file = list(files.upload().keys())[0]</span></pre><h1 id="b42c" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">读取文件内容</h1><p id="cf0e" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">让我们看看文件的内容。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="f184" class="lr jp hi li b fi ls lt l lu lv"># Read, then decode for py2 compat.</span><span id="1596" class="lr jp hi li b fi lw lt l lu lv">text = open(path_to_file, ‘rb’).read().decode(encoding=’utf-8')</span><span id="d591" class="lr jp hi li b fi lw lt l lu lv"># length of text is the number of characters in it</span><span id="564f" class="lr jp hi li b fi lw lt l lu lv">print (‘Length of text: {} characters’.format(len(text)))</span><span id="5d94" class="lr jp hi li b fi lw lt l lu lv">[ ]</span><span id="6ae4" class="lr jp hi li b fi lw lt l lu lv"># Take a look at the first 250 characters in text</span><span id="cbf6" class="lr jp hi li b fi lw lt l lu lv">print(text[:250])</span></pre><h1 id="27b1" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">编码</h1><p id="5e0a" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">因为这个文本还没有被编码，我们需要自己来做。我们将把每个独特的字符编码成一个不同的整数。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="f179" class="lr jp hi li b fi ls lt l lu lv">vocab = sorted(set(text))</span><span id="8e24" class="lr jp hi li b fi lw lt l lu lv"># Creating a mapping from unique characters to indices</span><span id="2059" class="lr jp hi li b fi lw lt l lu lv">char2idx = {u:i for i, u in enumerate(vocab)}</span><span id="1095" class="lr jp hi li b fi lw lt l lu lv">idx2char = np.array(vocab)</span><span id="7c11" class="lr jp hi li b fi lw lt l lu lv">def text_to_int(text):</span><span id="1309" class="lr jp hi li b fi lw lt l lu lv">return np.array([char2idx[c] for c in text])</span><span id="9f06" class="lr jp hi li b fi lw lt l lu lv">text_as_int = text_to_int(text)</span><span id="df45" class="lr jp hi li b fi lw lt l lu lv"># lets look at how part of our text is encoded</span><span id="e7cb" class="lr jp hi li b fi lw lt l lu lv">print(“Text:”, text[:13])</span><span id="f6bb" class="lr jp hi li b fi lw lt l lu lv">print(“Encoded:”, text_to_int(text[:13]))</span></pre><p id="ef17" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里我们将创建一个函数，可以将数值转换成文本。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="f0df" class="lr jp hi li b fi ls lt l lu lv">def int_to_text(ints):</span><span id="6e48" class="lr jp hi li b fi lw lt l lu lv">try:</span><span id="9880" class="lr jp hi li b fi lw lt l lu lv">ints = ints.numpy()</span><span id="a7dc" class="lr jp hi li b fi lw lt l lu lv">except:</span><span id="a782" class="lr jp hi li b fi lw lt l lu lv">pass</span><span id="04ff" class="lr jp hi li b fi lw lt l lu lv">return ‘’.join(idx2char[ints])</span><span id="8043" class="lr jp hi li b fi lw lt l lu lv">print(int_to_text(text_as_int[:13]))</span></pre><h1 id="856d" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">创建培训示例</h1><p id="4d80" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">记住我们的任务是给模型输入一个序列，并让它返回给我们下一个字符。这意味着我们需要将上面的文本数据分成许多更短的序列，我们可以将它们作为训练样本传递给模型。</p><p id="0f08" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将准备的训练示例将使用一个<em class="lx"> seq_length </em>序列作为输入，一个<em class="lx"> seq_length </em>序列作为输出，其中该序列是向右移动了一个字母的原始序列。例如:</p><p id="aa3a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du lf lg lh li b">input: Hell | output: ello</code></p><p id="43d2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们的第一步是从文本数据中创建一个字符流。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="16a2" class="lr jp hi li b fi ls lt l lu lv">seq_length = 100 # length of sequence for a training example</span><span id="6ab6" class="lr jp hi li b fi lw lt l lu lv">examples_per_epoch = len(text)//(seq_length+1)</span><span id="2d9b" class="lr jp hi li b fi lw lt l lu lv"># Create training examples / targets</span><span id="6a36" class="lr jp hi li b fi lw lt l lu lv">char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)</span></pre><p id="a344" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，我们可以使用batch方法将这个字符流转换成所需长度的批次。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="28c3" class="lr jp hi li b fi ls lt l lu lv">sequences = char_dataset.batch(seq_length+1, drop_remainder=True)</span></pre><p id="adac" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们需要使用这些长度为101的序列，并将它们分成输入和输出。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="5437" class="lr jp hi li b fi ls lt l lu lv">def split_input_target(chunk): # for the example: hello</span><span id="e44e" class="lr jp hi li b fi lw lt l lu lv">input_text = chunk[:-1] # hell</span><span id="8984" class="lr jp hi li b fi lw lt l lu lv">target_text = chunk[1:] # ello</span><span id="272a" class="lr jp hi li b fi lw lt l lu lv">return input_text, target_text # hell, ello</span><span id="87de" class="lr jp hi li b fi lw lt l lu lv">dataset = sequences.map(split_input_target) # we use map to apply the above function to every entry</span><span id="48ca" class="lr jp hi li b fi lw lt l lu lv">[ ]</span><span id="5e3d" class="lr jp hi li b fi lw lt l lu lv">for x, y in dataset.take(2):</span><span id="e77b" class="lr jp hi li b fi lw lt l lu lv">print(“\n\nEXAMPLE\n”)</span><span id="472d" class="lr jp hi li b fi lw lt l lu lv">print(“INPUT”)</span><span id="71f6" class="lr jp hi li b fi lw lt l lu lv">print(int_to_text(x))</span><span id="6560" class="lr jp hi li b fi lw lt l lu lv">print(“\nOUTPUT”)</span><span id="23a2" class="lr jp hi li b fi lw lt l lu lv">print(int_to_text(y))</span></pre><p id="e975" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，我们需要进行批量训练。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="e58a" class="lr jp hi li b fi ls lt l lu lv">BATCH_SIZE = 64</span><span id="d0c1" class="lr jp hi li b fi lw lt l lu lv">VOCAB_SIZE = len(vocab) # vocab is number of unique characters</span><span id="67cb" class="lr jp hi li b fi lw lt l lu lv">EMBEDDING_DIM = 256</span><span id="af77" class="lr jp hi li b fi lw lt l lu lv">RNN_UNITS = 1024</span><span id="cdb9" class="lr jp hi li b fi lw lt l lu lv"># Buffer size to shuffle the dataset</span><span id="7524" class="lr jp hi li b fi lw lt l lu lv"># (TF data is designed to work with possibly infinite sequences,</span><span id="76e4" class="lr jp hi li b fi lw lt l lu lv"># so it doesn’t attempt to shuffle the entire sequence in memory. Instead,</span><span id="2c42" class="lr jp hi li b fi lw lt l lu lv"># it maintains a buffer in which it shuffles elements).</span><span id="6c9d" class="lr jp hi li b fi lw lt l lu lv">BUFFER_SIZE = 10000</span><span id="3811" class="lr jp hi li b fi lw lt l lu lv">data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)</span></pre><h1 id="e5e7" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">构建模型</h1><p id="09b8" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">现在是建立模型的时候了。我们将使用一个LSTM嵌入层和一个密集层，后者包含训练数据中每个唯一字符的节点。密集层将给出所有节点的概率分布。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="6e4f" class="lr jp hi li b fi ls lt l lu lv">def build_model(vocab_size, embedding_dim, rnn_units, batch_size):</span><span id="af9e" class="lr jp hi li b fi lw lt l lu lv">model = tf.keras.Sequential([</span><span id="1602" class="lr jp hi li b fi lw lt l lu lv">tf.keras.layers.Embedding(vocab_size, embedding_dim,</span><span id="e77a" class="lr jp hi li b fi lw lt l lu lv">batch_input_shape=[batch_size, None]),</span><span id="e93b" class="lr jp hi li b fi lw lt l lu lv">tf.keras.layers.LSTM(rnn_units,</span><span id="fff1" class="lr jp hi li b fi lw lt l lu lv">return_sequences=True,</span><span id="423f" class="lr jp hi li b fi lw lt l lu lv">stateful=True,</span><span id="f740" class="lr jp hi li b fi lw lt l lu lv">recurrent_initializer=’glorot_uniform’),</span><span id="e430" class="lr jp hi li b fi lw lt l lu lv">tf.keras.layers.Dense(vocab_size)</span><span id="e870" class="lr jp hi li b fi lw lt l lu lv">])</span><span id="1025" class="lr jp hi li b fi lw lt l lu lv">return model</span><span id="7fbc" class="lr jp hi li b fi lw lt l lu lv">model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)</span><span id="03dc" class="lr jp hi li b fi lw lt l lu lv">model.summary()</span></pre><h1 id="02e6" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">创建损失函数</h1><p id="4aec" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">现在我们要为这个问题创建我们自己的损失函数。这是因为我们的模型将输出(64，sequence_length，65)形状的张量，该张量表示批量中每个序列的每个时间步中每个字符的概率分布。</p><p id="db8b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，在我们这样做之前，让我们来看看一个样本输入和来自我们的未训练模型的输出。这是为了让我们了解模型给了我们什么。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="2f36" class="lr jp hi li b fi ls lt l lu lv">for input_example_batch, target_example_batch in data.take(1):</span><span id="e00c" class="lr jp hi li b fi lw lt l lu lv">example_batch_predictions = model(input_example_batch) # ask our model for a prediction on our first batch of training data (64 entries)</span><span id="ff22" class="lr jp hi li b fi lw lt l lu lv">print(example_batch_predictions.shape, “# (batch_size, sequence_length, vocab_size)”) # print out the output shape</span><span id="6fde" class="lr jp hi li b fi lw lt l lu lv"># we can see that the predicition is an array of 64 arrays, one for each entry in the batch</span><span id="6376" class="lr jp hi li b fi lw lt l lu lv">print(len(example_batch_predictions))</span><span id="68f1" class="lr jp hi li b fi lw lt l lu lv">print(example_batch_predictions)</span><span id="2ed8" class="lr jp hi li b fi lw lt l lu lv"># lets examine one prediction</span><span id="6094" class="lr jp hi li b fi lw lt l lu lv">pred = example_batch_predictions[0]</span><span id="d1fa" class="lr jp hi li b fi lw lt l lu lv">print(len(pred))</span><span id="ed79" class="lr jp hi li b fi lw lt l lu lv">print(pred)</span></pre><p id="8527" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">请注意，这是一个长度为100的2d数组，其中每个内部数组都是每个时间步长下一个字符的预测值</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="4dad" class="lr jp hi li b fi ls lt l lu lv"># and finally well look at a prediction at the first timestep</span><span id="6387" class="lr jp hi li b fi lw lt l lu lv">time_pred = pred[0]</span><span id="3edd" class="lr jp hi li b fi lw lt l lu lv">print(len(time_pred))</span><span id="47dd" class="lr jp hi li b fi lw lt l lu lv">print(time_pred)</span><span id="d4ed" class="lr jp hi li b fi lw lt l lu lv"># and of course its 65 values representing the probabillity of each character occuring next</span></pre><p id="1a8d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们想确定预测的字符，我们需要对输出分布进行采样(根据概率选择一个值)</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="b956" class="lr jp hi li b fi ls lt l lu lv">sampled_indices = tf.random.categorical(pred, num_samples=1)</span></pre><p id="5c7e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们可以改变数组的形状，将所有的整数转换成数字，以查看实际的字符</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="21c2" class="lr jp hi li b fi ls lt l lu lv">sampled_indices = np.reshape(sampled_indices, (1, -1))[0]</span><span id="3556" class="lr jp hi li b fi lw lt l lu lv">predicted_chars = int_to_text(sampled_indices)</span><span id="e9a4" class="lr jp hi li b fi lw lt l lu lv">predicted_chars # and this is what the model predicted for training sequence 1</span></pre><p id="a0d8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，现在我们需要创建一个损失函数，将该输出与预期输出进行比较，并给出一些数值来表示两者有多接近。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="9574" class="lr jp hi li b fi ls lt l lu lv">def loss(labels, logits):</span><span id="8bef" class="lr jp hi li b fi lw lt l lu lv">return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)</span></pre><h1 id="a434" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">编译模型</h1><p id="f4c9" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">在这一点上，我们可以认为我们的问题是一个分类问题，模型预测每个独特的字母出现的概率。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="93f0" class="lr jp hi li b fi ls lt l lu lv">model.compile(optimizer=’adam’, loss=loss)</span></pre><h1 id="498e" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">创建检查点</h1><p id="7d4f" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">现在，我们将设置和配置我们的模型，以便在训练时保存checkpoinst。这将允许我们从检查点加载我们的模型，并继续训练它。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="1a93" class="lr jp hi li b fi ls lt l lu lv"># Directory where the checkpoints will be saved</span><span id="ef38" class="lr jp hi li b fi lw lt l lu lv">checkpoint_dir = ‘./training_checkpoints’</span><span id="3246" class="lr jp hi li b fi lw lt l lu lv"># Name of the checkpoint files</span><span id="dc19" class="lr jp hi li b fi lw lt l lu lv">checkpoint_prefix = os.path.join(checkpoint_dir, “ckpt_{epoch}”)</span><span id="c2fb" class="lr jp hi li b fi lw lt l lu lv">checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(</span><span id="6d3b" class="lr jp hi li b fi lw lt l lu lv">filepath=checkpoint_prefix,</span><span id="67cc" class="lr jp hi li b fi lw lt l lu lv">save_weights_only=True)</span></pre><h1 id="7dc7" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">培养</h1><p id="f87c" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">最后，我们将开始训练模型。</p><p id="e5f4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">如果这需要一段时间，转到运行时&gt;更改运行时类型并选择硬件加速器下的“GPU”。</strong></p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="8d85" class="lr jp hi li b fi ls lt l lu lv">history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])</span></pre><h1 id="b715" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">加载模型</h1><p id="92b9" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">我们将使用batch _ size 1从一个检查点重建模型，这样我们可以向模型提供一部分文本，并让它进行预测。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="2137" class="lr jp hi li b fi ls lt l lu lv">model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)</span></pre><p id="c73c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦模型完成训练，我们可以使用下面的代码找到存储模型权重的<strong class="is hj">最后一个检查点</strong>。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="a20f" class="lr jp hi li b fi ls lt l lu lv">model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))</span><span id="6cfc" class="lr jp hi li b fi lw lt l lu lv">model.build(tf.TensorShape([1, None]))</span></pre><p id="cb57" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以通过指定要加载的确切文件来加载<strong class="is hj">任何我们想要的检查点</strong>。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="d0fd" class="lr jp hi li b fi ls lt l lu lv">checkpoint_num = 10</span><span id="27de" class="lr jp hi li b fi lw lt l lu lv">model.load_weights(tf.train.load_checkpoint(“./training_checkpoints/ckpt_” + str(checkpoint_num)))</span><span id="ca16" class="lr jp hi li b fi lw lt l lu lv">model.build(tf.TensorShape([1, None]))</span></pre><h1 id="3870" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">生成文本</h1><p id="ccf7" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">现在，我们可以使用tensorflow提供的可爱函数，使用我们喜欢的任何起始字符串来生成一些文本。</p><pre class="lj lk ll lm fd ln li lo lp aw lq bi"><span id="b213" class="lr jp hi li b fi ls lt l lu lv">def generate_text(model, start_string):</span><span id="9708" class="lr jp hi li b fi lw lt l lu lv"># Evaluation step (generating text using the learned model)</span><span id="7150" class="lr jp hi li b fi lw lt l lu lv"># Number of characters to generate</span><span id="bd78" class="lr jp hi li b fi lw lt l lu lv">num_generate = 800</span><span id="6004" class="lr jp hi li b fi lw lt l lu lv"># Converting our start string to numbers (vectorizing)</span><span id="ac49" class="lr jp hi li b fi lw lt l lu lv">input_eval = [char2idx[s] for s in start_string]</span><span id="222d" class="lr jp hi li b fi lw lt l lu lv">input_eval = tf.expand_dims(input_eval, 0)</span><span id="420f" class="lr jp hi li b fi lw lt l lu lv"># Empty string to store our results</span><span id="5de4" class="lr jp hi li b fi lw lt l lu lv">text_generated = []</span><span id="3fdf" class="lr jp hi li b fi lw lt l lu lv"># Low temperatures results in more predictable text.</span><span id="c721" class="lr jp hi li b fi lw lt l lu lv"># Higher temperatures results in more surprising text.</span><span id="47cc" class="lr jp hi li b fi lw lt l lu lv"># Experiment to find the best setting.</span><span id="d9ae" class="lr jp hi li b fi lw lt l lu lv">temperature = 1.0</span><span id="81ab" class="lr jp hi li b fi lw lt l lu lv"># Here batch size == 1</span><span id="0e3f" class="lr jp hi li b fi lw lt l lu lv">model.reset_states()</span><span id="9e65" class="lr jp hi li b fi lw lt l lu lv">for i in range(num_generate):</span><span id="527e" class="lr jp hi li b fi lw lt l lu lv">predictions = model(input_eval)</span><span id="2fc3" class="lr jp hi li b fi lw lt l lu lv"># remove the batch dimension</span><span id="f642" class="lr jp hi li b fi lw lt l lu lv">predictions = tf.squeeze(predictions, 0)</span><span id="d479" class="lr jp hi li b fi lw lt l lu lv"># using a categorical distribution to predict the character returned by the model</span><span id="e570" class="lr jp hi li b fi lw lt l lu lv">predictions = predictions / temperature</span><span id="5237" class="lr jp hi li b fi lw lt l lu lv">predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()</span><span id="a9ec" class="lr jp hi li b fi lw lt l lu lv"># We pass the predicted character as the next input to the model</span><span id="3a52" class="lr jp hi li b fi lw lt l lu lv"># along with the previous hidden state</span><span id="19b9" class="lr jp hi li b fi lw lt l lu lv">input_eval = tf.expand_dims([predicted_id], 0)</span><span id="157f" class="lr jp hi li b fi lw lt l lu lv">text_generated.append(idx2char[predicted_id])</span><span id="6cd1" class="lr jp hi li b fi lw lt l lu lv">return (start_string + ‘’.join(text_generated))</span><span id="02f7" class="lr jp hi li b fi lw lt l lu lv">inp = input(“Type a starting string: “)</span><span id="7847" class="lr jp hi li b fi lw lt l lu lv">print(generate_text(model, inp))</span></pre><p id="9757" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="lx">和</em>这就是本模块的全部内容！我强烈建议修改我们刚刚创建的模型，看看你能让它做些什么！</p><h1 id="d301" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">来源</h1><ul class=""><li id="c0bf" class="kr ks hi is b it km ix kn jb ma jf mb jj mc jn kw kx ky kz bi translated">乔莱·françois.用Python进行深度学习。曼宁出版公司，2018。</li><li id="8414" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">"文本分类与RNN:张量流核心."张量流，<a class="ae lz" href="http://www.tensorflow.org/tutorials/text/text_classification_rnn" rel="noopener ugc nofollow" target="_blank">www.tensorflow.org/tutorials/text/text_classification_rnn</a>。</li><li id="75dc" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">"文本生成与RNN:张量流核心."张量流，<a class="ae lz" href="http://www.tensorflow.org/tutorials/text/text_generation" rel="noopener ugc nofollow" target="_blank">www.tensorflow.org/tutorials/text/text_generation</a>。</li><li id="c5a7" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">“了解LSTM网络”了解LSTM网络——科拉的博客，<a class="ae lz" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>。</li></ul></div></div>    
</body>
</html>
<html>
<head>
<title>Building a Data Lake and Warehouse on GCP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在GCP建立数据湖和数据仓库</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/building-a-data-lake-and-warehouse-on-gcp-5d49a0f5a592?source=collection_archive---------4-----------------------#2022-06-19">https://medium.com/geekculture/building-a-data-lake-and-warehouse-on-gcp-5d49a0f5a592?source=collection_archive---------4-----------------------#2022-06-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="bd58" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">从理论到实践，主要考虑因素和GCP服务</h2></div><p id="7e7e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这篇文章技术上不会很深。我们将讨论数据湖和数据仓库，应该考虑的重要原则以及可以使用的GCP服务。我将指导你与GCP服务，可用于建设和为什么你可能会考虑他们。</p><p id="427b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在介绍我的数据湖和数据仓库版本之前，我想介绍几个众所周知的架构，如果您对这个主题感兴趣的话，可能已经看过了。我建议的架构会比那些更通用。</p><div class="jt ju ez fb jv jw"><a href="https://cloud.google.com/architecture/build-a-data-lake-on-gcp" rel="noopener  ugc nofollow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">云存储作为数据湖|云架构中心|谷歌云</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">本文讨论了如何在Google Cloud上使用数据湖。数据湖为像您这样的组织提供了…</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">cloud.google.com</p></div></div><div class="kf l"><div class="kg l kh ki kj kf kk kl jw"/></div></div></a></div><div class="jt ju ez fb jv jw"><a href="https://cloud.google.com/architecture/marketing-data-warehouse-on-gcp" rel="noopener  ugc nofollow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">架构:营销数据仓库|云架构中心|谷歌云</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">本文档提供了一个参考架构，描述了如何构建可扩展的营销数据仓库…</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">cloud.google.com</p></div></div><div class="kf l"><div class="km l kh ki kj kf kk kl jw"/></div></div></a></div><p id="ef16" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我的数据湖和数据仓库版本中，我将讨论数据传输服务、Dataproc、云存储、云调度器、BigQuery、云SQL等GCP服务。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es kn"><img src="../Images/ecaa8417b06f65247fece8970cddeb47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ii3pjya7s9rR-HJpZWhZCA.png"/></div></div></figure><p id="0885" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将收集“难题”,看看为什么以及如何使用服务。</p><h1 id="e32e" class="ky kz hi bd la lb lc ld le lf lg lh li io lj ip lk ir ll is lm iu ln iv lo lp bi translated">主要考虑因素</h1><p id="bce0" class="pw-post-body-paragraph ix iy hi iz b ja lq ij jc jd lr im jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">在设计自己的数据湖和数据仓库时，有许多重要的问题需要考虑。当BigQuery几乎完全覆盖了数据仓库的特性时，数据湖的特性需要更仔细的分析和实现方法。所有这些“热门词汇”，如容错、可靠、可伸缩，也应该应用于数据湖。您希望数据到达存储时具有良好的质量，您希望快速处理，轻松集成新的数据源，等等。所有这些要点可分为三大类问题，它们涉及该系统的具体领域。我只想简短地提及它们，因为讨论它们不是本文的目的。</p><ol class=""><li id="6769" class="lv lw hi iz b ja jb jd je jg lx jk ly jo lz js ma mb mc md bi translated"><strong class="iz hj">数据传输。</strong>该组包括关于数据传输本身及其存储的要点。原始数据应该从你的主系统传输到数据湖，没有丢失的风险。它应该“按原样”存储。当传入数据发生意外变化时，数据湖应该不会失败。在传入数据量意外增加的情况下，系统仍应能够快速适应。数据源的新集成应该是几个小时(甚至几分钟)的事情，而不是几天。在停机和故障的情况下，不应丢失任何东西，系统应自动恢复。</li><li id="58e8" class="lv lw hi iz b ja me jd mf jg mg jk mh jo mi js ma mb mc md bi translated"><strong class="iz hj">数据组织。</strong>在这组问题中，您需要回答如何在Data Lake中存储和查询您的数据，这有其自身的挑战。传入的数据结构是随时间变化的主题。数据到达数据湖的时间和与数据对象相关的业务时间之间可能没有关联。相同的数据可能来自多个供应商。您需要决定如何对原始数据进行分区，以便经济高效地使用它们。您可能还需要考虑不同角色和用户的数据访问权限和安全性，尽管在这个级别上可以跳过这一步。</li><li id="6663" class="lv lw hi iz b ja me jd mf jg mg jk mh jo mi js ma mb mc md bi translated"><strong class="iz hj">数据处理。</strong>最后一组与数据处理技术有关。首先，您希望保持数据处理足够简单并且不依赖于工具，以便您的组织可以使用最好的工具来实现目标。当您知道数据结构时，编写新的处理逻辑应该像编写新的SQL脚本一样简单。处理逻辑应该同样能够处理小容量和大容量的数据。新的数据处理管道的构建应该是快速的，并且部署应该是容易的。创建实时或批处理、按需处理或那些持续运行的处理应该同样简单。</li></ol><p id="ec3a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这些组中问题的答案在很大程度上取决于您的业务需求和先决条件。在我和我的团队设计数据湖和数据仓库的过程中，我们讨论了很多问题。下面，我想在几个具体的问题上停下来，我们花了大部分时间来选择正确的方法。</p><h2 id="8517" class="mj kz hi bd la mk ml mm le mn mo mp li jg mq mr lk jk ms mt lm jo mu mv lo mw bi translated">在云存储和BigQuery之间选择数据湖</h2><p id="c08e" class="pw-post-body-paragraph ix iy hi iz b ja lq ij jc jd lr im jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">第一个问题是关于将BigQuery和云存储用于数据湖。如果您的数据具有非常稳定的结构，并且其模式随时间变化非常缓慢，并且您可以认为它在很长一段时间内保持不变，那么BigQuery可能是您的一个好选择。您可以拥有从云存储中读取文件的外部表，并且可以像使用其他表一样使用这些表。您可以将这些数据自动导入到数据仓库中的数据产品表中。在极少数情况下，当您的数据改变其结构时，您可以用半手工的过程来覆盖它，并且不会有同时支持各种模式的问题。您可以在BigQuery中使用SQL和预定查询构建数据处理，并让DBA非常轻松地掌握所需的技能。</p><p id="f61d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在数据结构变化更频繁的情况下，支持这种利用BigQuery的数据湖的工作可能会呈指数增长，并且变得太高和太复杂，以至于无法保持业务的成本效益。在这种情况下，云存储及其相关流程可能是更好的解决方案。这是我们为自己选择的，因为在我们的情况下，数据模式会随着记录的不同而变化，我们有多个数据管道和供应商。</p><div class="jt ju ez fb jv jw"><a href="https://cloud.google.com/learn/what-is-a-data-lake" rel="noopener  ugc nofollow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">什么是数据湖？|谷歌云</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">数据湖是一个集中的存储库，旨在存储、处理和保护大量结构化…</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">cloud.google.com</p></div></div><div class="kf l"><div class="mx l kh ki kj kf kk kl jw"/></div></div></a></div><h2 id="e7a8" class="mj kz hi bd la mk ml mm le mn mo mp li jg mq mr lk jk ms mt lm jo mu mv lo mw bi translated">选择原始数据格式:Parquet、Avro或JSON</h2><p id="30e9" class="pw-post-body-paragraph ix iy hi iz b ja lq ij jc jd lr im jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">数据格式之间的选择与数据湖的BigQuery和云存储之间的选择具有相同的基础。拼花和Avro文件格式要求在文件中嵌入特定的结构，以支持它们提供的功能。例如，Parquet对于查询非常有效，因为它是一种列式数据存储。它还提供了更好的压缩。但是它的写入开销更大，因为它需要更多的CPU和RAM来运行。对于写操作应该很快的情况，Avro看起来是更好的选择。您可以逐行追加到Avro文件，但不能对拼花文件这样做。这使得Avro在以流方式处理数据的场景中更受欢迎。我发现了一个非常好的解释这些格式的细节。</p><div class="jt ju ez fb jv jw"><a href="https://www.clairvoyant.ai/blog/big-data-file-formats" rel="noopener  ugc nofollow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">大数据文件格式</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">在这篇博客中，我将讨论什么是文件格式，通过一些常见的Hadoop文件格式的特点，并给出一个…</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">www.clairvoyant.ai</p></div></div></div></a></div><div class="jt ju ez fb jv jw"><a href="https://www.adaltas.com/en/2021/03/22/performance-comparison-of-file-formats/" rel="noopener  ugc nofollow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">流行文件格式的存储大小和生成时间</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">无论您的数据是通过网络传输还是静态存储，选择合适的文件格式都是至关重要的。每个…</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">www.adaltas.com</p></div></div><div class="kf l"><div class="my l kh ki kj kf kk kl jw"/></div></div></a></div><p id="8329" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，如果您的模式发展不快，并且您可以保证所有记录在一个文件中具有相同的结构，那么Avro或Parquet是非常好的选择。但是当结构频繁改变，并且需要用不同的模式保存相同的文件记录时，这就变得很有挑战性。</p><p id="e23e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Parquet和Avro要求将模式嵌入到文件中。如果您的数据记录的模式对于几乎每个记录都发生了变化，那么您将需要修补每个记录，以将它们调整到某个“目标”模式，或者创建许多具有稍微不同的模式的文件。Avro声明它支持添加和删除列的模式进化，但我们的研究表明，它仍然要求所有记录在同一个文件中具有相同的结构。这是有可能解决的，但是增加了系统的复杂性，并且增加了支持这种系统的额外故障点和开销。</p><p id="c842" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">相比之下，JSON格式没有前面提到的问题，并且被许多处理框架很好地支持。您可能会想到的唯一问题是包含JSON数据的文本文件的大小。Spark允许您读取不同模式的文件。当您指定某个预期的模式时，它会正确地加载它，即使记录具有明显不同的模式。您还可以将不同模式的记录存储到相同的文件中。最终，经过几次实验，我们选定了JSON。</p><h2 id="bdb4" class="mj kz hi bd la mk ml mm le mn mo mp li jg mq mr lk jk ms mt lm jo mu mv lo mw bi translated">为数据湖选择正确的数据组织</h2><p id="586c" class="pw-post-body-paragraph ix iy hi iz b ja lq ij jc jd lr im jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">数据组织的主要目的是在数据湖中提供统一的结构，并帮助执行有效的数据查询。如何存储数据，如何组织数据，如何分区，所有这些都会影响查询的性能。一个很好的关于这个的指南作为链接附在下面。主要思想是摄取日期应该是数据湖第一层的数据组织的一部分。</p><div class="jt ju ez fb jv jw"><a href="https://techcommunity.microsoft.com/t5/data-architecture-blog/how-to-organize-your-data-lake/ba-p/1182562" rel="noopener  ugc nofollow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">如何组织你的数据湖</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">数据湖是大数据革命的最佳成果之一，能够为各种类型的数据提供廉价可靠的存储</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">techcommunity.microsoft.com</p></div></div><div class="kf l"><div class="mz l kh ki kj kf kk kl jw"/></div></div></a></div><p id="a238" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第二层数据湖可能更适合业务需求。这将提供从第一层重建第二层的可能性，以防您决定重新组织准备好的数据。</p><p id="5435" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">务必将数据湖的第一层与数据结构隔离开来，不要将其与您的业务联系起来。这将使新供应商的集成和对数据湖中数据的进一步操作变得更加容易。</p><h1 id="6e99" class="ky kz hi bd la lb lc ld le lf lg lh li io lj ip lk ir ll is lm iu ln iv lo lp bi translated">数据湖和数据仓库的GCP服务</h1><p id="9933" class="pw-post-body-paragraph ix iy hi iz b ja lq ij jc jd lr im jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">现在我想谈谈可能的数据湖和数据仓库的构建模块。所有组件都是GCP服务，完全覆盖特定领域的需求:数据传输、处理和查询。我们不会配置任何服务，但我会概述它们的主要优势，以及它们为什么适合您的数据湖和数据仓库。你可能会找到我在文章开头谈到的系统的模式。</p><h2 id="a58d" class="mj kz hi bd la mk ml mm le mn mo mp li jg mq mr lk jk ms mt lm jo mu mv lo mw bi translated">数据传输服务</h2><p id="6ecb" class="pw-post-body-paragraph ix iy hi iz b ja lq ij jc jd lr im jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">为了将数据接收到您的数据湖中，您需要从源系统建立可靠的数据传输。数据传输服务有助于将数据从AWS、Azure、云存储或内部存储传输到另一个内部存储或云存储。该服务传输文件，验证传输，并可以在中断时恢复。可以扩展内部部署的代理，以提高数据传输的性能。</p><p id="df3c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们的案例中，我们将数据从内部传输到云存储。为了进行配置，应该在数据传输中创建代理池。所有代理都通过PubSub服务相互通信，以便在基础设施出现故障时协调发送哪些文件以及从哪里恢复。例如，如果一个代理死亡或连接中断，其他仍能工作的代理将继续传输。最后，所有代理协同工作，使数据传输容错，不跳过任何文件。</p><div class="jt ju ez fb jv jw"><a href="https://cloud.google.com/storage-transfer/docs/managing-on-prem-agents" rel="noopener  ugc nofollow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">管理传输代理|云存储传输服务文档| Google Cloud</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">存储转移服务代理是在Docker容器中运行的应用程序，它与存储…</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">cloud.google.com</p></div></div><div class="kf l"><div class="na l kh ki kj kf kk kl jw"/></div></div></a></div><h2 id="d0e3" class="mj kz hi bd la mk ml mm le mn mo mp li jg mq mr lk jk ms mt lm jo mu mv lo mw bi translated">Dataproc</h2><p id="ec6d" class="pw-post-body-paragraph ix iy hi iz b ja lq ij jc jd lr im jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">Dataproc是一个托管的Apache Spark和Apache Hadoop服务。它是原始数据文件ETL过程的核心。可以配置自动缩放，默认情况下使用可抢占的辅助工作节点，并基于纱线度量。可以创建一个包含1个主节点和2个工作节点的最小集群，并在自动缩放器的最大辅助工作节点中设置一些高值。主工作节点和辅助工作节点的区别在于，后者不能在其上存储数据。也可以在无服务器模式下使用Dataproc，提交批处理作业。我们经常每三分钟运行几个ETL。因此，我们的Dataproc集群一直在运行。</p><div class="jt ju ez fb jv jw"><a href="https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling" rel="noopener  ugc nofollow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">自动扩展集群| Dataproc文档| Google云</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">目标:了解如何配置和使用Dataproc自动缩放来自动和动态地调整工作虚拟机的大小…</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">cloud.google.com</p></div></div><div class="kf l"><div class="nb l kh ki kj kf kk kl jw"/></div></div></a></div><p id="f0b5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">文件路径中的通配符</strong></p><p id="b487" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Spark非常有趣的特性是它在加载文件的路径中使用通配符的方式。例如，这是可以指定为<code class="du nc nd ne nf b">spark.read.json</code>的源的内容。假设您的数据湖中有以下文件夹组织:<code class="du nc nd ne nf b">gs://object/year/month/day/hour/minute/instance.json</code>。几乎所有服务都支持在字符串末尾加一个星号的规范，比如:<code class="du nc nd ne nf b">gs://object/year/month/day/hour/*</code> —如果您想在某一天的某个特定时间加载数据。但是并不是所有人都支持这个功能:<code class="du nc nd ne nf b">gs://object/year/month/*/*/minute/*</code> —如果您想只加载整个月中某个特定分钟的数据。你的文件夹组织可能会比这个复杂一点。尤其是在数据湖的第二层，您将使用准备好的数据文件，其中存储组织可能包含业务信息。例如，您可能希望全年只加载某个客户的数据。</p><p id="ed62" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> SparkSQL </strong></p><p id="0f94" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">SparkSQL允许使用众所周知的DSL对数据进行操作。有许多内置函数可以让学习曲线变得非常快，尤其是对DBA而言。</p><div class="jt ju ez fb jv jw"><a href="https://spark.apache.org/docs/latest/api/sql/index.html" rel="noopener  ugc nofollow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">内置函数</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">逻辑非。示例: &gt;选择！真实；false &gt;选择！假的；true &gt;选择！NULL因为:1.0.0 expr1！=…</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">spark.apache.org</p></div></div></div></a></div><p id="f552" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您的所有操作都可以在<code class="du nc nd ne nf b">spark.sql</code>操作员内执行:</p><pre class="ko kp kq kr fd ng nf nh ni aw nj bi"><span id="b412" class="mj kz hi nf b fi nk nl l nm nn">spark.sql("""<br/>CREATE OR REPLACE TEMPORARY VIEW my_view <br/>AS<br/>   SELECT<br/>      CAST(field AS FLOAT) AS mapped_field<br/>   FROM another_view<br/>""")</span></pre><h2 id="d164" class="mj kz hi bd la mk ml mm le mn mo mp li jg mq mr lk jk ms mt lm jo mu mv lo mw bi translated">云调度程序</h2><p id="547b" class="pw-post-body-paragraph ix iy hi iz b ja lq ij jc jd lr im jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">Cloud Scheduler服务允许您使用定义的时间表触发一些URL或向发布/订阅主题发送消息。我们使用它通过URL触发数据传输任务和Dataproc工作流。调度程序使用服务的GCP REST API来启动操作。数据传输作业不支持每小时一次以上的计划。Dataproc工作流或作业根本没有时间表。可以使用Cloud Composer或云函数来触发Dataproc或Transfer Service上的操作，但是这会增加系统的复杂性，引入更多的组件和更多的失败位置。</p><h2 id="53ea" class="mj kz hi bd la mk ml mm le mn mo mp li jg mq mr lk jk ms mt lm jo mu mv lo mw bi translated">云SQL</h2><p id="3c74" class="pw-post-body-paragraph ix iy hi iz b ja lq ij jc jd lr im jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">这个组件没有出现在我们的数据湖和数据仓库的模式中，但是我们正在使用它。它的主要目的是操作的可追溯性，并确保所有文件在到达数据湖时都得到处理。有一些云函数会在新文件到达存储桶时触发，并将文件添加到云SQL上的元数据数据库中。为了克服瞬时错误，对函数启用重试。启用重试后，该函数将在接下来的7天内尝试添加一个文件，并最终保证每个文件都将被添加到数据库中，而不考虑暂时的错误。我们使用PostgreSQL。</p><h2 id="2920" class="mj kz hi bd la mk ml mm le mn mo mp li jg mq mr lk jk ms mt lm jo mu mv lo mw bi translated">其余的，但不是最不重要的</h2><p id="a2f2" class="pw-post-body-paragraph ix iy hi iz b ja lq ij jc jd lr im jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">GCP其余的服务不需要特别的保障。我们将云存储用于我们的数据湖，将BigQuery用于数据仓库。</p><p id="71e8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当您在云存储上组织数据湖时，为数据对象配置生命周期是值得的。在同一个桶中有不同类的对象是可能的。生命周期规则有助于根据对象的年龄将其类别从标准更改为近线等。即使数据存储的成本很低，但当您处理大量数据和读取操作时，这将产生明显的代价。管理对象类会降低一些成本。</p><p id="51c8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在BigQuery中，您可能希望同一个数据产品有多个分区。例如，相同的订单历史可以按客户和位置进行划分。BigQuery表不支持多个分区。我们通过拥有多个包含相同数据但分区不同的表来克服这个问题。属于同一个数据产品的一组表是由同一个ETL生成的，因此支持多个表并不困难。</p><h1 id="519a" class="ky kz hi bd la lb lc ld le lf lg lh li io lj ip lk ir ll is lm iu ln iv lo lp bi translated">编后记</h1><p id="40b6" class="pw-post-body-paragraph ix iy hi iz b ja lq ij jc jd lr im jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">从许多角度来看，数据湖和数据仓库的设计是一个有趣的过程:技术架构和实现、与来自不同业务领域的人的协作，以及学习系统的高级用法，这些在以前被用作“黑盒”。在以后的文章中，如果我能找到这些任务之间的时间，我可能会详细描述数据传输服务的使用、Dataproc工作流的触发以及其他。</p></div></div>    
</body>
</html>
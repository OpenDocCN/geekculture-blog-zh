<html>
<head>
<title>An Overview Of Encoder Transformers — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">编码器变压器概述(二)</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/an-overview-of-encoder-transformers-part-2-3a533620f6af?source=collection_archive---------19-----------------------#2022-04-10">https://medium.com/geekculture/an-overview-of-encoder-transformers-part-2-3a533620f6af?source=collection_archive---------19-----------------------#2022-04-10</a></blockquote><div><div class="ds hc hd he hf hg"/><div class="hh hi hj hk hl"><div class=""/><figure class="ev ex im in io ip er es paragraph-image"><div role="button" tabindex="0" class="iq ir di is bf it"><div class="er es il"><img src="../Images/3f2ef1055cfd06d897154899c7b19768.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*atXk9u8jG9iDeDRJGgdDPA.jpeg"/></div></div></figure><p id="9bec" class="pw-post-body-paragraph iw ix ho iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hh bi translated">本文将探讨构建编码器模型所需的其余组件，这是一个通常用于文本分类或命名实体识别等任务的转换器模型。</p><p id="33ed" class="pw-post-body-paragraph iw ix ho iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hh bi translated">如果你还记得以前的帖子，我们大概了解了注意力分数是如何计算的，以及如何构建多头注意力层。因此，我们遗漏的其余子层…</p></div></div>    
</body>
</html>
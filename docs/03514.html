<html>
<head>
<title>Approaches to Biomedical Text Mining with BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于BERT的生物医学文本挖掘方法</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/approaches-to-biomedical-text-mining-dca3408397b0?source=collection_archive---------24-----------------------#2021-06-09">https://medium.com/geekculture/approaches-to-biomedical-text-mining-dca3408397b0?source=collection_archive---------24-----------------------#2021-06-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="a5b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我强调了降低进入生物医学文本处理的门槛和加快这个影响人类的极其重要的领域的进展的方法。</p><h1 id="08f3" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">聚焦挑战</h1><p id="a31b" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">医学文档已经增长到这样的程度，即搜索引擎和生物医学研究文章的存储库PubMed每天增加4，000篇新论文，每年超过100万篇。用于挖掘这种数据雪崩的有监督的深度学习方法一直无法跟上，主要是因为医疗领域中标记训练数据的缺乏。</p><p id="a6e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">强调快速理解医学文本的需要的另一个数据点是疫情六个月内发表的30，000篇新冠肺炎文章(见下文)。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kg"><img src="../Images/726f5d7da228bb88186a76ef3278640f.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*dGr5IIptqBacQj5xHiraBQ.png"/></div></figure><h1 id="df83" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">使用BERT的统一解决方案</h1><p id="16ca" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">BERT(来自变压器的双向编码器表示)是一种基于神经网络的语言模型，它为利用来自未标记文本的训练和转移学习(来自有限小的已标记文本)提供了统一的基础，以支持广泛的文本应用，如每个单词的实体标记、回答问题和提取关系。</p><h1 id="b191" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">BERT的优势</h1><p id="a006" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">BERT包括预训练步骤和微调步骤。</p><h2 id="cad8" class="ko je hi bd jf kp kq kr jj ks kt ku jn iq kv kw jr iu kx ky jv iy kz la jz lb bi translated">(1)预训练步骤建立通用语言模型。</h2><p id="594d" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">BERT通过从左到右和从右到左的方式(双向)学习单词的上下文来建立语言模型。简而言之，它在训练过程中的工作方式是，所有单词的15%在序列中被随机屏蔽(屏蔽)。训练过程使用标准的误差反向传播方法来预测被屏蔽的单词。BERT还通过预测前一个句子之后的下一个句子是实际的<em class="lc">下一个</em>句子还是随机的句子来学习两个句子之间的关系。具体来说，当为每个训练样本选择两个句子A和B时，50%的情况下，B是A后面的实际下一个句子，50%的情况下，它是文本中的随机句子。注意，两种情况下都不需要昂贵的标签。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ld"><img src="../Images/c4e80f79c2e0da05782f32626dd2aef1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*3lTK04XMlXaKfv6LhaplwA.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx">BERT learns the masked word (“sun”) and Sentence B follows Sentence A</figcaption></figure><h2 id="4dd5" class="ko je hi bd jf kp kq kr jj ks kt ku jn iq kv kw jr iu kx ky jv iy kz la jz lb bi translated">(2)在用未标记数据进行预训练之后，用标记数据进行微调的步骤。</h2><p id="96a4" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">我强调下面几个微调任务中的一个，回答问题。微调受监督的下游任务的优势在于，只需用相对较小的带标签数据集学习几个附加参数。</p><p id="1baa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">伯特(在班上微调)学习两个额外的向量，标记答案跨度的开始和结束。它们是区分答案(来自段落)的开始标记分类器和结束标记分类器。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es li"><img src="../Images/ffc5ee0ff78d64a9bfb9d6eab7af375b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*CPV7slMJtSKJOhC7zV4T-w.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx">BERT fine-tuning on SQuAD learns the start and end vector that spans the answer</figcaption></figure><h2 id="e1e0" class="ko je hi bd jf kp kq kr jj ks kt ku jn iq kv kw jr iu kx ky jv iy kz la jz lb bi translated">在生物医学领域的应用</h2><p id="07c2" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">一般文本领域(维基百科和大量书籍)中的单词分布和上下文与生物医学领域(PubMed)非常不同，因此需要进行微调或调整(然而，架构保持不变)。医学文献中医学术语占优势；生物医学研究者容易理解的专有名词(例如，BRCA1，c.248T&gt;C)和术语(例如，转录、抗微生物)。</p><h1 id="a5c2" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">预训练生物机器人</h1><p id="875b" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">步骤1，用来自BERT(迁移学习)的权重初始化BioBERT。</p><p id="2cca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二步，对生物医学领域文本(PubMed文摘和PebMed中心全文文章)进行预训练。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lj"><img src="../Images/39bb4e42737ad0e25b199bd6fbdbf842.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*v-L-b1_NQdKyTblVdQH12g.png"/></div></figure><h1 id="50e4" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">针对特定任务微调BioBert</h1><p id="5409" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">为了显示这种方法在生物医学文本挖掘中的有效性，BioBERT在三个流行的生物医学文本挖掘任务(命名实体识别、关系提取和问题回答)上进行了微调和评估。我们展示了问答系统的高层架构。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ld"><img src="../Images/6321b7818f30d57214d5e99f2cd1d989.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*4-bAa1p4woUS2le2Y0FbbQ.png"/></div></figure><h1 id="7aaf" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">比较结果</h1><p id="fae9" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">请注意，BioBERT的所有变体在医学问题/答案上的表现都优于领域不可知的BERT，准确率提高了20%至30%。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lk"><img src="../Images/bbbfda98125c6c068d1824c5417cce6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*h9o6cd7q_r7oLBtKEoJOCw.png"/></div></figure><h1 id="aec7" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">导语，第二种方法</h1><p id="431a" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">微软的研究人员观察到，生物医学文本明显不同于普通领域的文本(维基百科和书籍),在普通领域的文本中，生物医学术语少之又少。以前补偿缺失术语的方法，如将它们分成子词，没有什么意义，例如，淋巴瘤被分成四个子词(l、#ym、#ph或##oma)。相比之下，将生物医学术语视为“一等公民”可以通过避免不相关的域外文本来节省建模和计算带宽(特别是，因为特定于域的数据集同样大，不需要标记)。</p><p id="115a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面，我展示了(1)传统的两步法(2)和直接法。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ll"><img src="../Images/94915fdc3f297c9ac68cfa36a3482b93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*4mNeysfdco7IXmbwOelxmA.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx">The conventional two-step approach</figcaption></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lm"><img src="../Images/81f15e06c35ab678953276b6348c1936.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*ZKB4c9dDr-bWmfoJ_k3oiw.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx">The direct approach</figcaption></figure><p id="9376" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了进一步验证这种方法，微软研究院为基于PubMed的生物医学文本应用程序创建了BLURB基准，<a class="ae ln" href="https://microsoft.github.io/BLURB/" rel="noopener ugc nofollow" target="_blank">生物医学语言理解和推理基准</a>。BLURB由六个不同微调任务中的13个公开可用数据集组成，即:</p><ul class=""><li id="15c4" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc lt lu lv lw bi translated">命名实体识别，</li><li id="a20f" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">循证医学信息提取，</li><li id="805a" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">关系提取，</li><li id="d672" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">句子相似度，</li><li id="3ffc" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">文件分类，以及</li><li id="0b34" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">问题回答</li></ul><p id="4c01" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BLURB的主要目标是降低进入生物医学文本处理的门槛(跳过与领域无关的处理步骤)。</p><h1 id="d0c7" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">结论</h1><p id="6ac9" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">BERT对于文本处理任务就像ResNet对于计算机视觉任务一样。</p><p id="43a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为主干子网，ResNet加快了目标检测和场景分割的创新，因为它从图像中创建了一流的学习特征。ResNet的关键创新在于它能够通过创建一个跳过连接来学习剩余函数。</p><p id="8efc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，BERT的语言建模和随后的向下游任务的转移，在特定领域的文本应用中也走上了类似的快速创新之路。BERT的自我注意层(也包括跳过连接)和自我监督的无标签学习方法构成了文本处理的主干。</p><h1 id="7a86" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">参考</h1><p id="7686" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated"><a class="ae ln" href="https://www.nature.com/articles/d41586-020-01733-7" rel="noopener ugc nofollow" target="_blank"> <em class="lc">人工智能工具旨在驯服冠状病毒</em> </a></p><p id="0861" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ln" href="http://bioasq.org/" rel="noopener ugc nofollow" target="_blank"> <em class="lc"> BioASQ，组织生物医学语义标引和问答挑战赛。</em> </a></p><p id="9988" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ln" href="https://www.microsoft.com/en-us/research/blog/domain-specific-language-model-pretraining-for-biomedical-natural-language-processing/" rel="noopener ugc nofollow" target="_blank"> <em class="lc">微软研究博客:面向生物医学自然语言处理的特定领域语言模型预处理</em> </a></p></div></div>    
</body>
</html>
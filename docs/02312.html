<html>
<head>
<title>Linear Regression Using Gradient Descent: Intuition and Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用梯度下降的线性回归:直觉和实现</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/linear-regression-using-gradient-descent-intuition-and-implementation-522d43453fc3?source=collection_archive---------16-----------------------#2021-05-10">https://medium.com/geekculture/linear-regression-using-gradient-descent-intuition-and-implementation-522d43453fc3?source=collection_archive---------16-----------------------#2021-05-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/2641e20e2e866c50c870b6160564afa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KX-o4mtq30YM_5TD"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx">Photo by <a class="ae hv" href="https://unsplash.com/@isaacmsmith?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Isaac Smith</a> on <a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="58cc" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是我正在撰写的系列文章中的第二篇，其中我们将讨论和定义介绍性的机器学习概念。如果你没有读过我的第一篇关于数据预处理的文章，一定要去看看。</p><p id="4512" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我们将讨论线性回归，以及在处理带标签的数据时如何使用线性回归进行预测。</p><p id="5ff0" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们开始吧。</p><h1 id="6bd8" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">个案研究</h1><p id="cb93" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">理解线性回归概念的最好方法是通过一个例子。因此，在阅读本文的其余部分时，想象自己处于以下场景中:</p><blockquote class="kw kx ky"><p id="ffd8" class="iv iw kz ix b iy iz ja jb jc jd je jf la jh ji jj lb jl jm jn lc jp jq jr js hb bi translated">你是住在波士顿的数据科学家。你的朋友找到你，告诉你他很难卖掉他的房子，他在市场上标价50万美元。你问他房子的细节，从他的描述中注意到他对房产的估价太高了。因此，您希望通过编写一个算法来帮助您的朋友，该算法将查看当前的房地产市场，并预测他的房子可以卖多少钱。</p></blockquote><h1 id="8b83" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">几个定义</h1><p id="4f8b" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在描述线性回归之前，我们必须了解一些基本概念:</p><ol class=""><li id="064c" class="ld le hy ix b iy iz jc jd jg lf jk lg jo lh js li lj lk ll bi translated"><strong class="ix hz">因变量vs自变量:</strong><strong class="ix hz"/>根据其他变量的变化而变化<em class="kz">的变量，称为因变量。在我们的住房场景中，因变量的一个例子是房子的价格，它根据房子的大小、卧室的数量等而变化。尺寸、卧室数量和浴室数量都是独立变量的例子。这些变量的行为不依赖于任何其他变量。</em></li><li id="91fd" class="ld le hy ix b iy lm jc ln jg lo jk lp jo lq js li lj lk ll bi translated"><strong class="ix hz">监督与非监督学习算法:</strong>监督学习算法是一种从预先存在的、<em class="kz">标记为</em>的数据中学习的算法，以便理解其行为并对未来数据进行预测。正如我们将看到的，回归算法是<em class="kz">监督</em>学习算法。另一方面，无监督学习算法是一种分析<em class="kz">未标记</em>数据集的算法，它自己学习并将不同的数据组分成具有共同属性的数据点集。如果你想了解更多的区别，这里有一篇关于<a class="ae hv" href="https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d" rel="noopener" target="_blank">监督和非监督学习</a>的文章。</li><li id="c654" class="ld le hy ix b iy lm jc ln jg lo jk lp jo lq js li lj lk ll bi translated"><strong class="ix hz">回归vs分类:</strong>回归是一种统计方法，利用一个或多个自变量<strong class="ix hz"> </strong>来预测因变量<strong class="ix hz"> </strong>的值。在回归中，我们的目标是找到一条最能描述我们的数据的直线，试图在以后预测新的、即将到来的数据。预测房价是回归问题的一个例子。在分类中，目标是根据属性将数据分类到离散的组中。例如，我们可以将一个有新冠肺炎症状的人分为携带者或非携带者。在这里，我们将患者分为两个不同的组。</li></ol><h1 id="add3" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">线性回归</h1><p id="7a20" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">线性回归预测一个<strong class="ix hz">连续</strong>因变量<strong class="ix hz"> </strong>的值。这里的关键术语是连续。在线性回归中，我们不希望将数据分成离散的不同组。相反，我们的预测理论上可以取任何实数。当因变量和自变量之间的关系是线性时，我们使用这种回归变量。</p><h2 id="7027" class="lr ju hy bd jv ls lt lu jz lv lw lx kd jg ly lz kh jk ma mb kl jo mc md kp me bi translated">直觉</h2><p id="1605" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">我们都见过直线的方程，对吧？</p><figure class="mg mh mi mj fd hk er es paragraph-image"><div class="er es mf"><img src="../Images/39f29aa1ac1c5dc1e834f6b956d1c9ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/1*UYdMy00266jxo2jAiTKrQQ.png"/></div></figure><p id="5e3e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该等式通过定义一条线，将<code class="du mk ml mm mn b">m</code>作为其<em class="kz"> </em>斜率，将<code class="du mk ml mm mn b">b</code>作为其<em class="kz"> </em> y轴截距，即该线与y轴相交的位置，给出了<code class="du mk ml mm mn b">y</code> <em class="kz">、</em>的值及其各自的<code class="du mk ml mm mn b">x</code> <em class="kz">、</em>的值。根据<code class="du mk ml mm mn b">m</code>和<code class="du mk ml mm mn b">b</code><em class="kz"/>的值，我们的线会发生变化。</p><p id="d31c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了后面会变得更清楚的目的，我们可以用下面的方式来表达直线的方程:</p><figure class="mg mh mi mj fd hk er es paragraph-image"><div class="er es mo"><img src="../Images/53e11007c05a4422d69059b4fc71b70e.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*bRA_RnnP7v5O-hUdihzqVQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 1: </strong>Univariate Linear Regression</figcaption></figure><p id="c7c6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中<code class="du mk ml mm mn b">Theta_1</code>和<code class="du mk ml mm mn b">Theta_0</code>分别对应<code class="du mk ml mm mn b">m</code>和<code class="du mk ml mm mn b">b</code>,<code class="du mk ml mm mn b">h</code>对应<code class="du mk ml mm mn b">y</code> <em class="kz">。我们也可以改变我们解释<code class="du mk ml mm mn b">x</code>和<code class="du mk ml mm mn b">h</code>T42的方式。</em>不要将<code class="du mk ml mm mn b">x</code>视为任意实数，而是将其视为影响<code class="du mk ml mm mn b">h</code>的描述性特征。例如，如果<code class="du mk ml mm mn b">h</code>是价格，那么<code class="du mk ml mm mn b">x</code>可能是房子的房间数。下图显示了改变<code class="du mk ml mm mn b">Theta_0</code>和<code class="du mk ml mm mn b">Theta_1</code>的效果</p><figure class="mg mh mi mj fd hk er es paragraph-image"><div class="er es mp"><img src="../Images/ec6764621f292f4cace7f070c44971ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*6Uw0dQ7ZXAL_3ZGydyEBiA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 1: </strong>m=0.8 and b=2</figcaption></figure><figure class="mg mh mi mj fd hk er es paragraph-image"><div class="er es mq"><img src="../Images/1e0f038d1cc3a55d3b6080f6c2d89441.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*Eu7Vi4NiRC8Z9cLJqDIuag.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 2: </strong>m=-0.8 and b=2</figcaption></figure><figure class="mg mh mi mj fd hk er es paragraph-image"><div class="er es mr"><img src="../Images/d4254bfe09fe93ce8dc3220c06a5292c.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*BHEJr5WEvv6Z5672l6KHvQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 3: </strong>m=2 and b=0</figcaption></figure><figure class="mg mh mi mj fd hk er es paragraph-image"><div class="er es mr"><img src="../Images/82964544fe3d1b204db337efd3765ff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*RKAAdIYL2wTrp8vZoxQB7g.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 4: </strong>m=1 and b=2</figcaption></figure><p id="0def" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">哪条线最好地描述了我们的点的行为？目测可以看出，要么是<strong class="ix hz">图1 </strong>画的，要么是<strong class="ix hz">图4 </strong>画的。有了这个，我们可以开始使用我们的图表进行预测。例如，<strong class="ix hz">图4 </strong>告诉我们，一栋有八个房间的房子大约要花费10万美元。</p><p id="a9e3" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是我们的模特不懂视觉。那么我们如何比较一条线和另一条线呢？我们如何知道哪个直线方程最能描述我们的数据点？为此，我们使用<strong class="ix hz">均方误差</strong>公式:</p><figure class="mg mh mi mj fd hk er es paragraph-image"><div class="er es ms"><img src="../Images/d8b70ce50f27842bc86df088dbe52396.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*faHopegLwzZjuNqZJ113gg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 2: </strong>Mean Squared Errors</figcaption></figure><p id="3697" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个等式非常简单。以<strong class="ix hz">图4 </strong>为例，MSE将计算每个红点和蓝线之间的平均距离。这个平均值越大，我们的线在描述数据点方面就越差。很直观吧？如果我们的点离线很远，那么它就不能恰当地描述我们数据的行为。由于我们处理的是线性回归，我们可以用直线方程代替<code class="du mk ml mm mn b">Y^_i</code>:</p><figure class="mg mh mi mj fd hk er es paragraph-image"><div class="er es mt"><img src="../Images/7d07cb22bc62eebd02ce18cee5e95000.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*gZrkwQhq6AvOOSyJoVKOtQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 3: </strong>Mean Squared Errors for Linear Regression</figcaption></figure><p id="4aa7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">线性回归背后的想法并不比确定<code class="du mk ml mm mn b">Theta_0</code>和<code class="du mk ml mm mn b">Theta_1</code>的值更复杂，这将给我们一条最适合我们的训练数据的线，即<strong class="ix hz">一条具有最小MSE </strong>的线。</p><p id="9f41" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，房子的价格仅仅是由房间数量决定的吗？当然不是。我们需要看它的大小，它的位置等等。为了说明这一点，我们可以推广<strong class="ix hz">等式1 </strong>:</p><figure class="mg mh mi mj fd hk er es paragraph-image"><div class="er es mu"><img src="../Images/fcf3ab9979a4a9a72f64ebfe1268bf12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*D9IgG2F-pgVvT07DkvLjxA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 4: </strong>Multivariate Linear Regression</figcaption></figure><p id="e993" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中<code class="du mk ml mm mn b">{x_1, x_2,...,x_j}</code>对应于房屋的不同特征，并且是由方程<code class="du mk ml mm mn b">h</code>接收的输入，以便得出预测。为了简单起见，我们将主要使用<strong class="ix hz">等式1 </strong>中所示的单变量公式，但是所有的概念都可以扩展到<strong class="ix hz">等式4 </strong>中所示的多变量场景。</p><p id="f0aa" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">那么，我们如何决定哪一个<code class="du mk ml mm mn b">{Theta_1, Theta_2,...,Theta_j}</code>最能描述我们的数据呢？回车，<em class="kz">梯度下降</em>。</p><h1 id="d356" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">梯度下降</h1><p id="1d9f" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在线性回归的情况下，梯度下降用于找到将<strong class="ix hz">最小化</strong>MSE的θ。请注意，这不是它唯一的用例。我们可以在许多其他优化和机器学习问题中使用这种算法，但我们不会在本文中深入讨论这一点。</p><p id="278c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">关于梯度下降算法可以说很多(我指的是很多)。在本文中，我们将触及对线性回归最重要的几点。如果你想得到更详细的理解，看看<a class="ae hv" href="https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3" rel="noopener" target="_blank">梯度下降算法及其变体</a>。</p><h2 id="15bd" class="lr ju hy bd jv ls lt lu jz lv lw lx kd jg ly lz kh jk ma mb kl jo mc md kp me bi translated">直觉</h2><p id="6548" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">我们希望找到最符合以下几点的线条:</p><figure class="mg mh mi mj fd hk er es paragraph-image"><div class="er es mp"><img src="../Images/9907b2d57352f0d005d2610f8ddd8589.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*RIhxdPQ1isXTNgIIa0Nnng.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 5: </strong>Boston Housing Prices vs Number of Rooms</figcaption></figure><p id="9ec0" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了使我们的图表更容易理解，让我们假设<code class="du mk ml mm mn b">Theta_0 = 0</code>。我们只剩下<code class="du mk ml mm mn b">Theta_1</code>要找了。下图比较了我们根据任意选择的<code class="du mk ml mm mn b">Theta_1</code>得到的MSE:</p><figure class="mg mh mi mj fd hk er es paragraph-image"><div class="er es mr"><img src="../Images/a5bf28a1488224db4e053ab5dc38a997.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*GG3N4q_tWD-IegF-3tr72Q.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 6: </strong>MSE vs Different Choices of Theta_1</figcaption></figure><p id="06cb" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">用这个图很容易找到最好的<code class="du mk ml mm mn b">Theta_1</code>，不是吗？我们只需要看看最低点，就能意识到我们的MSE在<code class="du mk ml mm mn b">Theta_1~=1.1</code>时处于最小值。同样，我们需要一种方法来精确计算这个值。这就是梯度下降的作用。基本想法是这样的:</p><figure class="mg mh mi mj fd hk er es paragraph-image"><div class="er es mv"><img src="../Images/ae5fa32482eadec613e631681e76da60.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*Xs28_z8qIaXkL1E_7rUyYQ.png"/></div></figure><p id="6dc7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们来分析一下。</p><p id="3779" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最重要的部分是偏导数。为什么我们要部分推导MSE？这么想吧:你的偏导数代表某一点的斜率。这个斜率可以是正的、负的或零。当它为正时，我们减小<code class="du mk ml mm mn b">Theta_j</code>的值。当它为负时，<code class="du mk ml mm mn b">Theta_j</code>增加。当它为零时，意味着我们已经达到最小值，并且<code class="du mk ml mm mn b">Theta_j</code>没有发生任何事情。考虑图6<strong class="ix hz">中的点<code class="du mk ml mm mn b">(0.8, 4)</code>。该点的斜率为负。如果我们运行梯度下降算法的一次迭代，值<code class="du mk ml mm mn b">Theta_1</code>将增加并越来越接近最小值。</strong></p><p id="68b2" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz"> Alpha </strong>被称为<strong class="ix hz">学习率</strong>，它代表我们向最小值前进了多大的一步。这个值不能太小，否则你的算法会运行得很慢，但也不能太大，否则你的算法永远不会终止。</p><h1 id="1370" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">履行</h1><p id="659b" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">让我们看看如何使用scikit learn <a class="ae hv" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" rel="noopener ugc nofollow" target="_blank">线性回归</a>和内置<a class="ae hv" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html" rel="noopener ugc nofollow" target="_blank"> Boston Housing Dataset </a>类来找到最佳拟合线。</p><p id="59a5" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，我们导入我们需要的库:</p><pre class="mg mh mi mj fd mw mn mx my aw mz bi"><span id="f4ef" class="lr ju hy mn b fi na nb l nc nd"># Scikit learn's built-in Boston Housing dataset<br/>from sklearn.datasets import load_boston</span><span id="c103" class="lr ju hy mn b fi ne nb l nc nd"># Library for scikit-learn compatible arrays and matrices<br/>import numpy as np</span><span id="c2c7" class="lr ju hy mn b fi ne nb l nc nd"># Library for plotting nice graphs<br/>import matplotlib.pyplot as plt</span></pre><p id="da92" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后将我们的特征(房间数量)与我们的目标变量(价格)分开:</p><pre class="mg mh mi mj fd mw mn mx my aw mz bi"><span id="03cd" class="lr ju hy mn b fi na nb l nc nd">dataset = load_boston() # Loads sklearn's Boston dataset</span><span id="d037" class="lr ju hy mn b fi ne nb l nc nd">X = dataset.data[:100,5] # Set x_1 as the number of rooms</span><span id="bf22" class="lr ju hy mn b fi ne nb l nc nd">y = dataset.target[:100] # Set h as the house's price</span></pre><p id="873c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后将它们分开，这样20%的数据用于测试，其余的用于训练我们的模型:</p><pre class="mg mh mi mj fd mw mn mx my aw mz bi"><span id="8016" class="lr ju hy mn b fi na nb l nc nd"># Split data into 20% testing and 80% training<br/>from sklearn.model_selection import train_test_split</span><span id="2125" class="lr ju hy mn b fi ne nb l nc nd">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)</span></pre><p id="5538" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的数据现在可以用于我们的线性回归模型了。我们需要做的第一件事是使用我们的训练集来训练它。请记住，线性回归是一种<em class="kz">监督学习算法，</em>意味着它从以前的数据中学习，以预测新的输入数据的值:</p><pre class="mg mh mi mj fd mw mn mx my aw mz bi"><span id="7459" class="lr ju hy mn b fi na nb l nc nd"># Train the model with our training set using linear regreesion</span><span id="f5ad" class="lr ju hy mn b fi ne nb l nc nd">from sklearn.linear_model import LinearRegression</span><span id="d50f" class="lr ju hy mn b fi ne nb l nc nd">regressor = LinearRegression()</span><span id="17b6" class="lr ju hy mn b fi ne nb l nc nd"># Run Gradient Descent to get the values of Theta_1 and Theta_0<br/>regressor.fit(X_train.reshape(-1,1),y_train)</span></pre><p id="736f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以看到获得的<code class="du mk ml mm mn b">Theta_1</code>和<code class="du mk ml mm mn b">Theta_0</code>的值:</p><pre class="mg mh mi mj fd mw mn mx my aw mz bi"><span id="ef9e" class="lr ju hy mn b fi na nb l nc nd">print(regressor.coef_) # Theta_1<br/>print(regressor.intercept_) # Theta_0<br/>&gt;&gt; [9.79185794]<br/>&gt;&gt; -38.79201598312946</span></pre><p id="4362" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们通过绘制图表来看看我们的模型相对于我们的训练集的表现如何。请注意，该数据集着眼于中间价格:</p><pre class="mg mh mi mj fd mw mn mx my aw mz bi"><span id="c253" class="lr ju hy mn b fi na nb l nc nd">plt.scatter(X_test,y_test,color='red')</span><span id="5881" class="lr ju hy mn b fi ne nb l nc nd">plt.plot(X_test, regressor.predict(X_test.reshape(-1,1)), color='blue')</span><span id="c644" class="lr ju hy mn b fi ne nb l nc nd">plt.title('Boston Housing Price vs Number of Rooms')</span><span id="bed4" class="lr ju hy mn b fi ne nb l nc nd">plt.xlabel('Number of Rooms')</span><span id="d91d" class="lr ju hy mn b fi ne nb l nc nd">plt.ylabel('Median Price (1000s)')</span><span id="4597" class="lr ju hy mn b fi ne nb l nc nd">plt.show()</span></pre><figure class="mg mh mi mj fd hk er es paragraph-image"><div class="er es mr"><img src="../Images/0a4708a82648911249a27b2cc1ffe4d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*W11V_uNkpYQ0WJdL2ttW7w.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 7:</strong> Linear Regression Predictions On Test Data</figcaption></figure><p id="62dd" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，我们可以开始用这条线做预测:</p><pre class="mg mh mi mj fd mw mn mx my aw mz bi"><span id="91fe" class="lr ju hy mn b fi na nb l nc nd">regressor.predict([[8]])<br/>&gt;&gt;&gt; array([39.54284755])</span></pre><p id="1adf" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，一个有8个房间的房子的中间价格大约是40 000美元。</p><h1 id="405c" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">结论</strong></h1><p id="1933" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在本文中，我们介绍了线性回归背后的理论，以及如何使用梯度下降算法来查找参数，从而为我们的数据点提供最佳拟合线。我们还研究了如何使用Scikit Learn的线性回归类在我们选择的数据集上轻松使用该模型。</p><p id="4c2e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">尽管线性回归很有用，但它只是回归的开始。以下是一些需要你思考的事情:</p><ul class=""><li id="d2ae" class="ld le hy ix b iy iz jc jd jg lf jk lg jo lh js nf lj lk ll bi translated">在本文看到的所有例子中，自变量和因变量之间的相关性大多是线性的。如果情况不是这样，会发生什么？还能用线性回归吗？</li><li id="805e" class="ld le hy ix b iy lm jc ln jg lo jk lp jo lq js nf lj lk ll bi translated">我们将<code class="du mk ml mm mn b">Theta_0</code>设置为零，以简化我们的图表。梯度下降如何扩展到多个特征(多个<code class="du mk ml mm mn b">Theta</code>)？</li><li id="0b99" class="ld le hy ix b iy lm jc ln jg lo jk lp jo lq js nf lj lk ll bi translated">图6 中<strong class="ix hz">的图形是二次形状，这令人惊讶吗？</strong></li><li id="9f51" class="ld le hy ix b iy lm jc ln jg lo jk lp jo lq js nf lj lk ll bi translated">我建议你至少在纸上为你选择的数据集描绘一次梯度下降迭代。这个练习会让你更好地理解如何找到<code class="du mk ml mm mn b">Theta</code>的最小值。</li></ul><h1 id="a21c" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">参考</h1><ol class=""><li id="d5bb" class="ld le hy ix b iy kr jc ks jg ng jk nh jo ni js li lj lk ll bi translated"><a class="ae hv" href="https://www.investopedia.com/terms/r/regression.asp#:~:text=Regression%20is%20a%20statistical%20method,(known%20as%20independent%20variables)" rel="noopener ugc nofollow" target="_blank"> Investopedia回归定义</a></li><li id="5a4e" class="ld le hy ix b iy lm jc ln jg lo jk lp jo lq js li lj lk ll bi translated"><a class="ae hv" href="https://www.youtube.com/watch?v=L-Lsfu4ab74&amp;ab_channel=TheCodingTrain" rel="noopener ugc nofollow" target="_blank">编码序列:梯度下降线性回归——智力和学习</a></li><li id="ae74" class="ld le hy ix b iy lm jc ln jg lo jk lp jo lq js li lj lk ll bi translated"><a class="ae hv" href="https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220" rel="noopener" target="_blank">机器学习基础(1):成本函数和梯度下降</a></li><li id="1569" class="ld le hy ix b iy lm jc ln jg lo jk lp jo lq js li lj lk ll bi translated"><a class="ae hv" href="https://www.coursera.org/learn/machine-learning?page=1" rel="noopener ugc nofollow" target="_blank">吴恩达的机器学习Coursera课程</a></li><li id="bc05" class="ld le hy ix b iy lm jc ln jg lo jk lp jo lq js li lj lk ll bi translated"><a class="ae hv" href="https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d" rel="noopener" target="_blank">监督与非监督学习</a></li><li id="2f4e" class="ld le hy ix b iy lm jc ln jg lo jk lp jo lq js li lj lk ll bi translated"><a class="ae hv" href="https://www.probabilitycourse.com/chapter9/9_1_5_mean_squared_error_MSE.php#:~:text=The%20mean%20squared%20error%20(MSE)%20of%20this%20estimator%20is%20defined,MSE%20among%20all%20possible%20estimators." rel="noopener ugc nofollow" target="_blank">概率、统计和随机过程介绍:均方误差</a></li></ol></div></div>    
</body>
</html>
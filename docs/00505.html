<html>
<head>
<title>Can I Create an Image Classifier Using Tensorflow to Identify the Theme of a Painting?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我可以使用Tensorflow创建一个图像分类器来识别一幅画的主题吗？</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/can-i-create-an-image-classifier-using-tensorflow-to-identify-the-theme-of-a-painting-46d78ee1db13?source=collection_archive---------2-----------------------#2021-02-22">https://medium.com/geekculture/can-i-create-an-image-classifier-using-tensorflow-to-identify-the-theme-of-a-painting-46d78ee1db13?source=collection_archive---------2-----------------------#2021-02-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/20729ca9ce431d7b451a1349125783e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LMSRLg4PqRKdjsjXBhhxuQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx"><a class="ae iu" href="https://upload.wikimedia.org/wikipedia/commons/e/e4/Leonardo_Da_Vinci_-_Vergine_delle_Rocce_%28Louvre%29.jpg" rel="noopener ugc nofollow" target="_blank">https://upload.wikimedia.org/wikipedia/commons/e/e4/Leonardo_Da_Vinci_-_Vergine_delle_Rocce_%28Louvre%29.jpg</a></figcaption></figure><p id="d899" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我可以教一个使用Tensorflow物体检测API的AI模型，如何识别一幅画的主题吗？</p><p id="afeb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这就是我想用这个项目回答的一般问题。</p><p id="354a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是，出于实际原因，我将进一步缩小我的问题。</p><p id="64b0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我想辨别一幅画是否是关于<strong class="ix hj">耶稣诞生</strong>的。这是一个更简单的二进制分类问题。</p><p id="3833" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但首先，让我解释一下什么是耶稣诞生画。一幅<strong class="ix hj">耶稣诞生画</strong>是一幅以耶稣基督诞生为主题的画，在基督教中非常受尊敬。在过去的两千年里，许多著名的艺术家，如达芬奇、米开朗基罗、卡拉瓦乔都被委托为教堂创作画作，所以应该有很多关于<strong class="ix hj">耶稣诞生</strong>的画作可供挑选。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/c710221cf4ed84d30464941d8d2dabc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*OtgXAwvFmzMgVEiI.jpg"/></div><figcaption class="iq ir et er es is it bd b be z dx"><a class="ae iu" href="https://upload.wikimedia.org/wikipedia/commons/9/9d/Botticelli_-_Adoration_of_the_Magi_%28Zanobi_Altar%29_-_Uffizi.jpg" rel="noopener ugc nofollow" target="_blank">https://upload.wikimedia.org/wikipedia/commons/9/9d/Botticelli_-_Adoration_of_the_Magi_%28Zanobi_Altar%29_-_Uffizi.jpg</a></figcaption></figure><h1 id="f5f8" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">开始前</h1><p id="edab" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">我用<a class="ae iu" href="https://www.tensorflow.org/tutorials/images/classification" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj"> Tensorflow关于图像分类的教程</strong> </a>中的一些代码创建了这个笔记本。您可以在下面的链接中找到原始教程:</p><p id="58a2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://www.tensorflow.org/tutorials/images/classification" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/images/classification</a></p><h1 id="6ae1" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">导入TensorFlow和其他库</h1><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="b9c5" class="lg jz hi lc b fi lh li l lj lk">import matplotlib.pyplot as plt <br/>import PIL<br/>import tensorflow as tf<br/>import os<br/>from tensorflow import keras<br/>from tensorflow.keras import layers<br/>from tensorflow.keras.models import Sequential</span><span id="6691" class="lg jz hi lc b fi ll li l lj lk">import pandas as pd<br/>import requests # to get image from the web<br/>import shutil # to save it locally<br/>import time<br/>import numpy as np</span></pre><h1 id="493e" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">下载训练、验证和测试图像数据集</h1><p id="6a03" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">为了训练一个图像分类器，我们需要一个<strong class="ix hj">训练</strong>图像数据集、<strong class="ix hj">验证</strong>数据集和<strong class="ix hj">测试</strong>数据集。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/73ff818c3c31cd2e978831672f5eb418.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v-GrwnvrCTNzgipdUPqZBQ.png"/></div></div></figure><p id="c24d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为我们正在训练二进制图像分类器，所以我们将有两个不同类别的图像:</p><ul class=""><li id="2e45" class="ln lo hi ix b iy iz jc jd jg lp jk lq jo lr js ls lt lu lv bi translated">出生</li><li id="cffa" class="ln lo hi ix b iy lw jc lx jg ly jk lz jo ma js ls lt lu lv bi translated">其他人</li></ul><p id="a7f2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在模型的训练过程中，我们将使用训练数据集来教模型如何对一幅画进行分类:要么是<strong class="ix hj">耶稣降生</strong>画，要么不是(<strong class="ix hj">其他</strong>)。</p><p id="49df" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在每个训练周期(时期)结束时，我们将使用验证数据集，通过计算<strong class="ix hj">精确度</strong>和<strong class="ix hj">损失</strong>来评估模型的表现。准确性衡量我们的模型得到正确答案的次数。<strong class="ix hj">越高</strong>越好。<strong class="ix hj">损失</strong>测量差值，即预测值和实际值之间的差值。<strong class="ix hj">越低</strong>越好。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/ed1d33da5c3df4da2784e59d64332c3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jUhjjYf8HRSsuRvfoRd3hQ.png"/></div></div></figure><p id="3137" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">重要的是，<strong class="ix hj">验证</strong>数据集与<strong class="ix hj">训练</strong>数据集<strong class="ix hj">分开</strong>，因为AI模型非常擅长偷工减料(即作弊)。如果你不把这两者分开，模型将简单地记住答案，而不是学习我们试图教给它的内在特征。</p><p id="c48c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在培训的最后，我们还将使用来自<strong class="ix hj">培训</strong>和<strong class="ix hj">验证</strong>数据集的单独的<strong class="ix hj">测试</strong>数据集，对模型性能进行独立的基准测试。</p><p id="88f5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您会注意到我们正在下载三个文件:</p><ul class=""><li id="6114" class="ln lo hi ix b iy iz jc jd jg lp jk lq jo lr js ls lt lu lv bi translated">nativity _ dataset.csv包含所有耶稣诞生绘画</li><li id="5028" class="ln lo hi ix b iy lw jc lx jg ly jk lz jo ma js ls lt lu lv bi translated">other _ dataset.csv包含许多绘画，但耶稣诞生绘画除外</li><li id="808c" class="ln lo hi ix b iy lw jc lx jg ly jk lz jo ma js ls lt lu lv bi translated">test _ dataset.csv包含带标签的绘画</li></ul><p id="1cff" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">等一下！我刚才不是说过训练数据集应该与验证数据集分开吗，那么为什么要把它保存在同一个文件中呢？</p><p id="586d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">可以，但是因为我们做的是数据探索，有一定的灵活性是好事。通常建议您拥有80%的训练数据和20%的验证数据。但是，这不是一个硬性规定。作为实验的一部分，我们可能想改变这些百分比，看看什么能给我们带来更好的结果。这也被称为<strong class="ix hj">超参数调整</strong>。另一方面，测试数据集应该是固定的，因此我们可以用一致的方式比较不同架构的不同模型。</p><p id="5845" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们在下面定义了一些实用函数来帮助从我们的图像数据集中下载图像。请注意，<strong class="ix hj"> getFileNameFromUrl() </strong>对Url中的<strong class="ix hj">文件名</strong>进行一些非常基本的<strong class="ix hj">清理</strong>和<strong class="ix hj">提取</strong>。</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="889b" class="lg jz hi lc b fi lh li l lj lk">def getFileNameFromUrl(url):<br/>  firstpos=url.rindex("/")<br/>  lastpos=len(url)<br/>  filename=url[firstpos+1:lastpos]<br/>  print(f"url={url} firstpos={firstpos} lastpos={lastpos} filename={filename}")<br/>  return filename</span><span id="3d7c" class="lg jz hi lc b fi ll li l lj lk">def downloadImage(imageUrl, destinationFolder):<br/>  filename = getFileNameFromUrl(imageUrl)<br/>  # Open the url image, set stream to True, this will return the stream content.<br/>  r = requests.get(imageUrl, stream = True)</span><span id="3385" class="lg jz hi lc b fi ll li l lj lk">  # Check if the image was retrieved successfully<br/>  if r.status_code == 200:<br/>      # Set decode_content value to True, otherwise the downloaded image file's size will be zero.<br/>      r.raw.decode_content = True</span><span id="6887" class="lg jz hi lc b fi ll li l lj lk">      # Open a local file with wb ( write binary ) permission.<br/>      filePath = os.path.join(destinationFolder, filename)<br/>      if not os.path.exists(filePath):<br/>        with open(filePath,'wb') as f:<br/>            shutil.copyfileobj(r.raw, f)<br/>        print('Image sucessfully Downloaded: ',filename)<br/>        print("Sleeping for 1 seconds before attempting next download")<br/>        time.sleep(1)<br/>      else:<br/>        print(f'Skipping image {filename} as it is already Downloaded: ')</span><span id="f653" class="lg jz hi lc b fi ll li l lj lk">  else:<br/>      print(f'Image url={imageUrl} and filename={filename} Couldn't be retreived. HTTP Status={r.status_code}')</span><span id="baa5" class="lg jz hi lc b fi ll li l lj lk">df = pd.read_csv("nativity_dataset.csv")</span><span id="53c0" class="lg jz hi lc b fi ll li l lj lk"># create directory to which we download if it doesn't exist<br/>destinationFolder = "/content/dataset/nativity"<br/>os.makedirs(destinationFolder, exist_ok=True)</span><span id="a2f8" class="lg jz hi lc b fi ll li l lj lk">for i, row in df.iterrows():<br/>  print(f"Index: {i}")<br/>  print(f"{row['Image URL']}n")</span><span id="07b2" class="lg jz hi lc b fi ll li l lj lk">  downloadImage(row["Image URL"], destinationFolder)</span></pre><p id="ae1a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">出局:</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="4e9e" class="lg jz hi lc b fi lh li l lj lk">Index: 0<br/>https://d3d00swyhr67nd.cloudfront.net/w1200h1200/collection/LSE/CUMU/LSE_CUMU_TN07034-001.jpg</span><span id="69f5" class="lg jz hi lc b fi ll li l lj lk">url=https://d3d00swyhr67nd.cloudfront.net/w1200h1200/collection/LSE/CUMU/LSE_CUMU_TN07034-001.jpg firstpos=68 lastpos=93 filename=LSE_CUMU_TN07034-001.jpg<br/>Image sucessfully Downloaded:  LSE_CUMU_TN07034-001.jpg<br/>Sleeping for 1 seconds before attempting next download<br/>Index: 1<br/>https://d3d00swyhr67nd.cloudfront.net/w1200h1200/collection/GMIII/MCAG/GMIII_MCAG_1947_188-001.jpg</span></pre><h1 id="0a2d" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">将所有图像的大小调整为不超过90000像素(宽x高)</h1><p id="9033" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">我们数据集中的一些图像大小超过80MB。如果我们尝试直接从Python中调整这些图像的大小，它会尝试将图像加载到内存中。不是个好主意。因此，我们将使用Imagemick以超快的速度完成这项工作。</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="7a46" class="lg jz hi lc b fi lh li l lj lk">!apt install imagemagick</span><span id="2e12" class="lg jz hi lc b fi ll li l lj lk">Reading package lists... Done<br/>Building dependency tree       <br/>Reading state information... Done<br/>imagemagick is already the newest version (8:6.9.7.4+dfsg-16ubuntu6.9).<br/>0 upgraded, 0 newly installed, 0 to remove and 17 not upgraded.</span></pre><p id="e6d2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们定义实用函数<strong class="ix hj"> resizeImages </strong>来调整图像大小，并从<strong class="ix hj"> sourceFolder </strong>复制到<strong class="ix hj"> destinationFolder </strong>。</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="48a6" class="lg jz hi lc b fi lh li l lj lk">def resizeImages(sourceFolder, destinationFolder, maxPixels=1048576):<br/>  os.makedirs(destinationFolder, exist_ok=True)<br/>  for path, subdirs, files in os.walk(sourceFolder):<br/>      relativeDir=path.replace(sourceFolder, "")<br/>      destinationFolderPath = destinationFolder + relativeDir<br/>      os.makedirs(destinationFolderPath,exist_ok=True)<br/>      for fileName in files:<br/>          sourceFilepath=os.path.join(path,fileName)<br/>          destinationFilepath=os.path.join(destinationFolderPath, fileName)<br/>          print(f"sourceFilepath={sourceFilepath} destinationFilepath={destinationFilepath}")<br/>          os.system(f"convert {sourceFilepath} -resize {maxPixels}@&gt; {destinationFilepath}")</span><span id="8bd8" class="lg jz hi lc b fi ll li l lj lk"># resize training images<br/>sourceFolder="/content/dataset"<br/>destinationFolder = "/content/resized/dataset"<br/>resizeImages(sourceFolder, destinationFolder, maxPixels=90000)</span><span id="c601" class="lg jz hi lc b fi ll li l lj lk"># resize testing images<br/>sourceFolder="/content/test_dataset"<br/>destinationFolder = "/content/resized/test_dataset"<br/>resizeImages(sourceFolder, destinationFolder, maxPixels=90000)</span><span id="7b88" class="lg jz hi lc b fi ll li l lj lk">sourceFilepath=/content/dataset/others/Quentin_Massys-The_Adoration_of_the_Magi-1526%2CMetropolitan_Museum_of_Art%2CNew_York.jpg destinationFilepath=/content/resized/dataset/others/Quentin_Massys-The_Adoration_of_the_Magi-1526%2CMetropolitan_Museum_of_Art%2CNew_York.jpg<br/>...</span></pre><h1 id="1551" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">将图像标签映射到数值</h1><p id="5a3f" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">我们使用二进制交叉熵进行分类，因此我们需要确保我们的标签是0或1。Nativity = 1，Others = 0</p><p id="8f57" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将文件夹重命名为0和1，因为这是<strong class="ix hj">TF . keras . preprocessing . image _ dataset _ from _ directory</strong>用来为数据集创建标签的。</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="698e" class="lg jz hi lc b fi lh li l lj lk"><br/>!mv /content/resized/dataset/nativity /content/resized/dataset/1<br/>!mv /content/resized/dataset/others /content/resized/dataset/0</span><span id="e9b1" class="lg jz hi lc b fi ll li l lj lk">!mv /content/resized/test_dataset/nativity /content/resized/test_dataset/1<br/>!mv /content/resized/test_dataset/others /content/resized/test_dataset/0</span></pre><p id="7f14" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下载后，我们现在应该有一个可用的数据集副本。总共有429幅图像:</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="a688" class="lg jz hi lc b fi lh li l lj lk">import pathlib<br/>data_dir = pathlib.Path("/content/resized/dataset")<br/>test_data_dir = pathlib.Path("/content/resized/test_dataset")</span><span id="68ad" class="lg jz hi lc b fi ll li l lj lk">image_count = len(list(data_dir.glob('*/*')))<br/>print(image_count)</span><span id="22d1" class="lg jz hi lc b fi ll li l lj lk">454</span></pre><p id="7e3d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里有一些耶稣诞生的画:</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="6969" class="lg jz hi lc b fi lh li l lj lk">nativity_label="1"<br/>nativity = list(data_dir.glob(f'{nativity_label}/*'))</span><span id="98a2" class="lg jz hi lc b fi ll li l lj lk">PIL.Image.open(str(nativity[0]))</span></pre><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/72112efa0ee314621277bf169a29a769.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/0*EKjIvV3VClIbg5Ri.png"/></div></figure><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="879f" class="lg jz hi lc b fi lh li l lj lk">PIL.Image.open(str(nativity[1]))</span></pre><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/1ee558c40d46be6c6bfdafe65cbd741b.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/0*NpQTyNqRb_Baf-hg.png"/></div></figure><p id="0fd1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">还有一些非耶稣诞生的画:</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="175e" class="lg jz hi lc b fi lh li l lj lk">others_label="0"<br/>others = list(data_dir.glob(f'{others_label}/*'))<br/>PIL.Image.open(str(others[1]))</span></pre><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es md"><img src="../Images/200b2aafdaf2aa301ce902d0d76e712f.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/0*BKY0YiK5KXAzKtuA.png"/></div></figure><p id="bab4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du me mf mg lc b">PIL.Image.open(str(others[2]))</code></p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/b8fd8fc14a0103e00ef70117d22fca1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/0*HMMm1Mxeesj15R4e.png"/></div></figure><h1 id="a8ee" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">使用keras .预处理加载</h1><p id="e50f" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">Keras提供了一系列非常方便的功能，让我们在使用Tensorflow时更加轻松。<strong class="ix hj">TF . keras . preprocessing . image _ dataset _ from _ directory</strong>就是其中之一。它将图像从文件加载到<strong class="ix hj"> tf.data.DataSet </strong>格式。</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="6f23" class="lg jz hi lc b fi lh li l lj lk">batch_size = 32<br/>img_height = 300<br/>img_width = 300</span></pre><p id="1628" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一般来说，建议使用80% 20%的拆分将数据分为训练数据和验证数据。请记住，这不是一个硬性规定。</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="e3c4" class="lg jz hi lc b fi lh li l lj lk">train_ds = tf.keras.preprocessing.image_dataset_from_directory(<br/>  data_dir,<br/>  validation_split=0.2,<br/>  subset="training",<br/>  seed=123,<br/>  image_size=(img_height, img_width),<br/>  batch_size=batch_size, label_mode='binary')</span><span id="9a2c" class="lg jz hi lc b fi ll li l lj lk">Found 452 files belonging to 2 classes.<br/>Using 362 files for training.</span><span id="ff1a" class="lg jz hi lc b fi ll li l lj lk">val_ds = tf.keras.preprocessing.image_dataset_from_directory(<br/>  data_dir,<br/>  validation_split=0.2,<br/>  subset="validation",<br/>  seed=123,<br/>  image_size=(img_height, img_width),<br/>  batch_size=batch_size,label_mode='binary')</span><span id="0780" class="lg jz hi lc b fi ll li l lj lk">Found 452 files belonging to 2 classes.<br/>Using 90 files for validation.</span><span id="8f65" class="lg jz hi lc b fi ll li l lj lk">#Retrieve a batch of images from the test set<br/>test_data_dir = pathlib.Path("/content/resized/test_dataset")<br/>test_batch_size=37<br/>test_ds = tf.keras.preprocessing.image_dataset_from_directory(<br/>  test_data_dir,<br/>  seed=200,<br/>  image_size=(img_height, img_width),<br/>  batch_size=test_batch_size,label_mode='binary')</span><span id="f548" class="lg jz hi lc b fi ll li l lj lk">Found 37 files belonging to 2 classes.</span></pre><p id="9c86" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您可以在这些数据集的class_names属性中找到类名。这些对应于按字母顺序排列的目录名。</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="13b4" class="lg jz hi lc b fi lh li l lj lk">class_names = train_ds.class_names<br/>print(class_names)</span><span id="915a" class="lg jz hi lc b fi ll li l lj lk">['0', '1']</span></pre><h1 id="d51e" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">将数据可视化</h1><p id="0659" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">这是训练数据集中的前9幅图像。</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="e986" class="lg jz hi lc b fi lh li l lj lk">import matplotlib.pyplot as plt</span><span id="3ac8" class="lg jz hi lc b fi ll li l lj lk">plt.figure(figsize=(10, 10))<br/>for images, labels in train_ds.take(1):<br/>  for i in range(9):<br/>    ax = plt.subplot(3, 3, i + 1)<br/>    plt.imshow(images[i].numpy().astype("uint8"))<br/>    if labels[i] == 1.0:<br/>      title = "Nativity"<br/>    else:<br/>      title = "Others"</span><span id="39a6" class="lg jz hi lc b fi ll li l lj lk">    plt.title(title)<br/>    plt.axis("off")</span></pre><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/938625346b42ebbbbe0d12fc311b3b96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/0*tQ2ovS4RCe6_pRyG.png"/></div></figure><p id="c94c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们检查image_batch和labels_batch变量。</p><p id="bf16" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">image_batch是形状的张量(32，300，300，3)。这是一批32个形状为300x300x3的图像(最后一个维度是指颜色通道RGB)。label_batch是形状(32)的张量，这些是32个图像的对应标签。</p><p id="00c4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你可以打电话。numpy()将它们转换为numpy.ndarray。</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="587e" class="lg jz hi lc b fi lh li l lj lk">for image_batch, labels_batch in train_ds:<br/>  print(image_batch.shape)<br/>  print(labels_batch.shape)<br/>  break</span><span id="6f0b" class="lg jz hi lc b fi ll li l lj lk">(32, 300, 300, 3)<br/>(32, 1)</span></pre><h1 id="b949" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">配置数据集以提高性能</h1><p id="1f3e" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">这段代码直接摘自<a class="ae iu" href="https://www.tensorflow.org/tutorials/images/classification" rel="noopener ugc nofollow" target="_blank"> Tensorflow教程</a>，它旨在帮助提高性能，因此我们不必总是从磁盘获取图像。</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="4243" class="lg jz hi lc b fi lh li l lj lk">AUTOTUNE = tf.data.AUTOTUNE</span><span id="9de9" class="lg jz hi lc b fi ll li l lj lk">train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)<br/>val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)</span></pre><p id="a798" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们定义了一个效用函数来衡量测试数据集的性能</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="4f53" class="lg jz hi lc b fi lh li l lj lk">labelMappings={"0":"Others","1":"Nativity",<br/>               0.0:"Others",1.0 :"Nativity"}</span><span id="aaa3" class="lg jz hi lc b fi ll li l lj lk">def predictWithTestDataset(model):<br/>  image_batch, label_batch = test_ds.as_numpy_iterator().next()<br/>  predictions = model.predict_on_batch(image_batch).flatten()</span><span id="4528" class="lg jz hi lc b fi ll li l lj lk">  predictions = tf.where(predictions &lt; 0.5, 0, 1)</span><span id="1d1a" class="lg jz hi lc b fi ll li l lj lk">  #print('Predictions:n', predictions.numpy())<br/>  #print('Labels:n', label_batch)<br/>  correctPredictions=0<br/>  plt.figure(figsize=(20, 20))<br/>  print(f"number predictions={len(predictions)}")<br/>  for i in range(len(predictions)):<br/>    ax = plt.subplot(8, 5, i +1)<br/>    plt.imshow(image_batch[i].astype("uint8"))<br/>    prediction = class_names[predictions[i]]<br/>    predictionLabel = labelMappings[prediction]<br/>    gtLabel = labelMappings[label_batch[i][0]]<br/>    if gtLabel == predictionLabel:<br/>      correctPredictions += 1<br/>    plt.title(f"P={predictionLabel} GT={gtLabel}")<br/>    plt.axis("off")</span><span id="bd77" class="lg jz hi lc b fi ll li l lj lk">  accuracy = correctPredictions/len(predictions)<br/>  print(f"Accuracy:{accuracy}")</span></pre><h1 id="4e74" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">使数据标准化</h1><p id="529b" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">RGB在[0，255]范围内。我们将这些值归一化到[0，1]之间，这是神经网络的首选方式</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="b02d" class="lg jz hi lc b fi lh li l lj lk">normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)</span></pre><p id="84d4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个规范化层将在稍后的模型定义中使用。</p><h1 id="b1c5" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">创建模型</h1><p id="0942" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">我们基于<strong class="ix hj"> Tensorflow图像分类模型</strong>中的架构，为我们的模型定义了一个初始架构。</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="b417" class="lg jz hi lc b fi lh li l lj lk">model = Sequential([<br/>  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),<br/>  layers.Conv2D(16, 3, padding='same', activation='relu'),<br/>  layers.MaxPooling2D(),<br/>  layers.Conv2D(32, 3, padding='same', activation='relu'),<br/>  layers.MaxPooling2D(),<br/>  layers.Conv2D(64, 3, padding='same', activation='relu'),<br/>  layers.MaxPooling2D(),<br/>  layers.Flatten(),<br/>  layers.Dense(128, activation='relu'),<br/>  layers.Dense(1, activation='sigmoid')<br/>])</span></pre><h1 id="a17a" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">编译模型</h1><p id="ca1e" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">对于本教程，我们选择优化器。亚当优化器和损失。<code class="du me mf mg lc b">BinaryCrossentropy</code>损失函数。为了查看每个训练时期的训练和验证准确性，我们传递了metrics参数。</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="cfec" class="lg jz hi lc b fi lh li l lj lk">model.compile(optimizer='adam', loss=keras.losses.BinaryCrossentropy(from_logits=True), metrics=[keras.metrics.BinaryAccuracy()])</span></pre><h1 id="9fd0" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">模型摘要</h1><p id="421d" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">使用模型的汇总方法查看网络的所有层:</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="6534" class="lg jz hi lc b fi lh li l lj lk">model.summary()</span><span id="8289" class="lg jz hi lc b fi ll li l lj lk">Model: "sequential_31"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>rescaling_27 (Rescaling)     (None, 300, 300, 3)       0         <br/>_________________________________________________________________<br/>conv2d_108 (Conv2D)          (None, 300, 300, 16)      448       <br/>_________________________________________________________________<br/>max_pooling2d_60 (MaxPooling (None, 150, 150, 16)      0         <br/>_________________________________________________________________<br/>conv2d_109 (Conv2D)          (None, 150, 150, 32)      4640      <br/>_________________________________________________________________<br/>max_pooling2d_61 (MaxPooling (None, 75, 75, 32)        0         <br/>_________________________________________________________________<br/>conv2d_110 (Conv2D)          (None, 75, 75, 64)        18496     <br/>_________________________________________________________________<br/>max_pooling2d_62 (MaxPooling (None, 37, 37, 64)        0         <br/>_________________________________________________________________<br/>flatten_20 (Flatten)         (None, 87616)             0         <br/>_________________________________________________________________<br/>dense_52 (Dense)             (None, 128)               11214976  <br/>_________________________________________________________________<br/>dense_53 (Dense)             (None, 1)                 129       <br/>=================================================================<br/>Total params: 11,238,689<br/>Trainable params: 11,238,689<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><h1 id="01ad" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">训练模型</h1><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="43e0" class="lg jz hi lc b fi lh li l lj lk">epochs=10<br/>history = model.fit(<br/>  train_ds,<br/>  validation_data=val_ds,<br/>  epochs=epochs<br/>)</span><span id="8bdb" class="lg jz hi lc b fi ll li l lj lk">Epoch 1/10<br/>12/12 [==============================] - 1s 85ms/step - loss: 1.9371 - binary_accuracy: 0.5107 - val_loss: 0.7001 - val_binary_accuracy: 0.4444<br/>Epoch 2/10<br/>12/12 [==============================] - 1s 49ms/step - loss: 0.6491 - binary_accuracy: 0.6737 - val_loss: 0.7258 - val_binary_accuracy: 0.4778<br/>Epoch 3/10<br/>12/12 [==============================] - 1s 49ms/step - loss: 0.5943 - binary_accuracy: 0.6958 - val_loss: 0.7169 - val_binary_accuracy: 0.5333<br/>Epoch 4/10<br/>12/12 [==============================] - 1s 49ms/step - loss: 0.5111 - binary_accuracy: 0.7762 - val_loss: 0.7201 - val_binary_accuracy: 0.5667<br/>Epoch 5/10<br/>12/12 [==============================] - 1s 49ms/step - loss: 0.4013 - binary_accuracy: 0.8427 - val_loss: 0.6920 - val_binary_accuracy: 0.5667<br/>Epoch 6/10<br/>12/12 [==============================] - 1s 49ms/step - loss: 0.3027 - binary_accuracy: 0.8921 - val_loss: 0.8354 - val_binary_accuracy: 0.5889<br/>Epoch 7/10<br/>12/12 [==============================] - 1s 50ms/step - loss: 0.2438 - binary_accuracy: 0.9049 - val_loss: 0.8499 - val_binary_accuracy: 0.5778<br/>Epoch 8/10<br/>12/12 [==============================] - 1s 49ms/step - loss: 0.1725 - binary_accuracy: 0.9292 - val_loss: 0.9742 - val_binary_accuracy: 0.5222<br/>Epoch 9/10<br/>12/12 [==============================] - 1s 50ms/step - loss: 0.2792 - binary_accuracy: 0.8878 - val_loss: 0.9390 - val_binary_accuracy: 0.5222<br/>Epoch 10/10<br/>12/12 [==============================] - 1s 50ms/step - loss: 0.1347 - binary_accuracy: 0.9658 - val_loss: 0.9914 - val_binary_accuracy: 0.5889</span></pre><h1 id="6fba" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">可视化培训结果</h1><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="17b9" class="lg jz hi lc b fi lh li l lj lk">print(history.history)<br/>acc = history.history['binary_accuracy']<br/>val_acc = history.history['val_binary_accuracy']<br/># acc = history.history['accuracy']<br/># val_acc = history.history['val_accuracy']</span><span id="2c4e" class="lg jz hi lc b fi ll li l lj lk">loss = history.history['loss']<br/>val_loss = history.history['val_loss']</span><span id="5172" class="lg jz hi lc b fi ll li l lj lk">epochs_range = range(epochs)</span><span id="bae3" class="lg jz hi lc b fi ll li l lj lk">plt.figure(figsize=(8, 8))<br/>plt.subplot(1, 2, 1)<br/>plt.plot(epochs_range, acc, label='Training Accuracy')<br/>plt.plot(epochs_range, val_acc, label='Validation Accuracy')<br/>plt.legend(loc='lower right')<br/>plt.title('Training and Validation Accuracy')</span><span id="7200" class="lg jz hi lc b fi ll li l lj lk">plt.subplot(1, 2, 2)<br/>plt.plot(epochs_range, loss, label='Training Loss')<br/>plt.plot(epochs_range, val_loss, label='Validation Loss')<br/>plt.legend(loc='upper right')<br/>plt.title('Training and Validation Loss')<br/>plt.show()</span><span id="f586" class="lg jz hi lc b fi ll li l lj lk">{'loss': [1.5215493440628052, 0.6543726325035095, 0.5897634625434875, 0.5006453990936279, 0.39839598536491394, 0.2903604209423065, 0.22604547441005707, 0.22543807327747345, 0.2558016777038574, 0.14820142090320587], 'binary_accuracy': [0.5580110549926758, 0.6491712927818298, 0.7099447250366211, 0.7845304012298584, 0.8425414562225342, 0.8867403268814087, 0.9116021990776062, 0.9060773253440857, 0.9033148884773254, 0.9558011293411255], 'val_loss': [0.7000985741615295, 0.7257931232452393, 0.7169376611709595, 0.7200638055801392, 0.6920430660247803, 0.8354127407073975, 0.8498525619506836, 0.9741556644439697, 0.9390344619750977, 0.9914490580558777], 'val_binary_accuracy': [0.4444444477558136, 0.47777777910232544, 0.5333333611488342, 0.5666666626930237, 0.5666666626930237, 0.5888888835906982, 0.5777778029441833, 0.5222222208976746, 0.5222222208976746, 0.5888888835906982]}</span></pre><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/5351f60a8d1217f1170a380e6fcbd2b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/0*hO8ObMcZNzzowSiz.png"/></div></figure><p id="a742" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">观察这些图，我们看到了<strong class="ix hj">过度拟合</strong>的典型迹象。<strong class="ix hj">过度拟合</strong>发生在模型与训练数据过于吻合，但与<strong class="ix hj">验证</strong>数据不吻合的时候。注意<strong class="ix hj">精度</strong>随着训练精度的<strong class="ix hj">时期</strong>而增加，但是对于验证数据，精度没有增加，在这种情况下，损失增加。</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="ece0" class="lg jz hi lc b fi lh li l lj lk">predictWithTestDataset(model)</span><span id="e737" class="lg jz hi lc b fi ll li l lj lk">number predictions=37<br/>Accuracy:0.6216216216216216</span></pre><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/af0614612c01f9db6ce8c045e2a5b2e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dqK-DOK0pPQg_p6E.png"/></div></div></figure><h1 id="e9c7" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">数据扩充</h1><p id="29d2" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">过度拟合通常发生在训练样本数量较少的时候。这并不奇怪，因为我们在两个类中总共只有455张图片。因此，我们需要找到一种方法来生成更多的训练数据。</p><p id="f36e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Keras有一些真正易于使用的图像数据增强转换功能，实际上产生非常体面的图像。</p><p id="9f37" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">图像增强的想法，一种正则化的形式，是为了使模型更难过度拟合。当数据非常少时，很容易发生过拟合。我们引入了可变性，希望它能帮助模型更好地概括。</p><p id="0229" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">用于图像增强的keras函数可在<strong class="ix hj">TF . keras . layers . experimental . preparatory .</strong>下获得</p><p id="84e4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们用图像增强变换创建一个层，并且我们可以在模型的创建中包括它，就像任何其他层一样。</p><p id="1ddf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于图像放大，我们做一个随机的水平翻转和一个简单的旋转。</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="8ecb" class="lg jz hi lc b fi lh li l lj lk">data_augmentation = keras.Sequential(<br/>  [<br/>    layers.experimental.preprocessing.RandomFlip("horizontal", <br/>                                                 input_shape=(img_height, <br/>                                                              img_width,<br/>                                                              3)),<br/>    layers.experimental.preprocessing.RandomRotation(0.1) </span><span id="2570" class="lg jz hi lc b fi ll li l lj lk">    ]</span><span id="9d3b" class="lg jz hi lc b fi ll li l lj lk">)</span></pre><p id="9b85" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们通过对同一幅图像多次应用数据增强来想象几个增强示例的样子:</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="a2b6" class="lg jz hi lc b fi lh li l lj lk">plt.figure(figsize=(10, 10))<br/>for images, _ in train_ds.take(1):<br/>  for i in range(9):<br/>    augmented_images = data_augmentation(images)<br/>    ax = plt.subplot(3, 3, i + 1)<br/>    plt.imshow(augmented_images[0].numpy().astype("uint8"))<br/>    plt.axis("off")</span></pre><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/05d210f60ed030ac3dfdcbefee9e0471.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/0*4OKL8Tff-G5psRPG.png"/></div></figure><p id="b7ff" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将使用数据增强来训练一个模型。</p><h1 id="a1f5" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">拒绝传统社会的人</h1><p id="251a" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">另一种减少过拟合的技术是在网络中引入漏失，这是一种正则化的形式。你可以把它想象成在训练过程中随机挑选神经网络中的神经元。我们希望防止模型只依赖于一个特征。每一层的剔除都基于一个分数(0.2)，我们将这个分数作为参数传递给<strong class="ix hj"> Dropout </strong>函数。</p><p id="10b8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们使用层创建一个新的神经网络。辍学，然后使用增强图像训练它。</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="2311" class="lg jz hi lc b fi lh li l lj lk">model = Sequential([<br/>  data_augmentation,<br/>  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),<br/>  layers.Conv2D(16, 3, padding='same', activation='relu'),<br/>  layers.MaxPooling2D(),<br/>  layers.Conv2D(32, 3, padding='same', activation='relu'),<br/>  layers.MaxPooling2D(),<br/>  layers.Conv2D(64, 3, padding='same', activation='relu'),<br/>  layers.MaxPooling2D(),<br/>  layers.Dropout(0.2),<br/>  layers.Flatten(),<br/>  layers.Dense(128, activation='relu'),<br/>  layers.Dense(1, activation='sigmoid')<br/>])</span></pre><h1 id="4b73" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">编译和训练模型</h1><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="d2dc" class="lg jz hi lc b fi lh li l lj lk">from tensorflow import optimizers<br/>model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=True),<br/>              optimizer=optimizers.RMSprop(lr=1e-4),<br/>                  metrics=[keras.metrics.BinaryAccuracy()])</span><span id="c8c9" class="lg jz hi lc b fi ll li l lj lk">model.summary()</span><span id="928f" class="lg jz hi lc b fi ll li l lj lk">Model: "sequential_34"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>sequential_32 (Sequential)   (None, 300, 300, 3)       0         <br/>_________________________________________________________________<br/>rescaling_29 (Rescaling)     (None, 300, 300, 3)       0         <br/>_________________________________________________________________<br/>conv2d_114 (Conv2D)          (None, 300, 300, 16)      448       <br/>_________________________________________________________________<br/>max_pooling2d_66 (MaxPooling (None, 150, 150, 16)      0         <br/>_________________________________________________________________<br/>conv2d_115 (Conv2D)          (None, 150, 150, 32)      4640      <br/>_________________________________________________________________<br/>max_pooling2d_67 (MaxPooling (None, 75, 75, 32)        0         <br/>_________________________________________________________________<br/>conv2d_116 (Conv2D)          (None, 75, 75, 64)        18496     <br/>_________________________________________________________________<br/>max_pooling2d_68 (MaxPooling (None, 37, 37, 64)        0         <br/>_________________________________________________________________<br/>dropout_26 (Dropout)         (None, 37, 37, 64)        0         <br/>_________________________________________________________________<br/>flatten_22 (Flatten)         (None, 87616)             0         <br/>_________________________________________________________________<br/>dense_56 (Dense)             (None, 128)               11214976  <br/>_________________________________________________________________<br/>dense_57 (Dense)             (None, 1)                 129       <br/>=================================================================<br/>Total params: 11,238,689<br/>Trainable params: 11,238,689<br/>Non-trainable params: 0<br/>_________________________________________________________________</span><span id="ed67" class="lg jz hi lc b fi ll li l lj lk">epochs = 25<br/>history = model.fit(<br/>  train_ds,<br/>  validation_data=val_ds,<br/>  epochs=epochs<br/>)</span><span id="0945" class="lg jz hi lc b fi ll li l lj lk">Epoch 1/25<br/>12/12 [==============================] - 2s 76ms/step - loss: 0.9417 - binary_accuracy: 0.5580 - val_loss: 0.7153 - val_binary_accuracy: 0.5222<br/>Epoch 2/25<br/>12/12 [==============================] - 1s 61ms/step - loss: 0.6869 - binary_accuracy: 0.5338 - val_loss: 0.7236 - val_binary_accuracy: 0.5333<br/>Epoch 3/25<br/>12/12 [==============================] - 1s 61ms/step - loss: 0.6557 - binary_accuracy: 0.5985 - val_loss: 0.8124 - val_binary_accuracy: 0.5222<br/>Epoch 4/25<br/>12/12 [==============================] - 1s 61ms/step - loss: 0.6447 - binary_accuracy: 0.6315 - val_loss: 0.6829 - val_binary_accuracy: 0.5556<br/>Epoch 5/25<br/>12/12 [==============================] - 1s 65ms/step - loss: 0.6482 - binary_accuracy: 0.6273 - val_loss: 0.6708 - val_binary_accuracy: 0.5778<br/>Epoch 6/25<br/>12/12 [==============================] - 1s 61ms/step - loss: 0.6482 - binary_accuracy: 0.6348 - val_loss: 0.6733 - val_binary_accuracy: 0.5556<br/>Epoch 7/25<br/>12/12 [==============================] - 1s 61ms/step - loss: 0.6325 - binary_accuracy: 0.6592 - val_loss: 0.6762 - val_binary_accuracy: 0.5333<br/>Epoch 8/25<br/>12/12 [==============================] - 1s 62ms/step - loss: 0.5994 - binary_accuracy: 0.6680 - val_loss: 0.6587 - val_binary_accuracy: 0.6111<br/>Epoch 9/25<br/>12/12 [==============================] - 1s 61ms/step - loss: 0.6204 - binary_accuracy: 0.6904 - val_loss: 0.7240 - val_binary_accuracy: 0.5333<br/>Epoch 10/25<br/>12/12 [==============================] - 1s 62ms/step - loss: 0.6343 - binary_accuracy: 0.6480 - val_loss: 0.6776 - val_binary_accuracy: 0.5667<br/>Epoch 11/25<br/>12/12 [==============================] - 1s 62ms/step - loss: 0.6439 - binary_accuracy: 0.6107 - val_loss: 0.6811 - val_binary_accuracy: 0.5556<br/>Epoch 12/25<br/>12/12 [==============================] - 1s 62ms/step - loss: 0.6361 - binary_accuracy: 0.6301 - val_loss: 0.6612 - val_binary_accuracy: 0.6222<br/>Epoch 13/25<br/>12/12 [==============================] - 1s 62ms/step - loss: 0.6025 - binary_accuracy: 0.6949 - val_loss: 0.6725 - val_binary_accuracy: 0.5778<br/>Epoch 14/25<br/>12/12 [==============================] - 1s 61ms/step - loss: 0.5977 - binary_accuracy: 0.6868 - val_loss: 0.7521 - val_binary_accuracy: 0.5444<br/>Epoch 15/25<br/>12/12 [==============================] - 1s 62ms/step - loss: 0.5713 - binary_accuracy: 0.6833 - val_loss: 0.6427 - val_binary_accuracy: 0.6444<br/>Epoch 16/25<br/>12/12 [==============================] - 1s 62ms/step - loss: 0.5918 - binary_accuracy: 0.6939 - val_loss: 0.6515 - val_binary_accuracy: 0.6333<br/>Epoch 17/25<br/>12/12 [==============================] - 1s 61ms/step - loss: 0.5831 - binary_accuracy: 0.7253 - val_loss: 0.6556 - val_binary_accuracy: 0.5889<br/>Epoch 18/25<br/>12/12 [==============================] - 1s 62ms/step - loss: 0.5626 - binary_accuracy: 0.7121 - val_loss: 0.6877 - val_binary_accuracy: 0.5667<br/>Epoch 19/25<br/>12/12 [==============================] - 1s 62ms/step - loss: 0.5476 - binary_accuracy: 0.7327 - val_loss: 0.6398 - val_binary_accuracy: 0.6556<br/>Epoch 20/25<br/>12/12 [==============================] - 1s 62ms/step - loss: 0.5551 - binary_accuracy: 0.7283 - val_loss: 0.6465 - val_binary_accuracy: 0.6333<br/>Epoch 21/25<br/>12/12 [==============================] - 1s 62ms/step - loss: 0.5436 - binary_accuracy: 0.7312 - val_loss: 0.7083 - val_binary_accuracy: 0.5667<br/>Epoch 22/25<br/>12/12 [==============================] - 1s 65ms/step - loss: 0.5987 - binary_accuracy: 0.6781 - val_loss: 0.8078 - val_binary_accuracy: 0.5222<br/>Epoch 23/25<br/>12/12 [==============================] - 1s 62ms/step - loss: 0.5534 - binary_accuracy: 0.7139 - val_loss: 0.6705 - val_binary_accuracy: 0.6111<br/>Epoch 24/25<br/>12/12 [==============================] - 1s 85ms/step - loss: 0.5617 - binary_accuracy: 0.7406 - val_loss: 0.6471 - val_binary_accuracy: 0.6111<br/>Epoch 25/25<br/>12/12 [==============================] - 1s 62ms/step - loss: 0.5541 - binary_accuracy: 0.7303 - val_loss: 0.6263 - val_binary_accuracy: 0.7000</span></pre><h1 id="9c24" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">可视化培训结果</h1><p id="d765" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">在应用数据增加和减少后，过度拟合比以前少了，并且训练和验证准确性非常一致。</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="7a1b" class="lg jz hi lc b fi lh li l lj lk">print(history.history)<br/>acc = history.history['binary_accuracy']<br/>val_acc = history.history['val_binary_accuracy']</span><span id="c947" class="lg jz hi lc b fi ll li l lj lk">loss = history.history['loss']<br/>val_loss = history.history['val_loss']</span><span id="9946" class="lg jz hi lc b fi ll li l lj lk">epochs_range = range(epochs)</span><span id="00a9" class="lg jz hi lc b fi ll li l lj lk">plt.figure(figsize=(8, 8))<br/>plt.subplot(1, 2, 1)<br/>plt.plot(epochs_range, acc, label='Training Accuracy')<br/>plt.plot(epochs_range, val_acc, label='Validation Accuracy')<br/>plt.legend(loc='lower right')<br/>plt.title('Training and Validation Accuracy')</span><span id="2965" class="lg jz hi lc b fi ll li l lj lk">plt.subplot(1, 2, 2)<br/>plt.plot(epochs_range, loss, label='Training Loss')<br/>plt.plot(epochs_range, val_loss, label='Validation Loss')<br/>plt.legend(loc='upper right')<br/>plt.title('Training and Validation Loss')<br/>plt.show()</span><span id="ed7c" class="lg jz hi lc b fi ll li l lj lk">{'loss': [0.8289327025413513, 0.6810811758041382, 0.6626855731010437, 0.6704213619232178, 0.650795042514801, 0.6398268938064575, 0.6561762690544128, 0.6122907400131226, 0.6228107810020447, 0.6147234439849854, 0.6149318814277649, 0.6190508604049683, 0.607336699962616, 0.5861756801605225, 0.593088686466217, 0.6063793301582336, 0.5983501672744751, 0.5894269347190857, 0.5698645114898682, 0.5585014224052429, 0.5401753783226013, 0.5774908065795898, 0.5512883067131042, 0.5710932016372681, 0.5434897541999817], 'binary_accuracy': [0.5386740565299988, 0.5386740565299988, 0.6077347993850708, 0.6160221099853516, 0.6022099256515503, 0.6325966715812683, 0.6325966715812683, 0.6685082912445068, 0.6961326003074646, 0.6602209806442261, 0.6408839821815491, 0.6574585437774658, 0.6712707281112671, 0.6933701634407043, 0.6574585437774658, 0.6767956018447876, 0.7044199109077454, 0.6933701634407043, 0.7154695987701416, 0.7265193462371826, 0.7265193462371826, 0.6906077265739441, 0.7154695987701416, 0.7071823477745056, 0.7375690340995789], 'val_loss': [0.7153488993644714, 0.7235575318336487, 0.8124216794967651, 0.6829271912574768, 0.6708189249038696, 0.673344612121582, 0.676236629486084, 0.6586815714836121, 0.7239749431610107, 0.677582323551178, 0.6810950636863708, 0.6611502170562744, 0.6725294589996338, 0.7520950436592102, 0.642659068107605, 0.6514749526977539, 0.6556094884872437, 0.687703013420105, 0.639808714389801, 0.6464514136314392, 0.7082778811454773, 0.8077911138534546, 0.670492947101593, 0.6470986008644104, 0.6263118386268616], 'val_binary_accuracy': [0.5222222208976746, 0.5333333611488342, 0.5222222208976746, 0.5555555820465088, 0.5777778029441833, 0.5555555820465088, 0.5333333611488342, 0.6111111044883728, 0.5333333611488342, 0.5666666626930237, 0.5555555820465088, 0.6222222447395325, 0.5777778029441833, 0.5444444417953491, 0.644444465637207, 0.6333333253860474, 0.5888888835906982, 0.5666666626930237, 0.6555555462837219, 0.6333333253860474, 0.5666666626930237, 0.5222222208976746, 0.6111111044883728, 0.6111111044883728, 0.699999988079071]}</span></pre><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/d41efaba37c005cd6048ae08eea55adb.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/0*rU5-WKWC72VRd_q4.png"/></div></figure><h1 id="302d" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">根据新数据进行预测</h1><p id="d5db" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">最后，让我们使用我们的模型对测试数据集上未包含在训练集或验证集中的图像进行分类。</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="c149" class="lg jz hi lc b fi lh li l lj lk">predictWithTestDataset(model)</span><span id="54b7" class="lg jz hi lc b fi ll li l lj lk">number predictions=37<br/>Accuracy:0.7027027027027027</span></pre><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/43f99d130d47e60a9176860c0512e3fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*a6wDSmvxoYfkh29i.png"/></div></div></figure><h1 id="5418" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">迁移学习</h1><p id="c399" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">我们已经使用图像增强来尝试从我们的模型中获得更好的结果，我不得不说结果一点也不差。我们能够在验证数据集上得到一个准确率接近70%的模型。当然，我们可以做得更好，如果我们能够为我们的训练和验证数据集多收集数百张，也许数千张图像。</p><p id="b5be" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们当然可以这样做，但还有另一种方法，不涉及收集更多训练数据的繁琐和昂贵的过程:<strong class="ix hj">迁移学习</strong>。</p><p id="63ea" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过迁移学习，我们可以借用一个已经针对成千上万张图像训练过的模型，并针对我们的用例重新训练它，但使用的图像比我们从头开始训练模型时可能使用的图像要少得多。</p><p id="e61d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为此，我们可以使用Keras下载一个预训练模型，该模型具有已经在<strong class="ix hj"> Imagenet </strong>上训练过的Xception架构。</p><p id="7bd0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">要执行迁移学习，我们需要冻结基本模型的权重，并像平常一样执行训练。你会注意到，我们仍然做图像放大和正则化。</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="96be" class="lg jz hi lc b fi lh li l lj lk">base_model = keras.applications.Xception(<br/>    weights="imagenet",  # Load weights pre-trained on ImageNet.<br/>    input_shape=(img_height, img_width, 3),<br/>    include_top=False,<br/>)  # Do not include the ImageNet classifier at the top.</span><span id="09b8" class="lg jz hi lc b fi ll li l lj lk"># Freeze the base_model<br/>base_model.trainable = False</span><span id="b9ef" class="lg jz hi lc b fi ll li l lj lk"># Create new model on top<br/>inputs = keras.Input(shape=(img_height, img_width, 3))<br/>x = data_augmentation(inputs)  # Apply random data augmentation</span><span id="1eed" class="lg jz hi lc b fi ll li l lj lk"># Pre-trained Xception weights requires that input be normalized<br/># from (0, 255) to a range (-1., +1.), the normalization layer<br/># does the following, outputs = (inputs - mean) / sqrt(var)<br/>norm_layer = keras.layers.experimental.preprocessing.Normalization()<br/>mean = np.array([127.5] * 3)<br/>var = mean ** 2<br/># Scale inputs to [-1, +1]<br/>x = norm_layer(x)<br/>norm_layer.set_weights([mean, var])</span><span id="d3c9" class="lg jz hi lc b fi ll li l lj lk"># The base model contains batchnorm layers. We want to keep them in inference mode<br/># when we unfreeze the base model for fine-tuning, so we make sure that the<br/># base_model is running in inference mode here.<br/>x = base_model(x, training=False)<br/>x = keras.layers.GlobalAveragePooling2D()(x)<br/>x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout<br/>outputs = keras.layers.Dense(1, activation="sigmoid")(x)<br/>model = keras.Model(inputs, outputs)</span><span id="96c0" class="lg jz hi lc b fi ll li l lj lk">model.summary()</span><span id="4734" class="lg jz hi lc b fi ll li l lj lk">Model: "model_13"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_28 (InputLayer)        [(None, 300, 300, 3)]     0         <br/>_________________________________________________________________<br/>sequential_32 (Sequential)   (None, 300, 300, 3)       0         <br/>_________________________________________________________________<br/>normalization_13 (Normalizat (None, 300, 300, 3)       7         <br/>_________________________________________________________________<br/>xception (Functional)        (None, 10, 10, 2048)      20861480  <br/>_________________________________________________________________<br/>global_average_pooling2d_13  (None, 2048)              0         <br/>_________________________________________________________________<br/>dropout_28 (Dropout)         (None, 2048)              0         <br/>_________________________________________________________________<br/>dense_59 (Dense)             (None, 1)                 2049      <br/>=================================================================<br/>Total params: 20,863,536<br/>Trainable params: 2,049<br/>Non-trainable params: 20,861,487<br/>_________________________________________________________________</span><span id="0cf8" class="lg jz hi lc b fi ll li l lj lk"># model.compile(optimizer=keras.optimizers.Adam(),<br/>#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),<br/>#               metrics=['accuracy'])<br/>model.compile(<br/>    optimizer=keras.optimizers.Adam(),<br/>    loss=keras.losses.BinaryCrossentropy(from_logits=True),<br/>    metrics=[keras.metrics.BinaryAccuracy()],<br/>)</span><span id="f6d2" class="lg jz hi lc b fi ll li l lj lk">epochs = 25<br/>history = model.fit(<br/>  train_ds,<br/>  validation_data=val_ds,<br/>  epochs=epochs<br/>)</span><span id="6918" class="lg jz hi lc b fi ll li l lj lk">Epoch 1/25<br/>12/12 [==============================] - 7s 376ms/step - loss: 0.6989 - binary_accuracy: 0.5116 - val_loss: 0.6324 - val_binary_accuracy: 0.5333<br/>Epoch 2/25<br/>12/12 [==============================] - 4s 322ms/step - loss: 0.6106 - binary_accuracy: 0.5943 - val_loss: 0.5748 - val_binary_accuracy: 0.6222<br/>Epoch 3/25<br/>12/12 [==============================] - 4s 322ms/step - loss: 0.5557 - binary_accuracy: 0.6647 - val_loss: 0.5378 - val_binary_accuracy: 0.6889<br/>Epoch 4/25<br/>12/12 [==============================] - 4s 326ms/step - loss: 0.5280 - binary_accuracy: 0.6333 - val_loss: 0.5127 - val_binary_accuracy: 0.7222<br/>Epoch 5/25<br/>12/12 [==============================] - 4s 329ms/step - loss: 0.4751 - binary_accuracy: 0.7638 - val_loss: 0.4912 - val_binary_accuracy: 0.7889<br/>Epoch 6/25<br/>12/12 [==============================] - 4s 331ms/step - loss: 0.4586 - binary_accuracy: 0.7535 - val_loss: 0.4775 - val_binary_accuracy: 0.7556<br/>Epoch 7/25<br/>12/12 [==============================] - 4s 335ms/step - loss: 0.4328 - binary_accuracy: 0.7778 - val_loss: 0.4625 - val_binary_accuracy: 0.8111<br/>Epoch 8/25<br/>12/12 [==============================] - 4s 339ms/step - loss: 0.3951 - binary_accuracy: 0.8387 - val_loss: 0.4519 - val_binary_accuracy: 0.8111<br/>Epoch 9/25<br/>12/12 [==============================] - 4s 344ms/step - loss: 0.3745 - binary_accuracy: 0.8427 - val_loss: 0.4435 - val_binary_accuracy: 0.8111<br/>Epoch 10/25<br/>12/12 [==============================] - 4s 348ms/step - loss: 0.3631 - binary_accuracy: 0.8373 - val_loss: 0.4395 - val_binary_accuracy: 0.7889<br/>Epoch 11/25<br/>12/12 [==============================] - 4s 350ms/step - loss: 0.3449 - binary_accuracy: 0.8705 - val_loss: 0.4302 - val_binary_accuracy: 0.8111<br/>Epoch 12/25<br/>12/12 [==============================] - 4s 355ms/step - loss: 0.3409 - binary_accuracy: 0.8623 - val_loss: 0.4249 - val_binary_accuracy: 0.8222<br/>Epoch 13/25<br/>12/12 [==============================] - 4s 356ms/step - loss: 0.3491 - binary_accuracy: 0.8848 - val_loss: 0.4214 - val_binary_accuracy: 0.8333<br/>Epoch 14/25<br/>12/12 [==============================] - 4s 356ms/step - loss: 0.3522 - binary_accuracy: 0.8569 - val_loss: 0.4173 - val_binary_accuracy: 0.8333<br/>Epoch 15/25<br/>12/12 [==============================] - 4s 354ms/step - loss: 0.3106 - binary_accuracy: 0.8641 - val_loss: 0.4120 - val_binary_accuracy: 0.8333<br/>Epoch 16/25<br/>12/12 [==============================] - 4s 348ms/step - loss: 0.3108 - binary_accuracy: 0.8973 - val_loss: 0.4059 - val_binary_accuracy: 0.8333<br/>Epoch 17/25<br/>12/12 [==============================] - 4s 348ms/step - loss: 0.3041 - binary_accuracy: 0.8840 - val_loss: 0.4043 - val_binary_accuracy: 0.8333<br/>Epoch 18/25<br/>12/12 [==============================] - 4s 364ms/step - loss: 0.3106 - binary_accuracy: 0.8548 - val_loss: 0.3994 - val_binary_accuracy: 0.8444<br/>Epoch 19/25<br/>12/12 [==============================] - 4s 343ms/step - loss: 0.3072 - binary_accuracy: 0.8774 - val_loss: 0.4031 - val_binary_accuracy: 0.8333<br/>Epoch 20/25<br/>12/12 [==============================] - 4s 341ms/step - loss: 0.3008 - binary_accuracy: 0.8870 - val_loss: 0.3960 - val_binary_accuracy: 0.8444<br/>Epoch 21/25<br/>12/12 [==============================] - 4s 342ms/step - loss: 0.2959 - binary_accuracy: 0.8738 - val_loss: 0.3969 - val_binary_accuracy: 0.8444<br/>Epoch 22/25<br/>12/12 [==============================] - 4s 340ms/step - loss: 0.2655 - binary_accuracy: 0.8874 - val_loss: 0.3959 - val_binary_accuracy: 0.8444<br/>Epoch 23/25<br/>12/12 [==============================] - 4s 340ms/step - loss: 0.2452 - binary_accuracy: 0.9098 - val_loss: 0.3957 - val_binary_accuracy: 0.8444<br/>Epoch 24/25<br/>12/12 [==============================] - 4s 359ms/step - loss: 0.2532 - binary_accuracy: 0.9214 - val_loss: 0.3906 - val_binary_accuracy: 0.8444<br/>Epoch 25/25<br/>12/12 [==============================] - 4s 340ms/step - loss: 0.2512 - binary_accuracy: 0.9202 - val_loss: 0.3963 - val_binary_accuracy: 0.8556</span><span id="88ba" class="lg jz hi lc b fi ll li l lj lk">print(history.history)<br/>acc = history.history['binary_accuracy']<br/>val_acc = history.history['val_binary_accuracy']</span><span id="a7c2" class="lg jz hi lc b fi ll li l lj lk">loss = history.history['loss']<br/>val_loss = history.history['val_loss']</span><span id="8797" class="lg jz hi lc b fi ll li l lj lk">epochs_range = range(epochs)</span><span id="35b2" class="lg jz hi lc b fi ll li l lj lk">plt.figure(figsize=(8, 8))<br/>plt.subplot(1, 2, 1)<br/>plt.plot(epochs_range, acc, label='Training Accuracy')<br/>plt.plot(epochs_range, val_acc, label='Validation Accuracy')<br/>plt.legend(loc='lower right')<br/>plt.title('Training and Validation Accuracy')</span><span id="4b97" class="lg jz hi lc b fi ll li l lj lk">plt.subplot(1, 2, 2)<br/>plt.plot(epochs_range, loss, label='Training Loss')<br/>plt.plot(epochs_range, val_loss, label='Validation Loss')<br/>plt.legend(loc='upper right')<br/>plt.title('Training and Validation Loss')<br/>plt.show()</span><span id="cdbe" class="lg jz hi lc b fi ll li l lj lk">{'loss': [0.6730840802192688, 0.6024077534675598, 0.5369390249252319, 0.5121317505836487, 0.47049736976623535, 0.43795397877693176, 0.4308575689792633, 0.409035861492157, 0.39621278643608093, 0.38378414511680603, 0.36360302567481995, 0.3390505313873291, 0.33790066838264465, 0.34375235438346863, 0.3241599202156067, 0.3240811824798584, 0.30652204155921936, 0.3121297359466553, 0.29941326379776, 0.31102144718170166, 0.29046544432640076, 0.2721157371997833, 0.2742222845554352, 0.273193895816803, 0.2644941210746765], 'binary_accuracy': [0.541436493396759, 0.580110490322113, 0.7099447250366211, 0.6546961069107056, 0.7762430906295776, 0.7817679643630981, 0.7734806537628174, 0.8121547102928162, 0.8093922734260559, 0.8066298365592957, 0.8563535809516907, 0.8535911440849304, 0.8701657652854919, 0.8618784546852112, 0.8535911440849304, 0.889502763748169, 0.8812154531478882, 0.8729282021522522, 0.8784530162811279, 0.8674033284187317, 0.8839778900146484, 0.8784530162811279, 0.8839778900146484, 0.9171270728111267, 0.8950276374816895], 'val_loss': [0.6324149966239929, 0.5748280882835388, 0.537817120552063, 0.5126944184303284, 0.4911538362503052, 0.47753846645355225, 0.4625410735607147, 0.45192599296569824, 0.44351860880851746, 0.4394892752170563, 0.430193156003952, 0.42491695284843445, 0.42138800024986267, 0.4172518849372864, 0.4119878113269806, 0.40589141845703125, 0.40429064631462097, 0.399353951215744, 0.40307578444480896, 0.39602288603782654, 0.39687782526016235, 0.39587682485580444, 0.39574024081230164, 0.39059093594551086, 0.3963099718093872], 'val_binary_accuracy': [0.5333333611488342, 0.6222222447395325, 0.6888889074325562, 0.7222222089767456, 0.7888888716697693, 0.7555555701255798, 0.8111110925674438, 0.8111110925674438, 0.8111110925674438, 0.7888888716697693, 0.8111110925674438, 0.8222222328186035, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8444444537162781, 0.8333333134651184, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.855555534362793]}</span></pre><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/438480106665e3d518e7bfc933d09f73.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/0*6Dh_5-NaxGRGgXy9.png"/></div></figure><p id="8579" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">根据测试数据集测试模型</p><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="9a9a" class="lg jz hi lc b fi lh li l lj lk">predictWithTestDataset(model)</span><span id="0100" class="lg jz hi lc b fi ll li l lj lk">WARNING:tensorflow:5 out of the last 12 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7f58803470d0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.</span><span id="23b7" class="lg jz hi lc b fi ll li l lj lk">number predictions=37<br/>Accuracy:0.7837837837837838</span></pre><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/86410a7e72b3d382d7b796a114a645ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ICC5EX4Ok278kaHv.png"/></div></div></figure><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="bb01" class="lg jz hi lc b fi lh li l lj lk"># Unfreeze the base_model. Note that it keeps running in inference mode<br/># since we passed `training=False` when calling it. This means that<br/># the batchnorm layers will not update their batch statistics.<br/># This prevents the batchnorm layers from undoing all the training<br/># we've done so far.<br/>base_model.trainable = True<br/>model.summary()</span><span id="448c" class="lg jz hi lc b fi ll li l lj lk">model.compile(<br/>    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate<br/>    loss=keras.losses.BinaryCrossentropy(from_logits=True),<br/>    metrics=[keras.metrics.BinaryAccuracy()],<br/>)</span><span id="db4e" class="lg jz hi lc b fi ll li l lj lk">epochs = 2<br/>history = model.fit(train_ds, epochs=epochs, validation_data=val_ds)</span><span id="921d" class="lg jz hi lc b fi ll li l lj lk">Model: "model_13"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_28 (InputLayer)        [(None, 300, 300, 3)]     0         <br/>_________________________________________________________________<br/>sequential_32 (Sequential)   (None, 300, 300, 3)       0         <br/>_________________________________________________________________<br/>normalization_13 (Normalizat (None, 300, 300, 3)       7         <br/>_________________________________________________________________<br/>xception (Functional)        (None, 10, 10, 2048)      20861480  <br/>_________________________________________________________________<br/>global_average_pooling2d_13  (None, 2048)              0         <br/>_________________________________________________________________<br/>dropout_28 (Dropout)         (None, 2048)              0         <br/>_________________________________________________________________<br/>dense_59 (Dense)             (None, 1)                 2049      <br/>=================================================================<br/>Total params: 20,863,536<br/>Trainable params: 20,809,001<br/>Non-trainable params: 54,535<br/>_________________________________________________________________<br/>Epoch 1/2<br/>12/12 [==============================] - 19s 1s/step - loss: 0.1918 - binary_accuracy: 0.9218 - val_loss: 0.3842 - val_binary_accuracy: 0.8556<br/>Epoch 2/2<br/>12/12 [==============================] - 16s 1s/step - loss: 0.1469 - binary_accuracy: 0.9509 - val_loss: 0.3520 - val_binary_accuracy: 0.8556</span><span id="274b" class="lg jz hi lc b fi ll li l lj lk">print(history.history)<br/>acc = history.history['binary_accuracy']<br/>val_acc = history.history['val_binary_accuracy']</span><span id="ea5a" class="lg jz hi lc b fi ll li l lj lk">loss = history.history['loss']<br/>val_loss = history.history['val_loss']</span><span id="023f" class="lg jz hi lc b fi ll li l lj lk">epochs_range = range(epochs)</span><span id="dca8" class="lg jz hi lc b fi ll li l lj lk">plt.figure(figsize=(8, 8))<br/>plt.subplot(1, 2, 1)<br/>plt.plot(epochs_range, acc, label='Training Accuracy')<br/>plt.plot(epochs_range, val_acc, label='Validation Accuracy')<br/>plt.legend(loc='lower right')<br/>plt.title('Training and Validation Accuracy')</span><span id="5a0d" class="lg jz hi lc b fi ll li l lj lk">plt.subplot(1, 2, 2)<br/>plt.plot(epochs_range, loss, label='Training Loss')<br/>plt.plot(epochs_range, val_loss, label='Validation Loss')<br/>plt.legend(loc='upper right')<br/>plt.title('Training and Validation Loss')<br/>plt.show()</span><span id="c714" class="lg jz hi lc b fi ll li l lj lk">{'loss': [0.1816166639328003, 0.14842070639133453], 'binary_accuracy': [0.9226519465446472, 0.950276255607605], 'val_loss': [0.38415849208831787, 0.3520398437976837], 'val_binary_accuracy': [0.855555534362793, 0.855555534362793]}</span></pre><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/93b65d0bf83c38e8718b42e21f97e070.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/0*C1UgElPaM0KArUUW.png"/></div></figure><pre class="ju jv jw jx fd lb lc ld le aw lf bi"><span id="628f" class="lg jz hi lc b fi lh li l lj lk">predictWithTestDataset(model)</span><span id="4fff" class="lg jz hi lc b fi ll li l lj lk">WARNING:tensorflow:6 out of the last 13 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7f58f13847b8&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.<br/>number predictions=37<br/>Accuracy:0.8378378378378378</span></pre><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="ab fe cl mn"><img src="../Images/8bd36827af2b0e10ae35cba953b660e0.png" data-original-src="https://miro.medium.com/v2/format:webp/0*6p2MHj_0zS8OS57L.png"/></div></figure><h1 id="f1e9" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">结论</h1><p id="6a64" class="pw-post-body-paragraph iv iw hi ix b iy kw ja jb jc kx je jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">我们已经能够用有限的数据集获得一些非常好的结果。这确实很有前途！神经网络似乎能够根据主题对艺术收藏品进行分类。</p><p id="793a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有许多方法可以进一步改善这些结果，从收集更多的图像，试验不同的图像大小，甚至尝试新的模型架构。</p><p id="d04f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">希望这篇文章对你有用。希望下次能见到你。编码快乐！</p><figure class="ju jv jw jx fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="3c01" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">资源</p><div class="mq mr ez fb ms mt"><a href="https://github.com/armindocachada/tensorflow-custom-classification-art-nativity" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hj fi z dy my ea eb mz ed ef hh bi translated">armindocachada/tensor flow-定制-分类-艺术-诞生</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">我可以教一个使用Tensorflow物体检测API的AI模型，如何识别一幅画的主题吗？唯一的方法是…</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">github.com</p></div></div></div></a></div><div class="mq mr ez fb ms mt"><a href="https://www.tensorflow.org/tutorials/images/classification" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hj fi z dy my ea eb mz ed ef hh bi translated">图像分类|张量流核心</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">当有少量训练样本时，模型有时会从噪声或不需要的细节中学习…</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">www.tensorflow.org</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh io mt"/></div></div></a></div><div class="mq mr ez fb ms mt"><a href="https://www.tensorflow.org/tutorials/images/transfer_learning" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hj fi z dy my ea eb mz ed ef hh bi translated">迁移学习和微调| TensorFlow核心</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">此外，您应该尝试微调一小部分顶层，而不是整个MobileNet模型。在大多数情况下…</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">www.tensorflow.org</p></div></div><div class="nc l"><div class="ni l ne nf ng nc nh io mt"/></div></div></a></div><figure class="ju jv jw jx fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure><figure class="ju jv jw jx fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure></div></div>    
</body>
</html>
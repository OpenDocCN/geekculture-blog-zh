<html>
<head>
<title>Locality Sensitive Hashing for Fast Search in High Dimension Data.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高维数据中快速搜索的位置敏感哈希算法。</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/locality-sensitive-hashing-for-fast-search-in-igh-dimension-data-a2cdef1b6eff?source=collection_archive---------3-----------------------#2021-03-25">https://medium.com/geekculture/locality-sensitive-hashing-for-fast-search-in-igh-dimension-data-a2cdef1b6eff?source=collection_archive---------3-----------------------#2021-03-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f662dc6ef1737caa130ec92070aed09a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sG5Xelv1JAREPC-zdpGX2w.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">image credits: Christopher Burns @ Unsplash</figcaption></figure><h1 id="a33e" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">现实世界的问题</h1><p id="8698" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在许多现实世界的问题中，我们必须处理和分析大量的文本数据。例如:文本挖掘、垃圾邮件过滤、产品推荐、在线广告等..</p><p id="221b" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">这种数据通常以高维度为特征，例如:谷歌新闻模型是在约1000亿词的数据集上训练的。该模型包含300维向量，包含300万个单词和短语。</p><p id="3aae" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">在某些情况下，我们会分析大量文档，以识别高维数据中的类似项目，例如:</p><ul class=""><li id="3166" class="kv kw hi ju b jv kq jz kr kd kx kh ky kl kz kp la lb lc ld bi translated">网络搜索和文本挖掘</li><li id="86e6" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">文件分类</li><li id="8a3d" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">剽窃</li><li id="0e07" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">聊天机器人</li></ul><p id="3e71" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">所有这些问题的一个共同问题是在高维空间中寻找最近的邻居。在某些情况下，我们感兴趣的是一组最近的邻居，而不是精确的匹配。</p><p id="0821" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">我们将关注位置敏感散列(LSH ),这是一种可以在高维数据中使用的技术，用于在具有几乎恒定的查找时间的大规模数据库中寻找最近的邻居。</p><p id="7da8" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">LSH是一种通用的算法，可应用于无数问题，包括:</p><ul class=""><li id="a505" class="kv kw hi ju b jv kq jz kr kd kx kh ky kl kz kp la lb lc ld bi translated"><strong class="ju hj">近似重复检测</strong> : LSH通常用于消除大量文档、网页和其他文件的重复数据。</li><li id="7fa1" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated"><strong class="ju hj">全基因组关联研究</strong>:生物学家经常使用LSH来识别基因组数据库中相似的基因表达。</li><li id="9a59" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated"><strong class="ju hj">大规模图片搜索</strong>:谷歌使用LSH和<a class="ae lj" href="https://en.wikipedia.org/wiki/PageRank" rel="noopener ugc nofollow" target="_blank"> PageRank </a>来构建他们的图片搜索技术<a class="ae lj" href="https://research.google.com/pubs/pub34634.html" rel="noopener ugc nofollow" target="_blank"> VisualRank </a>。</li><li id="7ffb" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated"><strong class="ju hj">音频/视频指纹</strong>:在多媒体技术中，LSH被广泛用作音频/视频数据的指纹技术。</li><li id="4f99" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated"><strong class="ju hj">欺诈检测</strong>:优步利用LSH快速检测平台滥用，从垃圾邮件到虚假账户和支付欺诈</li></ul><h1 id="ddd1" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">将文档表示为数字向量</h1><p id="db85" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">向量空间是自然语言处理中许多应用的基础。如果我们想要表示文档，以便我们可以应用数学模型来分析内容，我们需要将它编码为一个向量，其中维度是构成文档的单词或n-grams。</p><p id="2e7b" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">让我们用一个例子来理解。考虑下面的语句和一个查询词。这些声明在下文中称为文件。</p><ul class=""><li id="5176" class="kv kw hi ju b jv kq jz kr kd kx kh ky kl kz kp la lb lc ld bi translated"><em class="lk">文件一:猫追老鼠</em></li><li id="a31a" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated"><em class="lk">文件二:狗追猫</em></li><li id="e298" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated"><em class="lk">查询:谁在追老鼠</em></li></ul><p id="341e" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">在对文档进行预处理后，我们将它们表示为单词向量</p><ul class=""><li id="04fa" class="kv kw hi ju b jv kq jz kr kd kx kh ky kl kz kp la lb lc ld bi translated"><em class="lk">文件1:(猫，追逐，老鼠)</em></li><li id="19e4" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated"><em class="lk">文件二:(狗，追逐，猫)</em></li><li id="aa36" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated"><em class="lk">查询:(谁，追，鼠)</em></li></ul><p id="db7e" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">要检索的相关文档将是。</p><p id="f1bf" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated"><em class="lk">相似性得分(文档1，查询)&gt;相似性得分(文档2，查询)</em></p><p id="e555" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">我们可以将上述文档向量以数字格式表示为术语文档矩阵。</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/5a67d71774261a848bc4b8ad0865af38.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*gS6pNOzQaaRFZAiwuNUDWw.png"/></div></figure><p id="e138" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">在创建术语文档矩阵时，还有一些额外的步骤，如词干提取、词汇匹配、TF-IDF，为了简单起见，我们将跳过这些步骤。</p><p id="87f6" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">现在想象一下，如果我们想要绘制真实世界的数据，例如整个维基百科语料库有大约30亿个单词，而谷歌新闻数据集有超过1000亿个单词。你可以在https://code.google.com/archive/p/word2vec/的<a class="ae lj" href="https://code.google.com/archive/p/word2vec/" rel="noopener ugc nofollow" target="_blank">找到这两个数据集</a></p><p id="e533" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">维度的数量将大大增加，使得将查询与每个可能的文档进行比较以找到最相关的文档在计算上不可行或效率低下。</p><h1 id="9f09" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">维度的诅咒</h1><p id="645b" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">维数灾难指的是在分类、组织和分析高维数据时出现的问题，这些问题在低维空间中不会出现，特别是数据稀疏性和数据邻近性的问题。</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lq"><img src="../Images/b4dac59f09d72e48c0c4395abee41a2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sKDiZlFDCWDbKZI2pVtQ2g.png"/></div></div></figure><ul class=""><li id="bd50" class="kv kw hi ju b jv kq jz kr kd kx kh ky kl kz kp la lb lc ld bi translated">随着嵌入这些点的维数的增加，点与点之间的平均距离不断增加</li><li id="9e64" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">随着词汇量的增加，矩阵变得越来越稀疏，因为大多数文档不包含大多数术语。</li></ul><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/0285ddb0df70e7d8f0c4aff187d79ac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/1*waR6G0d24cwolFZCnENtMA.gif"/></div></figure><p id="30b1" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">一种解决维数灾难问题的方法是寻找近似结果，而不是搜索精确结果。在许多不需要100%准确性的应用中，搜索足够接近的结果比搜索精确的结果要快得多</p><h1 id="0960" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">位置敏感散列法</h1><p id="f9fc" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated"><em class="lk">位置敏感散列法</em> (LSH)是一套技术，可以显著加快数据的邻居搜索或近似重复检测。为了理解算法，让我们首先理解我们所说的散列和对位置敏感是什么意思。</p><p id="313f" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated"><strong class="ju hj">什么是哈希</strong></p><p id="98ae" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">散列函数的一个传统用途是在<em class="lk">散列表</em>中。提醒一下，哈希表中使用的哈希函数被设计为将一段数据映射到一个整数，该整数可用于在哈希表中的特定<em class="lk">桶</em>中查找，以检索或删除该元素。</p><p id="bd1d" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">您可以将哈希函数看作是一个接受任意大小的数据并将其映射到固定值的函数。返回的值被称为<em class="lk">哈希值</em>甚至<em class="lk">哈希值</em>。</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ls"><img src="../Images/03dca04141dbf8bb7389bc6bf3c9da92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gPZJrpqYunL8NwlOc-3GBQ.png"/></div></div></figure><p id="f1b0" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">上图显示了一个非常简单的散列函数的例子，它接受一个向量并返回一个散列值。然后，为了将这个散列值分配给一个桶，我们取模数值。</p><p id="98a4" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">现在给定一个输入，你不需要将它与所有其他的例子进行比较，你只需要将它与输入被散列到的同一个<em class="lk"> hash_bucket </em>中的所有值进行比较。</p><p id="57d3" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">在散列文档时，您希望将相似的文档散列到同一个桶中。用于比较文档的LSH技术之一是余弦相似度。</p><p id="40a8" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated"><strong class="ju hj">什么是区分位置的</strong></p><p id="126a" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">一个词的意义取决于上下文或该词使用的“位置”。</p><p id="847d" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">例如，如果像“皮”或“馅饼”这样的词也出现在当地，那么“苹果”指的是水果。但是如果诸如“iphone”或“computer”之类的词在附近，则可以指消费者技术。但是，“在本地”在低维环境中是一回事，在高维环境中又是另一回事。</p><p id="757c" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">对位置敏感的散列函数被设计为，对于两个距离<em class="lk">很近的输入值</em>，散列值冲突的可能性<em class="lk">大于距离很远的输入值</em>。对于不同的数据类型和不同的“靠近”定义，LSH函数有不同的实现方式。</p><p id="644b" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">根据数据类型，有不同的本地敏感散列函数:</p><ol class=""><li id="83ad" class="kv kw hi ju b jv kq jz kr kd kx kh ky kl kz kp lt lb lc ld bi translated">比特采样LSH(汉明距离)</li><li id="da74" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp lt lb lc ld bi translated">明哈什LSH(雅克卡德相似性)</li><li id="1b84" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp lt lb lc ld bi translated">欧几里得和曼哈顿LSH(欧几里得(L2)和曼哈顿(L1)的距离)</li><li id="c150" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp lt lb lc ld bi translated">聚类或K均值LSH(通过K均值学习哈希函数)</li><li id="a295" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp lt lb lc ld bi translated">有符号随机投影(余弦相似性)</li></ol><p id="06fd" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">一会儿我们将仔细看看余弦相似性的带符号随机投影LSH。</p><p id="98c8" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">具有相似内容和上下文的文档将具有相同的散列值，从而映射到相同或附近的散列桶。</p><p id="1995" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated"><strong class="ju hj">LSH的动机</strong></p><p id="08f2" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">假设我们需要在N = 100万个文档中找到近似重复的文档。</p><p id="ac6d" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">天真地说，我们必须为每一对文档计算成对的相似性，即N(N-1)/2 ≈ 5*10次比较。即使使用最先进的处理器，这也需要几天的时间来计算。</p><p id="0ae9" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">我们可以通过减少需要计算相似度的候选项来加快计算速度。这可以通过将文档向量散列到桶中来实现。</p><ul class=""><li id="94c9" class="kv kw hi ju b jv kq jz kr kd kx kh ky kl kz kp la lb lc ld bi translated">给定文件D1和D2</li><li id="5b83" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">如果我们能找到一个散列函数h，使得:</li><li id="ddb9" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">如果sim(D1，D2)很高，那么h(D1) = h(D2)的概率很高</li><li id="b84a" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">如果sim(D1，D2)低，那么h(D1)高概率≠ h(D2)</li><li id="209b" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">然后，我们可以将文档散列到桶中，并期望“大多数”接近重复的文档对将散列到同一个桶中，然后我们在每个桶中有一组候选文档对，以查看它们是否真的相似。</li></ul><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/6ad6d4ee68374d4ee90410f4d32d498e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*pATxCiHEVEa_s0vSwl8Q_w.gif"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Dividing vector space into hash buckets to reduce the number of candidate pairs to compare</figcaption></figure><h1 id="3d54" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated"><strong class="ak">文档相似度的余弦距离</strong></h1><p id="2e9d" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">余弦相似性是一种度量，用于确定文档的相似程度，而不考虑它们的大小。</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/b8cffa2037e60409a036fab8deb8acb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*SHaAokipYJRbfRSYgtZuwQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">As simplified representation of Documents and Query vectors in 2D vector space</figcaption></figure><p id="0624" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">在数学上，它测量的是在多维空间中投影的两个向量之间的角度余弦。</p><p id="486d" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">在我们的上下文中，向量空间由词汇表中的单词组成，向量是这些文档的数字表示。</p><p id="0be1" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">向量之间的余弦可以使用以下公式计算。</p><p id="43f2" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">余弦相似性函数表示为:</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/2e29251f4fc0f8b51fc2c8f8079b55be.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*jSlVcAHC9Ofz6WAcqHGkag.png"/></div></figure><p id="c202" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">𝐴和𝐵代表单词向量，𝐴𝑖或𝐵𝑖代表该向量的索引I。&amp;注意，如果a和b相同，你会得到𝑐𝑜𝑠(𝜃)=1.</p><ul class=""><li id="c41e" class="kv kw hi ju b jv kq jz kr kd kx kh ky kl kz kp la lb lc ld bi translated">否则，如果他们是完全相反的，意思是，𝐴=−𝐵，那么你会得到𝑐𝑜𝑠(𝜃)=−1.</li><li id="38b9" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">如果你得到𝑐𝑜𝑠(𝜃)=0，这意味着它们是正交的。</li><li id="6b95" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">介于0和1之间的数字表示相似性得分。</li><li id="5443" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">介于-1到0之间的数字表示相异分数。</li></ul><h1 id="33aa" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">余弦相似性的LSH</h1><p id="abaa" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">有符号随机投影(SRP)输出二进制值，但SRP对向量之间的角距离很敏感。</p><p id="a13b" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">与我们一直使用的典型桶不同，您可以考虑通过确定这些点是在线上还是线下来对它们进行聚类。现在，当我们进入更高维度时(比如说n维向量)，你将使用平面而不是直线。</p><p id="9ebe" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated"><strong class="ju hj">余弦距离的签名</strong></p><p id="423d" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">选择一些随机向量，并为每个向量散列你的数据。结果是每个数据点的+1和–1的签名(草图)</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/11f4b63ed544c2b07b3a0bc813eb6c42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*k1288_zv0nnhqT-QWGVz0w.gif"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Hash signature for two document vectors</figcaption></figure><p id="0f24" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">这个函数本质上是将空间一分为二，并将一半的点指定为1，另一半的点指定为-1。决策边界基于投影向量，投影向量与超平面边界正交。</p><p id="1fb4" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">然后可以通过使用AND和OR结构或散列函数来放大签名。</p><p id="7179" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">下图为为什么有符号随机投影对角度敏感提供了直观的解释。</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/2316d1d4061e0a4dd07c534e18d9ac5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cuZe9oserCIqknBVUv35LA.png"/></div></div></figure><p id="430d" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">h(v1)≠h(v2)的概率是随机选择的一条线落在<strong class="ju hj"> V1 </strong>和<strong class="ju hj"> V2 </strong>之间的概率。</p><p id="6ee2" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">如果我们随机选取一个角度，有θ/π的几率它落在两点之间并把它们分开。</p><p id="ee57" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">最后，我们可以识别相同桶中的近似相似的文档，从而减少用于比较的候选对的数量和问题的维度。</p><p id="eb8d" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">按照上述过程，我们可以列出在大型文本语料库中使用LSH进行相似性搜索所涉及的步骤。</p><ol class=""><li id="c8b9" class="kv kw hi ju b jv kq jz kr kd kx kh ky kl kz kp lt lb lc ld bi translated">将所有文档映射到向量空间模型中</li><li id="9488" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp lt lb lc ld bi translated">构建LSH散列表，将这些文档分组。</li><li id="2455" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp lt lb lc ld bi translated">对于新的查询，构造特征向量并计算查询的散列</li><li id="d37f" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp lt lb lc ld bi translated">基于散列值，在与查询相同的散列桶中识别候选对。</li><li id="3f3f" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp lt lb lc ld bi translated">对于每个候选对，计算文档和查询之间的距离度量</li><li id="345a" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp lt lb lc ld bi translated">最后，具有最小(阈值)距离的结果是与查询最相关的文档。</li></ol><h1 id="30c7" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated"><strong class="ak">结论</strong></h1><p id="bc0f" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">我们发展了对本地敏感散列的直觉和对算法背后的数学的高度理解。</p><p id="2527" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">我们可以看到LSH是如何成为搜索或查询大规模文本数据的高效技术的。LSH可以处理高维数据，有效地降低维数，加快搜索速度。</p><p id="165e" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">许多现实世界的应用程序使用LSH来查询或搜索大规模的大型数据库。</p><h1 id="3b22" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated"><strong class="ak">参考文献</strong>:</h1><ol class=""><li id="ec63" class="kv kw hi ju b jv jw jz ka kd ly kh lz kl ma kp lt lb lc ld bi translated"><a class="ae lj" href="https://web.stanford.edu/class/cs345a/slides/05-LSH.pdf" rel="noopener ugc nofollow" target="_blank">https://web.stanford.edu/class/cs345a/slides/05-LSH.pdf</a></li><li id="5cdc" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp lt lb lc ld bi translated">https://web.stanford.edu/class/cs345a/slides/05-LSH.pdf<a class="ae lj" href="https://web.stanford.edu/class/cs345a/slides/05-LSH.pdf" rel="noopener ugc nofollow" target="_blank"/></li><li id="f702" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp lt lb lc ld bi translated"><a class="ae lj" href="http://www.cs.jhu.edu/~vandurme/papers/VanDurmeLallACL10-slides.pdf" rel="noopener ugc nofollow" target="_blank">http://www . cs . jhu . edu/~ vandurme/papers/vandurmelallacl 10-slides . pdf</a></li><li id="1a29" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp lt lb lc ld bi translated"><a class="ae lj" href="https://www2.cs.duke.edu/courses/fall15/compsci590.4/slides/compsci590.04_fall15_lec12.pdf" rel="noopener ugc nofollow" target="_blank">https://www2 . cs . duke . edu/courses/fall 15/compsci 590.4/slides/compsci 590.04 _ fall 15 _ LEC 12 . pdf</a></li><li id="80db" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp lt lb lc ld bi translated"><a class="ae lj" href="https://eng.uber.com/lsh/" rel="noopener ugc nofollow" target="_blank">https://eng.uber.com/lsh/</a></li><li id="3688" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp lt lb lc ld bi translated"><a class="ae lj" href="https://randorithms.com/2019/09/19/Visual-LSH.html" rel="noopener ugc nofollow" target="_blank">https://randorithms.com/2019/09/19/Visual-LSH.html</a></li></ol></div></div>    
</body>
</html>
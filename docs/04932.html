<html>
<head>
<title>Basics of Natural Language Processing for Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">初学者自然语言处理基础</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/basics-of-natural-language-processing-for-beginners-d86351df9d09?source=collection_archive---------3-----------------------#2021-07-06">https://medium.com/geekculture/basics-of-natural-language-processing-for-beginners-d86351df9d09?source=collection_archive---------3-----------------------#2021-07-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/801057e4648007a8eedcd9464c4c8115.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*JCLSGNt5SQ7Jm7kjj6lm0Q.jpeg"/></div></figure><p id="c142" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">自然语言处理(NLP)是人工智能(AI)的一部分，处理人类语言的理解和处理。实际上，大部分数据以非结构化的形式存在，如文本、视频、图像等。非结构化类别中的大量数据将以文本形式出现。为了用机器学习算法处理这些文本数据，NLP开始发挥作用。</p><p id="9d5a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">NLP用例有语言翻译、语音识别、雇佣和招聘、聊天机器人、情感分析等等。</p><h2 id="e8bc" class="jk jl hi bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated"><strong class="ak">例子:</strong></h2><ol class=""><li id="cdb0" class="kf kg hi io b ip kh it ki ix kj jb kk jf kl jj km kn ko kp bi translated"><strong class="io hj">感伤分析</strong></li></ol><p id="b38c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">情感分析预测评论是正面还是负面。</p><p id="a74f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kq">正面评论:</em></p><ul class=""><li id="ecde" class="kf kg hi io b ip iq it iu ix kr jb ks jf kt jj ku kn ko kp bi translated">“爱手机。这款手机时尚、流畅、漂亮，我强烈推荐这款手机，你不会后悔买了这款手机。”</li></ul><p id="684d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kq">差评:</em></p><ul class=""><li id="cb08" class="kf kg hi io b ip iq it iu ix kr jb ks jf kt jj ku kn ko kp bi translated">“电话耳机质量很差，你打电话时没听懂戎语，听起来很糟糕，不推荐这篇文章。我想把它还回去，但是从委内瑞拉还回来很困难，而且很贵，因为在我的国家美元很少，而且会比…</li></ul><p id="0029" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 2。命名实体识别</strong></p><p id="df53" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">它的任务是从文本中识别和分类实体。</p><p id="29e4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kq">输入:</em></p><p id="37ca" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">多尼是印度队著名的板球运动员之一。</p><p id="7552" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kq">输出:</em></p><p id="572a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Dhoni |人</p><p id="5558" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">印度|国家</p><h1 id="9d1e" class="kv jl hi bd jm kw kx ky jq kz la lb ju lc ld le jx lf lg lh ka li lj lk kd ll bi translated"><strong class="ak">NLP中使用的术语</strong></h1><ul class=""><li id="3fe4" class="kf kg hi io b ip kh it ki ix kj jb kk jf kl jj ku kn ko kp bi translated"><strong class="io hj">文集</strong>指文档或文本文件的集合。来自twitter的tweet数据是一个语料库。</li><li id="58f0" class="kf kg hi io b ip lm it ln ix lo jb lp jf lq jj ku kn ko kp bi translated">定义为<strong class="io hj">的文本样本文档</strong>。每条推文都是一个文档。</li><li id="d870" class="kf kg hi io b ip lm it ln ix lo jb lp jf lq jj ku kn ko kp bi translated">文件由<strong class="io hj">句</strong>组成。每条推文都有一个或多个句子。</li><li id="93b4" class="kf kg hi io b ip lm it ln ix lo jb lp jf lq jj ku kn ko kp bi translated">一个文本被分成更小的单元，称为<strong class="io hj">记号</strong>。每个tweet句子都有一个或多个标记。</li></ul><h1 id="888b" class="kv jl hi bd jm kw kx ky jq kz la lb ju lc ld le jx lf lg lh ka li lj lk kd ll bi translated"><strong class="ak">文本的自然语言处理技术</strong></h1><p id="e7ad" class="pw-post-body-paragraph im in hi io b ip kh ir is it ki iv iw ix lr iz ja jb ls jd je jf lt jh ji jj hb bi translated">自然语言工具包(NLTK)是构建Python程序来处理人类语言数据的领先平台。它为分类、标记化、词干提取、标记和解析提供了预定义的功能。</p><p id="8687" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里探讨一下<strong class="io hj"/><a class="ae lu" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"><strong class="io hj">https://www.nltk.org/</strong></a></p><h1 id="f881" class="kv jl hi bd jm kw kx ky jq kz la lb ju lc ld le jx lf lg lh ka li lj lk kd ll bi translated"><strong class="ak">标记化</strong></h1><p id="97ab" class="pw-post-body-paragraph im in hi io b ip kh ir is it ki iv iw ix lr iz ja jb ls jd je jf lt jh ji jj hb bi translated">记号化是将文本分割成称为记号的更小单元的过程。这个记号包括句子、单词、符号、数字等。,</p><p id="0e43" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">有不同类型记号赋予器，例如空白记号赋予器、Tweet记号赋予器、Regex记号赋予器、单词记号赋予器、句子记号赋予器。</p><p id="87b2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">空白标记器是常用的一种，但它取决于使用案例。</p><p id="4ce8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">记号赋予器的例子:</strong></p><p id="3418" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们导入nltk和不同形式的记号赋予器。sent_tokenizer函数应用句子级标记化，word_tokenizer函数应用单词级标记化，TweetTokenizer函数应用twitter数据的标记化，如下面的代码示例所示。</p><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="7f59" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对于Sent_tokenizer输出，输入被分成两个单独的句子。在Word_tokenizer输出中，输入被分成四个单词。在TweetTokenizer输出中，输入被分成十个单词、符号等。,</p><h1 id="7bf9" class="kv jl hi bd jm kw kx ky jq kz la lb ju lc ld le jx lf lg lh ka li lj lk kd ll bi translated"><strong class="ak">归一化</strong></h1><blockquote class="mb mc md"><p id="5671" class="im in kq io b ip iq ir is it iu iv iw me iy iz ja mf jc jd je mg jg jh ji jj hb bi translated">规范化是从单词中去除屈折变化的过程。</p></blockquote><p id="4d39" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">文档可能包含提供有意义信息的语法，但这些单词不会影响文本处理。为了消除它，让我们使用诸如词干化和变元化之类的技术来消除音调变化。</p><p id="b629" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">示例:</p><ul class=""><li id="44d3" class="kf kg hi io b ip iq it iu ix kr jb ks jf kt jj ku kn ko kp bi translated">麻烦，麻烦，麻烦= &gt;麻烦</li><li id="71e3" class="kf kg hi io b ip lm it ln ix lo jb lp jf lq jj ku kn ko kp bi translated">是，是，是= &gt;是</li></ul><blockquote class="mb mc md"><p id="17c9" class="im in kq io b ip iq ir is it iu iv iw me iy iz ja mf jc jd je mg jg jh ji jj hb bi translated"><em class="hi">词干化</em>指的是砍掉词尾的过程，希望在大多数时候都能正确实现这一目标，通常还包括去除派生词缀。</p><p id="c67a" class="im in kq io b ip iq ir is it iu iv iw me iy iz ja mf jc jd je mg jg jh ji jj hb bi translated"><em class="hi">引理化</em>指的是通过使用词汇和单词的形态学分析来正确地做事情，通常旨在仅移除屈折词尾，并返回单词的基本形式或词典形式，这被称为<em class="hi">引理。</em></p></blockquote><p id="f59c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">词干对比词汇化</strong></p><p id="a370" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">词干化和词汇化都消除了单词的词形变化。然而，词干化会产生字典中没有的单词，因为它不使用任何词性标记、词汇和与单词的语法关系，而词汇化则一步一步地使用上述所有过程并产生所需的单词。</p><p id="4fa7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">下面的代码说明了词干化和词汇化。</p><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="lz ma l"/></div></figure><h1 id="a124" class="kv jl hi bd jm kw kx ky jq kz la lb ju lc ld le jx lf lg lh ka li lj lk kd ll bi translated"><strong class="ak">停止字</strong></h1><p id="9e72" class="pw-post-body-paragraph im in hi io b ip kh ir is it ki iv iw ix lr iz ja jb ls jd je jf lt jh ji jj hb bi translated">停用词是对文章没有多大意义的常用词。这些词需要在文本预处理阶段去除，否则会在处理文本时产生噪声。</p><p id="b3fe" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">示例:</strong></p><ul class=""><li id="3026" class="kf kg hi io b ip iq it iu ix kr jb ks jf kt jj ku kn ko kp bi translated">连词:for，and，nor，but，or，yet，so</li><li id="e7b8" class="kf kg hi io b ip lm it ln ix lo jb lp jf lq jj ku kn ko kp bi translated">文章:一个，一个，的</li></ul><p id="7b2d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">从nltk.corpus导入停用词</strong>允许导入停用词模块。NLTK有预定义的英语停用词，可以通过<strong class="io hj">停用词.单词('英语')导入。</strong></p><p id="3213" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Tokenizer用于从文本中获取标记，使用这些标记，停用词被移除并放置在text_without_stopwords中。</p><p id="2532" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">代码示例:</p><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="lz ma l"/></div></figure><h2 id="5507" class="jk jl hi bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated"><strong class="ak">鞠躬(包话)</strong></h2><p id="6d54" class="pw-post-body-paragraph im in hi io b ip kh ir is it ki iv iw ix lr iz ja jb ls jd je jf lt jh ji jj hb bi translated">如前所述，NLP使用机器学习算法处理文本数据。文本不能作为ML算法的输入，文本需要转换成向量。这个过程也称为从文本中提取特征。对于这种转换，可用的技术是BOW(单词包)和单词嵌入。</p><p id="aab7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">单词包表示单词在文档中的出现。通过实现标记化、忽略停用词、应用规范化和应用进一步的预处理步骤，我们跟踪并收集词。在这里，模型关注的是这个词是否存在于文档中，是否有唯一的集合。这种方法是从文档中提取特征的一种简单而灵活的方式。</p><p id="1ae8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">制作蝴蝶结的步骤</strong></p><ul class=""><li id="e58a" class="kf kg hi io b ip iq it iu ix kr jb ks jf kt jj ku kn ko kp bi translated">导入必要的库并加载文本。</li><li id="78ca" class="kf kg hi io b ip lm it ln ix lo jb lp jf lq jj ku kn ko kp bi translated">文本预处理和词汇收集。</li><li id="5394" class="kf kg hi io b ip lm it ln ix lo jb lp jf lq jj ku kn ko kp bi translated">实现矢量器。</li></ul><p id="8ed8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们从NLTK导入所有必要的库来执行前面讨论过的预处理技术，比如标记化、规范化、忽略停用词。此外，在下面的代码中，在执行词汇化时，特殊字符被删除，文本大小写改为小写，以便对所有文本执行相同的大小写。</p><p id="b024" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">导入必要的库&amp;加载文本</strong></p><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="bac2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">文本预处理&amp;词汇采集</strong></p><p id="0efb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">词汇收集可以通过两种方式来完成。简单的实现方法是使用Scikit-learn的计数矢量化工具，或者按照所有预处理步骤进行矢量化。</p><ul class=""><li id="ad0a" class="kf kg hi io b ip iq it iu ix kr jb ks jf kt jj ku kn ko kp bi translated"><strong class="io hj">不带计数矢量器的词汇集合:</strong></li></ul><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="7c34" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">输出:</p><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="lz ma l"/></div></figure><ul class=""><li id="bb8b" class="kf kg hi io b ip iq it iu ix kr jb ks jf kt jj ku kn ko kp bi translated"><strong class="io hj">使用计数矢量器收集词汇:</strong></li></ul><p id="4075" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">接下来，我们需要将文本转换成矢量。最简单的方法是用1表示有单词，用0表示没有单词。为此，我们需要实现<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">计数矢量器。</a></p><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="511f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">输出:</p><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="a279" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">矢量器</strong></p><ul class=""><li id="5283" class="kf kg hi io b ip iq it iu ix kr jb ks jf kt jj ku kn ko kp bi translated"><strong class="io hj">无计数矢量器:</strong></li></ul><p id="f07c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">检查文档中是否有单词，如果有，则追加1，否则追加0。</p><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="lz ma l"/></div></figure><ul class=""><li id="3764" class="kf kg hi io b ip iq it iu ix kr jb ks jf kt jj ku kn ko kp bi translated"><strong class="io hj">带计数矢量器:</strong></li></ul><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="047c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">输出:</p><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="63a6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">更好地理解:</p><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="75d1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们考虑第一句话“我喜欢这辆车”。现在观察count vectorizer给出的向量，句子中的单词(即‘I’，‘like’，‘this’，‘car’)除了标记为0之外，还标记为1。</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mh"><img src="../Images/10b786fca0c2fceb4c3f9473d9acfaea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zzS5df-an1jA6X_UBkpWRg.png"/></div></div></figure><h1 id="c094" class="kv jl hi bd jm kw kx ky jq kz la lb ju lc ld le jx lf lg lh ka li lj lk kd ll bi translated">单词嵌入</h1><blockquote class="mb mc md"><p id="aae7" class="im in kq io b ip iq ir is it iu iv iw me iy iz ja mf jc jd je mg jg jh ji jj hb bi translated">为什么要嵌入单词？</p><p id="70d2" class="im in kq io b ip iq ir is it iu iv iw me iy iz ja mf jc jd je mg jg jh ji jj hb bi translated">如果新句子包含新单词，那么我们的词汇量将增加，因此，向量的长度也将增加，向量表示将有许多零，这被称为<strong class="io hj">稀疏向量</strong>。这些向量在单词之间没有相邻或方向关系</p></blockquote><p id="dd22" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">单词嵌入技术有TF-IDF(词频—逆文档频率)、Word2vec、Glove(全局向量)。</p><h2 id="3254" class="jk jl hi bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated">1.TF-IDF</h2><blockquote class="mb mc md"><p id="dfb3" class="im in kq io b ip iq ir is it iu iv iw me iy iz ja mf jc jd je mg jg jh ji jj hb bi translated">词频定义为一个词在文档中出现的次数除以文档中的总字数。一般在某种意义上称为规范化。</p></blockquote><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/9935a6ce06800c4648c87af5823f22d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*NLoQitksRzLDjUNg6tqxqQ.png"/></div><figcaption class="mn mo et er es mp mq bd b be z dx">Term Frequency</figcaption></figure><blockquote class="mb mc md"><p id="0d61" class="im in kq io b ip iq ir is it iu iv iw me iy iz ja mf jc jd je mg jg jh ji jj hb bi translated">逆数据频率定义为文档总数除以包含该单词的文档数的对数。在下面的公式1中，可能包括也可能不包括，这取决于你的意愿。这只是为了标准化。</p></blockquote><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/1d74657b6890257b7a0a264b03cf5faf.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*zfJS99u5XbCIQk4MupMubw.png"/></div><figcaption class="mn mo et er es mp mq bd b be z dx">Inverse Document Frequency</figcaption></figure><p id="5e2c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">TF-IDF定义如下</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/18133cba595a458ba73f60cd54e1052d.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*qD-QNONXJrmCFXrdTVD9kw.png"/></div></figure><h2 id="688f" class="jk jl hi bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated"><strong class="ak">TF _ IDF计算示例:</strong></h2><ul class=""><li id="2ece" class="kf kg hi io b ip kh it ki ix kj jb kk jf kl jj ku kn ko kp bi translated">让我们假设单词“system”在10000单词的文档中出现6次。计算TF？</li></ul><p id="a095" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">TF=的数量。单词出现次数/总字数</p><p id="5408" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">TF= 6/10000=&gt;0.006</p><h2 id="8d91" class="jk jl hi bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated"><strong class="ak"> TF=0.006 </strong></h2><ul class=""><li id="26d6" class="kf kg hi io b ip kh it ki ix kj jb kk jf kl jj ku kn ko kp bi translated">让我们假设“系统”这个词在10000个文档中出现60次。计算IDF？</li></ul><p id="52d7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">IDF =日志(编号。文件总数/件数。包含该单词的文档)</p><p id="1431" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">IDF=log(10000/6)=&gt;3.21</p><h2 id="ee3b" class="jk jl hi bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated"><strong class="ak"> IDF=3.21 </strong></h2><p id="3d72" class="pw-post-body-paragraph im in hi io b ip kh ir is it ki iv iw ix lr iz ja jb ls jd je jf lt jh ji jj hb bi translated">TF-IDF=0.006 * 3.21</p><h2 id="e87f" class="jk jl hi bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated"><strong class="ak"> TF-IDF=0.01926 </strong></h2><p id="8dd7" class="pw-post-body-paragraph im in hi io b ip kh ir is it ki iv iw ix lr iz ja jb ls jd je jf lt jh ji jj hb bi translated">Scikit-learn的TfidfVectorizer允许应用TF-IDF单词嵌入技术。</p><p id="3cd7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">代码示例:</strong></p><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="ec72" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">输出:</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mt"><img src="../Images/e3c8a9f2720276a118b977ffd8fb5448.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2tAulu7fo-DVZnW-8Gtl2A.png"/></div></div></figure><p id="5158" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从上面的输出中，让我们尝试手动输入第一句话“我喜欢这辆车”。考虑TF-IDF计算文本中的“汽车”一词。</p><p id="b06b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">计算:</strong></p><p id="c298" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">总字数= 11</p><p id="1920" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">TF=1/4=&gt;0.25</p><p id="5ed6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">IDF=log(3+1/1+1)=&gt;1.308</p><p id="d88a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">TF-IDF=0.25 * 1.308=&gt;0.327</p><p id="a7b9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Scikit-learn实现在TF和IDF之间的产品上应用标准化</p><p id="9373" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">“车”字的归一化=平方根(0.327) =&gt; <strong class="io hj"> 0.57 </strong></p><h2 id="c518" class="jk jl hi bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated">2.<strong class="ak"> Word2vec </strong></h2><p id="e7f4" class="pw-post-body-paragraph im in hi io b ip kh ir is it ki iv iw ix lr iz ja jb ls jd je jf lt jh ji jj hb bi translated">2013年发明的Word2vec。它仅通过考虑预测单词的局部共现来预测向量。通过查看窗口大小的范围来进行单词预测。</p><p id="c48d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Word2vec结合了两种架构，如CBOW和Skip-Gram。在CBOW(连续单词包)中，我们试图通过循环许多单词来预测单词，而在Skip-Gram中，我们试图通过给定的特定单词来模拟上下文单词。下面的架构说明了同样的情况。</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mu"><img src="../Images/4deb626486147ded63573a51ee2725d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AJ6RNLf8MZjxFfLjyD59hg.png"/></div></div><figcaption class="mn mo et er es mp mq bd b be z dx"><a class="ae lu" href="https://arxiv.org/pdf/1309.4168v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1309.4168v1.pdf</a></figcaption></figure><p id="c0b5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在Word2Vec中，相似的单词一起位于向量空间中，并且对单词向量的算术运算可以提出语义关系。</p><p id="e2e9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">比如:“国王”——“男人”+“女人”= &gt;“女王”</p><p id="04d1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">跳格示例:</strong></p><p id="746f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">问:华特·迪士尼发明了迪士尼乐园。在这个句子中，让我们预测迪斯尼乐园这个词。</p><p id="9588" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Word2vec是由神经网络构建的。众所周知，我们不能将文本作为输入传递给模型，所以我们需要通过将预测单词标记为1而将其他单词标记为0来将单词转换为向量。为了预测“迪斯尼乐园”这个词，赋予它很高的权重。因此，输出层(软最大值)准确地预测具有高权重的单词..</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mv"><img src="../Images/a05c8a0cd1b9c96fbf482b8ba20de232.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EkW-fhUiyDfKDuEeY70JcQ.png"/></div></div><figcaption class="mn mo et er es mp mq bd b be z dx"><strong class="bd jm">Skip-Gram Example Architectur</strong>e</figcaption></figure><p id="6ecf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">CBOW的例子</strong></p><p id="23d7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">问:华特·迪士尼发明了迪士尼乐园。在这个句子中，让我们预测除了迪斯尼乐园以外的所有单词。</p><p id="292b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里，我们需要通过skip-gram模型的逆过程，通过对除“迪士尼乐园”之外的所有单词赋予高权重来预测单词</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mw"><img src="../Images/ae801f45844dbbfa7045ff3042f8447f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b1RxPozTiLEtdU9BIoSkVw.png"/></div></div><figcaption class="mn mo et er es mp mq bd b be z dx"><strong class="bd jm">CBOW Example Architecture</strong></figcaption></figure><p id="3c67" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">代码示例:</strong></p><p id="9ccc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在查看代码示例之前，我建议先了解一下<a class="ae lu" href="https://radimrehurek.com/gensim/models/word2vec.html" rel="noopener ugc nofollow" target="_blank"> Gensimword2vec </a></p><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="lz ma l"/></div></figure><h2 id="e2ed" class="jk jl hi bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated">3.手套(全局向量)</h2><p id="8cb2" class="pw-post-body-paragraph im in hi io b ip kh ir is it ki iv iw ix lr iz ja jb ls jd je jf lt jh ji jj hb bi translated"><a class="ae lu" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套</a>发明于2014年。这是一种无监督的学习算法，通过向量来表示单词。与word2vec相比，它可以更好地捕捉整个语料库中的单词-单词共现。换句话说，它通过估计语料库中其他词的概率来创建全局共现矩阵。</p><blockquote class="mb mc md"><p id="fbb7" class="im in kq io b ip iq ir is it iu iv iw me iy iz ja mf jc jd je mg jg jh ji jj hb bi translated">为什么Glove比Word2vec好？</p><p id="0069" class="im in kq io b ip iq ir is it iu iv iw me iy iz ja mf jc jd je mg jg jh ji jj hb bi translated">这两个模型本质上是相同的，它们从共现中学习向量。然而，Word2vec是<strong class="io hj">预测</strong>模型，而GloVe是<strong class="io hj">基于计数的</strong>模型。该预测模型学习向量以提高预测精度，但是基于模型整体统计的计数是共现的。在手套中，比word2vec更容易训练更多的数据。</p></blockquote><p id="7b01" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">手套示例:</strong></p><p id="1ee2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们考虑文本为“我爱NLP。我喜欢写博客”。首先，从文本中找出所有唯一的单词，包括任何存在的特殊字符。接下来找出每个单词的共现。</p><p id="8cf9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">例如，考虑单词“I”并找到“I”的左右单词(即，单词的同现)。所有同现词标记为1，其他标记为0。</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mx"><img src="../Images/d1ef95e5430e14f04dd4c88ba0648609.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6DjZLExk626uW6yfX9pxvg.png"/></div></div></figure><p id="143a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">上表汇总。</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es my"><img src="../Images/048adb052e03f5a2adbc5d97ac66d767.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*XpmvkzT9o-6T7-efZR__gA.png"/></div><figcaption class="mn mo et er es mp mq bd b be z dx">Glove Vectors Representation</figcaption></figure><h1 id="c7b1" class="kv jl hi bd jm kw kx ky jq kz la lb ju lc ld le jx lf lg lh ka li lj lk kd ll bi translated">资源</h1><ol class=""><li id="95ac" class="kf kg hi io b ip kh it ki ix kj jb kk jf kl jj km kn ko kp bi translated"><a class="ae lu" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/pubs/glove.pdf</a></li><li id="f81f" class="kf kg hi io b ip lm it ln ix lo jb lp jf lq jj km kn ko kp bi translated"><a class="ae lu" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank">https://www.nltk.org/</a></li><li id="add6" class="kf kg hi io b ip lm it ln ix lo jb lp jf lq jj km kn ko kp bi translated"><a class="ae lu" href="https://radimrehurek.com/gensim/" rel="noopener ugc nofollow" target="_blank">https://radimrehurek.com/gensim/</a></li><li id="e353" class="kf kg hi io b ip lm it ln ix lo jb lp jf lq jj km kn ko kp bi translated"><a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . feature _ extraction . text . count vectorizer . html</a></li><li id="8913" class="kf kg hi io b ip lm it ln ix lo jb lp jf lq jj km kn ko kp bi translated"><a class="ae lu" href="https://scikit-learn.org/stable/modules/feature_extraction.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/feature _ extraction . html</a></li><li id="6b5f" class="kf kg hi io b ip lm it ln ix lo jb lp jf lq jj km kn ko kp bi translated"><a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . feature _ extraction . text . tfidftransformer . html</a></li></ol><p id="8fa5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我很想听到一些反馈。感谢您的宝贵时间！T25】</p></div></div>    
</body>
</html>
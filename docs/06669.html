<html>
<head>
<title>Introduction to Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习导论</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/introduction-to-machine-learning-77350717a2b3?source=collection_archive---------38-----------------------#2021-08-25">https://medium.com/geekculture/introduction-to-machine-learning-77350717a2b3?source=collection_archive---------38-----------------------#2021-08-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="762c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">本文以全球AI Hub为内容。我准备了这篇文章，以便直观地解释和理解它。请访问全球人工智能中心，并表明你的赞赏，这是一个伟大的数据科学免费资源。[1] </strong></p><p id="9a0f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">你可以查看我的“数据科学基础知识”中间文章，了解定义和基础知识。</strong></p><h1 id="09e1" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">机器学习类型</strong></h1><h2 id="f247" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated"><strong class="ak">监督学习</strong></h2><p id="b5e0" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated"><strong class="ih hj">回归</strong>用于<strong class="ih hj">连续</strong>目标变量<strong class="ih hj"> </strong>(例如<strong class="ih hj">房价预测</strong>)。</p><p id="3ab9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">分类</strong>用于<strong class="ih hj">分类</strong>目标变量(例如<strong class="ih hj">医学影像</strong>)。</p><h2 id="cb56" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated"><strong class="ak">无监督学习</strong></h2><p id="3e30" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">不存在目标变量。</p><p id="dffd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">聚类</strong>(例如<strong class="ih hj">客户细分</strong>或<strong class="ih hj">关联</strong>)应用<strong class="ih hj"> </strong>(例如<strong class="ih hj">购物篮分析</strong>)。</p><h2 id="ded6" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated"><strong class="ak">半监督学习</strong></h2><p id="806f" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated"><strong class="ih hj">分类</strong>(例如<strong class="ih hj">文本分类</strong>)或<strong class="ih hj">聚类</strong>(例如<strong class="ih hj">GPS数据寻路</strong>)应用于<strong class="ih hj">分类</strong>目标变量。</p><h2 id="3df9" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated"><strong class="ak">强化学习</strong></h2><p id="eb2a" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated"><strong class="ih hj">控制</strong>用于<strong class="ih hj">无</strong>目标变量<strong class="ih hj"> </strong>(例如<strong class="ih hj">自动驾驶汽车</strong>)。</p><p id="d6e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">分类</strong>用于<strong class="ih hj">分类</strong>目标变量(例如<strong class="ih hj">优化营销</strong>)。</p><h1 id="789f" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">格式化数据以适应模型</strong></h1><p id="1670" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated"><strong class="ih hj">1 _清理数据</strong></p><p id="20a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">去掉不必要的数据，修正不正确的数据，聚合来自不同资源的数据。</p><p id="27b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">2 _确定数据类型</strong></p><p id="747b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">检查日期，数值，书写格式数据，并转换方便的数据类型。</p><p id="d9a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">3 _降维</strong></p><p id="b4c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">必要时做PCA(降维)，去掉不必要的列，分析相关性。</p><p id="63b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4_分析数据分布和规律性</strong></p><p id="c04f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最小-最大缩放和标准化</p><h1 id="dbd2" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">成功表现</strong></h1><h1 id="ff1b" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">分类指标</strong></h1><h2 id="e2a9" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated"><strong class="ak">混淆矩阵</strong></h2><p id="2285" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">TP(真阳性)，FN(假阴性)，FP(假阳性)，TN(真阴性)。</p><p id="4b2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">精度</strong>是模型的整体性能。它是全部正确预测值与所有预测值的比率。对于不均匀分布的数据集，它本身是不够的。</p><p id="3b67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> (TP+TN)/(TP+TN+FP+FN) </strong></p><p id="56ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">精度</strong>是正面预测的准确性。</p><p id="0948" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> TP/(TP+FP) </strong></p><p id="49b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">召回(灵敏度)</strong>是实际阳性样本的覆盖率。</p><p id="6bfc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> TP/(TP+FN) </strong></p><p id="4cfe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">特异性</strong>是实际阴性样本的覆盖率。</p><p id="019b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> TN/(TN+FP) </strong></p><p id="8ae5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> F1得分</strong>是对不平衡类别有用的混合指标。这是精确和回忆的调和平均值。</p><p id="8b1f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2TP/(2TP+FP+FN) </strong></p><h1 id="ff2e" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">回归指标</strong></h1><h2 id="11d2" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">平均绝对误差</h2><p id="4fe1" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">给出预测值和实际值的绝对差值的平均值。它不受训练集中异常值的影响。MAE的问题是，它在最小值时是不可导的。这可能会在训练机器学习模型时导致收敛。</p><h2 id="cf93" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">均方误差</h2><p id="74d0" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">MSE很常见，因为它简单、连续且可推导。当我们计算平均值时，由于考虑到一个大的异常值，它很有可能是一个巨大的值，因此它会导致很高的误差。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/9d68bdf8eed1e5920167682a5f8dbbe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wW_JP_i91tdmkMB0.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Regression metrics[3]</figcaption></figure><h1 id="f6ac" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">模型的优化</strong></h1><p id="d706" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">使用交叉验证调整超参数。</p><p id="bd3f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">做超参数搜索，比如gridsearch。</p><p id="c838" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尝试整体方法。组合您的最佳模型通常会产生更好的结果。</p><p id="597e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当您对您的最终模型更有把握时，使用测试数据集进行性能测试。</p><h1 id="3f39" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">模型部署</strong></h1><p id="bbed" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">ML部署是将机器学习模型的集成部署到已经存在的生产环境中(比如SaaS和PaaS)。</p><p id="5a5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Streamlit是一个python库，让你在不知道HTML和CSS的情况下创建交互式网站。</p><p id="011c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Docker是编码环境最流行的容器化方式。</p><p id="c244" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Heroku </strong>是云计算平台即服务基础设施服务。</p><h1 id="43ee" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">交叉验证</strong></h1><p id="cef2" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">它允许您选择一个不太依赖于第一个学习集的模型。</p><p id="f6c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们有一个数据集。我们将数据集的前%n作为验证，其余的作为训练集。这是第一次折叠，有一个E1验证错误。现在，我们只将数据集的下一个%n作为验证集，其余的作为训练集，并重复这个过程k次。最后的交叉验证误差:(E1+E2+…+Ek)/K</p><h1 id="a12d" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">偏差-方差权衡</strong></h1><p id="d0ed" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated"><strong class="ih hj">偏差:</strong>偏差是对给定点的预测和我们试图预测的正确模型之间的差异。</p><p id="1e58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">方差:“</strong>方差衡量集合中的每个数字与平均值的距离，以及与集合中其他数字的距离”。[2]</p><p id="600a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">方差权衡:</strong>越简单的现代，偏倚越高。模型越复杂，方差越高。</p><h1 id="4854" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">诊断欠拟合和过拟合</strong></h1><p id="dd72" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated"><strong class="ih hj">欠拟合:</strong>高训练误差，高偏差，训练误差类似于测试误差，模型学习不好。</p><p id="2ead" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">过拟合:</strong>非常低的训练误差，高方差，训练误差比测试误差小得多，模型记忆了数据。</p><p id="40a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">理想:</strong>训练误差略小于测试误差。</p><p id="75b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">不可约误差</strong>与噪声有关，唯一的解决方法是清除数据。</p><p id="11ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">防止欠拟合:</strong>可以增加训练数据，降低模型复杂度，训练时可以应用提前停止，可以应用正则化。</p><p id="6987" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">防止过拟合:</strong>可以通过添加新的特征值或层来增加模型的复杂度。可以从数据中去除噪声。在训练期间，历元的数量可以减少。可以给出更好的特征值(特征工程)。可以减少限制模型的值(例如正则化参数)。可以增加数据的数量。</p><h1 id="618d" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">回归分析技术的类型【4】</strong></h1><p id="a705" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated"><strong class="ih hj">线性回归</strong>:以直线形式进行预测。下面是表示线性回归的等式。</p><p id="4a3c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> y=xw+b </strong></p><p id="fa3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">y:因变量，x:自变量，w:斜率，b:偏差或截距。</p><p id="1cae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">岭回归</strong>:这是一种强大的回归方法，不容易过度拟合，通常在自变量之间存在高度相关性时首选。模型方程如下所示，(λ)解决了多重共线性问题。</p><p id="628b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">β = (X^{T}X + λ*I)^{-1}X^{T}y</p><p id="f852" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">套索回归:</strong>套索回归涉及特征选择。它防止回归系数的绝对大小增加，并且系数值接近零。该模型由下面的等式表示。</p><p id="6d5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">n^{-1}σ^{n}_{i=1}f(x_{i}，y_{I}，α，β)</p><p id="b357" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">多项式回归</strong>:自变量(x)和因变量(y)之间的联系在多项式回归中用n次表示(类似于多元线性回归，稍有修改)。下面是等式。</p><p id="46d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">l = β0+ β0x1+ε</p><h1 id="e33f" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak"> R平方</strong></h1><p id="81c4" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">它是对线性回归模型的适合度的度量。</p><p id="ddd9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">R =1-SS回归/SS总数</p><p id="1e4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> R2值并不总是意味着模型好，过度拟合时R也很高。</strong></p><h1 id="c3d6" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">提前停止</h1><p id="f602" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">如果当验证集误差低于或等于我们确定的最小误差值时，性能没有继续提高，我们的误差值将返回到最佳点，训练将停止。</p><h1 id="ea6b" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">价值函数</h1><p id="225e" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">它是模型的预测值和真实值之间的差异。训练完我们的模型后，我们需要看看它的效果如何。虽然准确性向我们展示了模型运行得有多好，但它并没有告诉我们如何改进它。我们需要一个成本函数来帮助我们达到训练不足的模型和训练过度的模型之间的微小点。我们需要一个成本函数来帮助我们确定模型何时最准确，因为我们需要达到欠训练模型和过训练模型之间的小点。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lk"><img src="../Images/aa5b6495a69cb7af996c1bffacb9fc93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nyPqqcs0fsSUpCxW9q3saA.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Cost function equation and graph[1]</figcaption></figure><h2 id="0f40" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">损失函数与成本函数</h2><p id="2de2" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">损失函数(误差函数)是针对单个训练示例的。</p><p id="c3ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">成本函数是整个训练数据集的平均损失。</p><h1 id="3801" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">梯度下降</h1><p id="c102" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">迭代地调整权重以最小化成本函数。这样做，直到误差值收敛到局部最小值。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ll"><img src="../Images/5e2be5edf3464950f4c1ac1f69daf103.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LAAuO5daHuArqk1yxvuTbg.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Gradient Descent formula on the right and its graph on the left[1]</figcaption></figure><p id="4d20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于所有的代价函数都不是凸的，该算法收敛于局部极小值。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lm"><img src="../Images/b9165a7913de959b380010b4381eb84d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uW4IAgy9gB5hNCYl7pq_kg.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Local minimum, global minimum, and plateau[1]</figcaption></figure><p id="63d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">像MSE和MAE这样的成本函数是凸的，所以我们可以确定只有一个全局最小值。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ln"><img src="../Images/22ccbc18303ddac49b56cac9cb4c9fa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nOum_rdR0trUMFpPTkg0_Q.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Gradient Descend Python Code[8]</figcaption></figure><h1 id="683b" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">正规化</h1><p id="496a" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">正则化是通过约束来简化模型，从而防止过度拟合的过程。</p><p id="8fd5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将比较2正则化算法L1拉索和L2岭。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lo"><img src="../Images/ed9724d6ae71d04f59719695bc0695fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QcH5NIHXvqjZ0rnoB4ce8w.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">L1 Lasso[1]</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lp"><img src="../Images/a08eaeeab0d58d0e4ddf120fc9e72213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MyEvyAVB2L796qDZZjF8SQ.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">L2 Ridge[2]</figcaption></figure><p id="9036" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">误差函数在L1，用权重的绝对值来惩罚，而在L2，用权重的平方来惩罚。</p><p id="32b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">L1的导数是k(一个与重量无关的常数),并从重量中减去该常数，其中L2的导数是2 *重量并去除重量的x%。</p><p id="096a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你拥有的特征多于观测值的数量(N)，L1最多拥有N个系数，而L2是解决这个问题的经典方法。</p><p id="b564" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">L1可以通过限制L1系数范数和固定某些系数值的级数来解决多重共线性问题。多重共线性意味着两个独立变量高度相关。L2可以通过限制L2系数范数和保护所有变量来解决多重共线性问题。</p><h1 id="d4a5" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">超参数定义</h1><p id="d202" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated"><strong class="ih hj">学习速率(α) </strong>:决定梯度下降时误差收敛到最优点的速率。</p><p id="e668" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Theta(θ) </strong>:特征值的权重。</p><p id="4938" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">正则项(λ) </strong>:是正则参数，是决定罚多少权的惩罚项。如果给的太多，我们的重量会接近于零。这防止了我们的模型过度拟合，但同时，创建的模型不会非常高效。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lq"><img src="../Images/da6c6fb18730a8b5630bddd6500a01a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xIXpJ7avSLWssXj7hsqyhQ.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Regularization term in the formula[1]</figcaption></figure><h1 id="4e64" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">分类</h1><p id="7e2e" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">将结果分成两个或更多不同的类别。可以在结构化和非结构化数据集上完成。这些类通常被称为目标、类别或标签。</p><h2 id="6896" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated"><strong class="ak">逻辑回归</strong></h2><p id="f8d5" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">这被称为回归，但实际上，它将结果分成两类(分类)，因此目标变量只能有两个值。这是一个快速和基本的算法。目标变量和自变量之间的关系由s形曲线表示。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lr"><img src="../Images/1557ea3271f51c68ed0cb20183c60b41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*53nBEKMYEkf6fNp9jhoApA.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Logistic Regression[1]</figcaption></figure><p id="c4da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑回归方程)如下所示。</p><p id="a09d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">logit(p)= ln(p/(1-p))= B0+b1x 1+B2 x2+b3x 3…+bkXk</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ls"><img src="../Images/043c6e5d1f8c15ce7f546c187fd21696.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*Q3WCeXA_iSEt5BUcsaWP7w.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Logistic regression formula[1]</figcaption></figure><h1 id="e976" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">激活功能</h1><p id="789b" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">我们将线性表达式转换成非线性表达式。</p><h2 id="739a" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">西格蒙德·丰克西约努</h2><p id="283f" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">sigmoid函数是非线性函数，其结果在[0，1]之间。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lt"><img src="../Images/76c30d7e7235fd1bc1aef92c57e29086.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*WDj19iHD0gepGbsEQyWvVA.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx">Sigmoid function and its graph[1]</figcaption></figure><p id="ffdf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果需要有多个输出类，应该怎么做？答案是softmax。</p><h2 id="f842" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">Softmax Fonksiyonu</h2><p id="0e8a" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">Softmax非常类似于sigmoid函数(性能更好)，适用于多个输出类。特别是用在输出层的深度学习。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lu"><img src="../Images/c9221ded4f58776e57c57114f68903cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*8N8mrTKi3rgfY4ctk6HQOQ.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx">softmax formula [1]</figcaption></figure><h1 id="d5e2" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">ROC曲线</h1><p id="c0bb" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">它是在各种阈值下对分类问题的性能测量。ROC是概率曲线，AUC是输出到类别的可分辨性的度量。ROC-AUC曲线是使用最广泛的指标之一。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lv"><img src="../Images/2072aa53b68569742213c174e6cdbfd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*uwq5wTaFmvvrOX52cfl_iw.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx">ROC curve graph[1]</figcaption></figure><h1 id="acf2" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">流行的分类算法</h1><h2 id="c967" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">k-最近邻(KNN)</h2><p id="78ce" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">它是一种具有非参数结构的监督学习算法，被认为是一种惰性分类器。模型上不进行任何训练。它通过查看最近的邻居来预测数据属于哪个类。</p><p id="48e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">选择邻居的数量(k ),并确定数据之间的欧几里得距离。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lw"><img src="../Images/53778b67a6c47386bda6cdfe41776d34.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*VlVJRWU5e1cyTZ8Vty_LnA.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx">Euclidean distance formula and graph[1]</figcaption></figure><p id="d643" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">欧几里德方程的结果是，如果A类中的邻居数量多于B类中的邻居数量，则推断未知数据来自A类。</p><p id="f8cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不同的距离计算器可以用来解决这个问题。</p><p id="7628" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当需要计算依赖于C值的距离时，使用Minkowski。当C=1公式给出曼哈顿距离，C=2给出奥克里德距离。汉明距离给出了两个二进制序列之间的相似性。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lx"><img src="../Images/2b7fd1a2b34d33719a3ac90cc2bd889e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lt9uz2FTmuXFwlAivXOW0g.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Different distance formulas[1]</figcaption></figure><h2 id="f2a3" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">支持向量机</h2><p id="e4bb" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">它是分类问题中使用的监督学习方法之一。画一条线来分隔放置在平面上的点，这条线的目的是在2类点内的最大距离处。复杂但适合中小型数据集。解决二元分类问题</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ly"><img src="../Images/ec5a794f7ce3a44300e0ea49d87ec901.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2tuWjFziH2-yVZvo.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">SVM[5]</figcaption></figure><p id="e025" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SVM通常对线性数据进行分类，但它也可以对非线性数据进行分类。目标是找到超序列，使两类之间的分离达到最佳。它也被应用于分类问题产生的后回归问题。</p><h2 id="a0fc" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">硬利润与软利润</h2><p id="0002" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">Margin是在到达数据点之前可以增加的宽度。边距是仅从直线到最近点的垂直距离。</p><p id="aafe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">如果数据是线性可分的，并且对异常值非常敏感，则硬边界</strong>有效。</p><p id="6add" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">软边界</strong>用于数据混乱且无法被超平面分隔的情况</p><p id="f88b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以用c超参数控制两者的平衡。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lz"><img src="../Images/6995b7835786f10d325d9b047b1ffaa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*tMgmpnLHcduH1Mirvwbx5Q.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Hard margin and soft margin [1]</figcaption></figure><h1 id="78cd" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">决策树</h1><p id="99b5" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">它是一种基于树的算法，在回归和分类中都使用。决策树往往会过度拟合。用于复杂的数据集中。决策树是基于树数据结构的，要了解更多，你可以查看我的“树数据结构”文章。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ma"><img src="../Images/2f82223c8a4dcc2e5119b8a983513eb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*ExOA84_Z04pRODp5HOqSUw.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx">Decision tree formula and representation [1]</figcaption></figure><h2 id="f539" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">决策树的优势</h2><p id="6289" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">对连续值和离散值都有效。</p><p id="1a41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不需要异常值检测和缩放(较少需要数据预处理)。</p><p id="2a51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单易懂，决策规则清晰可见。</p><p id="439d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用于多路输出。</p><h2 id="ce74" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">基尼指数</h2><p id="a5f5" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">纯意味着所有数据集值属于同一个类。</p><p id="4769" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基尼系数是对随机变量新样本错误分类的一种度量。</p><p id="f5d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基尼=(选择红球的概率+选择蓝球的概率)</p><h2 id="c1fa" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated"><strong class="ak">合奏学习</strong></h2><p id="f2b8" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">在这种情况下，一个以上的分类估计算法组合并产生改进的结果。</p><p id="2300" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">集成学习的一些例子是最大投票、平均、加权平均、堆叠、混合。</p><h2 id="1f3d" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">拔靴带</h2><p id="0ab5" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">使用随机抽样方法，这种策略可以估计几乎任何统计的抽样分布。它用于最小化数据集的高方差和噪声。[7]</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es mb"><img src="../Images/c2a28d6894fec898f9f4fc5c6045cc4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kTVoEoK758ufbtI2iMBoEg.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Bootstrapping Process[1]</figcaption></figure><h2 id="03bb" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">引导聚集</h2><p id="2b0d" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">它是用于机器学习集成的元算法，旨在提高统计分类和回归中使用的机器学习算法的稳定性和准确性。在从主数据集生成数据的随机子集之后，独立地训练多个弱模型。它还最小化了差异，并有助于避免过度拟合。虽然它通常与决策树一起使用(使用随机森林)，但它可以用于任何方法。[6]</p><h1 id="d9c2" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">无监督学习</h1><h2 id="67fc" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">流行的无监督学习算法</h2><p id="8a50" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated"><strong class="ih hj">聚类</strong></p><p id="123c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于质心、基于密度的空间聚类(DBSCAN)、聚类分析、相似性传播。</p><p id="b852" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">异常和新奇检测</strong></p><p id="3924" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一级SVM和隔离林。</p><p id="2488" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">关联规则学习</strong></p><p id="7877" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">先验和Eclat。</p><p id="70be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">可视化和降维</strong></p><p id="9cdc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">主成分分析，核主成分分析，局部线性嵌入(LLE)，t-分布随机邻居，t-分布随机邻居嵌入(t-SNE)。</p><h2 id="00e2" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated"><strong class="ak">主成分分析</strong></h2><p id="9b58" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">它减少了复杂数据集的大小，捕获了数据中的关键特征，并试图用更少的变量来显示它们。</p><h2 id="2553" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated"><strong class="ak">逐步PCA</strong></h2><p id="247d" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">标准化数据集。计算数据集中要素的协方差矩阵。计算协方差矩阵的特征值和特征向量。计算特征值和相应的特征向量。选择k个特征值，创建一个特征值矩阵。转换原始矩阵。</p><h2 id="f4ed" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">t分布随机邻居嵌入(t-SNE)</h2><p id="8fb6" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">它用于可视化高维样本。它通过保持相似的特征接近而不同的特征远离来降低维数。</p><h2 id="1c78" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">使聚集</h2><p id="7999" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">它是通过识别未标记的数据将数据分组到相似组的过程。</p><h2 id="91aa" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">亲和传播</h2><p id="4284" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">将每个数据点视为网络中的一个节点，并将所有数据点视为潜在样本。它存在一些问题，比如发现许多不必要的聚类，以及在处理大量数据时表现不佳。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mc"><img src="../Images/b9181a60d1d72d92e56f3c8fc2e1f50a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*6kmqA3hKLiNK0VaKc9pnBg.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx">Affinity Propagation [1]</figcaption></figure><h2 id="c87b" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated"><strong class="ak">层次聚类</strong></h2><p id="b797" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">它根据相似性将数据分为不同的级别和层次。创建了一个称为树状图的树状结构。</p><p id="29af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">层次聚类有两种工作方式，逻辑相似。</p><p id="667b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">集聚集群</strong></p><p id="e9e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个数据集在开始时都被视为一个聚类。实际的聚类是通过迭代组合彼此接近的聚类而形成的。</p><p id="1b0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">分裂聚类</strong></p><p id="9a5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有一个聚类在开始时包含所有数据集。它首先接受单个簇中的所有数据，然后根据邻近程度划分成簇。</p><h2 id="df42" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">基于密度的聚类</h2><p id="4f1e" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">它假设数据由高斯分布等分布组成，并根据密度进行聚类。当聚类时，考虑距中心的距离。它不需要聚类数量超参数，而是根据数据的密度来确定聚类的数量。</p><h2 id="f22e" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">基于质心的聚类</h2><p id="b433" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">在最大n个质心的帮助下，将数据分成n个簇。K-means是最流行的基于质心的算法。它对初始条件和异常值很敏感。</p><p id="8067" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> K均值聚类</strong></p><p id="e500" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它试图将样本分成n个方差相等的组。它的目标是最小化一个被称为平方误差函数的目标函数。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es md"><img src="../Images/5d4a8ab89a3d768b4689b1bb20f3c369.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*NDmBRdUiPfPcdTrqhc83-Q.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx">Squared error function[1]</figcaption></figure><h2 id="4e11" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">k-表示循序渐进</h2><p id="f87c" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">K-means算法为每个聚类分配随机质心。然后，我们将使用聚类指标来评估我们的模型。</p><p id="d784" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了找到聚类的最佳数量，使用了肘方法。画一个扭曲-k数的聚类图。图形看起来像一只手臂，肘点将显示最佳点。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es me"><img src="../Images/dcaefd3b9ce88acdb10859579194d99e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*XAKZa0HdyCDUjZfOELIHPQ.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">The Elbow Method[1]</figcaption></figure><h2 id="4ac3" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">剪影分数</h2><p id="8fbe" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">这是发现聚类技术如何聚类的标准。其结果介于-1和1之间。-1表示聚类是错误的，1表示聚类彼此相距很远，容易被分割。0表示聚类之间的距离没有任何意义。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es mf"><img src="../Images/20f8d50c4667a58c32a15e6d6699720a.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*lKTlRuWsGZa1-TSFNArqXg.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx">Silhouette equation and graph[1]</figcaption></figure><p id="5813" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算完成后，将重新计算质心。对于每个聚类，该算法通过平均聚类中的所有点来重新计算质心。重复这个过程。</p><h2 id="e251" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">K-NN与K-Means</h2><p id="5b40" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">K-NN是用于分类和回归的监督学习算法，而K-Means是用于聚类的非监督学习算法。</p><p id="1fdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">K-NN基于特征相似性，而K-Means基于距离和均值。</p><p id="2491" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在K-NN中，一个对象可以有许多簇，而在K-NN中，意味着一个数据属于一个簇。</p><h2 id="32f5" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">小批量K均值</h2><p id="246c" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">小批量用于减少计算时间。</p><p id="40ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每次迭代时，小批量接收随机选择的小堆栈。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mg"><img src="../Images/b292e450918a978023e6903d7999208d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*QWvshAqE0nFcfkVAnCqh4g.png"/></div></figure><p id="8972" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">小批量K-means更快，但给出的结果略有不同。</p><p id="e0e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你喜欢我的帖子，请别忘了鼓掌。谢谢你。</p><h2 id="060f" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated"><strong class="ak">参考文献:</strong></h2><p id="75a4" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">[1]全球人工智能中心机器学习介绍</p><p id="29f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">【https://globalaihub.com/ T4】</p><p id="6256" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2]亚当·海斯(2021年4月2日)，方差:</p><p id="7bf7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">https://www . investopedia . com/terms/v/variance . ASP #:~:text = The % 20 term % 20 variance % 20 refers % 20 to，other % 20 number % 20 in % 20 The % 20 set。</p><p id="8572" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[3]sada yeong，(2020年4月4日)，时间序列变异因素和模型:</p><div class="mi mj ez fb mk ml"><a href="https://sodayeong.tistory.com/19" rel="noopener  ugc nofollow" target="_blank"><div class="mm ab dw"><div class="mn ab mo cl cj mp"><h2 class="bd hj fi z dy mq ea eb mr ed ef hh bi">시계열의 변동요인과 모형</h2><div class="ms l"><h3 class="bd b fi z dy mq ea eb mr ed ef dx translated">하나또는여러사건(사상)에대해시간의흐름에따라일정한간격으로관찰하여기록한자료(데이터)前특정소비재의월별판매량혹은연도별농작물의생산량등어떠한경제현상이나…종합주가지수</h3></div><div class="mt l"><p class="bd b fp z dy mq ea eb mr ed ef dx translated">sodayeong.tistory.com</p></div></div><div class="mu l"><div class="mv l mw mx my mu mz le ml"/></div></div></a></div><p id="87a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[4] Pavan Vadapalli，(2020年7月27日)，机器学习中的后悔模型的类型:</p><p id="be71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae mh" href="https://www.upgrad.com/blog/types-of-regression-models-in-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://www . upgrad . com/blog/types-of-regression-models-in-machine-learning/</a></p><p id="86e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[5]维基百科，(2018年10月19日)，SVM_margin.png</p><p id="fc3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae mh" href="https://en.wikipedia.org/wiki/File:SVM_margin.png" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/File:SVM_margin.png</a></p><p id="7bca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[6]维基百科，(2021年7月16日)，自举汇总</p><p id="84fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae mh" href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Bootstrap_aggregating</a></p><p id="5e19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[7]维基百科，(2021年7月15日)，自举</p><p id="f05b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae mh" href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#cite_note-Varian-3" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Bootstrapping _(statistics)# cite _ note-Varian-3</a></p><p id="93e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[8]Coursera，(2021年10月1日)，作业:逻辑回归:</p><p id="2470" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae mh" href="https://www.coursera.org/learn/classification-vector-spaces-in-nlp/programming/P4CTb/assignment-logistic-regression/lab?path=%2Fnotebooks%2FWeek1%2FC1_W1_Assignment.ipynb" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/class ification-vector-spaces-in-NLP/programming/P4CTb/assignment-logistic-regression/</a></p></div></div>    
</body>
</html>
<html>
<head>
<title>Introduction to Deterministic Policy Gradient (DPG)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">确定性政策梯度介绍(DPG)</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/introduction-to-deterministic-policy-gradient-dpg-e7229d5248e2?source=collection_archive---------4-----------------------#2021-08-26">https://medium.com/geekculture/introduction-to-deterministic-policy-gradient-dpg-e7229d5248e2?source=collection_archive---------4-----------------------#2021-08-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7b00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将探索论文<a class="ae jd" href="http://proceedings.mlr.press/v32/silver14.pdf" rel="noopener ugc nofollow" target="_blank">确定性策略梯度算法</a> (Silver等人)之后的概念，实现论文中提出的算法COPDAC(兼容的非策略确定性参与者-批评家)，并在连续控制环境MountainCar上训练代理。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/9115d395f48778aa52fe2001654e0420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*yDszF3Oecx38DznVuIWQlA.gif"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Taken from OpenAI gym documentation</figcaption></figure><h1 id="69a4" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">简介</strong></h1><p id="ea00" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">在论文<a class="ae jd" href="http://proceedings.mlr.press/v32/silver14.pdf" rel="noopener ugc nofollow" target="_blank">确定性策略梯度算法</a>中，Silver提出了一类处理连续动作空间的新算法。本文从第一性原理推导出确定性政策梯度定理。确定性策略梯度的一个优点是，与随机策略梯度相比，确定性版本更简单，并且可以更有效地计算。然后，论文提出了利用确定性策略梯度的策略上和策略外行动者-批评家算法。这篇论文建立在我已经写过的几个概念之上，比如<a class="ae jd" rel="noopener" href="/nerd-for-tech/reinforcement-learning-introduction-to-policy-gradients-aa2ff134c1b">政策梯度</a>、<a class="ae jd" rel="noopener" href="/geekculture/actor-critic-implementing-actor-critic-methods-82efb998c273">行动者-批评家方法</a>和<a class="ae jd" href="https://chengxi600.medium.com/actor-critic-off-policy-actor-critic-algorithm-cca654845558" rel="noopener">非政策行动者-批评家</a>，所以当它们出现的时候我不会详细讨论它们。</p><h1 id="f533" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">确定性政策梯度定理</h1><p id="6f07" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">类似于随机策略梯度，我们的目标是最大化一个性能测量函数<em class="kt"> J(θ) = E[r_γ |π] </em>，它是遵循策略π的期望总贴现报酬，其中<em class="kt"> θ </em>是我们正在更新的函数的参数。这是一个扩展版本的绩效评估函数，我们对状态分布<em class="kt"> ρ </em>和每个状态中的行为的所有奖励进行求和。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ku"><img src="../Images/d31bcc62b7912fe2cc91ba0dc3ae1109.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*SFxJ1_-Jib1mmP4tpqWXxQ.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Taken from Determinist Policy Gradient Algorithms (Silver et al. 2014)</figcaption></figure><p id="d936" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于确定性策略，这有点不同。与随机策略不同，我们将我们的策略定义为函数<em class="kt"> μ_θ: S → A，</em>，其中μ接受状态空间<em class="kt"> S </em>并输出跟随参数<em class="kt"> θ的动作空间<em class="kt"> A </em>。</em>在这里，我们只需要对状态空间求和，而不是对动作进行积分，因为动作是确定的。然后我们可以把它写成一个期望，我们从状态分布<em class="kt"> ρ中抽取一个状态。</em></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kv"><img src="../Images/3501f1f57c0f3a24c2ac58cfe181b05a.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*lDUqRlMwoIANICSWliYrhw.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Taken from Determinist Policy Gradient Algorithms (Silver et al. 2014)</figcaption></figure><p id="df74" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大多数无模型学习算法都是基于策略迭代的，一般框架由<em class="kt">策略评估</em>和<em class="kt">策略改进组成。</em>策略评估涉及诸如估计动作-状态函数Q的方法，而策略改进方法通常涉及根据该动作-状态函数更新策略。这方面的一个例子是当我们为Q-learning建立一个策略时。在学习了策略评估函数Q之后，我们使用argmax构建一个策略来选择具有最高Q值的动作。</p><p id="b082" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于确定性策略，我们将做一些类似的事情。由于我们的行动空间是连续的，我们将在Q函数的梯度方向上移动我们的策略。策略将向最大Q值收敛，而不是通过采取argmax操作来最大化Q值。我们可以把这个梯度写成期望值:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kw"><img src="../Images/109646bacc1198d888bc1f769de1c7d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*rybaibq_77yrGFJBsy6uNQ.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Taken from Determinist Policy Gradient Algorithms (Silver et al. 2014)</figcaption></figure><p id="ba41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们可以应用一次链式法则，这样我们可以看到<em class="kt"> Q </em>相对于动作<em class="kt"> a </em>的梯度，以及<em class="kt"> μ </em>相对于参数<em class="kt"> θ: </em>的梯度</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kx"><img src="../Images/a76976256e0b5e9cbe6883c85cd07fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*nNxihIu80_yHzvYeR8ynBA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Taken from Determinist Policy Gradient Algorithms (Silver et al. 2014)</figcaption></figure><p id="4c11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是确定性政策梯度的扩展形式:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ky"><img src="../Images/935ec9ba5513c96468dfe5b6772faea5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*1Lej3d1W4udXhrHc7roeGg.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Taken from Determinist Policy Gradient Algorithms (Silver et al. 2014)</figcaption></figure><p id="7bac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种确定性政策梯度的证明在结构上类似于<a class="ae jd" href="https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> (Sutton et al. 1999) </a>中详述的政策梯度定理的证明。看看确定性政策梯度，你可能会注意到它有点类似于随机政策梯度。本文引入一个定理，证明确定性政策梯度实际上只是随机政策梯度的一个特例。</p><p id="10c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回想一下政策梯度定理:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kz"><img src="../Images/5ded0347a4dc3f694214d55a24e132e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/0*9pec6lsxfabrIgt-.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Taken from Sutton &amp; Barto, 2017</figcaption></figure><p id="4fa3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以用确定性策略<em class="kt"> μ_θ </em>和方差σ来参数化策略函数<em class="kt"> π </em>，类似于用于连续动作的策略参数化。定理指出，当σ接近0时，π = μ。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es la"><img src="../Images/fc1bb54f63f24dd6c1e8d46362ad717c.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*M-6_-11s7kLzl59ENReyFQ.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Taken from Determinist Policy Gradient Algorithms (Silver et al. 2014)</figcaption></figure><p id="17fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个定理的证明可以在论文的附录C中找到。这个定理很重要，因为它表明其他技术，如相容函数近似、自然政策梯度和经验重现，可以应用于确定性政策梯度。</p><h1 id="63d6" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">确定性策略梯度算法</h1><p id="2bce" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">利用确定性策略梯度，我们可以推导出不同种类的算法，例如针对策略上和策略外的行动者-批评家方法。本文从一个简单的策略算法开始，该算法使用SARSA来更新critical。类似于随机行动者-批评家方法，我们有一个行动者来更新策略，在这种情况下是确定性的，还有一个批评家，它将通过学习一组参数来逼近真实的行动状态函数Q。以下是算法的更新公式:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lb"><img src="../Images/2700e082a71342e42b3c6b67747b5c15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u90W9VDa6XMOY4nzlwB9Xg.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx">Taken from Determinist Policy Gradient Algorithms (Silver et al. 2014)</figcaption></figure><p id="b02c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">类似于策略上的算法，我们也可以将确定性策略梯度用于策略外的算法。我们可以使用静态行为策略来获得我们的动作，而不是使用从策略中采样的轨迹。我们可以用类似于随机政策外行动者评论家的方式推导出绩效评估梯度的期望值，我在我的<a class="ae jd" href="https://chengxi600.medium.com/actor-critic-off-policy-actor-critic-algorithm-cca654845558" rel="noopener">上一篇文章</a>中提到过:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lg"><img src="../Images/3a0930820ba8cd3cbf1fba53e206ddc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*ErbPHeFga3FwWwtMzj7lwQ.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Taken from Determinist Policy Gradient Algorithms (Silver et al. 2014)</figcaption></figure><p id="e688" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这与政策梯度的预期相同，只是状态是从单独的行为政策β中取样的。请注意，策略梯度没有使用重要性权重，因为我们没有对操作进行积分。我们还可以通过使用Q-learning来避免批评家的重要性权重:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lh"><img src="../Images/d93f10cfaea2af4e308d48e8da26daa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*I9KO7HAFOWLnNXm9bN402g.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Taken from Determinist Policy Gradient Algorithms (Silver et al. 2014)</figcaption></figure><p id="b382" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面提出的两个算法中，还有一个大问题。我们用一个近似值代替了确定性策略梯度中的动作状态函数，但是这并不保证近似梯度将遵循真实梯度。然后，本文提出了一类相容函数逼近器，以确保逼近将遵循真实梯度。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es li"><img src="../Images/b7d548825f3c28f6ae596e6e9ced3f28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*F4SCNwTofi5Lkh5zNqI7Lw.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Taken from Determinist Policy Gradient Algorithms (Silver et al. 2014)</figcaption></figure><p id="cbde" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个定理的证明如下:</p><p id="2053" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，如果<em class="kt"> w </em>最小化MSE，那么MSE相对于<em class="kt"> w </em>的梯度为0。我们知道这一点，因为如果<em class="kt"> w </em>使误差最小化，那么<em class="kt"> w </em>必定处于最小值，因此，梯度为0。然后，我们有以下内容:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lj"><img src="../Images/44b442709fb75405e96f4706f8a4dea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wYcBwD7VQ8QfD8VRbB2-iQ.png"/></div></div></figure><p id="b52f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于第(1)至(3)行，我们扩展了<em class="kt"> MSE </em>的梯度。然后，我们证明<em class="kt"> ∇_w ϵ(s，w) = ∇_θ μ(s，θ) </em>。我们在(4)和(5)中展开ϵ的梯度，然后将定理的条件1用于(6)。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lk"><img src="../Images/2aec59a34d20373519b9c1298bc4f9ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OaXe6ken7btLal9KZ_r4cg.png"/></div></div></figure><p id="006a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们可以在(8)中插入<em class="kt"> ∇μ </em>并在(9)中展开<em class="kt"> ϵ </em>。然后，我们将<em class="kt"> ∇μ </em>分布在(10)中，并使用近似器<em class="kt"> Q_w </em>证明确定性策略梯度定理在以下两个条件下是相容的。请注意，兼容函数逼近器也适用于非策略情况！</p><p id="a81c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后提出了满足定理两个条件的近似子Q_w的形式。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ll"><img src="../Images/786d347632e2c2e815a6ae597ad4c5df.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*ivk_FSk_-R8Od8fHNVGF9A.png"/></div></figure><p id="04ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，我们有了动作状态函数的基本形式。函数的组成部分很简单:优势函数<em class="kt"> A </em>估计选择动作<em class="kt"> a </em>相对于<em class="kt"> μ(s </em>)的优势，价值函数<em class="kt"> V </em>是状态<em class="kt"> s </em>的值。对于价值函数，我们可以仅使用具有参数<em class="kt"> v </em>和状态特征ϕ(s).的线性近似器</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lm"><img src="../Images/07e2bc1987bbaffeb958b5049763712f.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*_5MzyzQUXVq6-KrwEsoV3A.png"/></div></figure><p id="6e46" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于优势函数，我们也可以使用带有参数<em class="kt"> w </em>的线性近似器，并定义动作状态特征ϕ(s，a)。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ln"><img src="../Images/46cb54aea63d15b6677657baa6ae8789.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*eUg8PYizm9AB4XLiELgGWQ.png"/></div></figure><p id="3eb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的动作-状态特征与采取动作<em class="kt"> a </em>相对于<em class="kt"> μ(s) </em>的优势成正比，并且还满足相容性定理的条件1。以下是近似器的完整表格:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lo"><img src="../Images/a790c9e7bd3ba254d7ddb55c18cc7d47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MJx0KbYyk4qANpXQloCEkw.png"/></div></div></figure><p id="8caa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意，给定一个<em class="kt"> m </em>动作空间大小和<em class="kt"> n </em>策略参数，<em class="kt"> ∇μ(s) </em>将是一个雅可比矩阵，ϕ(s)将是一个<em class="kt"> n × 1 </em>向量，<em class="kt"> w </em>也将是一个<em class="kt"> n × 1 </em>向量。现在，我们必须满足定理的条件2，<em class="kt"> w </em>必须最小化Q_w的梯度和真实梯度之间的均方误差。这里的问题类似于学习一个价值函数。由于对真实梯度的无偏值进行采样太难，我们将仅使用近似方法(如SARSA或Q-learning)来近似真实梯度。该论文确实承认，尽管真实梯度的近似不会精确地最小化均方误差，但是该近似仍然可以近似满足第二个条件。</p><p id="ed3a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有鉴于此，该论文提出了一种兼容的非策略确定性行动者-批评家算法(COPDAC-Q ),该算法使用Q学习批评家:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lp"><img src="../Images/6fb5ff758d7a9189fb322bf8fb9aa4dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*aozj8UBtonnXKOvmRlX0FA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Taken from Determinist Policy Gradient Algorithms (Silver et al. 2014)</figcaption></figure><p id="f818" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可能注意到的一个问题是，偏离策略的Q-learning倾向于发散。我们可以做的一个改进是通过使用梯度时间差分方法，这种方法可以保证在非策略设置中的收敛，就像我们对<a class="ae jd" rel="noopener" href="/geekculture/actor-critic-off-policy-actor-critic-algorithm-cca654845558">非策略行动者-批评家</a>所做的一样。然后，该论文提出了另一种算法，COPDAC-GQ算法，该算法使用梯度Q学习评价器:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lq"><img src="../Images/398efce19b23f63fa5249b5809b4ee74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*by-GlvNAuMdHSbv0Qd_T_Q.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Taken from Determinist Policy Gradient Algorithms (Silver et al. 2014)</figcaption></figure><p id="7faf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，本文使用一个<a class="ae jd" href="https://proceedings.neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf" rel="noopener ugc nofollow" target="_blank">自然政策梯度</a>，它选择相对于费希尔信息度量的最陡梯度上升方向。我们使用费雪信息度量的确定性版本，当随机策略的方差接近0时，它是随机版本的特殊极限情况。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lr"><img src="../Images/e718f8cf997f362f7aa941fd60456ec4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*e7esILHMfNDAJeq7yVUyyw.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx">Taken from Determinist Policy Gradient Algorithms (Silver et al. 2014)</figcaption></figure><p id="6fbd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">幸运的是，通过我们的兼容函数逼近器，我们可以很容易地看到，相对于费希尔信息度量的最陡上升方向就是简单的<em class="kt"> w. </em></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ls"><img src="../Images/431bcbe5c1c70102409528de7b182784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*CxM21bDg-cptA0O6iH8EBQ.png"/></div></figure><h1 id="bee2" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">实现COPDAC-Q</h1><p id="ae2a" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">现在，让我们试着实现一个DPG！我将尝试实现一个带有自然政策梯度的COPDAC-Q。实施是在Pytorch中完成的，我们将对策略、Q函数和值函数使用线性近似。</p><p id="ed54" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，快速介绍一下登山车的环境。代理是一辆从山脚出发的汽车，想要到达山顶的绿旗。状态空间对于轿厢速度和轿厢位置是连续的，动作空间也是连续的，一个用于计算驱动力的功率系数。到达旗帜的奖励是100，而不像论文在每一步只有-1的奖励，并且代理在每一步的能量使用被惩罚。当代理在100集内达到90分时，环境被视为已解决。</p><p id="43ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是一些使用的超参数:</p><ul class=""><li id="23c2" class="lt lu hi ih b ii ij im in iq lv iu lw iy lx jc ly lz ma mb bi translated">γ(折扣系数)= 0.99</li><li id="7384" class="lt lu hi ih b ii mc im md iq me iu mf iy mg jc ly lz ma mb bi translated">最大步数(每集)= 5000</li><li id="dcae" class="lt lu hi ih b ii mc im md iq me iu mf iy mg jc ly lz ma mb bi translated">已解决_得分= 90</li><li id="43b7" class="lt lu hi ih b ii mc im md iq me iu mf iy mg jc ly lz ma mb bi">α_θ = 0.005</li><li id="964a" class="lt lu hi ih b ii mc im md iq me iu mf iy mg jc ly lz ma mb bi translated">α_w = 0.03</li><li id="43d0" class="lt lu hi ih b ii mc im md iq me iu mf iy mg jc ly lz ma mb bi translated">αv = 0.03</li><li id="475f" class="lt lu hi ih b ii mc im md iq me iu mf iy mg jc ly lz ma mb bi translated">BATCH_SIZE = 8</li><li id="eb27" class="lt lu hi ih b ii mc im md iq me iu mf iy mg jc ly lz ma mb bi translated">Numpy随机种子= 0</li><li id="1c76" class="lt lu hi ih b ii mc im md iq me iu mf iy mg jc ly lz ma mb bi translated">Pytorch随机种子= 0</li></ul><p id="049d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有趣的是，这里的随机种子确实很重要，因为我无法让我的实现长时间工作，但更改随机种子使它工作了。</p><p id="be5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这个实现，我们将利用经验重放，每10步进行一次批量更新。行为策略只是从动作空间中随机抽样。我们还将使用从10，000个随机状态采样中获得的平均值和标准差对状态进行标准化。请注意，我们为策略评估选择了较高的学习率，因为我们希望评估比策略收敛得更快，从而使策略朝着正确的方向移动。</p><p id="a80b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">先来训练我们的代理，100集，批量1。以下是培训历史:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mh"><img src="../Images/4b9c79ea2f14cbb76c1b50e04e95afb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*e1g_TCBQu7NPmLpNFGVNkg.png"/></div></figure><p id="756e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在100集之后，我们的代理能够在10次播放中获得82.06的平均分数，方差为0.009。由于算法的本质是确定性的，低方差是有意义的，因为唯一的不确定性是代理的随机起始状态。灰线是来自随机获得幸运的行为策略的训练分数，蓝线是我们的目标策略分数。一个有趣的事情是，即使政策最初在17/18集左右走向错误的方向，政策仍然能够极快地恢复。我假设这大约是Q和值函数开始收敛的时间，因此策略可以快速学习。它看起来是这样的:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mi"><img src="../Images/b52af51faef7705707959ac9765f53bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/1*wQ57UQL9cJDxXLBRpwzhgA.gif"/></div></figure><p id="582a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们用批量8来培训我们的代理。以下是培训历史:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mh"><img src="../Images/f98b3009be6aec1b34945e77613d6aaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*QdAFMTAY9BXNEH4pIVhIrA.png"/></div></figure><p id="95f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的代理能够获得90.10的平均分数，方差为0.0203。看看比分历史，这似乎是一个错误。目标政策1集后近100的高分起步。然而，在查看了训练参数并确保没有NaN值后，我尝试重新训练代理1集。令人惊讶的是，该代理的平均分为98±0.03，可以在短短1集内解决登山车环境！它看起来是这样的:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mj"><img src="../Images/709948a5310d52e111a18cb2dc614cd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/1*bPYwPXTRw1suX-cnO5N3JA.gif"/></div></figure><p id="366b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有趣的是。对于单集代理，购物车需要更长的时间才能到达旗帜，但得分更高。如果我们用DPG论文的指标来评估，那么它的得分会更低，因为它需要更长的时间才能到达旗帜。然而，手推车在这里使用较少的能量来建立到达旗帜的动力，导致较高的分数和能量使用惩罚。对于100集的代理，我猜想代理过度训练，并由于使用Q-learning评论家而转向一个次优的解决方案。</p><p id="d345" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">还有一点要注意的是，我得到的结果并不总是一致的，因为代理无法解决某些试验的环境。在修改奖励以奖励机械能的变化后，∇me =∇(0.5毫伏+兆瓦时)，代理人能够始终如一地解决环境问题。</p><h1 id="2031" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">最后的想法</h1><p id="03ff" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">我非常兴奋能够处理这个实现，因为我希望能够复制DPG论文的结果，而且它也是DDPG算法的重要组成部分。最后，我在Pytorch中使用线性近似成功地解决了MountainCar环境。在这个过程中，我确实学到了一些东西，那就是随机种子确实很重要。我的实现在很长一段时间内都没有工作，直到我更改了种子。我将来要做的一个改变是聚集来自不同种子的结果，因为在一个种子上做所有事情可能会给出有偏见的结果。直觉上，你会认为算法不应该依赖于随机种子，但这是强化学习的本质。在下一篇文章中，我将借鉴DPG和DQN的概念，探索使用深度学习的DDPG算法。</p><p id="99e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">code:<a class="ae jd" href="https://github.com/chengxi600/RLStuff/blob/master/Deterministic%20Policy%20Gradients/COPDAC-Q.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/cheng i600/rl stuff/blob/master/Deterministic % 20 policy % 20 gradients/COP DAC-q . ipynb</a></p><p id="c652" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><ul class=""><li id="cefa" class="lt lu hi ih b ii ij im in iq lv iu lw iy lx jc ly lz ma mb bi translated"><a class="ae jd" href="http://proceedings.mlr.press/v32/silver14.pdf" rel="noopener ugc nofollow" target="_blank">确定性政策梯度算法(Silver等人，2014年)</a></li><li id="799d" class="lt lu hi ih b ii mc im md iq me iu mf iy mg jc ly lz ma mb bi translated"><a class="ae jd" href="http://proceedings.mlr.press/v32/silver14-supp.pdf" rel="noopener ugc nofollow" target="_blank">确定性政策梯度算法:补充材料(Silver等人，2014年)</a></li></ul></div></div>    
</body>
</html>
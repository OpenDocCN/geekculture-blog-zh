<html>
<head>
<title>Speech Recognition Using CRNN, CTC Loss, DeepSpeech Beam Search Decoder, and KenLM Scorer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用CRNN、CTC Loss、DeepSpeech Beam Search解码器和KenLM Scorer的语音识别</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/audio-recognition-using-crnn-ctc-loss-beam-search-decoder-and-kenlm-scorer-24472e43fb2f?source=collection_archive---------5-----------------------#2021-03-26">https://medium.com/geekculture/audio-recognition-using-crnn-ctc-loss-beam-search-decoder-and-kenlm-scorer-24472e43fb2f?source=collection_archive---------5-----------------------#2021-03-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/5f639cf9029d099f5831177e8036293f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_bQ1Z1Wy13j6IIZ5WOJ2tg.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx">Photo by <a class="ae hv" href="https://unsplash.com/@mattbotsford" rel="noopener ugc nofollow" target="_blank"><strong class="bd hw">Matt Botsford</strong></a> on Unsplash</figcaption></figure><div class=""/><h1 id="0c23" class="iw ix hz bd hw iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">理论</h1><p id="b63e" class="pw-post-body-paragraph jt ju hz jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">今天最流行的三款端到端ASR(自动语音识别)机型分别是<a class="ae hv" href="https://arxiv.org/pdf/1904.03288.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="jv ia">Jasper</strong></a><a class="ae hv" href="https://arxiv.org/pdf/1712.09444.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="jv ia">wave 2 letter+</strong></a><strong class="jv ia"/><a class="ae hv" href="https://arxiv.org/pdf/1512.02595.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="jv ia">Deep Speech 2</strong></a>。现在它们<a class="ae hv" href="https://nvidia.github.io/OpenSeq2Seq/html/speech-recognition.html#speech-recognition" rel="noopener ugc nofollow" target="_blank"><strong class="jv ia"/></a>作为英伟达<a class="ae hv" href="https://github.com/NVIDIA/OpenSeq2Seq" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia"> OpenSeq2Seq </strong> </a>工具包的一部分。所有这些ASR系统都基于神经声学模型，该模型在每个时间步<strong class="jv ia"> t、</strong>的所有目标人物<strong class="jv ia"> Pt(c) </strong>上产生概率分布<strong class="jv ia">Pt(c)</strong>，该概率分布又由<a class="ae hv" href="https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia"> CTC损失</strong> </a> <strong class="jv ia"> </strong>函数评估<strong class="jv ia"><strong class="jv ia">:</strong></strong></p><figure class="ks kt ku kv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kr"><img src="../Images/ae8f93be5fb905dae590697da86a0766.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wLUVEkzDL-37RPa9.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx">Summarization of CTC ASR pipeline’s architectures by <a class="ae hv" href="https://nvidia.github.io/OpenSeq2Seq/html/speech-recognition.html#introduction" rel="noopener ugc nofollow" target="_blank"><strong class="bd hw">Nvidia</strong></a></figcaption></figure><p id="076c" class="pw-post-body-paragraph jt ju hz jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">本质上，本文中描述的端到端语音识别系统由几个简单的部分组成:</p><ul class=""><li id="f01d" class="lb lc hz jv b jw kw ka kx ke ld ki le km lf kq lg lh li lj bi translated">使用<a class="ae hv" href="https://librosa.org/doc/latest/index.html" rel="noopener ugc nofollow" target="_blank"><strong class="jv ia">【librosa】</strong></a><strong class="jv ia"/>或<strong class="jv ia"/><a class="ae hv" href="https://pytorch.org/audio/stable/index.html" rel="noopener ugc nofollow" target="_blank"><strong class="jv ia">torchaudio</strong></a>将原始波形转换为<a class="ae hv" href="https://en.wikipedia.org/wiki/Spectrogram" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">频谱图</strong> </a>。<a class="ae hv" rel="noopener" href="/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53"> <strong class="jv ia">本文</strong> </a>提供了对<strong class="jv ia"> mel光谱图</strong>和<a class="ae hv" href="https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">的直观理解。</strong></a></li></ul><figure class="ks kt ku kv fd hk"><div class="bz dy l di"><div class="lk ll l"/></div><figcaption class="hr hs et er es ht hu bd b be z dx">How to convert <strong class="ak">waveform to spectrogram</strong> using <strong class="ak">librosa</strong> and <strong class="ak">torchaudio</strong></figcaption></figure><figure class="ks kt ku kv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lm"><img src="../Images/2a49b5839e31f6443848103647f4f05d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YMlGvHq-GfF9RJ8TZDbmEA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx">Waveform converted to spectrogram</figcaption></figure><ul class=""><li id="ec95" class="lb lc hz jv b jw kw ka kx ke ld ki le km lf kq lg lh li lj bi translated">声谱图是一幅图像，我们可以用<a class="ae hv" href="https://d2l.ai/chapter_convolutional-neural-networks/conv-layer.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">卷积层</strong> </a>从中提取特征。在这篇文章中，我将使用一个流行的组合，一个<a class="ae hv" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia"> Conv2d </strong> </a>层和<a class="ae hv" href="https://pytorch.org/docs/stable/generated/torch.nn.GELU.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia"> GELU </strong> </a> <strong class="jv ia"> </strong>激活函数(因为<a class="ae hv" href="https://arxiv.org/pdf/1710.05941.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">比<a class="ae hv" href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html" rel="noopener ugc nofollow" target="_blank"><strong class="jv ia">ReLU</strong></a><strong class="jv ia"/>在一系列实验中更好)和<a class="ae hv" href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">退出</strong> </a>进行正则化。此外，我认为使用<a class="ae hv" href="https://arxiv.org/pdf/1607.06450.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">层归一化</strong> </a>和<a class="ae hv" href="https://arxiv.org/pdf/1712.09913.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">跳过连接</strong> </a>对于更快的收敛和更好的泛化是有益的。因此，神经网络的第一部分将由以下层组成:</strong></a></li></ul><pre class="ks kt ku kv fd ln lo lp lq aw lr bi"><span id="1e54" class="ls ix hz lo b fi lt lu l lv lw"><strong class="lo ia"># First Conv2d layer<br/></strong>Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))</span><span id="8f80" class="ls ix hz lo b fi lx lu l lv lw"><strong class="lo ia"># 7 blocks of these layers<br/># Skip connection is added to this Conv2d layer<br/></strong>LayerNorm((64,), eps=1e-05, elementwise_affine=True)<br/>GELU()<br/>Dropout(p=0.2, inplace=False)<br/>Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span></pre><ul class=""><li id="2c27" class="lb lc hz jv b jw kw ka kx ke ld ki le km lf kq lg lh li lj bi translated">同时，声谱图是时间序列数据，所以使用双向<a class="ae hv" href="https://d2l.ai/chapter_recurrent-neural-networks/rnn.html" rel="noopener ugc nofollow" target="_blank"><strong class="jv ia"/></a>层如<a class="ae hv" href="https://arxiv.org/pdf/1409.1259.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="jv ia">【GRU】</strong></a>从<strong class="jv ia"> CNN </strong>层检测到的特征中捕捉时频模式是很自然的。出于与之前相同的原因，我将使用一个图层正常化和下降:</li></ul><pre class="ks kt ku kv fd ln lo lp lq aw lr bi"><span id="acc6" class="ls ix hz lo b fi lt lu l lv lw"><strong class="lo ia"># I will use 5 blocks of these layers</strong><br/>LayerNorm((512,), eps=1e-05, elementwise_affine=True)<br/>GELU()<br/>GRU(512, 512, batch_first=True, bidirectional=True)<br/>Dropout(p=0.2, inplace=False)</span></pre><ul class=""><li id="39ac" class="lb lc hz jv b jw kw ka kx ke ld ki le km lf kq lg lh li lj bi translated">实际上，我们需要的是将声谱图图像的每个小垂直切片映射到某个字符，因此我们的模型将为每个垂直特征向量和每个字符产生一个概率分布。这可以使用<strong class="jv ia"> </strong> <a class="ae hv" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">线性</strong> </a> <strong class="jv ia"> </strong>(完全连接)层来完成:</li></ul><pre class="ks kt ku kv fd ln lo lp lq aw lr bi"><span id="579d" class="ls ix hz lo b fi lt lu l lv lw"><strong class="lo ia"># Input for this layer is an output from the last GRU layer<br/># We need to gradually reduce number of outputs from 1024 to the number of characters used in LibriSpeech dataset - 28 + 'blank'</strong><br/>Linear(in_features=<strong class="lo ia">1024</strong>, out_features=<strong class="lo ia">512</strong>, bias=True)<br/>GELU()<br/>Dropout(p=0.2, inplace=False)<br/>Linear(in_features=<strong class="lo ia">512</strong>, out_features=<strong class="lo ia">29</strong>, bias=True)</span></pre><ul class=""><li id="b6af" class="lb lc hz jv b jw kw ka kx ke ld ki le km lf kq lg lh li lj bi translated">然后我们可以使用<strong class="jv ia">贪婪解码器</strong>或<strong class="jv ia">波束搜索解码器</strong>来产生最终的转录。<br/>一个<strong class="jv ia"> </strong> <a class="ae hv" href="https://d2l.ai/chapter_recurrent-modern/beam-search.html#greedy-search" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">贪婪解码器</strong> </a>接受模型的输出，对于每个垂直特征向量，它选择概率最高的字符。<br/>一个<strong class="jv ia"> </strong> <a class="ae hv" href="https://d2l.ai/chapter_recurrent-modern/beam-search.html#id1" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">光束搜索解码器</strong> </a>稍微复杂一些。波束搜索基于一种试探法，该试探法假定具有高概率的随机变量链具有相当高的概率条件。基本上，它取<strong class="jv ia"> p(x1) </strong>的<strong class="jv ia"> k </strong>个最大概率解，然后对其中每一个取<strong class="jv ia"> k </strong>个最大概率解<strong class="jv ia"> p(x2|x1) </strong>。然后我们需要取那些值最高的<strong class="jv ia"> k </strong>为<strong class="jv ia"> p(x2|x1) * p(x1) </strong>并重复。<br/>我认为吴恩达的<a class="ae hv" href="https://www.youtube.com/watch?v=RLWuzLLSIgw&amp;ab_channel=DeepLearningAI" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">这个视频</strong> </a> <strong class="jv ia"> </strong>和<a class="ae hv" href="https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/?utm_source=dlvr.it&amp;utm_medium=twitter" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">这篇文章</strong> </a>是关于这个主题最直观的指南。<br/>据<strong class="jv ia"> </strong> <a class="ae hv" href="https://arxiv.org/pdf/1703.03906.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">本文</strong> </a> <strong class="jv ia"> </strong>关于NMT由谷歌、<strong class="jv ia">“…我们发现…一个调好的波束搜索对获得最先进的结果至关重要。”</strong></li><li id="c5fe" class="lb lc hz jv b jw ly ka lz ke ma ki mb km mc kq lg lh li lj bi translated"><strong class="jv ia"> CRNN </strong>车型照常，<a class="ae hv" href="https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia"> CTC损耗</strong> </a>将在训练过程中使用。你可以在这里<a class="ae hv" href="https://xiaodu.io/ctc-explained/" rel="noopener ugc nofollow" target="_blank"><strong class="jv ia">这里</strong></a><strong class="jv ia"/>或者<a class="ae hv" href="https://distill.pub/2017/ctc/" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">这里</strong> </a>阅读更多关于这个损失函数的内容。</li><li id="1f74" class="lb lc hz jv b jw ly ka lz ke ma ki mb km mc kq lg lh li lj bi translated">同样，使用<a class="ae hv" href="https://en.wikipedia.org/wiki/Levenshtein_distance" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia"> Levenshtein距离</strong> </a> <strong class="jv ia"> </strong>和<a class="ae hv" href="https://en.wikipedia.org/wiki/Word_error_rate" rel="noopener ugc nofollow" target="_blank"><strong class="jv ia">WER</strong></a><strong class="jv ia"/>作为衡量原始话语和生成的转录之间差异的度量也是相当方便的。</li></ul><p id="e0c0" class="pw-post-body-paragraph jt ju hz jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated"><strong class="jv ia">生成的模型具有以下架构:</strong></p><figure class="ks kt ku kv fd hk"><div class="bz dy l di"><div class="lk ll l"/></div><figcaption class="hr hs et er es ht hu bd b be z dx">Speech recognition model’s architecture <strong class="ak">(1,645,181 trainable parameters)</strong></figcaption></figure><h1 id="c6a8" class="iw ix hz bd hw iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">实践</h1><h2 id="96b7" class="ls ix hz bd hw md me mf jb mg mh mi jf ke mj mk jj ki ml mm jn km mn mo jr mp bi translated">资料组</h2><p id="cb7c" class="pw-post-body-paragraph jt ju hz jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在本文中，我使用了大约<strong class="jv ia"> 1000小时</strong>的分割和对齐的英语语音的<a class="ae hv" href="https://www.openslr.org/12" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia"> LibriSpeech ASR语料库</strong> </a> <strong class="jv ia"> </strong>，这些语音来自阅读有声读物。该数据集中的话语由<strong class="jv ia"> 28 </strong>个字符组成(目标类):</p><figure class="ks kt ku kv fd hk"><div class="bz dy l di"><div class="lk ll l"/></div></figure><p id="8f4b" class="pw-post-body-paragraph jt ju hz jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">然后我们需要使用<a class="ae hv" href="https://pytorch.org/audio/stable/transforms.html#melspectrogram" rel="noopener ugc nofollow" target="_blank"><strong class="jv ia">Mel spectrogram</strong></a><strong class="jv ia"/>将波形转换成频谱图，并定义函数将文本转换成整数，反之亦然:</p><figure class="ks kt ku kv fd hk"><div class="bz dy l di"><div class="lk ll l"/></div></figure><p id="b201" class="pw-post-body-paragraph jt ju hz jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">该校对功能对于准备我们的模型所需的张量<strong class="jv ia">光谱图和标签及其长度</strong>是必要的。这些“长度”张量稍后将由CTC损失函数使用:</p><figure class="ks kt ku kv fd hk"><div class="bz dy l di"><div class="lk ll l"/></div></figure><p id="30ca" class="pw-post-body-paragraph jt ju hz jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">现在，我们必须使用训练和验证数据集初始化<a class="ae hv" href="https://pytorch.org/docs/stable/data.html#loading-batched-and-non-batched-data" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">数据加载器</strong></a><strong class="jv ia"/>，并将随机种子设置为固定值，以获得可重现的结果:</p><figure class="ks kt ku kv fd hk"><div class="bz dy l di"><div class="lk ll l"/></div></figure><h2 id="517d" class="ls ix hz bd hw md me mf jb mg mh mi jf ke mj mk jj ki ml mm jn km mn mo jr mp bi translated">培养</h2><p id="ee61" class="pw-post-body-paragraph jt ju hz jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">下一步是定义训练和验证循环，选择优化器、超参数和指标来评估训练进度。我决定使用<a class="ae hv" href="https://pytorch.org/docs/stable/_modules/torch/optim/adamw.html#AdamW" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia"> AdamW </strong> </a>优化器，学习率相当低的<strong class="jv ia"> 5e-4，</strong>为<strong class="jv ia"> 25 </strong>个时期训练这个模型，并使用<a class="ae hv" href="https://pypi.org/project/levenshtein/" rel="noopener ugc nofollow" target="_blank"><strong class="jv ia">levenshtein</strong></a><strong class="jv ia"/>和<a class="ae hv" href="https://pypi.org/project/jiwer/" rel="noopener ugc nofollow" target="_blank"><strong class="jv ia">jiwer</strong></a><strong class="jv ia"/>包<strong class="jv ia"> </strong>来计算质量度量:</p><figure class="ks kt ku kv fd hk"><div class="bz dy l di"><div class="lk ll l"/></div><figcaption class="hr hs et er es ht hu bd b be z dx">Training and validation loops</figcaption></figure><figure class="ks kt ku kv fd hk"><div class="bz dy l di"><div class="lk ll l"/></div><figcaption class="hr hs et er es ht hu bd b be z dx">Model implementation directly follows the architecture described in the previous paragraph</figcaption></figure><p id="bfca" class="pw-post-body-paragraph jt ju hz jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">为了推论，我使用了<a class="ae hv" href="https://deepspeech.readthedocs.io/en/latest/Decoder.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">光束搜索解码器</strong> </a>通过<a class="ae hv" href="https://github.com/mozilla/DeepSpeech" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">深度语音</strong> </a> <strong class="jv ia">。</strong>它允许我们基于<a class="ae hv" href="https://kheafield.com/code/kenlm/" rel="noopener ugc nofollow" target="_blank"><strong class="jv ia">KenLM</strong></a><strong class="jv ia"/>工具包使用一个<a class="ae hv" href="https://deepspeech.readthedocs.io/en/latest/Scorer.html#scorer-scripts" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">外部语言模型评分器</strong> </a>。</p><figure class="ks kt ku kv fd hk"><div class="bz dy l di"><div class="lk ll l"/></div><figcaption class="hr hs et er es ht hu bd b be z dx">Generation of the <a class="ae hv" href="https://github.com/mozilla/DeepSpeech/blob/master/data/alphabet.txt" rel="noopener ugc nofollow" target="_blank"><strong class="ak">custom alphabet mapping file</strong></a> that will be used for <a class="ae hv" href="https://github.com/mozilla/DeepSpeech/blob/master/native_client/ctcdecode/__init__.py#L41" rel="noopener ugc nofollow" target="_blank"><strong class="ak">Alphabet</strong></a><strong class="ak"> </strong>initialization</figcaption></figure><figure class="ks kt ku kv fd hk"><div class="bz dy l di"><div class="lk ll l"/></div><figcaption class="hr hs et er es ht hu bd b be z dx">KenLM model and external .scorer generation</figcaption></figure><figure class="ks kt ku kv fd hk"><div class="bz dy l di"><div class="lk ll l"/></div><figcaption class="hr hs et er es ht hu bd b be z dx">Inference process using <a class="ae hv" href="https://pypi.org/project/ds-ctcdecoder/" rel="noopener ugc nofollow" target="_blank"><strong class="ak">ds-ctcdecoder</strong></a></figcaption></figure><figure class="ks kt ku kv fd hk er es paragraph-image"><div class="er es mq"><img src="../Images/2f60ae8361b343ef96ac9465a6e7616d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*gv0xUAtJXw2EOQFBaUCqHw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx">Training and validation results</figcaption></figure><p id="d1e7" class="pw-post-body-paragraph jt ju hz jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">这里可用<a class="ae hv" href="https://github.com/dredwardhyde/speech-recognition-examples/blob/main/weights.pth" rel="noopener ugc nofollow" target="_blank"><strong class="jv ia"/></a><strong class="jv ia"/>预训练模型权重，这里可用<strong class="jv ia"/><a class="ae hv" href="https://github.com/dredwardhyde/speech-recognition-examples/blob/main/librispeech.scorer" rel="noopener ugc nofollow" target="_blank"><strong class="jv ia"/></a>生成评分器。另请注意，批量大小是根据可用GPU内存量选择的，该型号在培训期间消耗了大约<strong class="jv ia">22 GB</strong><strong class="jv ia">VRAM</strong>:</p><figure class="ks kt ku kv fd hk er es paragraph-image"><div class="er es mr"><img src="../Images/a01ba84e21ca79cff7fc664045d646d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*xW3dUkSc2o1rMIgJv0o4SQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx">CUDA device used for this project</figcaption></figure><h2 id="9cbd" class="ls ix hz bd hw md me mf jb mg mh mi jf ke mj mk jj ki ml mm jn km mn mo jr mp bi translated">测试</h2><p id="73b6" class="pw-post-body-paragraph jt ju hz jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">出于测试目的，我从测试集中随机抽取了20个光谱图:</p><figure class="ks kt ku kv fd hk"><div class="bz dy l di"><div class="lk ll l"/></div><figcaption class="hr hs et er es ht hu bd b be z dx">Model inference and displaying spectrograms</figcaption></figure><figure class="ks kt ku kv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ms"><img src="../Images/8caf018cb7a162224c70b2b7f009da5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qRDaja7SLckdCU98c3ZVnA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx">Results for a few random samples from the test set</figcaption></figure><p id="ef1a" class="pw-post-body-paragraph jt ju hz jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">正如你所看到的，这个相对较小的模型肯定能够识别人类语音，并在LibriSpeech数据集上表现出良好的性能。</p><p id="cf4a" class="pw-post-body-paragraph jt ju hz jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated"><strong class="jv ia">这个项目在我的GitHub </strong>  <strong class="jv ia">上也有</strong> <a class="ae hv" href="https://github.com/dredwardhyde/speech-recognition-examples" rel="noopener ugc nofollow" target="_blank"> <strong class="jv ia">。</strong></a></p></div></div>    
</body>
</html>
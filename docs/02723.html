<html>
<head>
<title>ML algorithms 1.01: Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML算法1.01:线性回归</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/ml-algorithms-1-01-linear-regression-5829a9698aa9?source=collection_archive---------38-----------------------#2021-05-24">https://medium.com/geekculture/ml-algorithms-1-01-linear-regression-5829a9698aa9?source=collection_archive---------38-----------------------#2021-05-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/44504755170a34180bc0bf04d41f38eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D3kxgoFEpuuDFUgKR_Wjsw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Source: <a class="ae iu" href="https://unsplash.com/photos/P8b0bg-w_YA?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditShareLink" rel="noopener ugc nofollow" target="_blank">Joel &amp; Jasmin</a></figcaption></figure><h2 id="c7f4" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">介绍</h2><p id="8640" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">这是任何人踏入机器学习领域时学习的第一个算法。这个模型的优点在于简单。线性模型具有高偏差和低方差。此外，如果特征被归一化，那么在使用梯度下降时，它有助于算法更快地收敛。缺失值需要在线性回归中进行估算或剔除。离群值影响最佳拟合线的形成。应该使用箱线图或任何其他方法对它们进行过滤。</p><h2 id="d55e" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">假设</h2><ul class=""><li id="04e7" class="ko kp hi jv b jw jx ka kb jg kq jk kr jo ks kn kt ku kv kw bi translated">目标变量和预测变量之间的关系是线性的</li><li id="84dd" class="ko kp hi jv b jw kx ka ky jg kz jk la jo lb kn kt ku kv kw bi translated">这些特征是相互独立的</li><li id="05d5" class="ko kp hi jv b jw kx ka ky jg kz jk la jo lb kn kt ku kv kw bi translated">同方差性:随机变量具有相同的有限方差</li><li id="7a5d" class="ko kp hi jv b jw kx ka ky jg kz jk la jo lb kn kt ku kv kw bi translated">残差和目标变量之间没有关系</li><li id="054c" class="ko kp hi jv b jw kx ka ky jg kz jk la jo lb kn kt ku kv kw bi translated">残差呈正态分布</li></ul><h2 id="4626" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">优势</h2><ul class=""><li id="4e75" class="ko kp hi jv b jw jx ka kb jg kq jk kr jo ks kn kt ku kv kw bi translated">假设为真时表现非常好</li><li id="8731" class="ko kp hi jv b jw kx ka ky jg kz jk la jo lb kn kt ku kv kw bi translated">高模型可解释性</li></ul><h2 id="9d9b" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">不足之处</h2><ul class=""><li id="e90a" class="ko kp hi jv b jw jx ka kb jg kq jk kr jo ks kn kt ku kv kw bi translated">数据几乎不符合假设</li><li id="3e2b" class="ko kp hi jv b jw kx ka ky jg kz jk la jo lb kn kt ku kv kw bi translated">模型容易过度拟合</li></ul><h2 id="a1fe" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">模型</h2><p id="2308" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">设<strong class="jv hj"> X </strong>为具有<em class="lc"> m </em>个样本和<em class="lc"> n </em>个特征的特征集。设<strong class="jv hj"> y </strong>为连续响应。</p><p id="43e9" class="pw-post-body-paragraph jt ju hi jv b jw ld jy jz ka le kc kd jg lf kf kg jk lg ki kj jo lh kl km kn hb bi translated">模型的参数表示为:</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div class="er es li"><img src="../Images/cd9bd25b0699353858da27d8dfa6c049.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/0*gt0cinkZtdBdDZNA.png"/></div></figure><figure class="lj lk ll lm fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/6bbdd72df98b39a046742086fd0ba838.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/0*syPbOG3UU7MSxtaP.png"/></div></figure><p id="790e" class="pw-post-body-paragraph jt ju hi jv b jw ld jy jz ka le kc kd jg lf kf kg jk lg ki kj jo lh kl km kn hb bi translated">我们可以将模型的初始参数𝜷设置为接近0。让我们定义模型的损失(<em class="lc">成本</em>)函数:</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/884fc636faaeda32bbb1a50b0fde106d.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/0*mAoEqE8PDZ5s37CW.png"/></div></figure><figure class="lj lk ll lm fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/e563e57ee9fce30521ea502aaeb12175.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/format:webp/0*PJj4I9sykdOYXww2.png"/></div></figure><p id="e3f7" class="pw-post-body-paragraph jt ju hi jv b jw ld jy jz ka le kc kd jg lf kf kg jk lg ki kj jo lh kl km kn hb bi translated">成本函数的负梯度给出了我们应该为我们的模型优化𝜷的方向。我们以步长η(学习速率)向下移动损失函数的负梯度。注意，我们不更新截距项。</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/a67b37d9b3ad1aadb528bfbc1c78ecaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/0*3_6294gc5BG8JkvE.png"/></div></figure><figure class="lj lk ll lm fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/278cefdd98418274548b0b5f9ddb7b9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/0*lWTAVA13jnJrnxMg.png"/></div></figure><p id="ef46" class="pw-post-body-paragraph jt ju hi jv b jw ld jy jz ka le kc kd jg lf kf kg jk lg ki kj jo lh kl km kn hb bi translated">线性回归的代价函数是凸函数。因此，我们迭代更新𝜷，直到梯度为0。如果我们不能达到梯度0，我们迭代直到梯度小于某个ε值。经过这些步骤，我们获得了模型的最佳参数𝜷和数据的最佳拟合线。</p><h2 id="0dc1" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">估计</h2><p id="4707" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">现在，我们可以通过我们的模型估计连续变量，如下所示:</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/4760d368d915430221d9c1ceac02aace.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/0*_4hXE1ncIWfa0ZMt.png"/></div></figure><h2 id="e29c" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">超参数调谐</h2><p id="e5ee" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">在线性回归中不需要或不可能进行太多的调整。我们可以使用套索或岭回归技术来惩罚成本函数。</p><p id="1f04" class="pw-post-body-paragraph jt ju hi jv b jw ld jy jz ka le kc kd jg lf kf kg jk lg ki kj jo lh kl km kn hb bi translated">岭回归用于通过惩罚较高的参数值𝜷.来减少模型中的方差</p><p id="9837" class="pw-post-body-paragraph jt ju hi jv b jw ld jy jz ka le kc kd jg lf kf kg jk lg ki kj jo lh kl km kn hb bi translated"><em class="lc">岭回归成本函数:</em></p><figure class="lj lk ll lm fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/003708fb85675b9c4fb6b356714cf963.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/0*8xFA_Q-55nJZ4tdo.png"/></div></figure><p id="d5a4" class="pw-post-body-paragraph jt ju hi jv b jw ld jy jz ka le kc kd jg lf kf kg jk lg ki kj jo lh kl km kn hb bi translated">在岭回归中，参数向0收缩，但永远不会变成0。套索回归可以强制某些参数为0。这实现了自动特征选择。</p><p id="5706" class="pw-post-body-paragraph jt ju hi jv b jw ld jy jz ka le kc kd jg lf kf kg jk lg ki kj jo lh kl km kn hb bi translated"><em class="lc">拉索回归成本函数:</em></p><figure class="lj lk ll lm fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/1774b77a8a4289b798b7991993aa549c.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/0*9iph_br7d1rRFKql.png"/></div></figure><p id="296b" class="pw-post-body-paragraph jt ju hi jv b jw ld jy jz ka le kc kd jg lf kf kg jk lg ki kj jo lh kl km kn hb bi translated">* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *<em class="lc">示例代码:</em></p><p id="d523" class="pw-post-body-paragraph jt ju hi jv b jw ld jy jz ka le kc kd jg lf kf kg jk lg ki kj jo lh kl km kn hb bi translated">从sklearn.linear_model导入线性回归作为LR</p><p id="0de6" class="pw-post-body-paragraph jt ju hi jv b jw ld jy jz ka le kc kd jg lf kf kg jk lg ki kj jo lh kl km kn hb bi translated">lr =LR()</p><p id="0dae" class="pw-post-body-paragraph jt ju hi jv b jw ld jy jz ka le kc kd jg lf kf kg jk lg ki kj jo lh kl km kn hb bi translated">lr.fit(X_train，y_train)</p><p id="9c78" class="pw-post-body-paragraph jt ju hi jv b jw ld jy jz ka le kc kd jg lf kf kg jk lg ki kj jo lh kl km kn hb bi translated">y_hat = lr.predict(X_test)</p><p id="7d03" class="pw-post-body-paragraph jt ju hi jv b jw ld jy jz ka le kc kd jg lf kf kg jk lg ki kj jo lh kl km kn hb bi">*****************************************************************</p><h2 id="756a" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">参考</h2><div class="lu lv ez fb lw lx"><a href="https://www.coursera.org/learn/machine-learning?utm_source=gg&amp;utm_medium=sem&amp;utm_campaign=07-StanfordML-IN&amp;utm_content=07-StanfordML-IN&amp;campaignid=1950458127&amp;adgroupid=70479331563&amp;device=c&amp;keyword=andrew%20ng%20machine%20learning&amp;matchtype=e&amp;network=g&amp;devicemodel=&amp;adpostion=&amp;creativeid=351348153032&amp;hide_mobile_promo" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hj fi z dy mc ea eb md ed ef hh bi translated">《汽车杂志》</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">4，188，060已注册机器学习是让计算机在没有明确…</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">www.coursera.org</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml io lx"/></div></div></a></div><div class="lu lv ez fb lw lx"><a href="https://github.com/krishnaik06/Interview-Prepartion-Data-Science/blob/master/Interview%20Preparation-%20Day%202-%20Linear%20Regression.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hj fi z dy mc ea eb md ed ef hh bi translated">krishnaik 06/采访-准备-数据-科学</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">在GitHub上创建一个帐户，为krishnaik 06/采访准备数据科学的发展做出贡献。</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">github.com</p></div></div><div class="mg l"><div class="mm l mi mj mk mg ml io lx"/></div></div></a></div></div></div>    
</body>
</html>
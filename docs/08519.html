<html>
<head>
<title>Mathematics Behind Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降背后的数学</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/mathematics-behind-gradient-descent-f2a49a0b714f?source=collection_archive---------2-----------------------#2021-11-04">https://medium.com/geekculture/mathematics-behind-gradient-descent-f2a49a0b714f?source=collection_archive---------2-----------------------#2021-11-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7579" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">了解梯度下降算法在机器学习中的工作原理。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/d2836aa633acd991f17260cab9e90cc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PCchQItCclbDnJXD"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Photo by <a class="ae jt" href="https://unsplash.com/@aleskrivec?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ales Krivec</a> on <a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="fda4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你是一个机器学习爱好者，你可能会遇到梯度下降算法这个术语，甚至在你的一些模型中使用过它。你会知道这是一个优化算法，并且会使用sklearn库来应用它。但是你真的知道引擎盖下发生了什么吗？</p><p id="e4aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文将讨论梯度下降算法背后的数学直觉，它的各种类型以及如何在机器学习中使用它。</p><h2 id="6a80" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">简介:</h2><p id="bb3e" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">梯度下降是一种优化算法，用于寻找函数(线性回归、逻辑回归等)的参数值。)用于降低成本函数。简单来说，它可以帮助我们找到最合适的线。它是怎么做到的？在深入讨论之前，我们必须知道一些术语的含义。</p><ul class=""><li id="7a67" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated"><strong class="ih hj"> <em class="ld">衍生物</em> </strong></li></ul><p id="6407" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了理解什么是导数，我们必须知道什么是斜率。斜率定义为直线上任意两点之间的垂直变化与水平变化之比(斜率=δy/δx)。它既用来描述方向，也用来描述线有多陡。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es le"><img src="../Images/2d9825ef9d2785104e97ced3534f66b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2amFUgkt9ZeJ45yy7U1X6Q.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Source: Image by author</figcaption></figure><p id="2125" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们知道了，给定直线上的两点，如何求出直线的斜率。但是如何求直线上给定点的斜率呢？这里没什么好衡量的。这就是衍生产品出现的地方。对于导数，我们使用一个小的差值，然后让它缩小到零！</p><p id="ec9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设函数为f(x ),为了求出这个函数的导数，我们使用斜率公式。x从x变为x+δx，y从f(x)变为f(x+δx)。然后我们把方程简化后让δx向零收缩。这样我们就得到了直线上一点的导数。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es le"><img src="../Images/167b0e5edf02aebe697bcb117be4d634.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PRxSk6N2af6ogP8uwe2WMw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Source: Image by author</figcaption></figure><p id="0e81" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，设函数f(x) = x</p><p id="eda5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，为了找到导数或斜率，我们遵循以下步骤:</p><ol class=""><li id="bf2c" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc lf la lb lc bi translated">f(x) = x和f(x+δx)=(x+δx)= x+2xδx+(δx)</li><li id="a8e9" class="ku kv hi ih b ii lg im lh iq li iu lj iy lk jc lf la lb lc bi translated">斜率公式为(f(x+δx)-f(x))/δx</li><li id="f202" class="ku kv hi ih b ii lg im lh iq li iu lj iy lk jc lf la lb lc bi translated">代入斜率方程中第1点的方程，斜率公式变为(x+2xδx+(δx)-x)/δx</li><li id="9552" class="ku kv hi ih b ii lg im lh iq li iu lj iy lk jc lf la lb lc bi translated">简化第3点中的等式后，我们得到2x+δx</li><li id="9da5" class="ku kv hi ih b ii lg im lh iq li iu lj iy lk jc lf la lb lc bi translated">然后，随着δx向0收缩，斜率= 2x</li></ol><p id="04dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这样，我们就可以求出曲线上任意一点的导数或斜率。我们在寻找导数的时候，也有一定的规则要遵循。你可以在<a class="ae jt" href="https://www.mathsisfun.com/calculus/derivatives-rules.html" rel="noopener ugc nofollow" target="_blank">这里</a>的链接上读到它。</p><ul class=""><li id="aef6" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated"><strong class="ih hj"> <em class="ld">偏导数:</em> </strong></li></ul><p id="3242" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在数学中，偏导数的定义是“<em class="ld">通过保持其他变量不变来求一个多变量函数相对于一个单变量的导数</em>”。简单来说，如果一个函数有两个或两个以上的变量，我们通过保持一些变量不变来求导数。</p><p id="de32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，如果函数f(x) = x + y，那么f(x)对x的偏导数将是2x + 0(因为y是常数)。而f(x)对y的偏导数将是0 + 2y(因为y是常数)。也就是说，</p><p id="0a8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果f(x) = x + y，那么，f'ₓ = 2x，f'ᵧ = 2y</p><p id="3871" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">偏导数有时也表示为∂.</p><ul class=""><li id="c7b0" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated"><strong class="ih hj"> <em class="ld">成本函数</em> </strong></li></ul><p id="74cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我借助线性回归来解释这一点。为了简单起见，让我们这样来考虑这个问题:我们必须找出一个学生学习了多少小时的分数。当我们为一个虚构的数据集绘制这个图时，我们会得到一个散点图，看起来像这样:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ll"><img src="../Images/e8ec6c49f6eb5433a650e0033981319e.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*9q6MYHkOY6KJ3X8wXs5zqQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Source: Image by author</figcaption></figure><p id="766e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以有很多条线来连接这些数据点，但我们需要一条最合适的线。为了找到最佳拟合，我们引入了一个称为成本函数的术语，它就是误差函数。对于每个数据点，我们通过减去实际值和预测值来找到误差，然后对该差值求平方(因此结果不会偏斜，因为该差值也可以是负值)。然后将这些结果加在一起，我们得到最常用的成本函数，<em class="ld">均方误差</em> (MSE)。寻找它的公式如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lm"><img src="../Images/93602942a6e233dd02418ffe64cbd2fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*EEbvX8FZgVvUc2cUYHNPtg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Mean Squared Error</figcaption></figure><p id="df2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，第一个y项代表实际的y值，第二个y项代表预测的y值。预测的y由公式y = mx + b计算得出，其中m是直线的斜率，b是截距。因此，均方差现在变为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ln"><img src="../Images/4ce882b8449cfb68a3d73f7a7e46d03c.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*4ivDsrw_wahLTrGowW8dww.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Mean Squared Error</figcaption></figure><p id="16f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦我们找到给出最小成本函数的线，这将是这个问题的最佳拟合线。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lo"><img src="../Images/10a35cfb9b974bbc947a04cff2ec7735.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*Gs_6geg0OVjCJouqsuZxKQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Source: Image by author</figcaption></figure><h2 id="a0d5" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">梯度下降是如何工作的？</h2><p id="9361" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">既然现在我们知道什么是导数，偏导数和成本函数，我们现在可以学习梯度下降的概念。</p><p id="9d5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降是一种一阶迭代优化算法，用于寻找可微函数的局部/全局最小值。我们知道，我们必须找到一条成本最低的线，这可以通过梯度下降来实现。但是这是怎么发生的呢？</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lp"><img src="../Images/bea658532d8723044842f5a697abc61b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EIQa4WMKaZlmi-gQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Source: <a class="ae jt" href="https://www.infinitycodex.in/data-science-ss-106gradient-descent-and" rel="noopener ugc nofollow" target="_blank">Gradient Descent</a></figcaption></figure><p id="12ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为此，我们将首先绘制系数、斜率(m)和截距(b)以及MSE(成本)之间的3d图，如上所示。由于梯度下降算法是一种迭代方法，我们首先随机取m和b的值，然后改变它，使得成本函数变得越来越小，直到我们达到局部/全局最小值。</p><ol class=""><li id="a31a" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc lf la lb lc bi translated">首先，让m = 0和b = 0，对于这些，我们得到一个更高的MSE值(根据上面的图，该值大约为1000)。</li><li id="e955" class="ku kv hi ih b ii lg im lh iq li iu lj iy lk jc lf la lb lc bi translated">然后，我们采取一个步骤(这里的步骤指的是系数m和b的变化)来减小m和b的值，使得MSE减小(大约900)。这个步骤反复进行，直到我们达到局部/全局最小值。</li><li id="3bb9" class="ku kv hi ih b ii lg im lh iq li iu lj iy lk jc lf la lb lc bi translated">一旦我们达到局部/全局最小值，我们可以在我们的预测函数中使用m和b的这个值(y = mx + c)。</li></ol><p id="96ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们来看看m和b的值是如何变化的，这样我们每一步得到的MSE就变小了。为此，让我们画出MSE和截距(b)的曲线图。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lq"><img src="../Images/33328b3b3e7556b73b9045df6b7a17cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AUnQ_R8jcLiD4ItneEC61g.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Source: Image by author</figcaption></figure><p id="cc3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面我画的图中，蓝色的星是MSE的起点(值),红色的点是函数的局部/全局最小值。我们必须从蓝星到达红点，为此，我们首先尝试采取固定的步骤。但是正如我们在上面看到的，这种方法并不总是导致达到最小值，因此是无效的，因为梯度下降可能永远不会收敛。因此，我们可以采用另一种方法，每次缩小步长，如下图所示。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lq"><img src="../Images/fc366206d66e5fafa8eed34cdcbcb957.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q07XmGkRcXaSNk87sWC95A.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Source: Image by author</figcaption></figure><p id="0a37" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上图中我们可以看出，每次我们走一步，步长就不断减小，因此最终梯度下降收敛到局部/全局最小值。但现在的问题是，我们如何采取措施缩小尺寸？</p><p id="4027" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为此，我们必须找到每个点的斜率(或切线),这样我们就知道该往哪个方向走。这个斜率只不过是那个特定点的导数。因此，由于有两个值，斜率(m)和截距(b)，我们必须找到MSE(成本)相对于m和b的偏导数。我们已经在上述部分中了解了如何找到偏导数，因此最终结果如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/0cf2eba7a3681e6a16afe14bbcafdd3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*LPu2GMI5Qx-YDuyBp-kRSg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Partial derivatives of MSE</figcaption></figure><p id="e19d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除此之外，我们还有另一个称为学习率的参数，它决定了我们采取的步长。</p><p id="9153" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ld">学习率(</em>【α】<em class="ld">:</em></strong></p><p id="f2bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">学习参数(α)是一个超参数，负责确定步长，即系数可以改变多少。通常，用户将设置梯度下降算法的学习速率，并且该速率将在整个算法中保持不变。学习率的最佳值将导致在更少的步骤内更快的收敛。如果这个速率高于最优值，那么算法将发现很难收敛到局部/全局最小值，并且成本倾向于随时间增加。如果值太低，即使收敛到极小值，也要花很多时间。因此有必要找到学习率的最优值。学习率对收敛的影响如下图所示。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/3b232d6ebca0f257a520e56406bab702.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/0*zU7sxV7u6tRShi54"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Source: <a class="ae jt" href="https://cs231n.github.io/neural-networks-3/" rel="noopener ugc nofollow" target="_blank">Effect of learning rates on convergence</a></figcaption></figure><p id="2995" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦我们计算了导数并决定了学习速率，我们的下一步就是使用这两个值来改变系数m和b。这是通过以下公式完成的:</p><ul class=""><li id="5641" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated">m = m —学习率* ∂/∂m = m — α * ∂/∂m</li><li id="830f" class="ku kv hi ih b ii lg im lh iq li iu lj iy lk jc kz la lb lc bi translated">b = b—学习率* ∂/∂b = b — α * ∂/∂b</li></ul><h2 id="711a" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">概观</h2><p id="255e" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">我们已经涵盖了与梯度下降如何工作相关的所有必要概念。让我们来看一下梯度下降算法的步骤。</p><p id="8d36" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ld">第一步</em> </strong>:取系数m和b的一些随机值，计算MSE(代价函数)。</p><p id="adef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ld">第二步:</em> </strong>计算MSE关于m和b的偏导数</p><p id="0dd3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ld">第三步:</em> </strong>为学习率设定一个值。并使用以下公式计算m和b的变化。</p><ul class=""><li id="9452" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated">m = m —学习率* ∂/∂m = m — α * ∂/∂m</li><li id="4b66" class="ku kv hi ih b ii lg im lh iq li iu lj iy lk jc kz la lb lc bi translated">b = b —学习率* ∂/∂b = b — α * ∂/∂b</li></ul><p id="7d91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ld">第四步:</em> </strong>用m和b的这些值计算新的MSE。</p><p id="0bff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ld">第五步:</em> </strong>重复第二、三、四步，直到m和b的变化没有显著降低MSE(成本)为止。</p><h2 id="6035" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">梯度下降的类型</h2><p id="fbef" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">上面讨论的梯度下降算法有三种变体。让我告诉你这两者之间的主要区别。</p><ul class=""><li id="3078" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated"><strong class="ih hj">批量梯度下降:</strong></li></ul><p id="5102" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设数据集中有“n”个观察值。使用所有这些“n”个观察值来更新系数值m和b被称为批量梯度下降。它要求整个数据集在内存中可供算法使用。</p><ul class=""><li id="55e0" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated"><strong class="ih hj">随机梯度下降(SGD): </strong></li></ul><p id="38eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相比之下，SGD为数据集中的每个观察值更新m和b的值。这些系数的频繁更新提供了良好的改善率。然而，它们在计算上比批量梯度下降更昂贵。</p><ul class=""><li id="e4f5" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated"><strong class="ih hj">小批量梯度下降:</strong></li></ul><p id="e2c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">小批量梯度下降是SGD和批量梯度下降的结合。它将数据集分成批次，在每个批次结束时更新系数。</p><h2 id="4012" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">结论</h2><p id="0ec6" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">在这篇文章的结尾，你会学到梯度下降算法背后的数学直觉，以及它在机器学习中是如何工作的。你也会学到算法的三种变体。</p><p id="0fa7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望它增加了你对梯度下降的理解，让你对引擎盖后面发生的事情有一个清晰的了解。</p><p id="2702" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢您的阅读，如果您觉得这篇文章有用，请分享！</p></div></div>    
</body>
</html>
<html>
<head>
<title>Complete tutorial on Cross Validation with Implementation in python using Sklearn.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Sklearn完成python中交叉验证和实现的教程。</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/complete-tutorial-on-cross-validation-with-implication-in-python-using-sklearn-48100095788?source=collection_archive---------3-----------------------#2022-02-25">https://medium.com/geekculture/complete-tutorial-on-cross-validation-with-implication-in-python-using-sklearn-48100095788?source=collection_archive---------3-----------------------#2022-02-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="929f" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">@机器学习#交叉验证</h2><div class=""/><div class=""><h2 id="855f" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">CV的概念、类型和实际含义。</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/c569f061e3d075a15e10f3a51a5a5e6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kcgdlmbz6RSfbvQqlAisCg.jpeg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Photo by <a class="ae jw" href="https://unsplash.com/@homajob?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Scott Graham</a> on <a class="ae jw" href="https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="64a4" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">在本文中，我们将看到交叉验证背后的理论概念，不同类型的交叉验证，以及最后使用python和sklearn的实际应用。</p><p id="8868" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">但在此之前，我们为什么需要交叉验证？让我们理解。</strong></p><p id="0c97" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">在用给定的数据建立任何最大似然模型之前，我们将数据集分成测试集和训练集，其比例取决于数据集计数的可用性。大多数情况下，<code class="du kt ku kv kw b"><strong class="jz hs">Test Set : 20 -30 % of data </strong></code> <strong class="jz hs"> &amp; </strong> <code class="du kt ku kv kw b"><strong class="jz hs"> Train Set : 70–80 % of data</strong></code>其中，模型的准确性/性能将由测试数据集来检查。但是这70- 30 %的数据量是从所有数据点中随机选择的，这导致了精确度的波动。这是通过给变量<strong class="jz hs"> random_state </strong>赋值来控制的。</p><p id="37af" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><code class="du kt ku kv kw b"><strong class="jz hs"><em class="kx">Random state will decide the splitting of data into test and train set and using a particular finite number(It can take any positive value) will ensure same results will be reproduced again and again. But for different random_state splitting of test and train will be different and hence accuracy obtained will be different and results in fluctuation of accuracy</em></strong><em class="kx">.</em></code></p><p id="5266" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">我们需要验证我们的ML模型的准确性，交叉验证的作用就来了:</strong>这是一种通过使用不同的数据子集对模型进行一定次数的迭代来评估ML模型准确性的技术。模型的最终输出将是所有输出的平均值。它还减轻了过度拟合的影响。</p><p id="4e61" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">通常有6种类型的cv方法:</p><ol class=""><li id="94e8" class="ky kz hi jz b ka kb kd ke kg la kk lb ko lc ks ld le lf lg bi translated">坚持方法</li><li id="cfbf" class="ky kz hi jz b ka lh kd li kg lj kk lk ko ll ks ld le lf lg bi translated">遗漏一个交叉验证</li><li id="29e0" class="ky kz hi jz b ka lh kd li kg lj kk lk ko ll ks ld le lf lg bi translated">k折叠交叉验证</li><li id="71de" class="ky kz hi jz b ka lh kd li kg lj kk lk ko ll ks ld le lf lg bi translated">分层K折叠交叉验证</li><li id="eba4" class="ky kz hi jz b ka lh kd li kg lj kk lk ko ll ks ld le lf lg bi translated">时间序列交叉验证</li><li id="2cca" class="ky kz hi jz b ka lh kd li kg lj kk lk ko ll ks ld le lf lg bi translated">重复随机测试序列分割或蒙特卡罗交叉验证</li></ol><p id="d037" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">让我们一个一个来看:</p></div><div class="ab cl lm ln gp lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="hb hc hd he hf"><h1 id="39dd" class="lt lu hi bd lv lw lx ly lz ma mb mc md ix me iy mf ja mg jb mh jd mi je mj mk bi translated"><strong class="ak"> 1。保持方法</strong></h1><p id="1ec2" class="pw-post-body-paragraph jx jy hi jz b ka ml is kc kd mm iv kf kg mn ki kj kk mo km kn ko mp kq kr ks hb bi translated">这只是将数据分成训练集和测试集。训练数据的百分比大于测试数据。发布使用训练集训练模型和剩余测试集进行误差估计。</p><p id="a923" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs"> <em class="kx">缺点:</em> </strong>会有<a class="ae jw" rel="noopener" href="/geekculture/overfitting-underfitting-and-bias-variance-tradeoff-9e83f4a147c">高方差</a>的机会，因为任何随机的数据样本和与之相关的模式都可能被选入测试数据。由于我们用测试数据验证模型，准确性和模型泛化会受到负面影响。</p><h1 id="5681" class="lt lu hi bd lv lw mq ly lz ma mr mc md ix ms iy mf ja mt jb mh jd mu je mj mk bi translated"><strong class="ak"> 2。遗漏一项交叉验证(LOOCV): </strong></h1><p id="b852" class="pw-post-body-paragraph jx jy hi jz b ka ml is kc kd mm iv kf kg mn ki kj kk mo km kn ko mp kq kr ks hb bi translated">在这种情况下，在所有数据点<strong class="jz hs">中，一个数据</strong>被留下作为测试数据，其余的作为训练数据。因此，对于<strong class="jz hs"> n </strong>个数据点，我们必须执行<strong class="jz hs"> n </strong>次迭代来覆盖每个数据点。</p><p id="c137" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs"> <em class="kx">留P-Out交叉验证是一种特殊情况，留下P个数据点用于测试和验证，n-p用于训练模型。</em>T9】</strong></p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mv"><img src="../Images/defefd9b15d71a5dd842b5fb5ce457d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hoSJ0UTZBK8_Uz8-0iIbxg.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">LOOCV iterations. Credits: Author</figcaption></figure><p id="73d6" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs"> <em class="kx">缺点:</em> </strong></p><p id="f0a3" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">I)需要高计算能力，因为大数据集的每个数据点都需要多次迭代。</p><p id="79de" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">ii)由于<strong class="jz hs"> n-1个</strong>数据点被用作训练数据，因此<a class="ae jw" rel="noopener" href="/geekculture/overfitting-underfitting-and-bias-variance-tradeoff-9e83f4a147c">会发生过拟合，导致低偏差</a>，但不会产生概化模型，导致高误差和低精度。它很久以前就被使用了，现在没人使用它了。</p><h1 id="3364" class="lt lu hi bd lv lw mq ly lz ma mr mc md ix ms iy mf ja mt jb mh jd mu je mj mk bi translated"><strong class="ak"> 3。k折交叉验证:</strong></h1><p id="3f35" class="pw-post-body-paragraph jx jy hi jz b ka ml is kc kd mm iv kf kg mn ki kj kk mo km kn ko mp kq kr ks hb bi translated">其中，整个<strong class="jz hs"> n数据集</strong>被分成k个部分，其中<strong class="jz hs"> n/k =p </strong>，然后这个<strong class="jz hs"> p </strong>将作为每次迭代的测试数据，下一次迭代的下一个p，以此类推，直到<strong class="jz hs"> k </strong>次迭代。<strong class="jz hs"> </strong>为ex。5折交叉验证的20个数据点，20/5 =4，因此给定数据集将如下图所示进行划分。不同的折叠会有不同的设置。<strong class="jz hs">每个数据将在测试集中被考虑一次，在训练集中被考虑k-1次，增强了该方法的有效性</strong>。每个折叠将给出不同精度，最终精度将是所有这5个精度的平均值。此外，我们将能够获得该特定型号的最小<strong class="jz hs">精度和最大<strong class="jz hs">精度。</strong></strong></p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mw"><img src="../Images/a6d0c427a014692a385b748a6c2ea3b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAGbUIyhU6ugZaOzGYW8NQ.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">5-fold cross validation iterations. Credits : Author</figcaption></figure><p id="c406" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs"> <em class="kx">优点:</em> </strong></p><p id="23f6" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">I)有效使用数据，因为每个数据点都用于训练和测试目的。</p><p id="699b" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">低偏倚是因为大部分数据用于训练。</strong></p><p id="d5e3" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">低方差，因为几乎每个数据点都在测试集中使用。</strong></p><p id="ce84" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">ii)精确度高。</p><p id="ed8b" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">理想情况下，k值最好在5 -10之间，但也可以是任何值。较高的K值将导致类似于LOOCV方法的精确度。</strong></p><p id="1fb1" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs"> <em class="kx">缺点:</em> </strong></p><p id="0713" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">I)I<strong class="jz hs">m平衡数据集使用此方法</strong>导致低准确性，假设对于二进制分类问题，在测试数据中，我们有输出1的最大实例，因此它不会给出关于特定模型的准确结果。或者在价格预测中，为测试集选择的所有数据都有很高的价格，因此准确性也会受到影响。</p><p id="4897" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">为了克服这一点，我们使用分层交叉验证。</p><h1 id="49fe" class="lt lu hi bd lv lw mq ly lz ma mr mc md ix ms iy mf ja mt jb mh jd mu je mj mk bi translated"><strong class="ak"> 4。分层交叉验证:</strong></h1><p id="d611" class="pw-post-body-paragraph jx jy hi jz b ka ml is kc kd mm iv kf kg mn ki kj kk mo km kn ko mp kq kr ks hb bi translated">在这种情况下，在训练和测试数据集中填充的随机样本是这样的，即在训练和测试数据分割的每次迭代中，每个类的实例数量以是和否、0和1、或高和低的良好比例被采用，从而模型给出良好的准确性。</p><h1 id="7f4d" class="lt lu hi bd lv lw mq ly lz ma mr mc md ix ms iy mf ja mt jb mh jd mu je mj mk bi translated"><strong class="ak"> 5。时间序列交叉验证:</strong></h1><p id="23e6" class="pw-post-body-paragraph jx jy hi jz b ka ml is kc kd mm iv kf kg mn ki kj kk mo km kn ko mp kq kr ks hb bi translated">它完全适用于时间序列数据，如股票价格预测、销售预测。输入按顺序添加到训练数据中，如下所示。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mx"><img src="../Images/aa6ebb63d55df1d32f2f942b881c74b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bmDgoOUW9D8pCIFFbJFFfg.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Time Series CV. credits : Author</figcaption></figure><h1 id="7ec9" class="lt lu hi bd lv lw mq ly lz ma mr mc md ix ms iy mf ja mt jb mh jd mu je mj mk bi translated">6.重复随机测试训练分割或<strong class="ak">蒙特卡洛交叉验证</strong>:</h1><p id="7903" class="pw-post-body-paragraph jx jy hi jz b ka ml is kc kd mm iv kf kg mn ki kj kk mo km kn ko mp kq kr ks hb bi translated">它包括传统的训练测试分割和K-fold CV。这里，数据集被随机分割成训练集和测试集，然后分割和性能测量的进一步过程被重复我们指定的次数。执行交叉验证。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es my"><img src="../Images/df9f1732b90e4c58846fedc40efb3e42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-pKXbM1Wh9-HRyd7j2syPA.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Repeated Random Test Train Split. Credits: Author</figcaption></figure><p id="a8b9" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs"> <em class="kx">缺点:</em> </strong></p><p id="b453" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">I)它不适用于不平衡数据集。</p><p id="857f" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">ii)有可能一些样本没有被选择用于训练和测试数据。</p></div><div class="ab cl lm ln gp lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="hb hc hd he hf"><h1 id="3797" class="lt lu hi bd lv lw lx ly lz ma mb mc md ix me iy mf ja mg jb mh jd mi je mj mk bi translated">使用Sklearn和Python的实际意义:</h1><p id="fbe4" class="pw-post-body-paragraph jx jy hi jz b ka ml is kc kd mm iv kf kg mn ki kj kk mo km kn ko mp kq kr ks hb bi translated">现在，我们正在使用python和sklearn实现上述所有技术，以构建一个简单的ML模型。这只是为了理解交叉验证技术，所以回归分类器的其他超参数是默认值。</p><p id="cc1a" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">我们正在考虑一个癌症数据集，以在各种特征的基础上预测癌症的类型，即良性(B)和恶性(M)。</p><pre class="jh ji jj jk fd mz kw na nb aw nc bi"><span id="cece" class="nd lu hi kw b fi ne nf l ng nh">import pandas as pd<br/>data=pd.read_csv(r'/content/drive/MyDrive/cancer_dataset.csv')<br/>data.head()</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ni"><img src="../Images/6fe67267f1ed61192de1de95a850e1fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*soJ-FkQPaPMBWaa-elppzQ.png"/></div></div></figure><pre class="jh ji jj jk fd mz kw na nb aw nc bi"><span id="b3f8" class="nd lu hi kw b fi ne nf l ng nh"><strong class="kw hs">#Removing Null Values</strong><br/>data.isnull().sum()</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nj"><img src="../Images/71f81c47727c449ff38b191103d62032.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*YCxV6--4C1ZJAbxxxdvnRQ.png"/></div></figure><pre class="jh ji jj jk fd mz kw na nb aw nc bi"><span id="bf2c" class="nd lu hi kw b fi ne nf l ng nh"><strong class="kw hs">#last column have all NaN value so we can drop that</strong></span><span id="c50d" class="nd lu hi kw b fi nk nf l ng nh">data1=data.drop(['Unnamed: 32'],axis='columns')</span><span id="a813" class="nd lu hi kw b fi nk nf l ng nh"><strong class="kw hs">###  Dividing dataset into dependent &amp; independent feature</strong></span><span id="9cbf" class="nd lu hi kw b fi nk nf l ng nh"><strong class="kw hs">#diagnosis is the output and rest all are input features.</strong></span><span id="1252" class="nd lu hi kw b fi nk nf l ng nh">x=data1.iloc[:,2:]<br/>y=data1.iloc[:,1]</span><span id="5daa" class="nd lu hi kw b fi nk nf l ng nh"><strong class="kw hs">#to check if the dataset is balanaced or not</strong></span><span id="c75d" class="nd lu hi kw b fi nk nf l ng nh">y.value_counts()</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nl"><img src="../Images/2b053303054e01fab40f830524e82ec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S4shwe3CmweMXBi2Dv7wvA.png"/></div></div></figure></div><div class="ab cl lm ln gp lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="hb hc hd he hf"><p id="a739" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">现在我们正在使用不同的CV技术构建ML模型。</p><p id="cfd0" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs"> 1。维持验证</strong></p><pre class="jh ji jj jk fd mz kw na nb aw nc bi"><span id="b1c7" class="nd lu hi kw b fi ne nf l ng nh">from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.model_selection import train_test_splitx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30,random_state=0)<br/>model=DecisionTreeClassifier()<br/>model.fit(x_train,y_train)<br/>mod_score1=model.score(x_test,y_test)<br/>mod_score1</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nm"><img src="../Images/f061df3c7a1e5f018558ad467c18a7aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ujsg8ENKD0tzCsXSOs9t4A.png"/></div></div></figure><p id="e7af" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs"> 2。遗漏一个交叉验证(LOOCV) </strong></p><pre class="jh ji jj jk fd mz kw na nb aw nc bi"><span id="5017" class="nd lu hi kw b fi ne nf l ng nh">from sklearn.model_selection import LeaveOneOut<br/>from sklearn.model_selection import cross_val_score<br/>model=DecisionTreeClassifier()<br/>leave_val=LeaveOneOut()<br/>mod_score2=cross_val_score(model,x,y,cv=leave_val)</span><span id="3551" class="nd lu hi kw b fi nk nf l ng nh">print(np.mean(mod_score2))</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es my"><img src="../Images/565fad242a4c5a45908096ae3a13620f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OwypcRUSMR4wKdVt9HQOSA.png"/></div></div></figure><p id="92ea" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs"> 3。k折交叉验证</strong></p><pre class="jh ji jj jk fd mz kw na nb aw nc bi"><span id="f564" class="nd lu hi kw b fi ne nf l ng nh">from sklearn.model_selection import KFold<br/>model=DecisionTreeClassifier()<br/>kfold_validation=KFold(10)</span><span id="8391" class="nd lu hi kw b fi nk nf l ng nh">import numpy as np<br/>from sklearn.model_selection import cross_val_score<br/>mod_score3=cross_val_score(model,x,y,cv=kfold_validation)<br/>print(mod_score3)</span><span id="ba6f" class="nd lu hi kw b fi nk nf l ng nh"><strong class="kw hs">#Overall accuracy of the model will be average of all values.<br/></strong>print(np.mean(mod_score3))</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nn"><img src="../Images/24dd6cdc3fc05066b4c5e2ce35bdc327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6a30ftGDRRLGe22xDVHuMw.png"/></div></div></figure><p id="a0f5" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs"> 4。分层K倍交叉验证</strong></p><pre class="jh ji jj jk fd mz kw na nb aw nc bi"><span id="e8e5" class="nd lu hi kw b fi ne nf l ng nh">from sklearn.model_selection import StratifiedKFold<br/>sk_fold=StratifiedKFold(n_splits=5)<br/>model=DecisionTreeClassifier()<br/>mod_score4=cross_val_score(model,x,y,cv=sk_fold)<br/>print(np.mean(mod_score4))<br/>print(mod_score4)</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es no"><img src="../Images/bd261d9b7ff4980d238bb1ebadc2ccb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LCOFJCppvggTDx_8fF9wRw.png"/></div></div></figure><p id="908a" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs"> 5。重复随机测试-列车分裂</strong></p><pre class="jh ji jj jk fd mz kw na nb aw nc bi"><span id="e37c" class="nd lu hi kw b fi ne nf l ng nh">from sklearn.model_selection import ShuffleSplit<br/>model=DecisionTreeClassifier()<br/>s_split=ShuffleSplit(n_splits=10,test_size=0.30)<br/>mod_score5=cross_val_score(model,x,y,cv=s_split)<br/>print(mod_score5)<br/>print(np.mean(mod_score5))</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es np"><img src="../Images/de28d67d4a7e700ca6172609316ec929.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tyByKCZbzUKQaNCoubiEpg.png"/></div></div></figure><p id="2aa1" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">这样，我们几乎涵盖了交叉验证的每一点。</p><p id="814d" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">感谢阅读！！</p></div></div>    
</body>
</html>
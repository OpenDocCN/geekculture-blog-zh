<html>
<head>
<title>Text Feature Extraction (2/3): TF-IDF Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本特征提取(2/3): TF-IDF模型</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/text-feature-extraction-2-3-tf-idf-model-c3a8f7a92bc9?source=collection_archive---------5-----------------------#2021-06-29">https://medium.com/geekculture/text-feature-extraction-2-3-tf-idf-model-c3a8f7a92bc9?source=collection_archive---------5-----------------------#2021-06-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/10f13f4d7fe8578d75cd7c3fc0810135.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dudfjJ-iDfR4WzYjahhgXw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx"><a class="ae iu" href="https://unsplash.com/s/photos/books-and-laptop" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="862f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jt translated"><span class="l ju jv jw bm jx jy jz ka kb di">在</span>自然语言处理中，任何基于文本的问题都需要转换成可以建模的形式。一个简单的文本可以用各种技术转换成特征，比如单词包(BOW)、TF-IDF或单词嵌入。在文本特征提取系列的<a class="ae iu" rel="noopener" href="/geekculture/text-feature-extraction-1-3-bag-of-words-model-649dbeeade79">最后一篇博客</a>中，我们从零开始研究了计数向量器及其在文本分类中的用例。我们在那里破译了它有一个缺乏语义的主要缺点。<strong class="ix hj">计数矢量器</strong>考虑字数来创建特征，因此<strong class="ix hj">不考虑句子结构和顺序</strong>。此外，它还会产生一个很大的稀疏矩阵。这样，TF-IDF就出现了。</p><p id="e078" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个博客系列展示了详细的TF-IDF技术。那么，我们开始吧。</p><h2 id="ca64" class="kc kd hi bd ke kf kg kh ki kj kk kl km jg kn ko kp jk kq kr ks jo kt ku kv kw bi translated"><strong class="ak">瞄准</strong></h2><p id="2b82" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">TF-IDF技术给出了相关的术语。这个相关术语是一个可以理解整个上下文而不是阅读整个文本的术语。</p><h2 id="b5cd" class="kc kd hi bd ke kf kg kh ki kj kk kl km jg kn ko kp jk kq kr ks jo kt ku kv kw bi translated">直觉</h2><ul class=""><li id="c767" class="lc ld hi ix b iy kx jc ky jg le jk lf jo lg js lh li lj lk bi translated">这个词出现多次意味着它的重要性(TF)。</li><li id="146c" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated">但同时，如果它频繁出现在多个文档中，则可能不相关(IDF)。我们可以称这些词为停用词，如the、this等。</li></ul><h2 id="7cfc" class="kc kd hi bd ke kf kg kh ki kj kk kl km jg kn ko kp jk kq kr ks jo kt ku kv kw bi translated">理论与概念</h2><ul class=""><li id="56c7" class="lc ld hi ix b iy kx jc ky jg le jk lf jo lg js lh li lj lk bi translated">TF-IDF(词频-逆文档频率)刻画了一个词的重要性。它是TF和IDF的点积的计算。在此之前，我们先单独了解一下术语。</li><li id="ad44" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated">词频(TF): <br/> -它演示了单词对文档的重要性，直觉上文档中的词越多意味着重要性越高。</li></ul><figure class="lr ls lt lu fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lq"><img src="../Images/66a17d998662d6d3d37f264b578f97f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*Mlws6f7IVUrYq16pWrIwFw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">equation-1</figcaption></figure><ul class=""><li id="780e" class="lc ld hi ix b iy iz jc jd jg lv jk lw jo lx js lh li lj lk bi translated">反向文档频率(IDF): <br/> -显示一个术语实际上是如何相关的。在某些文档中，频繁出现的术语不一定是相关的，例如停用词(the，of等)。停用词不能揭示上下文，因此应该避免使用。以色列国防军的工作方式忽略了他们。<br/> -它惩罚在文档中频繁出现的单词<br/>-IDF分数对相关术语较高，而对停用词的权重较低<br/> -它考虑自然对数函数，也称为log e</li></ul><figure class="lr ls lt lu fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lq"><img src="../Images/5fbf47a15ff15b33c859052a3e14b814.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*pm_UDKx71xCZqdLTmmYrmQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">equation-2</figcaption></figure><ul class=""><li id="4fcb" class="lc ld hi ix b iy iz jc jd jg lv jk lw jo lx js lh li lj lk bi translated">简而言之，TFIDF值与doc相关，而IDF取决于语料库</li><li id="cc62" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated">手动计算TF-IDF和Sklearn的TF-IDF不一样。<br/> <strong class="ix hj">区别:</strong> TF术语保持不变，IDF术语不同。让我们开始吧！<br/> <strong class="ix hj"> <em class="ly">标准TF-IDF </em> <br/> </strong>标准符号如下所示，其中N是跨语料库的文档，N是具有术语m的文档</li></ul><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/20847a9240d5a8d27073beb659a5497d.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*nkn9gLX4rH8tKY4431sldw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">equation-3</figcaption></figure><p id="a2b7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"><em class="ly">Sklearn TF-IDF</em><br/></strong>根据定义，Tfidf应该按照上面的公式工作但是sk learn提供了一些更高级的计算。TfidfVectorizer和TfidfTransformer的计算与标准的不同之处如下:<br/> -给分子加1<br/>-给分母加1以防止零除法<br/> -给整个对数项加1作为平滑常数<br/> -得到的TF * IDF向量然后用L2(欧几里德)归一化</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/00bf7f054a9b02a492dd53aee6146a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*2iTNn68Z4WKSUBY2hgtDqw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">equation-4</figcaption></figure><p id="2102" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">想了解更多信息，请访问这个<a class="ae iu" href="https://stackoverflow.com/questions/24032485/difference-in-values-of-tf-idf-matrix-using-scikit-learn-and-hand-calculation" rel="noopener ugc nofollow" target="_blank"> StackOverflow链接</a>和一个有深入细节的<a class="ae iu" href="https://towardsdatascience.com/how-sklearns-tf-idf-is-different-from-the-standard-tf-idf-275fa582e73d" rel="noopener" target="_blank">博客</a>。</p><h2 id="275c" class="kc kd hi bd ke kf kg kh ki kj kk kl km jg kn ko kp jk kq kr ks jo kt ku kv kw bi translated">过程</h2><p id="13d4" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">让我们切入正题，研究构建TF-IDF模型的步骤。我们将遵循标准符号<em class="ly">等式-3 </em>。</p><p id="7da7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">科珀斯= "她是一个旅游癖"，"她很可爱"</p><ol class=""><li id="445e" class="lc ld hi ix b iy iz jc jd jg lv jk lw jo lx js ma li lj lk bi translated"><strong class="ix hj">计算TF </strong>:参考等式1。</li></ol><p id="cf4f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">TF_doc2("是")= 1/3 <br/> TF_doc2("可爱")= 1/3</p><p id="c08c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 2。计算IDF </strong>:参考等式-3。</p><p id="3990" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">IDF(" is ")= log(2/2)= 0<br/>IDF(" lovely ")= log(2/1)= 0.30<br/>可以清楚地看到，“is”的权重小于“lovely”。因此可爱似乎更贴切。</p><ul class=""><li id="f186" class="lc ld hi ix b iy iz jc jd jg lv jk lw jo lx js lh li lj lk bi translated"><strong class="ix hj">TF和IDF字的点积:</strong></li></ul><p id="6ca5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">TF-IDF ("is") = TF。IDF = (1/3) * 0 = 0 <br/> TF-IDF("可爱")= (1/3) * 0.3 = 0.09</p><p id="d838" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">结果显示，单词“是”是不相关的，而“可爱”具有一定的重要性。仅仅读“可爱”这个词就能使句子与众不同。</p><p id="5a4f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">结果总结如下:</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/2271c81e06fefbfbc2e76e333a892762.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*yntwSSsJGiCgQ_0DdN4__Q.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">table-1</figcaption></figure><h2 id="44db" class="kc kd hi bd ke kf kg kh ki kj kk kl km jg kn ko kp jk kq kr ks jo kt ku kv kw bi translated">履行</h2><p id="017d" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">为了理解TF-IDF模型，让我们先来看看如何手动实现，然后再为Sklearn实现will。</p><ol class=""><li id="83d3" class="lc ld hi ix b iy iz jc jd jg lv jk lw jo lx js ma li lj lk bi translated"><strong class="ix hj">手动</strong></li></ol><ul class=""><li id="39a5" class="lc ld hi ix b iy iz jc jd jg lv jk lw jo lx js lh li lj lk bi translated">让我们创建我们的句子语料库，并将它们转换成小写，以便不区分“this”和“This”。</li></ul><pre class="lr ls lt lu fd mc md me mf aw mg bi"><span id="2bec" class="kc kd hi md b fi mh mi l mj mk">doc = "She is a wanderlust”, “She is lovely”<br/>#Convert into lowercase<br/>doc = list(map(str.lower, doc))</span></pre><ul class=""><li id="0536" class="lc ld hi ix b iy iz jc jd jg lv jk lw jo lx js lh li lj lk bi translated">为语料库中的每个文档创建一个单词包</li></ul><pre class="lr ls lt lu fd mc md me mf aw mg bi"><span id="24ed" class="kc kd hi md b fi mh mi l mj mk">cv = CountVectorizer()<br/>count_occurrences = cv.fit_transform(corpus)</span><span id="594b" class="kc kd hi md b fi ml mi l mj mk">#For doc1<br/>bagOfWords_1 = dict.fromkeys(cv.get_feature_names())<br/>for ind,key in enumerate(bagOfWords_1):<br/>    bagOfWords_1[key] = count_occurrences.toarray()[0][ind]<br/>bagOfWords_1<br/>Out[2]:<br/>{'is': 1, 'lovely': 0, 'she': 1, 'wanderlust': 1}</span><span id="cde0" class="kc kd hi md b fi ml mi l mj mk">#For doc2<br/>bagOfWords_2 = dict.fromkeys(cv.get_feature_names())<br/>for ind,key in enumerate(bagOfWords_2):<br/>    bagOfWords_2[key] = count_occurrences.toarray()[1][ind]<br/>bagOfWords_2<br/>Out[3]:<br/>{'is': 1, 'lovely': 1, 'she': 1, 'wanderlust': 0}</span></pre><ul class=""><li id="c6f4" class="lc ld hi ix b iy iz jc jd jg lv jk lw jo lx js lh li lj lk bi translated">计算TF</li></ul><pre class="lr ls lt lu fd mc md me mf aw mg bi"><span id="3be0" class="kc kd hi md b fi mh mi l mj mk">def compute_tf(bow, doc):<br/>    tf_dict ={}<br/>    doc_count = len(doc)<br/>    for word, count in bow.items():<br/>        tf_dict[word] = count/doc_count<br/>    return tf_dict<br/>tf_doc1 = compute_tf(bagOfWords_1, corpus[0].split(' '))<br/>tf_doc2 = compute_tf(bagOfWords_2, corpus[1].split(' '))<br/>tf_doc1</span><span id="fbb4" class="kc kd hi md b fi ml mi l mj mk">Out[4]:<br/>{'is': 0.25, 'lovely': 0.0, 'she': 0.25, 'wanderlust': 0.25}</span></pre><ul class=""><li id="b1b1" class="lc ld hi ix b iy iz jc jd jg lv jk lw jo lx js lh li lj lk bi translated">计算IDF</li></ul><pre class="lr ls lt lu fd mc md me mf aw mg bi"><span id="5fb0" class="kc kd hi md b fi mh mi l mj mk">def compute_idf(docs):    <br/>    N = len(docs)<br/>    idfDict = dict.fromkeys(docs[0].keys(),0)<br/>    for doc in docs:<br/>        for word, val in doc.items():<br/>            if val &gt; 0:<br/>                idfDict[word] +=1 <br/>    <br/>    for word, val in idfDict.items():<br/>        # standard notation<br/>        idfDict[word] = math.log(N / float(val))<br/>        #sklearn notation<br/>        #idfDict[word] = (math.log((N+1) / (val+1))) + 1<br/>    return idfDict<br/>idfs = compute_idf([bagOfWords_1,bagOfWords_2])<br/>idfs</span><span id="1f3c" class="kc kd hi md b fi ml mi l mj mk">Out[5]:<br/>{'is': 0.0,<br/> 'lovely': 0.6931471805599453,<br/> 'she': 0.0,<br/> 'wanderlust': 0.6931471805599453}</span></pre><ul class=""><li id="ebdd" class="lc ld hi ix b iy iz jc jd jg lv jk lw jo lx js lh li lj lk bi translated">计算TF * IDF</li></ul><pre class="lr ls lt lu fd mc md me mf aw mg bi"><span id="da79" class="kc kd hi md b fi mh mi l mj mk">def compute_tfidf(tf,idf):<br/>    tfidf = {}<br/>    for word, tfVal in tf.items():<br/>        tfidf[word] = tfVal * idf[word]<br/>    return tfidf<br/>tfidf_doc1 = compute_tfidf(tf_doc1, idfs)<br/>tfidf_doc2 = compute_tfidf(tf_doc2, idfs)<br/>tfidf_doc1</span><span id="eed7" class="kc kd hi md b fi ml mi l mj mk">Out[6]:<br/>{'is': 0.0, 'lovely': 0.0, 'she': 0.0, 'wanderlust': 0.17328679513998632}</span></pre><p id="b8d7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 2。上面的步骤只需要几行代码就可以完成。Scikit-learn提供了一个名为TfidfVectorizer的库，用于计算Tfidf权重。</strong></p><pre class="lr ls lt lu fd mc md me mf aw mg bi"><span id="645a" class="kc kd hi md b fi mh mi l mj mk">vectorizer = TfidfVectorizer()<br/>vectors = vectorizer.fit_transform(corpus).todense()<br/>vectors<br/>"""<br/>matrix([[0.50154891, 0.        , 0.50154891, 0.70490949],<br/>        [0.50154891, 0.70490949, 0.50154891, 0.        ]])<br/>"""</span></pre><h2 id="fafc" class="kc kd hi bd ke kf kg kh ki kj kk kl km jg kn ko kp jk kq kr ks jo kt ku kv kw bi translated">缺点</h2><ul class=""><li id="d788" class="lc ld hi ix b iy kx jc ky jg le jk lf jo lg js lh li lj lk bi translated"><strong class="ix hj">上下文理解</strong><br/>bagowords和TFIDF技术缺乏对上下文的理解。TFIDF可以破译句子结构，但不能破译上下文。</li><li id="74e9" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated"><strong class="ix hj">大词汇量</strong> <br/>在词汇量很大的情况下，特征变得浩如烟海，对内存和时间提出了挑战。</li></ul><p id="ecfb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了克服上述缺点，出现了一种新的特征提取方法，称为单词嵌入。让我们在文本特征提取(3/3):单词嵌入模型中了解一下这个。</p><p id="ecb6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">查看我的</strong><a class="ae iu" href="https://github.com/shachi01/NLP/blob/main/TFIDF_Sklearn_Scratch.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="ix hj">GitHub repo</strong></a><strong class="ix hj">总结了这里演示的所有代码。</strong></p><p id="cbef" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">此外，您还可以通过本</strong> <a class="ae iu" href="https://github.com/shachi01/NLP/blob/main/TFIDF_MovieReviews.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj"> GitHub知识库</strong> </a> <strong class="ix hj">了解更多关于如何开发TF-IDF模型来预测电影评论情绪的信息。</strong></p><h1 id="4526" class="mm kd hi bd ke mn mo mp ki mq mr ms km mt mu mv kp mw mx my ks mz na nb kv nc bi translated">参考</h1><div class="nd ne ez fb nf ng"><a href="https://www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab dw"><div class="ni ab nj cl cj nk"><h2 class="bd hj fi z dy nl ea eb nm ed ef hh bi translated">如何在Python中使用TF-IDF处理文本数据</h2><div class="nn l"><h3 class="bd b fi z dy nl ea eb nm ed ef dx translated">我的计算机擅长处理数字，但不擅长处理文本数据。最广泛使用的一种…</h3></div><div class="no l"><p class="bd b fp z dy nl ea eb nm ed ef dx translated">www.freecodecamp.org</p></div></div><div class="np l"><div class="nq l nr ns nt np nu io ng"/></div></div></a></div><div class="nd ne ez fb nf ng"><a href="https://towardsdatascience.com/how-sklearns-tf-idf-is-different-from-the-standard-tf-idf-275fa582e73d" rel="noopener follow" target="_blank"><div class="nh ab dw"><div class="ni ab nj cl cj nk"><h2 class="bd hj fi z dy nl ea eb nm ed ef hh bi translated">Sklearn的「TF-IDF」与标准的「TF-IDF」有何不同？</h2><div class="nn l"><h3 class="bd b fi z dy nl ea eb nm ed ef dx translated">让我们来看看不同之处，并逐步分析计算Sklearn的TF-IDF所采用的方法</h3></div><div class="no l"><p class="bd b fp z dy nl ea eb nm ed ef dx translated">towardsdatascience.com</p></div></div><div class="np l"><div class="nv l nr ns nt np nu io ng"/></div></div></a></div></div><div class="ab cl nw nx gp ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="hb hc hd he hf"><p id="0e8a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你喜欢这位作者的博客，请随意关注，因为这位作者向你保证会带来更多有趣的与人工智能相关的东西。<br/> 谢谢，<br/>学习愉快！😄</p><p id="78fc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="ly">可以通过</em></strong><a class="ae iu" href="https://www.linkedin.com/in/kaul-shachi" rel="noopener ugc nofollow" target="_blank"><strong class="ix hj"><em class="ly">LinkedIn</em></strong></a><strong class="ix hj"><em class="ly">取得联系。</em> </strong></p></div></div>    
</body>
</html>
<html>
<head>
<title>A 2021 Guide to improving CNN's-Recent Optimizers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2021年CNN优化指南</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/a-2021-guide-to-improving-cnns-recent-optimizers-a340456f6b2d?source=collection_archive---------29-----------------------#2021-06-21">https://medium.com/geekculture/a-2021-guide-to-improving-cnns-recent-optimizers-a340456f6b2d?source=collection_archive---------29-----------------------#2021-06-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="cac5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这将是我在<em class="jd"> </em>上的第四篇文章，我的系列文章<em class="jd">是2021年改进CNN的指南。</em></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/e46c81de4cb748657d2f99fd3036e61f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*U0F_5VStC3duG7dd"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Photo by <a class="ae ju" href="https://unsplash.com/@ewitsoe?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Erik Witsoe</a> on <a class="ae ju" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="0bf2" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">高级优化器(最新优化器)</h1><p id="577d" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">在前一篇比较Adam和SGD的泛化能力的文章中，我们得出的结论是，由于Adam包含SGD，在足够多的超参数搜索下，Adam的表现肯定优于SGD。</p><p id="01ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">自Adam在2014年提出以来，一阶优化器有了许多突破，这可能有利于训练最佳实践DL模型。每个优化器都建立在独特的直觉上，可能会加快DL模型的训练时间。</p><p id="6ecb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">优化器研究的范例得出结论，SGD比Adam更通用，但Adam比SGD更快。许多优化器都建立在这种直觉的基础上，并结合了Adam和SGD的优点。然而，这样的论点受到了实验[8]的挑战。这个结果可能会挑战这些优化器的基本背景。</p><p id="5c79" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与其他方法相比，更好的优化器可能速度更快，并且对最终验证的准确性有很大的影响。我们将回顾这篇文章中提出的一些最重要的优化器。</p><h2 id="0388" class="ky jw hi bd jx kz la lb kb lc ld le kf iq lf lg kj iu lh li kn iy lj lk kr ll bi translated">SWATS[7]</h2><p id="17fd" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">SWitches from Adam To Sgd是一个优化器，当条件满足时，它会在训练期间从Adam切换到Sgd。这是基于这样一个发现，即SGD在训练的后期概括得更好，但在训练的早期阶段比Adam更慢。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lm"><img src="../Images/225e0c00a031a8d135ecd79c33f5fb2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*x_CUF5bf4QXUhgVIml1ozA.png"/></div></figure><p id="21f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图显示了在特定时期切换时训练DenseNet的结果。有趣的是，最后的误差被列为切换点。当下面的等式成立时，SWATS建议从Adam转换到SGD。该条件不会引入新的超参数，这是SWATS转换的一个优点。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ln"><img src="../Images/43c16d4645fd857167d2dbe5ecbe8629.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*V-hwVoXyDWK1wPVbgxOBRw.png"/></div></figure><p id="8952" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然SWATS在测试误差方面并不落后于SGD，因此可以被视为一个合适的转换位置，但训练速度似乎并没有从亚当阶段中受益匪浅。也许混合这两种优化器的想法是没有用的。最重要的是，在训练的“中间阶段”, SWATS的误差不会比两个主要优化器都好。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lo"><img src="../Images/0d1d57772d0d194f6c72f453897d6a17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JKG2pBz5oiIJeZErRKJRhA.png"/></div></div></figure><h2 id="278a" class="ky jw hi bd jx kz la lb kb lc ld le kf iq lf lg kj iu lh li kn iy lj lk kr ll bi translated">Adabound[4]</h2><p id="363f" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">Adabound对学习速率采用动态限制，以实现从自适应方法到SGD的逐步过渡。这篇论文认为自适应方法的极端学习率(大的和小的)是有问题的。训练后获得的自适应学习率非常极端(例如，小于10^-4，大于10⁴)，如下图所示，该图显示了自适应学习率的对数。鉴于这种极端的适应性学习率，亚当可以表现出怪异的行为。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lp"><img src="../Images/681702b1fc2f07cbae3423981f041c1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*9uuCUsISnxIYLGC4Z12YqQ.png"/></div></figure><p id="4364" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了解决这个问题，Adabound提出将自适应学习速率限制在某个阈值。当剪裁到0和inf时，Adam可以被视为Adabound的特例。限幅参数被自适应地控制为t的函数，t是被训练的时期的数量。准确地说，学习率被裁剪为下面的等式。论文还提出了亚当不收敛的证据。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lq"><img src="../Images/a71314dad3b51b685bb536b7acfcd176.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*ZMxhJ3OjMeFUze_asrZa6w.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lr"><img src="../Images/09de1f9c5262e6d00d7aeaa495b95e25.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*oIUD_1VlJOXqQABvPGudZw.png"/></div></figure><p id="afa6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最终的优化器显示出比Adam更快的优化器，同时通过限制极端的学习速率而像SGD一样准确。训练的早期阶段类似于Adam，而最终精度过渡到SGD。实验结果如下图所示。Adabound引入了许多新的超参数。</p><div class="jf jg jh ji fd ab cb"><figure class="ls jj lt lu lv lw lx paragraph-image"><img src="../Images/f1c6984e72215e899ae686f458a9ba06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*SpKE_ydVm3Nxo3Qp3lHDdg.png"/></figure><figure class="ls jj ly lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><img src="../Images/4baab668384e7991fb6020e19dd9d45f.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*0RYKhW8ykBTL1o65qPVCGw.png"/></div></figure></div><h2 id="96d6" class="ky jw hi bd jx kz la lb kb lc ld le kf iq lf lg kj iu lh li kn iy lj lk kr ll bi translated">前瞻[3]</h2><p id="0f30" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">与以前的建议不同，以前的建议主要是改进Adam族方法，LookAhead提出了一种完全不同的方法来加速优化。在朝着最终快速权重的方向更新“慢速权重”一次之前，前瞻使用两个优化器来首先更新“快速权重”k次。这一过程将以可忽略的计算成本减少慢权重的方差。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lz"><img src="../Images/1ecbd839a56bc513ee22b8c333417306.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q2S7xsp318ZwZYIuxTNLvg.png"/></div></div></figure><p id="8fc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了澄清，权重仅前进了前一状态与快速权重的最终权重之间的差的一部分α(θt，kφt 1)。根据该论文，我们可以受益于该方法的稳定性并使用更大的学习速率。</p><h2 id="c8f4" class="ky jw hi bd jx kz la lb kb lc ld le kf iq lf lg kj iu lh li kn iy lj lk kr ll bi translated">坚定的信念</h2><p id="2e1b" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">AdaBelief被提出来同时实现三个目标:如自适应方法中的快速收敛、如SGD中的良好泛化以及训练稳定性。AdaBelief根据当前梯度方向上的“信念”来调整步长。将梯度的指数移动平均值(EMA)视为下一时间步的梯度预测值，如果观察到的梯度与预测值相差很大，优化程序会采取较小的步骤。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ma"><img src="../Images/5d756fbea2452d663710b4c1e32820b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lhmB7-HshhvFjAxN4VTtnA.png"/></div></div></figure><p id="652b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度的EMA实际上是Adam优化器中的动量项，直观地将动量值m_t视为梯度g_t的预测值，当预测值与真实值相似时((g_t-m_t)较小)，AdaBelief迈出较大的一步，否则迈出较小的一步。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mb"><img src="../Images/4a3b53c574ba1986978d0b1f7a2b9b5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HFkNp7VnHWKQgJhzif739A.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mc"><img src="../Images/601bf5fd9c140fa5c01f71e508f7b7ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PdH7O6dL23l6EPiUXJd6nA.png"/></div></div></figure><p id="2440" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">据观察，AdaBelief具有以下效果:</p><ul class=""><li id="9fca" class="md me hi ih b ii ij im in iq mf iu mg iy mh jc mi mj mk ml bi translated">自适应方法中的快速收敛。</li><li id="4d0f" class="md me hi ih b ii mm im mn iq mo iu mp iy mq jc mi mj mk ml bi translated">良好或更好的概括，如在SGD家族中。</li><li id="5f7c" class="md me hi ih b ii mm im mn iq mo iu mp iy mq jc mi mj mk ml bi translated">在GANs等复杂环境中训练稳定性。</li><li id="a47c" class="md me hi ih b ii mm im mn iq mo iu mp iy mq jc mi mj mk ml bi translated">没有从Adam引入新的超参数。</li></ul><h2 id="ae0e" class="ky jw hi bd jx kz la lb kb lc ld le kf iq lf lg kj iu lh li kn iy lj lk kr ll bi translated">更多优化器</h2><p id="16d8" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">优化器的有效性可能高度依赖于训练配置和超参数，因此可能使用冗长的搜索过程找到现实任务中最佳的工作优化器。这种努力可能值得，也可能不值得，因为优化器的好处没有得到普遍认可，而且非常不清楚。然而，根据上述论文中的实验，优化器的选择似乎对泛化性能有很大的影响。</p><p id="684f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面的库提供了本文中介绍的所有优化器的PyTorch实现，以及更多。</p><div class="mr ms ez fb mt mu"><a href="https://github.com/jettify/pytorch-optimizer" rel="noopener  ugc nofollow" target="_blank"><div class="mv ab dw"><div class="mw ab mx cl cj my"><h2 class="bd hj fi z dy mz ea eb na ed ef hh bi translated">jettify/py torch-优化器</h2><div class="nb l"><h3 class="bd b fi z dy mz ea eb na ed ef dx translated">torch-optimizer —与optim模块兼容的PyTorch优化器集合。安装过程很简单…</h3></div><div class="nc l"><p class="bd b fp z dy mz ea eb na ed ef dx translated">github.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni jo mu"/></div></div></a></div><h1 id="c8cc" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">学习率计划</h1><p id="ef64" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">学习率计划是指在训练过程中修改学习率以提高成绩的方法。我们通常在一定次数的迭代之后以一定的比率衰减学习率(步长衰减)，或者采用输出学习率的当前迭代的一些数值函数。</p><p id="d648" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">令人惊讶的是，高级学习率计划，尤其是我们即将讨论的余弦LR衰减对神经网络的性能有很大的影响。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es nj"><img src="../Images/a8f3b1a43fc67e77ef5192cac35c3dee.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*N8QAtWmiFPL15dGfrAKPRw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx">Image from [10]</figcaption></figure><h2 id="1924" class="ky jw hi bd jx kz la lb kb lc ld le kf iq lf lg kj iu lh li kn iy lj lk kr ll bi translated">SGDR[6]</h2><p id="8501" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">简而言之，SGDR使用余弦退火来衰减学习速率，如下式所述。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es nk"><img src="../Images/45eef48ec6a1142b39a8c785c96a9e00.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*YjEYGcXzIEehUlXiZ-P3Dw.png"/></div></figure><p id="1cfa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除了余弦退火之外，本文还在每个T_i周期使用模拟热重启，并在训练过程中逐渐增加。这是为了<em class="jd">对输入信息</em>进行去噪，因为梯度和损失值在不同批次的数据之间变化很大。</p><p id="dd09" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ju" href="https://towardsdatascience.com/https-medium-com-reina-wang-tw-stochastic-gradient-descent-with-restarts-5f511975163" rel="noopener" target="_blank">这篇文章</a>描述了这种学习率重启背后的直觉。简而言之，学习率的激增理论上可以将模型推出糟糕的局部极小值。学习率时间表可以绘制如下图。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es nl"><img src="../Images/0915aa2361534f6bec24a6e9e77b5b37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U1Zmxr6P-3d3j2--F6SZ0w.png"/></div></div></figure><p id="cc78" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">实验结果如下图所示。与SGDR学习率计划相比，阶梯学习率衰减(蓝色、红色)不稳定、缓慢，甚至最终表现更差。我们可以看到，热重启实际上显示了显著的速度提升，因为经常重启的深绿色和紫色调度显示了非常快的初始性能。</p><p id="1f52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最重要的是，提出的SGDR能够将训练速度提高几个数量级。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es nm"><img src="../Images/235e0d8675954aa56044fa3ce4c8482d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wohz89a8I0zuldPvIQ68og.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es nn"><img src="../Images/5b143a6cd17853c8b25f31bf8e6d36e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_k4ufvn33V6foTG6hJ-d_g.png"/></div></div></figure><h1 id="bbfc" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">结论</h1><p id="02b2" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">在这篇文章中，我们回顾了一些现代优化的方法和背景。我们观察到，优化器的选择对训练速度和最终性能有重大影响。我们看到，新的AdaBelief在它所呈现的实验中胜过其他优化器。</p><p id="e45f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，我不确定在没有进一步调整超参数的情况下，这种优越的性能是否可以推广到任何训练设置。因为尽管现代优化器具有潜在的准确性和速度增益，但是诸如Adam和SGD之类的经典优化器仍然很流行，所以这种优化器的一般应用仍然存在不确定性。</p><p id="a19a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">需要注意的一点是，大多数优化器都是建立在SGD比Adam等自适应方法更好的基础上的。然而，还不清楚这种说法是否正确，因为存在相反的结果[8]。SGD和Adam之间的泛化性能这个话题在我的<a class="ae ju" rel="noopener" href="/geekculture/a-2021-guide-to-improving-cnns-optimizers-adam-vs-sgd-495848ac6008">上一篇文章中探讨过。</a></p><h2 id="95c1" class="ky jw hi bd jx kz la lb kb lc ld le kf iq lf lg kj iu lh li kn iy lj lk kr ll bi translated">参考</h2><p id="25c8" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">[1] Loshchilov，I .，&amp; Hutter，F. (2017)。去耦权重衰减正则化。<em class="jd"> arXiv预印本arXiv:1711.05101 </em>。</p><p id="81a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2]杜奇、哈赞和辛格(2011年)。在线学习和随机优化的自适应次梯度方法。<em class="jd">机器学习研究杂志</em>，<em class="jd"> 12 </em> (7)。</p><p id="cf97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[3]张，M. R .，卢卡斯，j .，辛顿，g .，&amp;巴，J. (2019)。前瞻优化器:向前k步，向后1步。<em class="jd"> arXiv预印本arXiv:1907.08610 </em>。</p><p id="3ba3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[4]罗，李，熊，杨，刘，杨，孙，谢(2019).学习速率动态限制的自适应梯度方法。<em class="jd"> arXiv预印本arXiv:1902.09843 </em>。</p><p id="e81d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[5]庄，j .，唐，t .，丁，y .，塔蒂孔达，s .，德沃内克，n .，帕帕德米特里斯，x .，&amp;邓肯，J. S. (2020)。Adabelief优化器:通过对观测梯度的信任来调整步长。<em class="jd"> arXiv预印本arXiv:2010.07468 </em>。</p><p id="85cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[6] Loshchilov，I .，&amp; Hutter，F. (2016年)。Sgdr:带有热重启的随机梯度下降。<em class="jd"> arXiv预印本arXiv:1608.03983 </em>。</p><p id="8790" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[7]凯斯卡尔，N. S .，&amp;索彻，R. (2017年)。通过从adam切换到sgd来提高泛化性能。<em class="jd"> arXiv预印本arXiv:1712.07628 </em>。</p><p id="1d84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[8]Choi d .、Shallue、C. J .、Nado z .、Lee j .、马迪森、C. J .、&amp; Dahl、G. E. (2019年)。深度学习优化器的实证比较。<em class="jd"> arXiv预印本arXiv:1910.05446 </em>。</p><p id="6d4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[9]威尔逊、罗洛夫斯、斯特恩、斯雷布罗和雷希特(2017年)。机器学习中自适应梯度方法的边际价值。arXiv预印本arXiv:1705.08292 。</p><p id="5a65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[10] Bello，I .、Fedus，w .、Du，x .、Cubuk，E. D .、Srinivas，a .、Lin，T. Y .、… &amp; Zoph，B. (2021年)。改进的训练和扩展策略。<em class="jd"> arXiv预印本arXiv:2103.07579 </em>。</p><p id="a0b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[11]你，y，李，j .，Reddi，s .，Hseu，j .，Kumar，s .，Bhojanapalli，s，... &amp; Hsieh，C. J. (2019)。深度学习大批量优化:76分钟训练bert。<em class="jd"> arXiv预印本arXiv:1904.00962 </em>。</p><p id="d532" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[12]哈特，m .，雷希特，b .，辛格，Y. (2016年6月)。训练更快，推广更好:随机梯度下降的稳定性。在<em class="jd">机器学习国际会议</em>(第1225–1234页)。PMLR。</p><p id="7442" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[13] Reddi，s .，Zaheer，m .，Sachan，d .，Kale，s .，和Kumar，S. (2018年)。非凸优化的自适应方法。在<em class="jd">第32届神经信息处理系统会议录(NIPS 2018) </em>中。</p></div></div>    
</body>
</html>
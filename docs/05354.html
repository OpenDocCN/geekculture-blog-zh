<html>
<head>
<title>Action Recognition Using Pytorch Geometric</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于Pytorch几何的动作识别</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/action-recognition-using-pytorch-geometric-d50ca8de0f6e?source=collection_archive---------11-----------------------#2021-07-19">https://medium.com/geekculture/action-recognition-using-pytorch-geometric-d50ca8de0f6e?source=collection_archive---------11-----------------------#2021-07-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="2e01" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我曾在海德拉巴国际信息技术学院(IIITH)的Ravikiran Sarvadevabhatla博士的指导下研究动作识别。我的工作是使用<a class="ae jd" href="https://pytorch-geometric.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> Pytorch几何库</a>实现基于骨架的动作识别 (STGCN)的研究论文<a class="ae jd" href="https://arxiv.org/pdf/1801.07455.pdf" rel="noopener ugc nofollow" target="_blank">时空图卷积网络。所用的数据集是</a><a class="ae jd" href="https://github.com/shahroudy/NTURGB-D" rel="noopener ugc nofollow" target="_blank"> NTU-60 </a>数据集(所用的行动类别是A1-A60)。</p><blockquote class="je jf jg"><p id="b9cb" class="if ig jh ih b ii ij ik il im in io ip ji ir is it jj iv iw ix jk iz ja jb jc hb bi translated">由于下面的文章使用了图卷积网络，如果您不熟悉这个概念，请阅读“<a class="ae jd" rel="noopener" href="/analytics-vidhya/getting-the-intuition-of-graph-neural-networks-a30a2c34280d">获得图神经网络的直觉</a>”、“<a class="ae jd" href="https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b" rel="noopener" target="_blank">了解用于节点分类的图卷积网络</a>”以对图神经网络有一个基本的了解。</p></blockquote><h1 id="97fe" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated"><strong class="ak">什么是NTU 60数据集？</strong></h1><p id="337b" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">NTU-60数据集将人体骨骼建模为一组25个关节，每个关节有3个值(x，y，z)来表示空间信息。当执行一个动作时，25个关节的值随时间而变化。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es ko"><img src="../Images/870cd7cff6eb1a92015731cfdd23eb8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*mv_fJfhZbNwxH3A_28g7ng.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx">Fig 1: 25 body joints in NTU dataset</figcaption></figure><h1 id="9ca8" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">现有STGCN代码</h1><p id="8603" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated"><a class="ae jd" href="https://github.com/open-mmlab/mmskeleton/tree/master/deprecated/origin_stgcn_repo" rel="noopener ugc nofollow" target="_blank"> STGCN代码</a>旨在捕捉每个时间帧(空间卷积)的这些特征值如何在空间中变化(即，对于给定关节，其连接关节的值如何变化)以及每个关节的值如何在时间帧中变化(时间卷积)，以将视频分类为特定动作。网络的输入是联合值，它在最终分类层之前通过多个STGCN块(即执行空间和时间卷积的块)。通过每个块后，生成更高级的特征图(即每个关节的数值增加)。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es la"><img src="../Images/7343f41036ce144843e5135f796681b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*O1OppFfaKscdXGyY8dUDFA.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx">Fig 2: STGCN skeleton sequence</figcaption></figure><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lb"><img src="../Images/1e1628e4eccb9510ee756276508161e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_VvTpdN8eHhGq9ryUpLXXw.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx">Fig 3: Spatial and Temporal Convolutions performed multiple times on the sequence before classification.</figcaption></figure><p id="6561" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于数据是以批处理的方式进行处理的，因此在传递到网络之前，数据具有以下形状:N x C x T x V x M其中:</p><p id="d79b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">n:批量大小，C:通道/特征的数量(开始时为3)，T:时间帧的数量，V:顶点的数量(总是25)，M:时间帧中出现的人数。</p><p id="032c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了批量处理不同的视频，一批视频的尺寸必须相同。但是数据集的视频具有不同的时间帧，并且在这些帧中存在不同数量的人(1或2)。为了确保统一的尺寸，视频用0填充，使得数据集中的时间帧的数量为300，并且每个帧中的人数为2(如果一个帧有一个人，则复制该帧，使得第二个人的关节的特征值为0)。现在传递给网络的数据有以下维度:N x C x 300 x V x 2。</p><p id="f6a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据在通过第一个STGCN块之前被整形为N*2 x C x T x V。</p><p id="f07b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">STGCN代码使用了<a class="ae jd" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html" rel="noopener ugc nofollow" target="_blank"> nn。用于空间和时间卷积的Conv2d </a>包。对于<a class="ae jd" href="https://github.com/open-mmlab/mmskeleton/blob/master/deprecated/origin_stgcn_repo/net/utils/tgcn.py" rel="noopener ugc nofollow" target="_blank">空间卷积</a>，使用<a class="ae jd" href="https://github.com/open-mmlab/mmskeleton/blob/master/deprecated/origin_stgcn_repo/net/utils/graph.py" rel="noopener ugc nofollow" target="_blank">邻接矩阵</a>的概念来确定骨架中存在的邻居，并且在给定的时间帧中，空间特征被卷积以生成新的空间特征图。对于<a class="ae jd" href="https://github.com/open-mmlab/mmskeleton/blob/master/deprecated/origin_stgcn_repo/net/st_gcn.py" rel="noopener ugc nofollow" target="_blank">时间卷积</a>，使用的核大小是(9，1)，即它捕获9个时间帧中每个关节的特征值，并卷积成单个特征。体系结构如下所示。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lg"><img src="../Images/924ce9d7e65b2c9960dd9b4e11f7a7c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dTgSZoKiClf_rCD6qmNvaA.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx">Fig 4: Original STGCN Code Architecture</figcaption></figure><h1 id="852d" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">此方法的问题:</h1><p id="4bb2" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">在这种方法中，填充输入数据以执行批处理和卷积的必要性意味着消耗更多的空间，并且对填充的数据(用0填充)执行不必要的卷积。</p><p id="400a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了避免使用填充，我尝试使用Pytorch几何库重写相同的代码。</p><h1 id="071f" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">Pytorch基于几何的代码:</h1><p id="8e1d" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">Pytorch几何库提供了在不规则数据结构上实现深度学习网络的各种方法。我做图形卷积用的包是<a class="ae jd" href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv" rel="noopener ugc nofollow" target="_blank"> GCNConv </a>。Pytorch Geometric允许<a class="ae jd" href="https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Batch" rel="noopener ugc nofollow" target="_blank">批处理</a>节点数量可变但特征数量相同的图。为了更好地利用这一点，我将NTU 60数据集视为一组图表。这些图具有不同数量的时间节点，但特征保持不变。</p><p id="afa9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图形的形状是T×V×C(T:时间节点，V:顶点，C:通道数)。批量大小为5意味着将5个图作为单个不连接的图一起批量处理，其中值T将是每个图的时间节点的数量之和。</p><p id="0586" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这也解决了两个人所需的填充，方法是将每个人视为一个图，并将他们作为一个不连通的图分批在一起。</p><p id="697d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面的图4展示了一个玩具数据集，显示了这些图形是如何组合在一起的。在该数据集中，折点数(V)固定为4，要素数(C)固定为3。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lh"><img src="../Images/7924e1c3a70d7cb6eca7639133f620b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QPmxf9ydGh4Z4UCwIvu13Q.jpeg"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx">Fig 5: The orange-colored nodes are the temporal nodes and within each node, there is a graph of dimension VxC.</figcaption></figure><p id="5062" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<a class="ae jd" href="https://github.com/hariharannatesh/Action-Recognition-using-Pytorch-Geometric/blob/master/expt_6/expt_6g/Code_pytorch_geom/utils_temp/stgcn_dataset.py" rel="noopener ugc nofollow" target="_blank">处理完数据</a>后，图表通过<a class="ae jd" href="https://github.com/hariharannatesh/Action-Recognition-using-Pytorch-Geometric/blob/master/expt_6/expt_6g/Code_pytorch_geom/utils_temp/gstcn.py" rel="noopener ugc nofollow" target="_blank"> STGCN模块</a>并通过最终分类层。由于Pytorch STGCN代码的数据格式不同，因此<a class="ae jd" href="https://github.com/hariharannatesh/Action-Recognition-using-Pytorch-Geometric/blob/master/expt_6/expt_6g/Code_pytorch_geom/utils_temp/pyg_batchnorm.py" rel="noopener ugc nofollow" target="_blank">批量标准化</a>也必须进行编码，以使其等同于原始的1-D、2-D批量标准化。体系结构如下所示。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es li"><img src="../Images/7010a003176cebc79e4db9117789a544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JrLcYKpMIYs-o9EL_V9NSg.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx">Fig 6: Pytorch STGCN Code Architecture</figcaption></figure><p id="164a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">显示了原始STGCN代码和Pytorch几何代码的曲线。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lj"><img src="../Images/cc9f783fb7871698eebbf3964a179b60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FS6_p0xrWVN4tMEz0aFLqQ.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx">Fig 7:Training and Validation Accuracy plot for original STGCN Code.</figcaption></figure><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lj"><img src="../Images/9e09be15c3abccf3c4355fda1337c267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vPfeFMbMkwYMYJv_qGobAw.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx">Fig 8:Training and Validation Accuracy plot for Pytorch Geometric STGCN Code.</figcaption></figure><p id="f100" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以看出，原始STGCN代码具有大约78%的验证准确度，而基于Pytorch几何的STGCN代码具有大约58%的验证准确度。尽管这两种代码在概念上执行空间和时间卷积，但卷积模块中涉及的参数数量是不同的。</p><h1 id="e8d2" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">Pytorch几何模型为什么不行？</h1><p id="8001" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">在Conv2d包中，如果内核大小给定为(9，1)，则卷积运算的可学习参数数为[9 x输入特征数x输出特征数]。除此之外，这还意味着在给定的数据点，该数据点两侧的4个邻居的特征被卷积，即给定的数据点有8个邻居。</p><p id="2596" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了使用GCNConv包执行等效操作，需要预先定义邻接矩阵，使得任意节点具有8个节点的邻居。基本的图卷积运算是归一化邻接矩阵、图、可学习权重矩阵的乘积。权重矩阵的形状为[输入要素的数量x输出要素的数量]。这意味着尽管邻接矩阵照顾到了邻居，但是每个STGCN操作的可学习操作符的数量减少了9倍(即内核大小)。邻域特征被同等地呈现。这种类型的内核被称为各向同性内核。</p><h1 id="637f" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">结论:</h1><p id="6cd8" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">如果内核不是像<a class="ae jd" href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.SplineConv" rel="noopener ugc nofollow" target="_blank"> SplineConv </a>包所使用的那种各向同性内核，那么精确度可能会提高。然而，SplineConv包只能容纳二维图形，而我们的图形是三维的。如果一个图形包可以容纳三维图形，并且没有各向同性的内核，那么我相信在不使用填充数据的情况下可以达到更高的精度。</p></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><p id="aa36" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jh">链接到Pytorch几何知识库:</em><a class="ae jd" href="https://github.com/hariharannatesh/Action-Recognition-using-Pytorch-Geometric" rel="noopener ugc nofollow" target="_blank"><em class="jh">https://github . com/hariharannatesh/Action-Recognition-using-py torch-Geometric</em></a></p></div></div>    
</body>
</html>
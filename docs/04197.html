<html>
<head>
<title>Unsupervised Learning and the Intuition Behind K-Means Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无监督学习和K-均值聚类背后的直觉</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/unsupervised-learning-and-the-intuition-behind-k-means-clustering-9805ed89fa0?source=collection_archive---------52-----------------------#2021-06-22">https://medium.com/geekculture/unsupervised-learning-and-the-intuition-behind-k-means-clustering-9805ed89fa0?source=collection_archive---------52-----------------------#2021-06-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/5f3a01c8f7b1905aacba8a9d32b67a6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sahtQd6AdJ1W_s5c"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx">Photo by <a class="ae hv" href="https://unsplash.com/@colorflores?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Omar Flores</a> on <a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="d148" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是我正在从事的系列文章的第八部分，其中我们将讨论和定义介绍性的机器学习算法和概念。在这篇文章的最后，你会找到这个系列的所有前几篇文章。我建议你按顺序读这些。原因很简单，因为我在那里介绍了一些概念，这些概念对于理解本文中讨论的概念至关重要，我将在许多场合引用它们。</p><p id="3b6c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">今天我们来看看我们第一个(也是唯一一个)无监督学习算法。我们将定义无监督意味着什么，然后看看K-Means背后的直觉以及回答一些常见问题。</p><p id="2751" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们开始吧。</p><h1 id="74cf" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">无监督学习算法</h1><p id="48a4" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在我们之前的所有文章中，我们已经处理了如下所示的训练数据:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kw"><img src="../Images/a46cf8ce0da3272f0d83fb3a5b118fa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*M7sJomicWzN_i5h2.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 1:</strong> Classification Problem with Two Features and Two Classes</figcaption></figure><p id="56d6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中所有的<code class="du lb lc ld le b">{x^(1), x^(2),...x^(m)}</code>(训练输入)都有各自的<code class="du lb lc ld le b">{y^(1),y^(2),...y^(m)}</code>值(训练标签)，表明<strong class="ix hz">实际的</strong>值是多少。有了这些信息，我们就能够使用训练输入来训练我们的模型，并将我们的<strong class="ix hz">预测值</strong>与实际训练标签进行比较。我们称之为监督学习。在无监督学习中，我们的数据现在看起来如下所示:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lf"><img src="../Images/0ba13a3889e72aba9b30d9e3a9f331cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_upyYCipK2pabnZk-lF2qQ.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 2:</strong> Unsupervised Algorithm Training Data Example</figcaption></figure><p id="faf4" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们只有训练输入<code class="du lb lc ld le b">{x^(1), x^(2),...x^(m)}</code>，没有对这些输入标签的理解，也就是没有<code class="du lb lc ld le b">{y^(1),y^(2),...y^(m)}</code>。无监督学习算法的目标是通过基于点之间的相似性创建不同的组，在这些数据中找到某种顺序。这些组被称为<strong class="ix hz">簇</strong>。下面是一个无监督学习算法如何从图2 所示的训练数据中创建不同聚类的示例:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lg"><img src="../Images/e005c1744422280622336df62f7c10a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t4RfXTMwFtBh6MnoZ1ULpA.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 3:</strong> Grouping Training Data Into Two Different Clusters</figcaption></figure><p id="e275" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最流行和最常用的聚类算法之一是K-Means。</p><h1 id="c658" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">k均值</h1><h2 id="f6f3" class="lh ju hy bd jv li lj lk jz ll lm ln kd jg lo lp kh jk lq lr kl jo ls lt kp lu bi translated">直觉</h2><p id="9892" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">理解这个算法的最好方法是通过一个例子。因此，考虑下图:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lv"><img src="../Images/6daf1a2fa0d156d12004dbd98ca94225.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4zC20amyBLthwzq4wEouhA.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 4:</strong> Classification Problem with Two Features and Two Classes</figcaption></figure><p id="ccd9" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们希望将相似的数据点分组。K-Means要做的第一件事就是选择<code class="du lb lc ld le b">k</code>任意点。我们将这些点称为<strong class="ix hz">质心</strong>。对于我们的示例，我们将选择<code class="du lb lc ld le b">k = 3</code>，标记为红色、蓝色和黑色十字:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lw"><img src="../Images/746aa51a573757ca8a2637a181cb8425.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JQ-D9Egw-O-gB-ptfz-fww.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 4:</strong> K-Means Selecting K Arbitrary Centroids</figcaption></figure><p id="5f19" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，我们将计算每个训练点到这两个质心的距离，使用您选择的距离方程，例如<a class="ae hv" href="https://en.wikipedia.org/wiki/Euclidean_distance#:~:text=In%20mathematics%2C%20the%20Euclidean%20distance,being%20called%20the%20Pythagorean%20distance." rel="noopener ugc nofollow" target="_blank">欧几里德距离</a>。我们将训练点分配给它最接近的质心:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lx"><img src="../Images/128417df1f5d354411314706f4b546db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gWKFVT5a09qh2NLXh90k0w.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 5:</strong> K-Means Assigning Training Points to the Closes Centroid</figcaption></figure><p id="3724" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们通过使用以下公式计算指定给质心的所有训练点的平均值来重新定位质心:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ly"><img src="../Images/144c7dfe5c9c8f202992a901f962cfbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lTk0JGXl5P-2FKDJcmHt9Q.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 5:</strong> K-Means Assigning Training Points to the Closes Centroid</figcaption></figure><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lz"><img src="../Images/21f061bc6418cea059ca6717d0af577a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EG3wyNVYoM2p3G5UXv5nOA.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 6:</strong> K-Means Repositioning the Centroids</figcaption></figure><p id="91ce" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上面的<strong class="ix hz">等式1 </strong>中，<code class="du lb lc ld le b">m^K</code>表示分配给聚类<code class="du lb lc ld le b">K</code>的训练样本的数量。最后，我们重复该过程，直到我们不再看到质心位置的变化(直到我们收敛)，即从训练点到它们的指定质心的平均距离最小:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ma"><img src="../Images/46c72ee09cbcb29dbae96a2986074cf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DSHaBo2IWGYcFNImhXn0UA.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 7:</strong> K-Means Converging</figcaption></figure><p id="b6da" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在你知道了。以下是该算法的官方伪代码，摘自Coursera上<a class="mb mc ge" href="https://medium.com/u/592ce2a67248?source=post_page-----9805ed89fa0--------------------------------" rel="noopener" target="_blank">吴恩达</a>的机器学习课程介绍:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es md"><img src="../Images/d1daba1bcb172b736c7bd6e0d4a15027.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CQYr1Yie_XOIEEC1TmRsbg.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 8:</strong> K-Means Algorithm Pseudocode</figcaption></figure><p id="12ad" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果您一直关注这个系列，那么您可能会注意到一些奇怪的事情。有一个步骤我们甚至还没有考虑提及。这个算法的代价函数是什么？我们怎么知道什么时候找到了最优解。信不信由你，我们已经在<strong class="ix hz">等式1 </strong>中引入了K均值成本函数。就像我们找到每个训练点与其分配的质心之间的平均距离一样，我们可以找到所有<em class="me">训练点与其分配的质心之间的平均距离，这为我们提供了模型执行情况的总体指示:</em></p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mf"><img src="../Images/453e97bd4ad5c5d8fad8e86404b87983.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n3I-cm7e2mujVRm08FaugA.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 2:</strong> Mean of Distance of All Training Points to Their Assigned Centroid</figcaption></figure><p id="ba3a" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中<code class="du lb lc ld le b">m</code>现在是总训练点数。这是我们试图最小化的方程，这是K均值的成本函数。</p><h1 id="2c70" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">需要考虑的事项</h1><p id="d755" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在结束之前，对于这个算法，我想提出几个常见的问题。</p><h2 id="d5b8" class="lh ju hy bd jv li lj lk jz ll lm ln kd jg lo lp kh jk lq lr kl jo ls lt kp lu bi translated">我们会一直收敛到全局最优吗？</h2><p id="c20f" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">答案是否定的。请看下图:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es mg"><img src="../Images/6b06f136d42a0911a784b5c4d4e48669.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*GqcelFf2nNiSZy3Y_aRDXw.jpeg"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 9:</strong> Random Points on a Graph</figcaption></figure><p id="4dfb" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们在这些训练点上运行K-Means。根据初始化质心的方式，可以获得以下任何结果:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es mh"><img src="../Images/cecaf18ab263cb5fa885a5788d700c9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*AXqH6GYUGXywuGtpwUi7fg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 10:</strong> Optimal Solution</figcaption></figure><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es mi"><img src="../Images/819c99c6521b28b4078232ba3ebf0a83.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*G2F3CVV4eCm_RscKhxBaDQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 11:</strong> Non-Optimal Solution One</figcaption></figure><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es mj"><img src="../Images/f9adf0d2756867d91710d7956f12091b.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*H6Vu_d2lKetE-PSEpTXI1w.jpeg"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 12:</strong> Non-Optimal Solution Two</figcaption></figure><p id="4923" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">虽然<strong class="ix hz">图10 </strong>中的图表显然是正确的解决方案，但是K-Means很可能收敛到局部最优而不是全局最优，从而给我们留下不准确的结果，如图11和12所示。从数学上证明这一点很困难，但有一个很好的方法来形象化它。考虑这两点:</p><pre class="kx ky kz la fd mk le ml mm aw mn bi"><span id="682c" class="lh ju hy le b fi mo mp l mq mr">.     .<br/>.     .</span></pre><p id="756c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如下所示的解决方案(其中<code class="du lb lc ld le b">c</code>是集群):</p><pre class="kx ky kz la fd mk le ml mm aw mn bi"><span id="2402" class="lh ju hy le b fi mo mp l mq mr">.  c  .<br/>.  c  .</span></pre><p id="0d8e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">是有效的，但更好的解决方案应该是:</p><pre class="kx ky kz la fd mk le ml mm aw mn bi"><span id="d43b" class="lh ju hy le b fi mo mp l mq mr">.     .<br/>c     c<br/>.     .</span></pre><p id="37e9" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">要准确得多。我们可以通过选择正确数量的质心<code class="du lb lc ld le b">K</code>，以及有系统地随机初始化它们，来尽量减少这种情况发生的可能性。</p><h2 id="cf5d" class="lh ju hy bd jv li lj lk jz ll lm ln kd jg lo lp kh jk lq lr kl jo ls lt kp lu bi translated">如何随机初始化质心？</h2><p id="58ae" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">一种方法是让选择完全随机。虽然这是一个有效的解决方案，但是这是一个非常不稳定和危险的选择质心位置的方法。</p><p id="3a9a" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最广泛使用的方法是选择<code class="du lb lc ld le b">K</code>训练点，并使用这些坐标作为质心的初始位置。例如，如果我们有:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es mg"><img src="../Images/6b06f136d42a0911a784b5c4d4e48669.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*GqcelFf2nNiSZy3Y_aRDXw.jpeg"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 13:</strong> Random Points on a Graph</figcaption></figure><p id="9b79" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，我们可以选择任意点，并将它们用作质心的初始位置，例如:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es mg"><img src="../Images/238a84e6d8554f6f93da5638b6e23402.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*XbpUk3xWitmvHFkBbTCSKA.jpeg"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 14:</strong> Training Points Positions Used as Centroid Initial Positions</figcaption></figure><p id="d747" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注意，这仍然<strong class="ix hz">不能</strong>保证我们不会陷入局部最优，但是它减少了机会。</p><h2 id="fe39" class="lh ju hy bd jv li lj lk jz ll lm ln kd jg lo lp kh jk lq lr kl jo ls lt kp lu bi translated">怎么挑K？</h2><p id="6f41" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">这个问题的最佳答案是，根本没有选择正确k的具体方法。有一些技巧，但没有一个保证有效。在一天结束时，选择您的<code class="du lb lc ld le b">K</code>的最佳方式是通过手动浏览您的数据可视化，并决定您认为最佳的决策。</p><p id="21a0" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在可用的几种技术中，<strong class="ix hz">肘法</strong>应用最为广泛。它的工作原理是比较集群的成本和数量<code class="du lb lc ld le b">K</code>:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ms"><img src="../Images/b75c368613b0eaf9607a24c68c81e3ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BeK-oJy9FB3HUyYkFS44-w.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 15:</strong> Elbow Method Example</figcaption></figure><p id="70d6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这种情况下，我们看到<code class="du lb lc ld le b">K=1</code>和<code class="du lb lc ld le b">K=2</code>之间以及<code class="du lb lc ld le b">K=2</code>和<code class="du lb lc ld le b">K=3</code>之间的成本显著降低，但之后它开始以慢得多的速度降低，因此我们可以得出结论<code class="du lb lc ld le b">K=3</code>是正确的质心数量。但是，尽管尝试一下这种方法总是值得的，但通常情况下，您的图表看起来会像这样:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mt"><img src="../Images/72711d29aa43b872217525b6160c4b7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MxI1ZkvfoFgput28rRRR-A.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 16:</strong> Elbow Method Counterexample</figcaption></figure><p id="f8b6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中<code class="du lb lc ld le b">K</code>值之间的误差减少最小，留给我们的信息很少，无法做出有价值的结论。</p><p id="6b0b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">同样，目前最好的技术是简单地手动查看您的数据。</p><h1 id="08e5" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结论</h1><p id="5193" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在本文中，我们定义了无监督学习算法的概念。然后，我们采用这个想法，并用它来理解最流行的监督学习算法之一K-Means背后的理论。在最后，我们回答了一些算法最常见的问题，并得出结论，尽管有正式的方法，优化超参数的最佳方式是基于可视化手动进行。</p><p id="a614" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在下一篇文章中，我们将讨论降维。</p><h1 id="44e6" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">过去的文章</h1><ol class=""><li id="4528" class="mu mv hy ix b iy kr jc ks jg mw jk mx jo my js mz na nb nc bi translated"><strong class="ix hz">第一部分:</strong> <a class="ae hv" href="https://ali-h-khanafer.medium.com/data-pre-processing-ee81bbe5cc77" rel="noopener">数据预处理</a></li><li id="36b3" class="mu mv hy ix b iy nd jc ne jg nf jk ng jo nh js mz na nb nc bi translated"><strong class="ix hz">第二部分:</strong> <a class="ae hv" href="https://ali-h-khanafer.medium.com/linear-regression-using-gradient-descent-intuition-and-implementation-522d43453fc3" rel="noopener">梯度下降线性回归:直觉与实现</a></li><li id="9fff" class="mu mv hy ix b iy nd jc ne jg nf jk ng jo nh js mz na nb nc bi translated"><strong class="ix hz">第三部分:</strong> <a class="ae hv" rel="noopener" href="/geekculture/logistic-regression-using-gradient-descent-intuition-and-implementation-36a8498afdcb">使用梯度下降的逻辑回归:直觉和实现</a></li><li id="3ca5" class="mu mv hy ix b iy nd jc ne jg nf jk ng jo nh js mz na nb nc bi translated"><strong class="ix hz">第四部分— 1: </strong> <a class="ae hv" rel="noopener" href="/geekculture/neural-networks-part-1-terminology-motivation-and-intuition-73675fc43947">神经网络第一部分:术语、动机和直觉</a></li><li id="ae15" class="mu mv hy ix b iy nd jc ne jg nf jk ng jo nh js mz na nb nc bi translated"><strong class="ix hz">第四部分— 2: </strong> <a class="ae hv" rel="noopener" href="/geekculture/neural-networks-part-2-backpropagation-and-gradient-checking-4f8d1350fb0b">神经网络第二部分:反向传播和梯度检测</a></li><li id="5660" class="mu mv hy ix b iy nd jc ne jg nf jk ng jo nh js mz na nb nc bi translated"><strong class="ix hz">第六部分:</strong> <a class="ae hv" rel="noopener" href="/geekculture/evaluating-your-hypothesis-and-understanding-bias-vs-variance-86512cce4253">评估你的假设，理解偏差与方差</a></li><li id="b6ff" class="mu mv hy ix b iy nd jc ne jg nf jk ng jo nh js mz na nb nc bi translated"><strong class="ix hz">第七部分:</strong> <a class="ae hv" rel="noopener" href="/geekculture/support-vector-machines-and-kernels-8b064ee53fc3">支持向量机和核</a></li></ol><h1 id="641f" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">无耻的插头</h1><ul class=""><li id="1d21" class="mu mv hy ix b iy kr jc ks jg mw jk mx jo my js ni na nb nc bi translated"><strong class="ix hz">推特:【twitter.com/ali_khanafer2】T22</strong></li><li id="1f8e" class="mu mv hy ix b iy nd jc ne jg nf jk ng jo nh js ni na nb nc bi translated"><strong class="ix hz">领英</strong>:<a class="ae hv" href="https://www.linkedin.com/in/ali-khanafer-319382152/" rel="noopener ugc nofollow" target="_blank">linkedin.com/in/ali-khanafer-319382152/</a></li></ul><h1 id="3d6e" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">参考</h1><ol class=""><li id="18b6" class="mu mv hy ix b iy kr jc ks jg mw jk mx jo my js mz na nb nc bi translated"><a class="ae hv" href="https://www.coursera.org/learn/machine-learning?page=1" rel="noopener ugc nofollow" target="_blank">吴恩达的机器学习Coursera课程</a></li><li id="b6e9" class="mu mv hy ix b iy nd jc ne jg nf jk ng jo nh js mz na nb nc bi translated"><a class="ae hv" href="https://stackoverflow.com/questions/14577329/why-doesnt-k-means-give-the-global-minima" rel="noopener ugc nofollow" target="_blank">堆栈溢出:为什么K-Means不能给出全局最小值？</a></li></ol></div></div>    
</body>
</html>
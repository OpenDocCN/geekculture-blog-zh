<html>
<head>
<title>Optimization Techniques for better and faster Model Training.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">更好更快模型训练的优化技术。</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/optimization-techniques-for-better-and-faster-model-training-44b41ef7bd31?source=collection_archive---------12-----------------------#2022-04-25">https://medium.com/geekculture/optimization-techniques-for-better-and-faster-model-training-44b41ef7bd31?source=collection_archive---------12-----------------------#2022-04-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="4132" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">模型训练基本上包括学习一个使实际值和预测值之间的损失最小化的等式。通过学习方程，意味着调整方程中的各种参数/权重。这就是优化发挥作用的地方。</p><blockquote class="jd je jf"><p id="eec6" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">优化器是一种更新各种参数的方法或算法，可以以更少的努力减少损失。</p></blockquote><p id="9481" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jg">让我们直接进入不同的优化技术:</em></p><p id="bad2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jk translated"><span class="l jl jm jn bm jo jp jq jr js di">G</span><strong class="ih hj">radient Descent</strong><em class="jg">(1874年由柯西提出)</em></p><p id="5dd8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是最早出现的优化算法。使用以下等式更新权重:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es jt"><img src="../Images/0bd24d2b2c3e0e328509bec6e5622080.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*Sv3pxas-eSwG020zAxuHYg.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx">Figure 1: Gradient Descent algorithm , <em class="kf">Image by author</em></figcaption></figure><p id="b9c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GD通过直接减去成本函数<em class="jg"> J(θ) w.r.t </em> θ乘以学习率α的梯度(导数)来更新权重θ。但是为什么成本函数是梯度的呢？成本函数是我们要最小化的函数，它的导数基本上是斜率的方向。因此，GD以速率α沿斜坡下降，直到收敛并达到最小值。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kg"><img src="../Images/b9a0097f3aad2fe0816f4d54737cd58b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*W-Ry-D1V_-8gCP-zMWvAdg.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx">Figure 2: Gradient Descent Illustration, <em class="kf">Image by author</em></figcaption></figure><p id="3277" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">α，是这里的超参数，叫做学习率。它决定了步长的大小。</p><ol class=""><li id="9854" class="kh ki hi ih b ii ij im in iq kj iu kk iy kl jc km kn ko kp bi translated">如果α很大，可能无法收敛，会超调最小值。</li><li id="c4a0" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">如果α非常小，则需要很长时间才能收敛，并且计算量很大。</li></ol><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kv"><img src="../Images/80ac39feef3da3ce5361496aaf5d3b6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*GoFOHGVnX9mVrSLjXqXVNQ.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx">Figure 3: When, α is very large and the steps keep on oscillating instead of reaching the minima and diverge. <em class="kf">Image by author</em></figcaption></figure><p id="c631" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看在不同学习速率的情况下，w.r.t .对各代的损失。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kw"><img src="../Images/73a3a02e6fb3b1b8b61f3d231d9bef88.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/0*_pv3fU5efDmNaJMe.jpeg"/></div><figcaption class="kb kc et er es kd ke bd b be z dx">Figure 4: Gradient descent with different learning rates. <a class="ae kx" href="http://cs231n.github.io/neural-networks-3/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><p id="54f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jk translated"><span class="l jl jm jn bm jo jp jq jr js di">G</span><strong class="ih hj"/><em class="jg">(鲍里斯·波亚克于1964年提出)</em></p><p id="71ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">传统梯度下降算法的问题是，它没有考虑先前的梯度是什么，如果梯度很小，它下降得非常慢。</p><p id="9313" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当存在多个局部最小值或具有近似零梯度的鞍点时，这产生了问题，这又导致权重没有更新或更新很少，并且学习停止。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es lf"><img src="../Images/d2ec5b301b455732f772f79fafda1af7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*6bnX4AjWm5VtzvS5F-ORrA.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx">Figure 5: Gradient Descent stuck at local minima instead of going down to global minima. <em class="kf">Image by author</em></figcaption></figure><p id="3645" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一方面，动量优化非常关心先前的梯度。它考虑了上一次迭代中步长的变化(动量增加),并使用它来更新下一步。我们可以想象一个滚动的球，它开始很慢，但很快获得动量，直到它到达底部(全局最小值)。使用以下等式更新权重:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es lg"><img src="../Images/bb3a0425649a628560d8697a11a70d34.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*FELAX0zRKQc5Eq96KQIaXA.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx">Figure 6: Momentum Optimization algorithm. <em class="kf">Image by author</em></figcaption></figure><p id="787b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">β，称为动量，是调整前面的变化，设置在<em class="jg"> 0(高摩擦)</em>到<em class="jg"> 1(低摩擦)</em>之间。建议值为0.9。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lh"><img src="../Images/877879a25dc9f1c3c9a5459ef0ae6287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VP2bzoO_xDppAeZclfWMCw.png"/></div></div><figcaption class="kb kc et er es kd ke bd b be z dx">Figure 7: Gradient Descent with Momentum Illustration, doesn’t get stuck at local minima and reach the global minima using the momentum gained from previous iterations. <em class="kf">Image by author</em></figcaption></figure></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><p id="a602" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jk translated"><span class="l jl jm jn bm jo jp jq jr js di">N</span>T22】埃斯特罗夫加速梯度T24】(由尤里·内斯特罗夫于1983年提出)</p><p id="9855" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它是动量优化的一个小更新，不是计算局部位置的梯度，而是计算动量方向稍微靠前的梯度。这个调整是有效的，因为即使动量优化也指向正确的方向，所以在稍远的点测量梯度是有意义的，反过来它会收敛得更快一些。</p><p id="e758" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">权重使用以下公式更新:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es lm"><img src="../Images/560944aeb1a39a73c2a1cfbf478587cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*gNAH_q3hQWDNqR9ln971rw.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx">Figure 8: Nesterov Accelerated Gradient algorithm. <em class="kf">Image by author</em></figcaption></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es ln"><img src="../Images/e9067d455e7dec5522b96949693c4d55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*ksBU3WtAbsQxoQXLypicQw.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx">Figure 9: Nesterov Accelerated Gradient, converges much faster than Momentum optimization. <em class="kf">Image by author</em></figcaption></figure></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><p id="79cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jk translated"><span class="l jl jm jn bm jo jp jq jr js di">阿</span> <strong class="ih hj">达格拉德</strong> <em class="jg">(约翰·杜奇于2011年提出)</em></p><p id="a958" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们有多个参数/权重并且函数是非凸的而不是凸的时，我们可能在不同的点需要不同的学习速率。</p><p id="1065" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在AdaGrad优化中，学习速率根据梯度进行调整。使用以下等式更新权重:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es lo"><img src="../Images/ae21489a960574660be255011507590b.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*iV-xTSBZ_8kcj8Qq1HLgZQ.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx">Figure 10: AdaGrad algorithm. <em class="kf">Image by author</em></figcaption></figure><p id="ebd2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，学习速率α除以参数s，它累加梯度的平方。这意味着，当斜率较小时，α将较大，反过来，当我们接近最小值且斜率较陡时，将出现较大的台阶。</p><p id="5c10" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们先考虑一个梯度下降的情况。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es lp"><img src="../Images/784c910aab1eb490794940118504d307.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*V5Gqp3flQW_xmBYilcsgPw.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx">Figure 11: GD oscillating around global minima, until it reaches there after a lot of iterations, with a constant learning rate, α. <em class="kf">Image by author</em></figcaption></figure><p id="6e24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，如果使用非常小学习速率来防止其振荡，它仍将花费更多的迭代，且在计算上将是昂贵的。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es lq"><img src="../Images/31487367c8a28499f7d02bd8346c05ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*m_8k3DdllTJu15Rvy5b2Yg.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx">Figure 12: AdaGrad Illustration. Global Minima is reached much faster with the slower learning rate near the minima than at beginning. <em class="kf">Image by author</em></figcaption></figure></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><p id="d19f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jk translated"><span class="l jl jm jn bm jo jp jq jr js di">R</span><strong class="ih hj">ms prop</strong><em class="jg">(Geoffrey hint on&amp;Tijmen tie leman于2012年提出)</em></p><p id="bf26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于在极小值附近收敛较慢，AdaGrad可能会遇到速度降得太快而永远无法收敛到全局最优的风险。RMSProp通过在第一步中使用指数衰减来解决这个问题，并减少斜率对学习速率的影响。</p><p id="9413" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">权重使用以下公式更新:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es lr"><img src="../Images/e2884b3c4164cbb8c5b0ce5e23171ffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*-K1jng7WABS6kzdRCWhvzg.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx">Figure 13: RMSProp algorithm. <em class="kf">Image by author</em></figcaption></figure><p id="1283" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">衰减率β通常设置为0.9，适用于大多数问题。</p></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><p id="09f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jk translated"><span class="l jl jm jn bm jo jp jq jr js di">A</span><strong class="ih hj">dam</strong><em class="jg">(Diederik和Jimmy于2014年提出)</em></p><p id="23b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它代表自适应动量估计，结合了<em class="jg">动量优化</em>和<em class="jg"> RMSProp </em>的思想。</p><p id="c45f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">权重使用以下公式更新:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es ls"><img src="../Images/69cd0cb22b4f727ef10df8ac1cae9b7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*VBYU_CHWU8XPhJBvugSnZQ.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx">Figure 14: Adam algorithm. <em class="kf">Image by author</em></figcaption></figure><p id="a0a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤1、2和5与动量优化和RMSProp非常相似。</p><p id="c16f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">动量衰减参数<em class="jg"> β1 </em>通常初始化为<em class="jg"> 0.9 </em>，而比例衰减参数<strong class="ih hj"> </strong> <em class="jg"> β2 </em>初始化为<em class="jg"> 0.999 </em>。</p><blockquote class="jd je jf"><p id="90a4" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">在实践中，Adam可以很好地解决几乎所有问题，并且通常比RMSProp稍好一些。因此，我们可以在所有的深度学习模型中使用Adam作为默认优化器。但是，在一些数据集中，我们可以尝试使用内斯特罗夫加速梯度作为替代。</p></blockquote><p id="103d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">亚当有2个变种，分别是<strong class="ih hj"> AdaMax </strong>和<strong class="ih hj"> Nadam。</strong></p><p id="a487" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">阿达马克斯</strong> : <strong class="ih hj"> </strong>它将等式的第2步替换为</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es lt"><img src="../Images/1ad6c84c74b861b6637dbdeb7076e35b.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*Onf7toERfYm4t5MYyQ3mkw.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx">Figure 15: change in Step 2 in AdaMax algorithm. <em class="kf">Image by author</em></figcaption></figure><p id="0723" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，它省略了步骤4，并直接按系数<strong class="ih hj"> <em class="jg"> s </em> </strong>降低学习速率。在某些情况下，AdaMax比Adam更稳定，但这取决于数据集。</p><p id="d510" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">那达慕</strong>:是亚当+内斯特罗夫加速梯度，有助于其收敛速度略快于亚当。同样，这取决于手头的数据集。</p></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><p id="b98a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看如何在Keras，tensorflow中实现每个优化器。我们可以拿fashion_mnist作为样本数据集。数据集中有10个类:<em class="jg"> </em> <code class="du lu lv lw lx b"><em class="jg">[‘top’, ‘trouser’, ‘pullover’, ‘dress’, ‘coat’, ‘sandal’, ‘shirt’, ‘sneaker’, ‘bag’, ‘ankle boot’]</em></code></p><pre class="ju jv jw jx fd ly lx lz ma aw mb bi"><span id="9b9c" class="mc md hi lx b fi me mf l mg mh">import tensorflow as tf<br/>from tensorflow import keras<br/>fashion_mnist = keras.datasets.fashion_mnist<br/>(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()<br/>X_valid, X_train = X_train[:5000]/255.0, X_train[5000:]/255.0<br/>Y_valid, Y_train = Y_train[:5000] , Y_train[5000:]</span></pre><p id="0784" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将创建一个简单的顺序模型。</p><pre class="ju jv jw jx fd ly lx lz ma aw mb bi"><span id="2eba" class="mc md hi lx b fi me mf l mg mh">model = keras.models.Sequential()<br/>model.add(keras.layers.Flatten(input_shape=[28,28]))<br/>model.add(keras.layers.Dense(300, activation="relu"))<br/>model.add(keras.layers.Dense(100, activation="relu"))<br/>model.add(keras.layers.Dense(10, activation="softmax"))</span></pre><p id="6800" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">定义SGD(梯度下降)优化器</p><pre class="ju jv jw jx fd ly lx lz ma aw mb bi"><span id="60fa" class="mc md hi lx b fi me mf l mg mh">optimizer = keras.optimizers.SGD(learning_rate=0.001)<br/>model.compile(loss="sparse_categorical_crossentropy", optimizer = optimizer, metrics=["accuracy"])<br/>history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid))</span></pre><p id="cc93" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">类似地，我们可以定义各种其他优化器，并在<em class="jg"> model.compile </em>中传递它们</p><pre class="ju jv jw jx fd ly lx lz ma aw mb bi"><span id="4a27" class="mc md hi lx b fi me mf l mg mh"># Momentum Optimization<br/>optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)</span><span id="0c25" class="mc md hi lx b fi mi mf l mg mh"># Nesterov Accelerated Gradient<br/>optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9,nesterov=True)</span><span id="17d7" class="mc md hi lx b fi mi mf l mg mh"># AdaGrad<br/>optimizer = keras.optimizers.Adagrad(learning_rate=0.001)</span><span id="3139" class="mc md hi lx b fi mi mf l mg mh">#RMSProp<br/>optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)</span><span id="095e" class="mc md hi lx b fi mi mf l mg mh">#Adam<br/>optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)</span><span id="54ab" class="mc md hi lx b fi mi mf l mg mh">#AdaMax<br/>optimizer = keras.optimizers.Adamax(lr=0.001, beta_1=0.9, beta_2=0.999)</span><span id="61b5" class="mc md hi lx b fi mi mf l mg mh">#Nadam<br/>optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)</span></pre><p id="abc9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">完整的代码可以在<a class="ae kx" href="https://github.com/GoyalShreya/keras-tf2-notebooks/blob/main/comparing%20_optimizers8.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">这里找到</strong> </a>。</p><p id="1b69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">各种优化器在给定模型上的性能如下:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mj"><img src="../Images/f1b3c3373e1fd21e47d84f0659af7f87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0y18ptKMTiQ4wchCaWKpPQ.png"/></div></div><figcaption class="kb kc et er es kd ke bd b be z dx">Figure 16: Comparison of different Optimizers on Fashion Mnist Data classification Model. <em class="kf">Image by author</em></figcaption></figure><p id="f48d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从图表中可以得出以下结论:</p><ol class=""><li id="bcfd" class="kh ki hi ih b ii ij im in iq kj iu kk iy kl jc km kn ko kp bi translated">正如我们所看到的，后面的优化技术比<strong class="ih hj"> SGD </strong>收敛得更快，甚至在很少的时期内达到全局最小值。</li><li id="608e" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated"><strong class="ih hj">亚当变体</strong>超越所有，并在更短的时期内达到最小值。</li><li id="6785" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">同样，在这种情况下，<strong class="ih hj">内斯特罗夫</strong>的表现优于<strong class="ih hj"> RMSProp </strong>。</li><li id="855a" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated"><strong class="ih hj"> AdaGrad </strong>开始时达到最小值，之后就没有太多更新了。</li><li id="13ef" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">因此，在大多数情况下，我们可以很容易地使用Adam作为默认的优化器。</li></ol></div></div>    
</body>
</html>
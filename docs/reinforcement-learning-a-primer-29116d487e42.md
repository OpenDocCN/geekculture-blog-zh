# 强化学习——入门

> 原文：<https://medium.com/geekculture/reinforcement-learning-a-primer-29116d487e42?source=collection_archive---------15----------------------->

![](img/41c44ba4be91c334ac1ec3d775bcdca6.png)

**强化学习(RL)** 是机器学习的一个令人兴奋的领域，自 20 世纪 50 年代以来就一直存在。它已经产生了几个有趣的应用，特别是在游戏方面(例如，你可能听说过 DeepMind 的 AlphaGo，这是第一个击败职业人类围棋选手的计算机程序)。AlphaGo 是如何实现这样的壮举的？嗯，通过强化学习，它在学习游戏和做出更好的顺序决策方面变得更好。它必须与自己对弈数千次，并从实际的围棋比赛中学习。这就是 RL 的强大之处。虽然 RL 的应用并不局限于游戏。RL 发现在线推荐系统中的应用，例如帮助客户发现他们可能购买的新产品。其他应用包括机器人、多智能体交互、车辆导航/自动驾驶和工业物流。强化学习使一个“代理”能够使用来自自身经验的反馈(正面/负面)通过试错从其环境中学习——就像人类学习一样。**智能体最终学会做出最优的、经过计算的决策序列。**

**当我们构思 RL 问题时，有几个重要的术语需要知道。**为了更好地理解这些术语，让我们举一个经典棋盘游戏——蛇&梯子的例子。让我们假设在这个游戏中，我们允许我们的婴儿代理人控制掷骰子的数字。

> **:: State (agent 的位置):**这是环境返回的 agent 的当前状态/当前情况。在我们的例子中，这意味着玩家/代理在游戏板上的当前位置。
> 
> **::环境(代理人的世界):**代理人运作的世界(我们的整个游戏棋盘，有蛇、梯子和数字的位置)。注意，马尔可夫决策过程(MDP)正式描述了强化学习的环境。马尔可夫性质基本上是说“给定现在，未来独立于过去”。
> 
> **::奖励(来自环境的回报):**标量反馈信号，告诉代理在特定时间做得有多好。代理人的目标是最大化累积报酬。例如，代理人可以在较短的梯子上获得+5 点，在较大的梯子上获得+10 点，在较短的蛇上获得 5 点，在较长的蛇上获得 10 点。
> 
> **::策略(代理的策略):**这是从状态到动作的映射。它决定代理如何选择动作(骰子上的数字)。
> 
> **:::价值函数(有一定折扣的长期回报):**特定政策下未来回报的预期贴现总和。这是一个状态好/坏的指示器。

简而言之，在每个时间步，代理执行一个动作，接收一个观察和一个标量奖励。随着时间的推移，代理人将选择行动来最大化未来的总回报。请注意，行动可能会产生长期后果，放弃短期回报以获得更多长期回报可能更好(比如金融投资)。**那么，是什么让强化学习不同于监督学习等其他机器学习类型呢？**答案是，在 RL 中没有真正的主管。当监督学习工作在现有的或给定的数据样本上时，RL 工作在与环境的交互上。RL 是关于顺序做出好的决策，而在监督机器学习中，输出决策是根据初始输入做出的。此外，RL 代理具有可能被延迟并且不是瞬时的奖励/反馈信号(不像受监督的机器学习)。

让我们来看看强化学习的一些**利弊。从好的方面来说，RL 可以用来解决一些复杂的问题，这些问题是其他机器学习方法无法轻松解决的。学习模型就像人类学习的方式，因此它接近于实现良好的长期结果，并且更具可扩展性。此外，在缺乏训练数据集的情况下，RL 可以是从代理在环境中导航的经验中学习的强大技术。相反，正如我们所知，RL 需要大量数据，并且计算量很大/很昂贵。如果我们在训练模型的资源上受到限制，那么使用 RL 来完成任务可能不是一种实用的方法。那么，**我们什么时候应该考虑应用强化学习(什么时候不应该)？**目前 RL 还不能解决所有问题。在我们不能犯错误的情况下，我们应该避免使用 RL。例如，我们应该非常小心地使用强化学习在医院环境中对病人进行操作。当所有的环境变量没有被量化或绘制出来时，RL 也可能难以使用。部分信息可能导致不准确和次优的结果。时间是另一个限制。如果大部分学习是在线进行的，那么必须进行多次试验才能产生有效的政策。另一方面，在我们有能力犯错误、有时间和资源进行培训，并可能从探索/利用环境中受益以最大化累积折扣奖励的情况下，RL 可能是一个很好的应用。**

**我们经常谈到“离线”学习和“在线”强化学习设置。有什么区别？**简单来说，在线强化学习是一种技术，其中算法一次一个观察地吸收数据。在某些情况下，由于数据收集极其昂贵和耗时，在线学习可能不切实际。RL 中的大多数研究/实施都围绕在线学习设置。另一方面，离线或批量学习一次摄取静态数据来建立机器学习模型。当我们无法承担在线学习的风险时，离线强化学习通常是唯一的选择。在离线 RL 中，不需要与环境进一步交互，算法利用先前收集的数据。**离线强化学习算法可以将大型数据集转化为强大的决策引擎。让我们举一个来自现实世界的离线 RL 的例子来更好地理解这一点。**假设我们在医院环境中诊断一名患者。这里，动作被映射到某些诊断测试，而观察对应于诊断测试的结果。在这种情况下，我们可以使用真实患者的诊断测试的历史数据和医疗保健提供者给出的最佳建议，以便确定我们的离线强化学习问题的范围。很漂亮吧？请注意，离线 RL 面临的最大挑战之一是，我们无法通过探索环境来提高回报，因为我们使用的是静态数据。

总的来说，强化学习框架的吸引力正在上升。**在医疗保健、推荐引擎、自主系统、机器人等垂直领域应用可靠、高效的强化方法。为工业和研究创造了巨大的机会。如果你想继续学习更多关于 RL 的知识，我强烈推荐 David Silver 的关于 RL 的课程。**

希望这篇文章对你有所帮助。快乐学习！
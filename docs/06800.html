<html>
<head>
<title>Cross-Validation Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">交叉验证技术</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/cross-validation-techniques-33d389897878?source=collection_archive---------4-----------------------#2021-08-30">https://medium.com/geekculture/cross-validation-techniques-33d389897878?source=collection_archive---------4-----------------------#2021-08-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="6146" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">本文旨在解释不同的交叉验证技术以及它们是如何工作的。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/a0f995a3ed9ef478cfebb70647a6c3be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gBroDVHOfwcIYDy1i3EmlQ.png"/></div></div></figure><h1 id="2419" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">内容:</h1><p id="3357" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated"><strong class="kd hj"> →简介<br/> →什么是交叉验证？<br/> →不同类型的交叉验证</strong> <br/> <em class="kx"> 1。保持方法<br/> 2。k折法<br/> 3。重复K折法<br/> 4。分层K折法<br/> 5。组K折法<br/> 6。洗牌拆分法<br/> 7。分层洗牌拆分法<br/> 8。分组洗牌拆分方法<br/> 9。留一法<br/> 10。排除法<br/> 11。留一组法<br/> 12。离开P组方法<br/> 13。时间序列交叉验证方法<br/> 14。受阻交叉验证方法<br/> 15。嵌套交叉验证法</em> <strong class="kd hj"> <br/> →结论<br/> →参考</strong></p><h1 id="8d37" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">介绍</h1><p id="e5c7" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">想象一下，在数据集上建立一个模型，但在看不见的数据上却失败了。我们不能只是让模型适合我们的训练数据，然后就指望它能在真实的未知数据中表现出色。<br/>这是一个<strong class="kd hj"> <em class="kx">过拟合</em> </strong>的情况，其中我们的模型已经学习了训练数据的所有模式和噪声，为了避免这种情况，我们需要某种方式来保证我们的模型已经捕获了大多数模式，并且没有拾取数据中的每个噪声(低偏差和低方差)，处理这种情况的许多技术之一是<strong class="kd hj">交叉验证</strong>。</p><h1 id="0fbb" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">什么是交叉验证？</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ky"><img src="../Images/e0a4a810e7e49313e9231d580e429831.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/0*DeV6isRiANaUE5q_.png"/></div></figure><ul class=""><li id="67e8" class="kz la hi kd b ke lb kh lc kk ld ko le ks lf kw lg lh li lj bi translated">在机器学习中，交叉验证是一种通过在输入数据的子集上训练几个ML模型并在数据的互补子集上评估它们来评估任何ML模型的技术。</li><li id="fd39" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">它主要用于估计适合数据和模型的任何定量度量。</li><li id="8c56" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">在交叉验证方法中，测试结果通常不会有偏差，因为用于训练和测试的数据大多是不重叠的。</li></ul><p id="fa11" class="pw-post-body-paragraph kb kc hi kd b ke lb ij kg kh lc im kj kk lp km kn ko lq kq kr ks lr ku kv kw hb bi translated"><strong class="kd hj">让我们首先创建两个变量，我将用它们来进一步演示:</strong></p><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="5f2f" class="lx jk hi lt b fi ly lz l ma mb">data = ['Subset1', 'Subset2', 'Subset3', 'Subset4', 'Subset5', 'Subset6', 'Subset7', 'Subset8', 'Subset9', 'Subset10']</span><span id="f46e" class="lx jk hi lt b fi mc lz l ma mb">Y = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]</span><span id="cb80" class="lx jk hi lt b fi mc lz l ma mb">df = {"data":data, "Y":Y}</span><span id="6667" class="lx jk hi lt b fi mc lz l ma mb">df = pd.DataFrame(df)</span><span id="fbe0" class="lx jk hi lt b fi mc lz l ma mb">df</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es md"><img src="../Images/3a64e0945b409040351cf1ed8e1892ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*tDS_Rs83KpsfyB-rD_Gx7g.png"/></div></figure><h1 id="4812" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">交叉验证的不同方法有:</h1><h2 id="b616" class="lx jk hi bd jl me mf mg jp mh mi mj jt kk mk ml jv ko mm mn jx ks mo mp jz mq bi translated">→保持方法:</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mr"><img src="../Images/550f00e17ad1a133735bcd495e4a003c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/0*T2FFdVvVPf1q_rpM.jpg"/></div></figure><ul class=""><li id="f840" class="kz la hi kd b ke lb kh lc kk ld ko le ks lf kw lg lh li lj bi translated">这是一种简单的训练测试分割方法。</li><li id="0c89" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">一旦训练测试分割完成，我们可以进一步将测试数据分割成验证数据和测试数据。<br/>例如:<br/> 1。假设有1000个数据，我们将数据分成80%的训练和20%的测试。<br/> 2。我的训练数据由800个数据点组成，测试将包含200个数据点。<br/> 3。然后我们将测试数据分成50%的验证数据和50%的测试数据。</li></ul><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="f8d1" class="lx jk hi lt b fi ly lz l ma mb">x_train, x_test, y_train, y_test = model_selection.train_test_split(df.data, df.Y, test_size = 0.2)</span><span id="5202" class="lx jk hi lt b fi mc lz l ma mb">for i,n in zip(x_train, y_train):<br/>  print(i, "::", n)<br/>for i, n in zip(x_test, y_test):<br/>  print(i, "::", n)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ms"><img src="../Images/51fd4d1616296bfe93692621e892bb27.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*TJw8hIIiQq8Wvud77WoReQ.png"/></div><figcaption class="mt mu et er es mv mw bd b be z dx">Train Set</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mx"><img src="../Images/91f2982d9f3b4cffd7b338a158d4d0b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*aRxxyggfFJTt3zXFiKSHvw.png"/></div><figcaption class="mt mu et er es mv mw bd b be z dx">Test Set</figcaption></figure><blockquote class="my"><p id="65c8" class="mz na hi bd nb nc nd ne nf ng nh kw dx translated">* *注意:测试集只有一个类，这会导致有偏差的结果</p></blockquote><p id="a880" class="pw-post-body-paragraph kb kc hi kd b ke ni ij kg kh nj im kj kk nk km kn ko nl kq kr ks nm ku kv kw hb bi translated"><strong class="kd hj">使用分层参数</strong></p><blockquote class="nn no np"><p id="79ed" class="kb kc kx kd b ke lb ij kg kh lc im kj nq lp km kn nr lq kq kr ns lr ku kv kw hb bi translated"><strong class="kd hj">分层是重新排列数据的过程，以确保每个折叠都是整体的良好代表。<br/> </strong>例如，<strong class="kd hj">在每个类包含50%数据的二元分类问题中，最好安排数据，使得在每个文件夹中，每个类包含大约一半的实例。</strong></p></blockquote><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="60b6" class="lx jk hi lt b fi ly lz l ma mb">x_train, x_test, y_train, y_test = model_selection.train_test_split(df.data, df.Y, test_size = 0.2, stratify = df.Y)</span><span id="c461" class="lx jk hi lt b fi mc lz l ma mb">for i,n in zip(x_train, y_train):<br/>  print(i, "::", n)<br/>for i, n in zip(x_test, y_test):<br/>  print(i, "::", n)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nt"><img src="../Images/c378be6177399417fc0a99a463cab40f.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*KiIA8xifsQoAqN_VWXQu8Q.png"/></div><figcaption class="mt mu et er es mv mw bd b be z dx">Train Set</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nu"><img src="../Images/b9663d81fb9cc2a18964138342999de3.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/format:webp/1*NF_RZ0VsjxA_kAh7WvmKrQ.png"/></div><figcaption class="mt mu et er es mv mw bd b be z dx">Test Set</figcaption></figure><h2 id="6c83" class="lx jk hi bd jl me mf mg jp mh mi mj jt kk mk ml jv ko mm mn jx ks mo mp jz mq bi translated">→ K折叠法:</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nv"><img src="../Images/823608debf7f0c760e27cb055fffeaba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PO5MpOVRb9RUqMIY.png"/></div></div></figure><ul class=""><li id="888d" class="kz la hi kd b ke lb kh lc kk ld ko le ks lf kw lg lh li lj bi translated">在这种方法中，我们将数据集分成k个子集(称为折叠)，然后对所有子集进行训练，但留下一个(k-1)子集用于评估训练的模型。</li><li id="6fd4" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">我们迭代k次，每次为测试目的保留不同的子集。</li><li id="d534" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">它确保原始数据集中的每个观察值都有机会出现在训练和测试集中。</li><li id="fae1" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">来自折叠的k个结果然后可以被平均(或以其他方式组合)以产生单个估计。这种方法的优点是，所有的观察值都用于训练和验证，并且每个观察值只用于验证一次。</li></ul><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="d54c" class="lx jk hi lt b fi ly lz l ma mb">kfold = model_selection.KFold(n_splits=5)</span><span id="6e9f" class="lx jk hi lt b fi mc lz l ma mb">print("Train", "||", "Test")<br/>for train, test in kfold.split(df.data, df.Y):<br/>  print(train, "||", test)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nw"><img src="../Images/0700cc7cf83f0d4f585965c7bbf81694.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*2beJ0bYu9oobnIvF30rsrQ.png"/></div></figure><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="73c3" class="lx jk hi lt b fi ly lz l ma mb">tn = []<br/>tt = []<br/>for train, test in kfold.split(data):<br/>  tn.append(np.take(data,train))<br/>  tt.append(np.take(data,test))</span><span id="ec6c" class="lx jk hi lt b fi mc lz l ma mb">kfold_df = pd.DataFrame({"train":tn, "test":tt})<br/>kfold_df</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nx"><img src="../Images/12264445770dd53c13bd22624a25bdb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5NWWHNdhSt17OVk4QVV3rQ.png"/></div></div></figure><p id="6b8a" class="pw-post-body-paragraph kb kc hi kd b ke lb ij kg kh lc im kj kk lp km kn ko lq kq kr ks lr ku kv kw hb bi translated">现在，如果我们传入<code class="du ny nz oa lt b">shuffle</code>参数:</p><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="5059" class="lx jk hi lt b fi ly lz l ma mb">kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=1)<br/>print("Train", "||", "Test")<br/>for train, test in kfold.split(data):<br/>  print(train, "||", test)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ob"><img src="../Images/6a71b6076d6ff7b0a44a4688d97f261c.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*2JCxJffX3gghpvMJIK7e8Q.png"/></div></figure><p id="2fde" class="pw-post-body-paragraph kb kc hi kd b ke lb ij kg kh lc im kj kk lp km kn ko lq kq kr ks lr ku kv kw hb bi translated">我们可以看到，拆分不再是有序的，这是因为数据被打乱，然后被分开。</p><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="f63d" class="lx jk hi lt b fi ly lz l ma mb">tn = []<br/>tt = []<br/>for train, test in kfold.split(data):<br/>  tn.append(np.take(data,train))<br/>  tt.append(np.take(data,test))</span><span id="ef7c" class="lx jk hi lt b fi mc lz l ma mb">kfold_df = pd.DataFrame({"train":tn, "test":tt})<br/>kfold_df</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es oc"><img src="../Images/c6d4dec65b9a098fab636333e6f48572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sUaGyQiy7kilz-TgVZ5GWQ.png"/></div></div></figure><h2 id="6919" class="lx jk hi bd jl me mf mg jp mh mi mj jt kk mk ml jv ko mm mn jx ks mo mp jz mq bi translated">→重复K折法:</h2><ul class=""><li id="e885" class="kz la hi kd b ke kf kh ki kk od ko oe ks of kw lg lh li lj bi translated">重复K-fold方法使用K-fold交叉验证，并根据用户的需要重复n次。</li><li id="8514" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">单次运行k-fold交叉验证程序可能会导致模型性能的高噪声估计。不同的数据分割可能会产生完全不同的结果。</li><li id="f132" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">对模型性能的有噪声的估计导致混乱，不知道应该使用哪个模型来比较和选择最终模型来解决问题。</li><li id="b3c6" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">降低估计模型性能中的噪声的一个解决方案是增加k值。但是，它会增加方差。</li><li id="c113" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">多次重复k折叠交叉验证过程，并报告所有折叠和所有重复的平均性能。</li></ul><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="344c" class="lx jk hi lt b fi ly lz l ma mb">Rkfold = model_selection.RepeatedKFold(n_splits=5, n_repeats=5, random_state=2)</span><span id="d183" class="lx jk hi lt b fi mc lz l ma mb">print("Train", "||", "Test")<br/>for train, test in Rkfold.split(data):<br/>  print(train, "||", test)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es og"><img src="../Images/a6868455a4118106fa4524bd3205f8c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*b9TLcB_IORhPtGOk_19yDg.png"/></div></figure><p id="d744" class="pw-post-body-paragraph kb kc hi kd b ke lb ij kg kh lc im kj kk lp km kn ko lq kq kr ks lr ku kv kw hb bi translated">我们可以看到，数据被分成12组(5 n_splits * 5重复)，没有两个测试集是重复的</p><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="8f9d" class="lx jk hi lt b fi ly lz l ma mb">tn = []<br/>tt = []<br/>for train, test in Rkfold.split(data):<br/>  tn.append(np.take(data,train))<br/>  tt.append(np.take(data,test))</span><span id="1442" class="lx jk hi lt b fi mc lz l ma mb">Rkfold_df = pd.DataFrame({"train":tn, "test":tt})<br/>Rkfold_df</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es et"><img src="../Images/66e9cee9b84073143fc7c1500352be74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gz456U0TV2oiED0NSif5lA.png"/></div></div></figure><h2 id="157d" class="lx jk hi bd jl me mf mg jp mh mi mj jt kk mk ml jv ko mm mn jx ks mo mp jz mq bi translated">→分层K折法:</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es oh"><img src="../Images/12c0caf519e18d61db6352a9db242983.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D9r0Xi2JotlOpO0Ozz5euw.png"/></div></div></figure><ul class=""><li id="d2dd" class="kz la hi kd b ke lb kh lc kk ld ko le ks lf kw lg lh li lj bi translated">K-Fold交叉验证和分层K-Fold之间的区别在于，K-Fold将数据分成K个“随机”折叠，这意味着子集由随机选取和放置的数据点组成。<br/>然而，在分层交叉验证中，将数据分成k个折叠，确保每个折叠都是原始数据的适当代表。(类别分布、均值、方差等)。</li><li id="6903" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">任何分类问题的最大问题是由于不平衡的分类而产生的问题。如果我们对不平衡数据使用K-Fold CV方法，我们可能会导致训练偏向某一类。在K-Fold中，我们随机取出K个子集，并且我们很有可能得到由多数类组成的折叠。为了处理这种类型的问题，<strong class="kd hj"> <em class="kx">分层K-Fold在分层过程的帮助下被使用</em> </strong>。</li></ul><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="6a5e" class="lx jk hi lt b fi ly lz l ma mb">strkfold = model_selection.StratifiedKFold(n_splits=5)</span><span id="273f" class="lx jk hi lt b fi mc lz l ma mb">tn_x = []<br/>tn_y = []<br/>tt_x = []<br/>tt_y = []<br/>for train, test in strkfold.split(data, Y):<br/>  tn_x.append(np.take(data,train))<br/>  tn_y.append(np.take(Y,train))<br/>  tt_x.append(np.take(data,test))<br/>  tt_y.append(np.take(Y,test))</span><span id="1612" class="lx jk hi lt b fi mc lz l ma mb">strkfold_train = pd.DataFrame({"train_x":tn_x, "train_y":tn_y})<br/>strkfold_test = pd.DataFrame({'test_x':tt_x, "test_y":tt_y})</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es oi"><img src="../Images/380e04c265c5ba1e7589cbff0dd5c471.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nfGZFHtSOAWNJqFlbc-Phg.png"/></div></div><figcaption class="mt mu et er es mv mw bd b be z dx">Train Set</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es oj"><img src="../Images/b84323c7ef0138077804e5631d783854.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*HTIYfrn0iquxTVWI7gjVIQ.png"/></div><figcaption class="mt mu et er es mv mw bd b be z dx">Test Set</figcaption></figure><h2 id="1743" class="lx jk hi bd jl me mf mg jp mh mi mj jt kk mk ml jv ko mm mn jx ks mo mp jz mq bi translated">→K倍分组法:</h2><ul class=""><li id="239b" class="kz la hi kd b ke kf kh ki kk od ko oe ks of kw lg lh li lj bi translated">K-Folds组是一种考虑到作为参数传递的组的方法。</li><li id="fef7" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">每隔一个K倍和分组K倍之间的区别在于当数据被分成两组时。它为训练集取出单独的组。</li></ul><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="604f" class="lx jk hi lt b fi ly lz l ma mb">grpkfold = model_selection.GroupKFold(n_splits=5)<br/>print("Train", "||", "Test")<br/>for train, test in grpkfold.split(data, Y, groups=groups):<br/>  print(train, "||", test)</span></pre><div class="iy iz ja jb fd ab cb"><figure class="ok jc ol om on oo op paragraph-image"><img src="../Images/cf9d20ec2feae5e0f79a762e9778479e.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*X2q26fDQixRAfF2MJi3h8Q.png"/></figure><figure class="ok jc oq om on oo op paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/fc0515128f34012b35a0d15196385e6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*tDS_Rs83KpsfyB-rD_Gx7g.png"/></div></figure></div><blockquote class="my"><p id="8ba1" class="mz na hi bd nb nc nd ne nf ng nh kw dx translated">→比较上面两幅图像，我们可以看到位于索引8和9的组首先被取出。<br/> →之后是索引6和7，依此类推。</p></blockquote><p id="4393" class="pw-post-body-paragraph kb kc hi kd b ke ni ij kg kh nj im kj kk nk km kn ko nl kq kr ks nm ku kv kw hb bi translated">这种分割是以单个组为基础的</p><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="f826" class="lx jk hi lt b fi ly lz l ma mb">tn = []<br/>tt = []<br/>for train, test in grpkfold.split(data, Y, groups=groups):<br/>  tn.append(np.take(data,train))<br/>  tt.append(np.take(data,test))</span><span id="bd80" class="lx jk hi lt b fi mc lz l ma mb">grpkfold_df = pd.DataFrame({"train":tn, "test":tt})<br/>grpkfold_df</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es or"><img src="../Images/b7087eae789ce81198432ba05648ed0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6NOcDiGlc_I7kOfbJ2-NUw.png"/></div></div></figure><h2 id="8e2a" class="lx jk hi bd jl me mf mg jp mh mi mj jt kk mk ml jv ko mm mn jx ks mo mp jz mq bi translated">→洗牌拆分法:</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es os"><img src="../Images/23d3dacbcbfc7c8593a4c0604df0293b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HJFgXguXtFWKq0-Y.png"/></div></div></figure><ul class=""><li id="bda9" class="kz la hi kd b ke lb kh lc kk ld ko le ks lf kw lg lh li lj bi translated"><strong class="kd hj">重复随机子采样验证</strong>也称为<strong class="kd hj"> <em class="kx">蒙特卡洛交叉验证</em> </strong>将数据集随机分为训练和验证。不太可能将数据集k倍交叉验证拆分为非分组或折叠，但在这种情况下是随机拆分。</li><li id="8618" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">迭代的次数不是固定的，而是由分析决定的。然后对分割的结果进行平均。</li><li id="c48f" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">当您欠采样或者当您有上述情况时，您不希望每个观察值出现在k-1个折叠中，随机子采样(例如，bootstrap采样)是更可取的。</li><li id="458c" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">训练和验证分割的比例不依赖于迭代或分区的数量。</li><li id="5b59" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">有些样本可能不会被选择用于训练或验证。</li><li id="da07" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">不适合不平衡的数据集。</li></ul><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="c073" class="lx jk hi lt b fi ly lz l ma mb">shsplit = model_selection.ShuffleSplit(n_splits=5, test_size=0.2, random_state=3)</span><span id="d685" class="lx jk hi lt b fi mc lz l ma mb">print("Train", "||", "Test")<br/>for train, test in shsplit.split(data):<br/>  print(train, "||", test)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ot"><img src="../Images/4044cf35b102dbcb2df18bca56c7ddf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*yQ1kdHjdSB3hRsUlJGqlOg.png"/></div></figure><p id="c59f" class="pw-post-body-paragraph kb kc hi kd b ke lb ij kg kh lc im kj kk lp km kn ko lq kq kr ks lr ku kv kw hb bi translated">随机子集被提取和分离</p><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="95ae" class="lx jk hi lt b fi ly lz l ma mb">tn = []<br/>tt = []<br/>for train, test in shsplit.split(data):<br/>  tn.append(np.take(data,train))<br/>  tt.append(np.take(data,test))</span><span id="9584" class="lx jk hi lt b fi mc lz l ma mb">shsplit_df = pd.DataFrame({"train":tn, "test":tt})<br/>shsplit_df</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ou"><img src="../Images/b5dddafb006bdc2a3bc6603a71278ca3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v9jK4Oxj7qidPws9ensnNA.png"/></div></div></figure><h2 id="fe9b" class="lx jk hi bd jl me mf mg jp mh mi mj jt kk mk ml jv ko mm mn jx ks mo mp jz mq bi translated">→分层洗牌拆分法:</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ov"><img src="../Images/f74a5c45d52ad53f8247234c37f7d6dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/0*C8BFfAFLaXJXbjwT.png"/></div></figure><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="d9ca" class="lx jk hi lt b fi ly lz l ma mb">strshsplit = model_selection.StratifiedShuffleSplit(n_splits=5, test_size=0.2)<br/>print("Train", "||", "Test")<br/>for train, test in strshsplit.split(data, Y):<br/>  print(train, "||", test)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ow"><img src="../Images/e7e92953db82ce528ec6c6b0db4961a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*A6BH0F6kg_7ZAVvkaA38dw.png"/></div></figure><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="6728" class="lx jk hi lt b fi ly lz l ma mb">tn_x = []<br/>tn_y = []<br/>tt_x = []<br/>tt_y = []<br/>for train, test in strshsplit.split(data, Y):<br/>  tn_x.append(np.take(data,train))<br/>  tn_y.append(np.take(Y,train))<br/>  tt_x.append(np.take(data,test))<br/>  tt_y.append(np.take(Y,test))</span><span id="4be4" class="lx jk hi lt b fi mc lz l ma mb">strshsplit_train = pd.DataFrame({"train_x":tn_x, "train_y":tn_y})<br/>strshsplit_test = pd.DataFrame({'test_x':tt_x, "test_y":tt_y})</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ox"><img src="../Images/a9d61903461d67465bf63bc2444e1289.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3dISI0lJY1N2NHiIZjZcfA.png"/></div></div><figcaption class="mt mu et er es mv mw bd b be z dx">Train Set</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es oy"><img src="../Images/0706659c06b06566228b3de59aa3cf03.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*WUPn0qxS2S0-0KWK4LgHaA.png"/></div><figcaption class="mt mu et er es mv mw bd b be z dx">Test Set</figcaption></figure><h2 id="c80a" class="lx jk hi bd jl me mf mg jp mh mi mj jt kk mk ml jv ko mm mn jx ks mo mp jz mq bi translated">→分组洗牌拆分法:</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es oz"><img src="../Images/051dc0f3126f2499d4bcb9408a0cb684.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*gWUAJ45fyETmwgl4.png"/></div></figure><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="29d0" class="lx jk hi lt b fi ly lz l ma mb">grpshsplit = model_selection.GroupShuffleSplit(n_splits=5)<br/>print("Train", "||", "Test")<br/>for train, test in grpshsplit.split(data, groups=groups):<br/>  print(train, "||", test)</span></pre><div class="iy iz ja jb fd ab cb"><figure class="ok jc pa om on oo op paragraph-image"><img src="../Images/92df32ab0b2900fd16e42f7f6f44e3f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*fBE9Ou3cAQP381URkY_oGg.png"/></figure><figure class="ok jc pb om on oo op paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/558b0ccefc3bbbc1cb785129f95f2fae.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*tDS_Rs83KpsfyB-rD_Gx7g.png"/></div></figure></div><blockquote class="my"><p id="ddcd" class="mz na hi bd nb nc nd ne nf ng nh kw dx translated">→分组混洗分割倾向于重复相同的分割，因为分割是在混洗数据后选择的。<br/> →这种重复行为可能会使模型一般化。</p></blockquote><p id="3fb5" class="pw-post-body-paragraph kb kc hi kd b ke ni ij kg kh nj im kj kk nk km kn ko nl kq kr ks nm ku kv kw hb bi translated">分组混洗分割产生重复的验证集</p><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="df7a" class="lx jk hi lt b fi ly lz l ma mb">tn = []<br/>tt = []<br/>for train, test in grpshsplit.split(data, groups=groups):<br/>  tn.append(np.take(data,train))<br/>  tt.append(np.take(data,test))</span><span id="45cc" class="lx jk hi lt b fi mc lz l ma mb">grpshsplit_df = pd.DataFrame({"train":tn, "test":tt})<br/>grpshsplit_df</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es pc"><img src="../Images/4835cf234616c0f9bdfd1e4b97ea1309.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S4DwYh0jr80_O1QKOA60bg.png"/></div></div></figure><h2 id="3c27" class="lx jk hi bd jl me mf mg jp mh mi mj jt kk mk ml jv ko mm mn jx ks mo mp jz mq bi translated">→留一法:</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es pd"><img src="../Images/bc7cb19f0e10d0842e2cb443d8fdca16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Hrgw_8h6H79hxisg.png"/></div></div></figure><ul class=""><li id="7025" class="kz la hi kd b ke lb kh lc kk ld ko le ks lf kw lg lh li lj bi translated"><strong class="kd hj">留一交叉验证</strong>或<strong class="kd hj"> LOOCV </strong>，当机器学习算法用于对未用于训练模型的数据进行预测时，该程序用于评估机器学习算法的性能。</li><li id="9a44" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">这种方法在训练数据中留出1个数据点，即，如果原始样本中有n个数据点，则n-1个样本用于训练模型，p个点用作验证集。</li><li id="33ff" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">对原始样本可以这样分离的所有组合重复这一过程，然后对所有试验的误差进行平均，以给出总体有效性。</li></ul><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="d170" class="lx jk hi lt b fi ly lz l ma mb">loo = model_selection.LeaveOneOut()</span><span id="aed7" class="lx jk hi lt b fi mc lz l ma mb">print("Train", "||", "Test")<br/>for train, test in loo.split(data):<br/>  print(train, "||", test)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es oy"><img src="../Images/cfd806477e4630844e358bfa7c782c12.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*iyKmfQ7PAW1u70OVAtgsZQ.png"/></div></figure><p id="8843" class="pw-post-body-paragraph kb kc hi kd b ke lb ij kg kh lc im kj kk lp km kn ko lq kq kr ks lr ku kv kw hb bi translated">每次分离都将一个特定的子集作为验证集，因此，如果将数据分成n个子集，将会进行n次拆分。</p><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="b0dd" class="lx jk hi lt b fi ly lz l ma mb">tn = []<br/>tt = []<br/>for train, test in loo.split(data):<br/>  tn.append(np.take(data,train))<br/>  tt.append(np.take(data,test))</span><span id="86a6" class="lx jk hi lt b fi mc lz l ma mb">loo_df = pd.DataFrame({"train":tn, "test":tt})<br/>loo_df</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es pe"><img src="../Images/c6242709a631ed0afa0da9a96697e8b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nUuU4ny3LdEwbbJNyYo8ww.png"/></div></div></figure><h2 id="89fb" class="lx jk hi bd jl me mf mg jp mh mi mj jt kk mk ml jv ko mm mn jx ks mo mp jz mq bi translated">→留P法:</h2><ul class=""><li id="abd2" class="kz la hi kd b ke kf kh ki kk od ko oe ks of kw lg lh li lj bi translated">这与<strong class="kd hj"> <em class="kx">留一法</em> </strong>类似，但是，在这种方法中，我们从包含n个数据点的数据集中的数据点总数中取出p个数据点。</li><li id="a67b" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">该模型在(n-p)个数据点上训练，并在p个数据点上测试。</li></ul><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="ffad" class="lx jk hi lt b fi ly lz l ma mb">lpo = model_selection.LeavePOut(2)</span><span id="603a" class="lx jk hi lt b fi mc lz l ma mb">tn = []<br/>tt = []<br/>for train, test in lpo.split(data):<br/>  tn.append(np.take(data,train))<br/>  tt.append(np.take(data,test))</span><span id="4dc8" class="lx jk hi lt b fi mc lz l ma mb">lpo_df = pd.DataFrame({"train":tn, "test":tt})<br/>lpo_df</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es pf"><img src="../Images/91ad949d2cbace65102a6eb7dd44d5b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tm_UGI0_cTuQFrHMYeZOsw.png"/></div></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es pg"><img src="../Images/7ba2e28aa8b05fc68c449924938fc510.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TRuQJLSJXNyLrMN1qXlXgg.png"/></div></div></figure><h2 id="07c8" class="lx jk hi bd jl me mf mg jp mh mi mj jt kk mk ml jv ko mm mn jx ks mo mp jz mq bi translated">→留一组法:</h2><p id="d8ca" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">这基本上类似于K-folds组方法，除了K-Folds的分割顺序从末尾开始，而Leave-One-Group从开头开始。</p><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="22eb" class="lx jk hi lt b fi ly lz l ma mb">logo = model_selection.LeaveOneGroupOut()<br/>print("Train", "||", "Test")<br/>for train, test in logo.split(data, groups=groups):<br/>  print(train, "||", test)</span></pre><div class="iy iz ja jb fd ab cb"><figure class="ok jc pa om on oo op paragraph-image"><img src="../Images/5876a91ce62f049c51db8960430e68fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*b6xmcKRcAB_d512bcuOIAg.png"/></figure><figure class="ok jc pb om on oo op paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/558b0ccefc3bbbc1cb785129f95f2fae.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*tDS_Rs83KpsfyB-rD_Gx7g.png"/></div></figure></div><h2 id="4a7e" class="lx jk hi bd jl me mf mg jp mh mi mj jt kk mk ml jv ko mm mn jx ks mo mp jz mq bi translated">→离开P-组-排除法:</h2><ul class=""><li id="fd8c" class="kz la hi kd b ke kf kh ki kk od ko oe ks of kw lg lh li lj bi translated">它将p组排除在外，在n-p个子集上训练模型，然后在p组上进行测试。</li></ul><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="6afb" class="lx jk hi lt b fi ly lz l ma mb">lpgo = model_selection.LeavePGroupsOut(2)</span><span id="9b6c" class="lx jk hi lt b fi mc lz l ma mb">print("Train", "||", "Test")<br/>for train, test in lpgo.split(data, groups=groups):<br/>  print(train, "||", test)</span></pre><div class="iy iz ja jb fd ab cb"><figure class="ok jc ph om on oo op paragraph-image"><img src="../Images/4c37f909fd03cdf9690ae26d8819717a.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*sF8i172ABiHV8OjuQtGATw.png"/></figure><figure class="ok jc pi om on oo op paragraph-image"><img src="../Images/3a64e0945b409040351cf1ed8e1892ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*tDS_Rs83KpsfyB-rD_Gx7g.png"/></figure></div><h2 id="c18f" class="lx jk hi bd jl me mf mg jp mh mi mj jt kk mk ml jv ko mm mn jx ks mo mp jz mq bi translated">→时间序列交叉验证</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es pj"><img src="../Images/d1b7217997e1b783135772f24a7ca385.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/0*tTm6Mciv0iNy1aBO.png"/></div></figure><ul class=""><li id="e60c" class="kz la hi kd b ke lb kh lc kk ld ko le ks lf kw lg lh li lj bi translated">数据的顺序对于时间序列相关的问题非常重要。对于与时间相关的数据集，将数据随机分割或k倍分割成训练和验证可能不会产生好的结果。</li><li id="0780" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">对于时间序列数据集，将数据拆分成训练序列并根据时间进行验证，也称为<strong class="kd hj">前向链接法</strong>或<strong class="kd hj">滚动交叉验证</strong>。对于特定的迭代，训练数据的下一个实例可以被视为验证数据。</li></ul><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="b629" class="lx jk hi lt b fi ly lz l ma mb">tsplit = model_selection.TimeSeriesSplit(n_splits=9, max_train_size=10)</span><span id="10fc" class="lx jk hi lt b fi mc lz l ma mb">print("Train", "||", "Test")<br/>for train, test in tsplit.split(data):<br/>  print(train, "||", test)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es pk"><img src="../Images/efe19cc9d8aa55c9e754bc16e55bcc65.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*J6V6quW2pTJbQWXVK41MMg.png"/></div></figure><p id="388b" class="pw-post-body-paragraph kb kc hi kd b ke lb ij kg kh lc im kj kk lp km kn ko lq kq kr ks lr ku kv kw hb bi translated">该模型是在连续子集上训练的，而不是在随机选择的子集上训练的。</p><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="a0e6" class="lx jk hi lt b fi ly lz l ma mb">tn = []<br/>tt = []<br/>for train, test in tsplit.split(data):<br/>  tn.append(np.take(data,train))<br/>  tt.append(np.take(data,test))</span><span id="7d07" class="lx jk hi lt b fi mc lz l ma mb">tsplit_df = pd.DataFrame({"train":tn, "test":tt})<br/>tsplit_df</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es pl"><img src="../Images/953e2150b08845ae08f1c9b482a0df59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u738XqE7ytgNCcgpsCiNNQ.png"/></div></div></figure><h2 id="c5a7" class="lx jk hi bd jl me mf mg jp mh mi mj jt kk mk ml jv ko mm mn jx ks mo mp jz mq bi translated">→交叉验证受阻</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es pm"><img src="../Images/31eb73c6fd67f488f5aca8e7b1168af3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OqesMfSfesElm8DXUcAZwQ.png"/></div></div></figure><ul class=""><li id="b6f4" class="kz la hi kd b ke lb kh lc kk ld ko le ks lf kw lg lh li lj bi translated">阻塞交叉验证过程类似于上述标准形式。</li><li id="17cf" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">不同之处在于，没有初始的随机观察。</li><li id="a6a8" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">在时间序列中，这将呈现K个连续观测数据块。观察的自然顺序保持在每个区块内，但在它们之间被打破。</li><li id="0c64" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">但是，这可能会将未来数据泄漏到模型中。该模型将观察未来的模式进行预测，并试图记住它们。</li><li id="71e3" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">这就是引入阻塞交叉验证的原因。它的工作原理是在两个位置增加边距。第一个是在训练和验证折叠之间，以防止模型观察到两次使用的滞后值，一次作为回归变量，另一次作为响应。</li><li id="8066" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">第二个是在每次迭代中使用的折叠之间，以防止模型从一次迭代到下一次迭代记住模式。</li></ul><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="3daa" class="lx jk hi lt b fi ly lz l ma mb">blocks = 2<br/>n = len(data) // 2</span><span id="72c3" class="lx jk hi lt b fi mc lz l ma mb">for i in [data[i:i + n] for i in range(0, len(data), n)]:<br/>  train, test = model_selection.train_test_split(i)<br/>  print(train, "||", test)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es pn"><img src="../Images/b08b7a1dc6311876f862f696b6946d58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*sdZwEy8Bj-FKuaNTp0yMuw.png"/></div></figure><h2 id="4392" class="lx jk hi bd jl me mf mg jp mh mi mj jt kk mk ml jv ko mm mn jx ks mo mp jz mq bi translated">→嵌套交叉验证</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es po"><img src="../Images/7ec286ed52c2fe97230d2c69b43d5982.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dit6DYhsHaDmrNzq.png"/></div></div></figure><ul class=""><li id="f4d8" class="kz la hi kd b ke lb kh lc kk ld ko le ks lf kw lg lh li lj bi translated"><strong class="kd hj">嵌套交叉验证</strong> ( <strong class="kd hj"> Nested-CV </strong>)嵌套交叉验证和超参数调整。</li><li id="2aa7" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">它用于评估机器学习算法的性能，还用于估计底层模型及其超参数搜索的泛化误差。</li></ul><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="af25" class="lx jk hi lt b fi ly lz l ma mb">outter_cv = model_selection.KFold(n_splits=5)<br/>inner_cv = model_selection.KFold(n_splits=4)</span><span id="f4a2" class="lx jk hi lt b fi mc lz l ma mb">for train, test in outter_cv.split(data):<br/>  tr = np.take(data, train)<br/>  te = np.take(data, test)<br/>  print(tr, "||", te)<br/>  print("-----------------------------------")<br/>  for in_train, in_test in inner_cv.split(tr):<br/>    in_tr = np.take(tr, in_train)<br/>    in_te = np.take(tr, in_test)<br/>    print(in_tr, "||", in_te)<br/>  print("&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;-----------------------------------&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;")</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es pp"><img src="../Images/6ae3d5503889cc59c7d6f148f8a89116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*ACWc_gXape6kUEuaxAKx3A.png"/></div></figure></div><div class="ab cl pq pr gp ps" role="separator"><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv"/></div><div class="hb hc hd he hf"><h1 id="d995" class="jj jk hi bd jl jm px jo jp jq py js jt io pz ip jv ir qa is jx iu qb iv jz ka bi translated">结论</h1><ul class=""><li id="cd8f" class="kz la hi kd b ke kf kh ki kk od ko oe ks of kw lg lh li lj bi translated">没有一种交叉验证方法可以说是对您的数据有效的技术。</li><li id="815d" class="kz la hi kd b ke lk kh ll kk lm ko ln ks lo kw lg lh li lj bi translated">时间序列和分块CV是防止模型过度拟合的最好方法。</li></ul><h1 id="e959" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">参考</h1><p id="58dc" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated"><a class="ae qc" href="https://scikit-learn.org/stable/modules/cross_validation.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/cross _ validation . html</a></p></div><div class="ab cl pq pr gp ps" role="separator"><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv"/></div><div class="hb hc hd he hf"><p id="3d7c" class="pw-post-body-paragraph kb kc hi kd b ke lb ij kg kh lc im kj kk lp km kn ko lq kq kr ks lr ku kv kw hb bi translated">喜欢我的文章？请为我鼓掌并分享它，因为这将增强我的信心。此外，请查看我的另一篇文章，并与未来关于数据科学和机器学习基础系列的文章保持联系。</p><p id="a302" class="pw-post-body-paragraph kb kc hi kd b ke lb ij kg kh lc im kj kk lp km kn ko lq kq kr ks lr ku kv kw hb bi translated">还有，一定要在<a class="ae qc" href="http://www.linkedin.com/in/abhigyan-singh-b13651121" rel="noopener ugc nofollow" target="_blank"><strong class="kd hj">LinkedIn</strong></a>T5上联系我。</p><div class="qd qe ez fb qf qg"><a href="https://www.linkedin.com/in/abhigyan-singh-b13651121/" rel="noopener  ugc nofollow" target="_blank"><div class="qh ab dw"><div class="qi ab qj cl cj qk"><h2 class="bd hj fi z dy ql ea eb qm ed ef hh bi translated">Abhigyan Singh -印度卡纳塔克邦班加罗尔市区|职业简介| LinkedIn</h2><div class="qn l"><h3 class="bd b fi z dy ql ea eb qm ed ef dx translated">数据科学和机器学习爱好者。我在medium上写关于数据科学和机器学习的文章…</h3></div><div class="qo l"><p class="bd b fp z dy ql ea eb qm ed ef dx translated">www.linkedin.com</p></div></div><div class="qp l"><div class="qq l qr qs qt qp qu jh qg"/></div></div></a></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es qv"><img src="../Images/76419a3d41dc5048aa45d09d8151e748.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xnm4WBsvKtdGO5UZ"/></div></div><figcaption class="mt mu et er es mv mw bd b be z dx">Photo by <a class="ae qc" href="https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Markus Spiske</a> on <a class="ae qc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div>    
</body>
</html>
<html>
<head>
<title>Review — Look, Listen and Learn (Self-Supervised Learning)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">复习——看、听、学(自我监督学习)</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/review-look-listen-and-learn-self-supervised-learning-ff89a7dee980?source=collection_archive---------16-----------------------#2021-08-29">https://medium.com/geekculture/review-look-listen-and-learn-self-supervised-learning-ff89a7dee980?source=collection_archive---------16-----------------------#2021-08-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="74a5" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">针对<strong class="ak">视听通信任务(AVC) </strong>使用L -Net的自我监督学习</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/a2f603bc3485de3e9fc5f3b7893bff3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bRRgsTaFLWYbL_fm1ArOCQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx"><strong class="bd jn">Audio-visual correspondence task (AVC)</strong>: By seeing and hearing many <strong class="bd jn">unlabelled </strong>examples, a network should learn to determine whether a pair of (video frame, short audio clip) correspond to each other or not.</figcaption></figure><p id="68a8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi kk translated"><span class="l kl km kn bm ko kp kq kr ks di">在</span>这个故事中，<strong class="jq hj">看、听、学</strong>(L-Net)，由DeepMind和牛津大学的VGG共同完成，回顾。本文考虑了一个问题:</p><blockquote class="kt ku kv"><p id="d504" class="jo jp kw jq b jr js ij jt ju jv im jw kx jy jz ka ky kc kd ke kz kg kh ki kj hb bi translated">通过看和听大量<strong class="jq hj">无标签</strong>视频可以学到什么？</p></blockquote><ul class=""><li id="9875" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated"><strong class="jq hj">视听通信(AVC)学习任务</strong>被引入以从零开始训练视觉和听觉网络<strong class="jq hj">，除了原始的无约束视频本身之外</strong>没有任何额外的监督，导致<strong class="jq hj">良好的视觉和听觉表现</strong>。</li></ul><p id="859c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这是一篇发表在<strong class="jq hj"> 2017 ICCV </strong>的论文，引用超过<strong class="jq hj"> 400次</strong>。(<a class="lj lk ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----ff89a7dee980--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl ll lm gp ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="hb hc hd he hf"><h1 id="ebc5" class="ls lt hi bd jn lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated">概述</h1><ol class=""><li id="e09c" class="la lb hi jq b jr mj ju mk jx ml kb mm kf mn kj mo lg lh li bi translated"><strong class="jq hj">核心理念</strong></li><li id="f272" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj mo lg lh li bi translated"><strong class="jq hj"> L -Net:网络架构</strong></li><li id="aaab" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj mo lg lh li bi translated"><strong class="jq hj">训练数据采样</strong></li><li id="acd9" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj mo lg lh li bi translated"><strong class="jq hj">视听通信(AVC)结果</strong></li><li id="1e67" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj mo lg lh li bi translated"><strong class="jq hj">转移学习结果</strong></li><li id="2a58" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj mo lg lh li bi translated"><strong class="jq hj">定性分析</strong></li></ol></div><div class="ab cl ll lm gp ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="hb hc hd he hf"><h1 id="3b71" class="ls lt hi bd jn lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated">1.核心理念</h1><h2 id="ba0c" class="mu lt hi bd jn mv mw mx lx my mz na mb jx nb nc md kb nd ne mf kf nf ng mh nh bi translated">1.1.二元分类任务</h2><ul class=""><li id="54b4" class="la lb hi jq b jr mj ju mk jx ml kb mm kf mn kj lf lg lh li bi translated">通过看到和听到许多人拉小提琴的例子和狗叫的例子，并且从来没有，或者至少很少看到有人拉小提琴而听到狗叫，反之亦然，应该有可能得出小提琴和狗看起来和听起来像什么，而不需要明确地被教导什么是小提琴或狗。</li><li id="01ad" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">AVC任务是一个简单的<strong class="jq hj">二进制分类任务</strong>:给定一个示例视频帧和一个短音频剪辑，判断它们<strong class="jq hj">是否相互对应。</strong></li></ul><h2 id="53e9" class="mu lt hi bd jn mv mw mx lx my mz na mb jx nb nc md kb nd ne mf kf nf ng mh nh bi translated">1.2.困难</h2><ul class=""><li id="c402" class="la lb hi jq b jr mj ju mk jx ml kb mm kf mn kj lf lg lh li bi translated"><strong class="jq hj">对应的(正)对</strong>是从同一视频中同时提取的<strong class="jq hj">，而<strong class="jq hj">错配的(负)对</strong>是从<strong class="jq hj">不同的视频中提取的。</strong></strong></li><li id="9698" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">网络<strong class="jq hj">从头开始学习</strong>视觉和听觉特征和概念<strong class="jq hj"/><strong class="jq hj">而从未见过一个标签</strong>。</li><li id="2550" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">视频可能非常嘈杂<strong class="jq hj"/>，<strong class="jq hj">音频源在视频中不一定可见</strong>(例如，摄像师说话、视频旁白、声源看不见或被遮挡等。).</li><li id="38a4" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">音频和视频内容可以是<strong class="jq hj">完全不相关的</strong>(例如，具有<strong class="jq hj">添加的音乐</strong>、<strong class="jq hj">非常低音量的声音</strong>、<strong class="jq hj">环境声音</strong>，例如尽管存在其他音频事件，但是风主导音频轨道等。).</li></ul></div><div class="ab cl ll lm gp ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="hb hc hd he hf"><h1 id="51c7" class="ls lt hi bd jn lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated"><strong class="ak"> 2。L -Net:网络架构</strong></h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ni"><img src="../Images/1fb7f439cff388b4c7c3fd5cef71f352.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZF3iAz0hg0DXin5n5sLouA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx"><strong class="bd jn">L³-Net: Network Architecture</strong></figcaption></figure><ul class=""><li id="5efa" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated">该网络具有三个不同的部分 : <strong class="jq hj">分别提取视觉和音频特征的视觉和音频子网</strong>，以及<strong class="jq hj">将这些特征考虑在内以产生最终决策的融合网络</strong>。</li></ul><h2 id="ff03" class="mu lt hi bd jn mv mw mx lx my mz na mb jx nb nc md kb nd ne mf kf nf ng mh nh bi translated">2.1.视觉子网络</h2><ul class=""><li id="b5a4" class="la lb hi jq b jr mj ju mk jx ml kb mm kf mn kj lf lg lh li bi translated">视觉子网络的输入是224×224彩色图像。</li><li id="835c" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated"><a class="ae nj" rel="noopener" href="/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11?source=post_page---------------------------">使用VGGNet </a>设计风格，3×3卷积滤波器，2×2最大池层，步幅2，无填充。</li><li id="c8a3" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">网络可以被分割成<strong class="jq hj">conv+conv+池层</strong>的四个块，使得在每个块内，两个conv层具有相同数量的滤波器，而连续的块具有双倍的滤波器数量:64、128、256和512。</li><li id="7564" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated"><strong class="jq hj">在最末端</strong>，跨所有空间位置执行最大汇集(即<strong class="jq hj">全局最大汇集</strong>，以产生<strong class="jq hj">单个512-D特征向量。</strong></li><li id="a624" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">每个conv层之后是<a class="ae nj" href="https://sh-tsang.medium.com/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">批量归一化</a>和ReLU。</li></ul><h2 id="6b31" class="mu lt hi bd jn mv mw mx lx my mz na mb jx nb nc md kb nd ne mf kf nf ng mh nh bi translated">2.2.音频子网</h2><ul class=""><li id="3ab9" class="la lb hi jq b jr mj ju mk jx ml kb mm kf mn kj lf lg lh li bi translated">音频子网的输入是一个1秒钟的声音剪辑，转换成对数频谱图，作为灰度257×199图像处理。</li><li id="010b" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">音频子网的架构<strong class="jq hj">与vision one </strong>相同，除了输入是1D信号而不是3D信号。</li><li id="ada7" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated"><strong class="jq hj">最终音频特征</strong>也是<strong class="jq hj"> 512-D </strong>。</li></ul><h2 id="be2a" class="mu lt hi bd jn mv mw mx lx my mz na mb jx nb nc md kb nd ne mf kf nf ng mh nh bi translated">2.3.融合网络</h2><ul class=""><li id="4535" class="la lb hi jq b jr mj ju mk jx ml kb mm kf mn kj lf lg lh li bi translated">两个512-D视觉和音频特征被<strong class="jq hj">连接成一个1024-D向量。</strong></li><li id="4201" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">它由<strong class="jq hj">两个完全连接的层</strong>组成，其间夹有ReLU，<strong class="jq hj"> 128-D </strong>的<strong class="jq hj">中间特征尺寸</strong>，对<strong class="jq hj">产生一个2路分类输出</strong>，即视觉和听觉是否对应。</li></ul></div><div class="ab cl ll lm gp ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="hb hc hd he hf"><h1 id="9147" class="ls lt hi bd jn lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated"><strong class="ak"> 3。训练数据采样&amp;数据集</strong></h1><h2 id="4334" class="mu lt hi bd jn mv mw mx lx my mz na mb jx nb nc md kb nd ne mf kf nf ng mh nh bi translated">3.1.培训数据采样和其他细节</h2><ul class=""><li id="2524" class="la lb hi jq b jr mj ju mk jx ml kb mm kf mn kj lf lg lh li bi translated"><strong class="jq hj">非对应的帧-音频对</strong>通过随机采样两个不同的视频并从一个视频中选取随机帧和从另一个视频中选取随机的1秒音频剪辑来编译。</li><li id="8d88" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated"><strong class="jq hj">通过对随机视频进行采样，在该视频中选取随机帧，然后选取与采样帧在时间上重叠的随机1秒音频剪辑，来创建相应的帧-音频对</strong>。</li><li id="9af7" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated"><strong class="jq hj">使用标准的数据扩充</strong>技术:每个训练图像被统一缩放，使得最小尺寸等于256，然后<strong class="jq hj">随机裁剪</strong>成224 × 224，<strong class="jq hj">随机水平翻转</strong>，以及<strong class="jq hj">亮度和饱和度抖动</strong>。</li><li id="5c9e" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">音频仅通过<strong class="jq hj">将音量随机改变10%来增强</strong>，但在整个样本中保持一致。</li><li id="3890" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">网络在<strong class="jq hj"> 16个GPU上并行</strong>训练，同步训练在<strong class="jq hj"> TensorFlow </strong>中实现，每个工人处理一个<strong class="jq hj"> 16元素批次</strong>，从而使得<strong class="jq hj">有效批次大小为256个</strong>。</li><li id="012f" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">一套<strong class="jq hj"> 400k 10秒视频</strong>的训练集，网络<strong class="jq hj">训练两天</strong>，期间见过<strong class="jq hj"> 60M帧-音频对</strong>。</li></ul><h2 id="54e1" class="mu lt hi bd jn mv mw mx lx my mz na mb jx nb nc md kb nd ne mf kf nf ng mh nh bi translated">3.2.数据集</h2><p id="4acc" class="pw-post-body-paragraph jo jp hi jq b jr mj ij jt ju mk im jw jx nk jz ka kb nl kd ke kf nm kh ki kj hb bi translated"><strong class="jq hj">两个视频数据集</strong>用于训练网络:Flickr-SoundNet和Kinetics-Sounds。</p><h2 id="11d2" class="mu lt hi bd jn mv mw mx lx my mz na mb jx nb nc md kb nd ne mf kf nf ng mh nh bi translated">3.2.1.Flickr-声音网</h2><ul class=""><li id="0388" class="la lb hi jq b jr mj ju mk jx ml kb mm kf mn kj lf lg lh li bi translated">这是一个巨大的<strong class="jq hj">未标记的</strong>数据集，包含来自Flickr的完全不受约束的<strong class="jq hj">视频。</strong></li><li id="156d" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">它包含<strong class="jq hj">超过200万个视频</strong>，但出于实际原因<strong class="jq hj">仅使用50万个视频</strong>的随机子集(40万个训练、50万个验证和50万个测试)<strong class="jq hj">仅使用每个视频</strong>的前10秒。</li><li id="2bdb" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">该数据集用于<strong class="jq hj">迁移学习实验</strong>。</li></ul><h2 id="cb8e" class="mu lt hi bd jn mv mw mx lx my mz na mb jx nb nc md kb nd ne mf kf nf ng mh nh bi translated">3.2.2.动力学-声音</h2><ul class=""><li id="e64b" class="la lb hi jq b jr mj ju mk jx ml kb mm kf mn kj lf lg lh li bi translated">这是一个用于<strong class="jq hj">定量评估</strong>的带标签数据集。<strong class="jq hj">使用了动力学数据集</strong>的一个子集(比Flickr-SoundNet小得多),其中包含为10秒裁剪的人类动作手动注释的YouTube视频<strong class="jq hj">。</strong></li><li id="4cfd" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">该子集包含<strong class="jq hj"> 19k 10秒视频剪辑</strong> (15k训练，1.9k验证，1.9k测试)，通过过滤<strong class="jq hj"> 34种人体动作类别</strong>的动力学数据集形成，例如:</li></ul><ol class=""><li id="de6f" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj mo lg lh li bi translated"><strong class="jq hj">比如演奏各种乐器</strong>(吉他、小提琴、木琴等。),</li><li id="3a75" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj mo lg lh li bi translated"><strong class="jq hj">使用工具</strong>(割草、铲雪等。)，以及</li><li id="1e0f" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj mo lg lh li bi translated"><strong class="jq hj">表演杂耍动作</strong>(踢踏舞、保龄球、大笑、唱歌、擤鼻涕等。).</li></ol><ul class=""><li id="585d" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated">它<strong class="jq hj">仍然包含相当多的噪音</strong>，例如:保龄球运动经常伴随着保龄球馆的嘈杂音乐，人声(摄像师或视频解说)经常掩盖感兴趣的声音，许多视频包含音轨。</li></ul></div><div class="ab cl ll lm gp ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="hb hc hd he hf"><h1 id="0a77" class="ls lt hi bd jn lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated">4.视听通信(AVC)结果</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nn"><img src="../Images/d71bf12ae809113ede2b56f6fd8958b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*BEfJAgFOVaTgLSYO31j1-Q.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx"><strong class="bd jn">Audio-visual correspondence (AVC) results</strong></figcaption></figure><ul class=""><li id="ddb7" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated">显示了L3-Net的AVC任务的测试集准确性，以及标记的动力学-声音数据集的两个监督基线。</li><li id="3bd1" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">正反数相同，所以几率得到50%。所有方法都在各自数据集的训练集上进行训练。</li><li id="1946" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated"><strong class="jq hj">视觉网络</strong>具有与我们的视觉子网络(第2.1节)相同的特征提取干线，在其顶部<strong class="jq hj">附着了两个完全连接的层</strong>(大小:512×128和128× <strong class="jq hj"> 34 </strong>，因为有<strong class="jq hj"> 34个动力学-声音类。</strong></li><li id="a11a" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">音频网络的构建与此类似。</li><li id="af24" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated"><strong class="jq hj">监督直接</strong>:直接组合基线计算音视频对应得分为<strong class="jq hj">34-D网络softmax输出</strong>之间的标量积，如果得分大于阈值则判定音视频<strong class="jq hj">对应。</strong></li><li id="d0aa" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated"><strong class="jq hj">监督预训练</strong>:它从两个训练好的网络中提取特征提取干线，通过连接特征并添加两个完全连接的层，将它们组装到我们的网络架构中。<strong class="jq hj">特征提取器的权重被冻结</strong>并且完全连接的层在AVC任务上被训练。</li><li id="7a05" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated"><strong class="jq hj"> L -Net </strong>在两个数据集上分别达到<strong class="jq hj"> 74%和78%，其中机会为50%。(即使是人类也很难判断一个孤立的帧和一个孤立的一秒钟的音频是否对应)</strong></li><li id="6e23" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated"><strong class="jq hj">监督基线无法击败L -Net </strong>，因为“监督预训练”的性能与L -Net不相上下，而“监督直接组合”的效果明显更差，因为与“监督预训练”不同，它没有针对AVC任务进行过训练。</li></ul></div><div class="ab cl ll lm gp ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="hb hc hd he hf"><h1 id="2f72" class="ls lt hi bd jn lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated">5.转移学习结果</h1><ul class=""><li id="1dfa" class="la lb hi jq b jr mj ju mk jx ml kb mm kf mn kj lf lg lh li bi translated">在上述AVC实验中的自监督训练之后，子网络应该被很好地预训练，并且可以被用作其他监督数据集的权重初始化。</li></ul><h2 id="b4d0" class="mu lt hi bd jn mv mw mx lx my mz na mb jx nb nc md kb nd ne mf kf nf ng mh nh bi translated">5.1.ESC-50和DCASE上的音频功能</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es no"><img src="../Images/0a3a0fdcddedd0cf1c95858a266ee788.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*zPxc4zeRJ2u7MX7ODA98Fw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx"><strong class="bd jn">Sound Classification</strong></figcaption></figure><ul class=""><li id="bbc1" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated"><strong class="jq hj">环境声音分类(ESC-50) </strong> : 2000个音频剪辑，每个5秒，在50个类别之间均衡。</li><li id="b314" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated"><strong class="jq hj">声学场景和事件的检测和分类(DCASE) </strong> : 10类，每类10个训练和100个测试片段，每个片段长30秒。</li><li id="3268" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">音频特征是通过将ReLU之前的音频子网络的最后一个卷积层(conv4_2)最大汇集成4×3×512 = 6144维表示来获得的。</li><li id="2991" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">使用<strong class="jq hj"> z分数归一化</strong>对特征进行预处理。一个<strong class="jq hj">多类一对所有线性SVM </strong>被训练，并且在测试时，记录的类分数被计算为其子片段的类分数的平均值。</li><li id="3a41" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated"><strong class="jq hj">“我们的随机”</strong>是一个<strong class="jq hj">附加基线</strong>，它显示了我们的网络在没有L3训练的情况下的性能。</li><li id="3789" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">在这两个基准测试中，L -training令人信服地以5.1%和5%的绝对优势击败了之前最先进的SoundNet [3]。对于ESC-50，L -training将之前的最佳结果与人的表现之间的差距减少了72%,而对于DCASE，L -training <strong class="jq hj"> </strong>将误差减少了42%。</li></ul><blockquote class="kt ku kv"><p id="442c" class="jo jp kw jq b jr js ij jt ju jv im jw kx jy jz ka ky kc kd ke kz kg kh ki kj hb bi translated">提议的L3-training(我们的)在这两项基准测试中<strong class="jq hj">领先</strong>一大截。</p></blockquote><h2 id="5a5f" class="mu lt hi bd jn mv mw mx lx my mz na mb jx nb nc md kb nd ne mf kf nf ng mh nh bi translated">5.2.ImageNet上的视频功能</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es np"><img src="../Images/9110060e0812813334bdadb95dd0839a.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*4eVlml10uSnDnFqJy2KjmA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx"><strong class="bd jn">Visual classification on ImageNet</strong></figcaption></figure><ul class=""><li id="276c" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated">conv4_2特征在ReLU之后被提取，并且以相等的内核和步幅大小执行最大池化，直到特征维数低于10k在这种情况下，这会产生4×4×512 = <strong class="jq hj"> 8192-D特征</strong>。</li><li id="7c9d" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated"><strong class="jq hj">添加单个全连接层</strong>以对<strong class="jq hj"> 1000 ImageNet类</strong>进行线性分类。</li><li id="96c9" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">除了在ImageNet训练集上用交叉熵损失训练的最终分类层之外，所有权重都被冻结为它们的L -Net训练值。</li><li id="a975" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated"><strong class="jq hj">所提出的L -Net训练的特征实现了32.3%的准确度</strong>，这与其他最先进的自监督方法【7，8，22，36】(<a class="ae nj" href="https://sh-tsang.medium.com/review-unsupervised-visual-representation-learning-by-context-prediction-self-supervised-51a1d7ce6aff" rel="noopener">上下文预测</a>【7】)不相上下，同时令人信服地击败了随机初始化、数据相关初始化【17】和<a class="ae nj" href="https://sh-tsang.medium.com/review-context-encoders-feature-learning-by-inpainting-bd181e48997" rel="noopener">上下文编码器</a>【25】。</li><li id="841e" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">需要考虑的一个重要事实是<strong class="jq hj">所有竞争方法在训练</strong>时实际上都使用ImageNet图像。尽管它们不使用标签，但基本的图像统计信息是相同的:</li><li id="4280" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">相比之下，<strong class="jq hj"> L -Net使用完全独立的训练数据源</strong>，以Flickr视频帧的形式。此外，视频帧具有与静止图像非常不同的低级统计，具有强烈的伪影，例如运动模糊。</li></ul><blockquote class="kt ku kv"><p id="8b2c" class="jo jp kw jq b jr js ij jt ju jv im jw kx jy jz ka ky kc kd ke kz kg kh ki kj hb bi translated">令人印象深刻的是，在Flickr视频上经过L -Net训练的拟议视觉功能的表现与在ImageNet上训练的自我监督的最先进水平不相上下。</p></blockquote></div><div class="ab cl ll lm gp ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="hb hc hd he hf"><h1 id="778f" class="ls lt hi bd jn lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated">6.定性结果</h1><h2 id="6aa1" class="mu lt hi bd jn mv mw mx lx my mz na mb jx nb nc md kb nd ne mf kf nf ng mh nh bi translated">6.1.视觉特征</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nq"><img src="../Images/4f8c40a701b1da795e80574d361b441e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qcusllco40q14fuBcMDysg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx"><strong class="bd jn">Learnt visual concepts</strong></figcaption></figure><ul class=""><li id="8660" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated">上图显示了激活pool4中特定单位最多的图像(即，根据其大小排名最高)。</li></ul><blockquote class="kt ku kv"><p id="6279" class="jo jp kw jq b jr js ij jt ju jv im jw kx jy jz ka ky kc kd ke kz kg kh ki kj hb bi translated"><strong class="jq hj">视觉子网络在没有任何明确监督的情况下自动学习识别语义实体</strong>，如吉他、手风琴、键盘等。</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nr"><img src="../Images/30062e424095955a58a475224225baa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qT_BbMbNBwxlCyQWKQO4yw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx"><strong class="bd jn">Visual semantic heatmap</strong></figcaption></figure><ul class=""><li id="8b3a" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated"><strong class="jq hj">上面的热图显示，尽管有明显的混乱和遮挡，物体还是被成功检测出来</strong>。</li></ul><h2 id="5a9d" class="mu lt hi bd jn mv mw mx lx my mz na mb jx nb nc md kb nd ne mf kf nf ng mh nh bi translated">6.2.音频功能</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ns"><img src="../Images/96c52c91d6c4f0748692a5c033981671.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NXPuKhNWvmJiZhcVj0Kpeg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx"><strong class="bd jn">Learnt audio concepts</strong></figcaption></figure><ul class=""><li id="2967" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated">以上图像显示了与声音相对应的视频帧。</li></ul><blockquote class="kt ku kv"><p id="db25" class="jo jp kw jq b jr js ij jt ju jv im jw kx jy jz ka ky kc kd ke kz kg kh ki kj hb bi translated"><strong class="jq hj">同样没有任何监督的音频子网络设法学习各种语义实体</strong>，以及执行精细分类(“拨弄手指”与“弹奏低音吉他”)。</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nt"><img src="../Images/5f1285a1a944d068ea4d457a1eafad4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*BgzSmrSg4ZT8uFUPUOWajw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx"><strong class="bd jn">Audio semantic heatmaps</strong></figcaption></figure><ul class=""><li id="68fa" class="la lb hi jq b jr js ju jv jx lc kb ld kf le kj lf lg lh li bi translated">上图显示了光谱图及其语义热图。</li><li id="7956" class="la lb hi jq b jr mp ju mq jx mr kb ms kf mt kj lf lg lh li bi translated">例如，当检测低音吉他时，它显示出对低频的明显偏好；当检测剪草机时，它显示出对宽频率范围的关注；当检测拨弄手指和踢踏舞时，它显示出时间“步长”。</li></ul></div><div class="ab cl ll lm gp ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="hb hc hd he hf"><h2 id="56fb" class="mu lt hi bd jn mv mw mx lx my mz na mb jx nb nc md kb nd ne mf kf nf ng mh nh bi translated">参考</h2><p id="25f8" class="pw-post-body-paragraph jo jp hi jq b jr mj ij jt ju mk im jw jx nk jz ka kb nl kd ke kf nm kh ki kj hb bi translated">【2017 ICCV】【L-Net】<br/><a class="ae nj" href="https://arxiv.org/abs/1705.08168" rel="noopener ugc nofollow" target="_blank">看、听、学</a></p><h2 id="4445" class="mu lt hi bd jn mv mw mx lx my mz na mb jx nb nc md kb nd ne mf kf nf ng mh nh bi translated">自我监督学习</h2><p id="38bb" class="pw-post-body-paragraph jo jp hi jq b jr mj ij jt ju mk im jw jx nk jz ka kb nl kd ke kf nm kh ki kj hb bi translated"><strong class="jq hj">2014</strong><a class="ae nj" href="https://sh-tsang.medium.com/review-exemplar-cnn-discriminative-unsupervised-feature-learning-with-convolutional-neural-fa68abe937cc" rel="noopener">Exemplar-CNN</a><strong class="jq hj">2015</strong><a class="ae nj" href="https://sh-tsang.medium.com/review-unsupervised-visual-representation-learning-by-context-prediction-self-supervised-51a1d7ce6aff" rel="noopener">上下文预测</a><strong class="jq hj">2016</strong><a class="ae nj" href="https://sh-tsang.medium.com/review-context-encoders-feature-learning-by-inpainting-bd181e48997" rel="noopener">上下文编码器</a><strong class="jq hj">2017</strong><a class="ae nj" href="https://sh-tsang.medium.com/review-look-listen-and-learn-self-supervised-learning-ff89a7dee980" rel="noopener">L-Net</a></p><h2 id="fc9f" class="mu lt hi bd jn mv mw mx lx my mz na mb jx nb nc md kb nd ne mf kf nf ng mh nh bi translated"><a class="ae nj" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>
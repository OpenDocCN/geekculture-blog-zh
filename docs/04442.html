<html>
<head>
<title>A 2021 Guide to improving CNNs-Weak supervision: Semi-supervised learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">改进CNN的2021指南——弱监督:半监督学习</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/a-2021-guide-to-improving-cnns-weak-supervision-semi-supervised-learning-6191816dcb5d?source=collection_archive---------23-----------------------#2021-06-27">https://medium.com/geekculture/a-2021-guide-to-improving-cnns-weak-supervision-semi-supervised-learning-6191816dcb5d?source=collection_archive---------23-----------------------#2021-06-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="84fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这将是我在《T2 2021年CNN改进指南》上的第六篇也是最后一篇文章。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/3b6d3cce92727edfa6672e3b613eadbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SKAaZffajQmStSPb"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Photo by <a class="ae ju" href="https://unsplash.com/@chrislawton?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Chris Lawton</a> on <a class="ae ju" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="0480" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">半监督学习</h2><p id="7e97" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">半监督学习(SSL)处理这样的情况，其中很少标记的训练样本与大量<em class="jd">未标记的</em>样本一起可用。尽管有违直觉，但SSL不仅在小数据集上，而且在ImageNet上也显示出显著的性能提升。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kv"><img src="../Images/f2188e648a90298b7774c0f482a34a6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/0*E_4kMrJAbUm3zMxo.png"/></div></figure><p id="9a4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面关于流行的双月数据集的图表提供了半监督学习的概述。给定6个数据点和未标记数据的分布，算法必须尽最大努力推广到可能是真实的数据分布。</p><p id="1d23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[1]将SSL方法分为以下几类:</p><ul class=""><li id="99b4" class="kw kx hi ih b ii ij im in iq ky iu kz iy la jc lb lc ld le bi translated">一致性正则化</li><li id="0204" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">代理标记方法</li><li id="42d9" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">生成模型</li><li id="f6f0" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">基于图形的方法</li></ul><p id="f129" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本帖中，我们将回顾SSL的一些最基本和最强大的概念和方法，以及这些方法的直觉。这个帖子的顺序和内容是基于调查论文[1]的。由于SSL是一个非常受欢迎的话题，有许多不同的变体，所以我们将只对每种方法进行简短的探讨。我们必须注意到，对于提高深度学习的性能来说，它可能是一个有用的<em class="jd">欺骗</em>。</p><h2 id="081f" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">SSL的假设</h2><p id="c40f" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">SSL算法仅基于对数据的某些假设而工作，并基于对数据的以下假设而开发。如果没有这样的假设，就不可能从一个有限的训练集推广到一组可能无限多的看不见的测试用例。这些假设是:</p><ul class=""><li id="313f" class="kw kx hi ih b ii ij im in iq ky iu kz iy la jc lb lc ld le bi translated"><strong class="ih hj">聚类假设:</strong>如果点在同一个聚类中，那么它们<em class="jd">可能</em>属于同一个类。</li><li id="0758" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated"><strong class="ih hj">平滑度假设</strong>:如果两个输入属于同一类，属于同一簇，那么它们对应的输出<em class="jd">需要</em>接近。相反，如果这两个点是分开的，则输出必定彼此远离。</li><li id="34a4" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated"><strong class="ih hj">流形假设:</strong>(高维)数据(大致)位于低维流形上。高维空间中的距离(例如图像中的像素差异)与类别类型无关。但是，我们可以使用这个假设找到一个低维表示，并使用未标记的数据。</li></ul><h1 id="f5ff" class="lk jw hi bd jx ll lm ln kb lo lp lq kf lr ls lt ki lu lv lw kl lx ly lz ko ma bi translated">一致性正则化</h1><p id="275e" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">基于<em class="jd">聚类假设，</em>如果对未标记的数据点进行小的修改，预测应该不会发生显著变化。使用一致性正则化技术，我们训练一个模型，为相似的数据点提供一致的预测。具体地，给定一个未标记的数据点x，以及它的修改版本x̂，并且模型是f，目标是最小化d( f( x)，f( x ̂)).一些流行的距离度量是MSE，KL散度，JS散度。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mb"><img src="../Images/7694f8ee89fd166b44c870797e8244ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*oMcz1fd0SMGAqdTBUzH9mg.png"/></div></figure><h2 id="39a0" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">梯形网络[2]</h2><p id="55ae" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">该网络由3个网络组成:编码器、噪声编码器和解码器。在每次迭代中，输入被提供给两个编码器。高斯噪声被注入到噪声编码器的每一层。这将给出两个预测，干净的预测y和损坏的预测ỹ.ỹ被馈送到解码器中以重构y和来自ỹ.的干净编码器的激活</p><p id="97b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">无监督损失被测量为每层的干净编码器和解码器的激活之间的MSE。这种一致性正则化方法在编码器中使用高斯噪声来生成修改版本的x̂。当测量无监督损失时，数据x不需要标签。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mc"><img src="../Images/0258f44949bf9a2cecfadaec6960d11d.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*Yb6aihwNtt1RmPLId1VBcg.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es md"><img src="../Images/0a07db0f07cd4e3992d715a678a3f00b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*d7T245T2Hk3OdqAr_lFrjQ.png"/></div></figure><p id="1463" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯形网络的一个问题是计算量大，因为需要三次转发。在以下主题中，我们将解释梯形模型的许多变体。一种变型是，γ模型简单地比较y和ỹ，以降低解码器的计算成本。</p><h2 id="8e01" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">π模型，时间集合[3]</h2><p id="c32b" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">梯形网络的另一种变体，π模型[3]通过移除损坏的编码器并对损坏的和普通的输入使用相同的网络，进一步简化了框架。这样，π模型可以利用数据增加或减少作为<em class="jd">噪声源</em>，而不是在网络中插入高斯噪声。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es me"><img src="../Images/372b45de642604fdc95fc4dc56ad234c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9eb56zKsKD0yYXiRlxg_Lg.png"/></div></div></figure><p id="267c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，目标<em class="jd"> y </em>值在训练期间快速变化，这降低了训练的效率和稳定性。因此，作者建议稳定目标y值。通过将y计算为先前预测的集合，提出了时间集合来实现这一点。准确地说，目标y将通过指数移动平均线(EMA)累积，如下式所示。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mf"><img src="../Images/81cf9284e69b22fd4f6a182736f9cee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*rCX30Vd3BavswqjgoiU_dA.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mg"><img src="../Images/eebb6d4db7d76062334934595ee3d548.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iagfCEeHKIy5xDiEDJPjPQ.png"/></div></div></figure><p id="822b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这样，每次迭代只需要一次向前传递，并且变得更加稳定。但是这需要保持所有训练样本的均线值。</p><p id="d908" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于大型数据集，新学习的信息应用缓慢，因为每个目标每个时期更新一次，并且一个时期的跨度非常长。重要的是，目标的均线经常会对数据进行错误分类。如果无监督的损失大于有监督的损失，模型将在受到轻微惩罚的同时不断做出错误的预测，这导致了<em class="jd">确认偏差</em>。</p><h2 id="31c2" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">刻薄的老师[4]</h2><p id="e108" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">Mean Teacher[4]是为了更快地整合学习到的信号，并避免确认偏差而提出的。教师模型的权重计算为先前状态的加权平均值，如下式所示。平均教师方法可以解释为π模型，它使用教师模型进行目标预测。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mh"><img src="../Images/deced77aad1add1f90a85d2f78106339.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*5dmAlpQ9S0IB9h9LSXoLag.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mi"><img src="../Images/8336717f24da82f5ebc3ebc29df9d4ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*_Ig64NRoIoBbxeWuXAy2UA.png"/></div></figure><h2 id="2bde" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">双学生[5]</h2><p id="d086" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">使用Mean Teacher的一个问题是，随着大量的训练迭代，教师模型会收敛到学生模型，它打算解决的初始问题会继续。在双学生中，将同时训练两个<em class="jd">不同</em>重量的学生模型，其中一个将为另一个提供目标。每个模型都在以下条件下进行稳定性测试。</p><ul class=""><li id="b357" class="kw kx hi ih b ii ij im in iq ky iu kz iy la jc lb lc ld le bi translated">清洁和修改的预测是相似的:f(x) ~ f(x̃)</li><li id="1a69" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">两个预测都很有把握:f(x)，f(x̃)&gt;ε</li></ul><p id="348c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除了监督和非监督的损失，我们强迫其中一个学生根据他们的稳定性做出与对手相似的预测。只有当两个模型都是<em class="jd">稳定的</em>时，相对不太稳定的模型才使用额外的双学生损失进行训练。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mj"><img src="../Images/bb17564a24f5e3d966097194c34f9e83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*9IlDUTbJziUBOPTSEdZ_nA.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx">Supervised+Unsupervised loss</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mk"><img src="../Images/7944fb20880963b5e686cc60e5404ac9.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*moS1GBD6kjdfm-fDZmDavQ.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx">Additional dual student loss</figcaption></figure><p id="9261" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用这个简单的技巧，DualStudents (DS)可以从Mean Teacher(MT)方法中获得很大的性能提升，如下表所示。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ml"><img src="../Images/92e31423c3f6ccb766afdef061b90c18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*GdmqTyltMR2j4lzWE9N8Dw.png"/></div></figure><h2 id="81e7" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">插值一致性训练(ICT)〔7〕</h2><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mm"><img src="../Images/03570beb3547f4cd8ee60808d956c547.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-wPY9YnPq0-gMhpoTKeKBw.png"/></div></div></figure><p id="73b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ICP建议将混合作为随机修改，并将无监督损失作为混合图像的预测和两个独立图像的预测的混合输出之间的差异来测量。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mn"><img src="../Images/746704c2454497ae51f6b9ad830b4917.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*6pGVhd6RB2fmlKFmMozxOQ.png"/></div></figure><h2 id="d005" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">虚拟对抗训练(VAT) [6]</h2><p id="8eaa" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">源自梯形网络的方法将随机变换应用于输入，并鼓励模型具有与未标记数据及其修改版本相似的输出。</p><p id="06b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，随机扰动在高维中可能是低效的，因为只有一小部分修改会有效地将模型推出决策边界，并提供有意义的反馈。为了加强修改的鲁棒性，一些方法提出在<em class="jd">对抗方向</em>上进行修改，这将最大化模型预测的变化<em class="jd">。</em></p><p id="4920" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">VAT希望每个数据点周围的输出分布同样平滑。增值税的<em class="jd">对抗方向</em>的测量方式将<em class="jd">最大程度地改变模型的预测</em>。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mo"><img src="../Images/d787df29e46c63e17b2f698228a0c597.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*Ca6QCO5wFWhP_KRRDW28sw.png"/></div></figure><p id="de67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，采样与输入x维数相同的高斯噪声r。接下来，测量关于损失的梯度，该梯度测量对x和(x+r)的预测之间的差异。最终的对抗性修改r_adv通过归一化和缩放梯度来实现。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mp"><img src="../Images/d1f7f3b35fb6b8eb17748b8c21cfcdbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*Qkeg1eZwsb5pU-jukv2jIQ.png"/></div></figure><p id="cc6e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们将获得的对抗性修改应用于图像，并测量无监督损失。一个缺点是寻找敌对方向需要其自身的向前传递，并且所需的计算增加，使得这种方法不那么吸引人。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mq"><img src="../Images/39fe5e03f334bc70d567d6ad52315139.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*pnnw-Ev8kUDh4ECK9DteUw.png"/></div></figure><p id="6fb5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">提出了更多使用不同类型的数据扩充和随机化推断的一致性正则化方法。</p><h1 id="c4ad" class="lk jw hi bd jx ll lm ln kb lo lp lq kf lr ls lt ki lu lv lw kl lx ly lz ko ma bi translated">代理标记方法</h1><p id="6106" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">代理标记方法使用一些特定的方法在未标记的数据上生成合成的代理标记，通常涉及预测本身或其他监督。这些带有代理标签的数据与原始标签数据一起用作目标。这些方法主要基于两个分支:自我训练和多视角学习。</p><h2 id="e02d" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">自我训练</h2><p id="2b98" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">首先根据标记的数据训练初始模型。在自训练中，训练的模型本身被用于将标签分配给未标记的数据点。不同方法之间的细节有所不同，但是通常，当置信度足够大时，将(x，argmax f(x))的未标记数据对添加到标记集。重复这个用标记集训练模型，然后向标记集添加可信伪标记的过程。</p><p id="90cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">模型通常在最后阶段根据正确标记的数据进行精细训练[8]。学生-教师模型也用于生成代理标签。</p><p id="383f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种方法的一个显而易见的问题是，该模型通常不能纠正有偏见的和错误的分类，并且可以自信地做出错误的代理标签。</p><h2 id="3e8c" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">伪标记</h2><p id="587b" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">类似于自训练，基于伪标记的方法为未标记的数据生成代理标记。最初，伪标注天真地将数据标注为概率最高的类。</p><h2 id="9295" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">转导SSL[9]</h2><p id="9739" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">直推式SSL[9]提出将未标记示例的标签视为变量，并尝试利用它们的损失函数来学习最优标签以及模型参数。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mr"><img src="../Images/a212c0fd21a04ed8f2b824304ef82ec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*C0gcM1ns2lY66JZK3skw3w.png"/></div></figure><p id="feaf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，r_i是样本X_i的置信水平，并且是基于人口密集区域中的样本更可能被分配正确标签的假设自洽地计算的。</p><p id="341b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">众所周知，朴素伪标记具有确认偏差，会产生不正确的标记。根据[10]，混合正则化在减轻这种偏差方面是有效的。</p><h2 id="3883" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">元伪标签(MPL)〔11〕</h2><p id="87f6" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">MPL使用学生-教师设置。教师模型负责生成代理标签，并受学生模型反馈的指导。准确的说，老师先生成目标标签，学生在生成的标签上训练。然后，教师用策略梯度(PPO)训练学生的确认损失。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ms"><img src="../Images/725a9679236adfe15e05697c149f90a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*m0P95bsl661wbKKLHVKBTg.png"/></div></figure><h2 id="562a" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">整体方法[12]</h2><p id="b9c6" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">SSL中的整体方法结合了主流方法，并设计了一个可以实现更好性能的框架。诸如MixMatch、ReMixMatch和FixMatch之类的整体方法基于一致性正则化和代理标记的直觉，并设计了一种改进的方法来组合这些方法。</p><p id="d3f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">FixMatch[12]管道是最近的一个重要方法，它结合了一致性正则化和伪标记，如下图所示。对于带标签的示例，使用提供的目标。对于未标记的示例，模型对弱增强图像的预测被用作代理标签，而强增强图像用代理标签来训练。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mt"><img src="../Images/b7334eb04693615b5243757b85948d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*zelgGnE57rFqe-DUEQP_Nw.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mu"><img src="../Images/08944d26713dd5406e6c8209efa7db59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*q_O9hlg72sLRfv_bHViOsQ.png"/></div></figure><h1 id="fceb" class="lk jw hi bd jx ll lm ln kb lo lp lq kf lr ls lt ki lu lv lw kl lx ly lz ko ma bi translated">生成模型</h1><p id="6859" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">诸如GANs、自动编码器之类的生成模型可以用来增强分类器的鲁棒性。因为大多数生成模型可以用未标记的数据来训练，所以我们可以将SSL用于生成模型。这种生成模型以各种方式结合到训练过程中。</p><h2 id="bb52" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">自动编码器/可变自动编码器(VAE)</h2><p id="63c8" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">M1模型直接在模型中使用VAEs，首先使用标记和未标记的图像预训练自动编码器，并对潜在变换z而不是输入图像x执行分类</p><p id="ea6b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">M2模型在M1模型的训练中使用标签。如果类别标签不可用，y将被视为潜在变量z之外的潜在变量。然而，我并不真正理解M2模型的概念。</p><h2 id="e78f" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">生成对抗网络</h2><p id="b91f" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">GAN框架可以通过多种方式集成到SSL中。例如，分类器可以通过在鉴别器之后添加完全连接的层来利用鉴别器特征，作为迁移学习方法。这种从鉴别器到分类器的迁移学习是主要的方法。</p><p id="f8d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，这种将鉴别器集成到分类器的通用方法具有局限性，因为与例如VAEs相比，特征表示不够丰富。这使得GAN对于SSL来说不是一个有吸引力的选择。然而，我们可以使用特定的GAN框架，如甘比，它具有将数据映射到潜在表示的编码器网络。</p><h1 id="13b0" class="lk jw hi bd jx ll lm ln kb lo lp lq kf lr ls lt ki lu lv lw kl lx ly lz ko ma bi translated">基于图形的方法</h1><p id="eef9" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">在基于图的SSL方法中，每个数据点x，包括标记的和未标记的，都被表示为图中的一个节点。连接每对节点的边反映了它们的相似性。但是，图表的具体特征可能会有所不同。对于在SSL设置中对数据之间的关系进行建模，图是一种强大的结构。</p><h2 id="d00a" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">标签传播</h2><p id="2909" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">标签传播中的主要假设是同一流形中的数据点很可能共享同一标签。总之，标签传播根据数据流形结构和图的边相似性将已标记数据点的标签传播到未标记数据点。</p><p id="ca6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">传播指的是标签被分配给图中的节点并沿着图的边传播到连接的节点的迭代性质。从标有已知标签的节点1、2、…、l和标有0的节点l + 1、…、n开始，每个节点开始<em class="jd">向其邻居传播</em>其标签，并且重复该过程直到收敛。</p><p id="398c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用图形来测量数据接近度以应用SSL的其他方法</p><ul class=""><li id="1127" class="kw kx hi ih b ii ij im in iq ky iu kz iy la jc lb lc ld le bi translated">图形嵌入</li><li id="9ca6" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">图形神经网络</li></ul><p id="70d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我不得不诚实地说，基于图形的SSL方法并不直观(至少对我来说是这样)，我也不太了解这些方法。有关基于图形的SLL的更多信息，您可以查看调查[1]和调查参考资料。</p><h1 id="74d7" class="lk jw hi bd jx ll lm ln kb lo lp lq kf lr ls lt ki lu lv lw kl lx ly lz ko ma bi translated">结论</h1><p id="5e1e" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">总结半监督学习，一致性正则化是基于这样的假设，即模型应该为相似的数据输出相似的预测。因此，基于<em class="jd">梯形模型</em>的方法对未标记的数据进行随机修改，并期望预测是一致的。确认偏差是指模型不断地做出错误的预测，同时受到无监督损失的轻微惩罚。为了解决这个问题，提出了“平均教师法”。还提出了对抗而不是随机修改的方法来加速学习有用的信息。</p><p id="f0ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代理标记方法使用诸如伪学习的自我训练方法，为未标记的数据提供合成标记。还提出了改进的方法，如使用师生设置的元伪学习。</p><p id="118b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SSL还可以利用用标记和未标记数据训练的生成模型的丰富功能来训练用于迁移学习的鉴别器，或者使用在自动编码器中训练的模型。基于图的SSL方法将数据空间建模为图，并应用图搜索技术和距离度量来执行SSL。</p><h1 id="38f1" class="lk jw hi bd jx ll lm ln kb lo lp lq kf lr ls lt ki lu lv lw kl lx ly lz ko ma bi translated">参考</h1><p id="d06d" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">[1]纽约州奥阿利市、加利福尼亚州赫德洛特市和塔米市(2020年)。深度半监督学习综述。<em class="jd"> arXiv预印本arXiv:2006.05278 </em>。</p><p id="03a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2]拉斯穆斯、瓦尔波拉、洪卡拉、贝里隆德和莱科(2015年)。梯形网络的半监督学习。<em class="jd"> arXiv预印本arXiv:1507.02672 </em>。</p><p id="6f2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[3]萨穆利·莱恩和蒂莫·艾拉。半监督学习的时态集成。arXiv预印本arXiv:1610.02242，2016。</p><p id="d307" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[4] Antti Tarvainen和Harri Valpola。平均教师是更好的榜样:加权平均一致性目标提高半监督深度学习结果。《神经信息处理系统进展》，第1195-1204页，2017年。</p><p id="3371" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[5]柯志忠，王，丁，严，秦，任，刘瑞伟(2019)。双重学生:在半监督学习中打破老师的限制。在<em class="jd">IEEE/CVF计算机视觉国际会议论文集</em>(第6728–6736页)。</p><p id="1418" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[6]t . miya to，Maeda，S. I .，Koyama，m .，&amp; Ishii，S. (2018年)。虚拟对抗训练:监督和半监督学习的正则化方法。<em class="jd"> IEEE模式分析与机器智能汇刊</em>，<em class="jd"> 41 </em> (8)，1979–1993。</p><p id="ce7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[7]维尔马，v .，川口，k .，兰姆，a .，坎纳拉，j .，本吉奥，y .，洛佩斯-帕斯，D. (2019)。半监督学习的插值一致性训练。<em class="jd"> arXiv预印本arXiv:1903.03825 </em>。</p><p id="23a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[8] Yalniz，I. Z .，Jégou，h .，Chen，k .，Paluri，m .，和Mahajan，D. (2019年)。用于图像分类的亿级半监督学习。<em class="jd"> arXiv预印本arXiv:1905.00546 </em>。</p><p id="8a71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[9]石文伟，龚玉英，丁春春，陶正明，郑，倪(2018)。使用最小-最大特征的直推式半监督深度学习。在<em class="jd">欧洲计算机视觉会议(ECCV) </em>(第299–315页)的会议记录中。</p><p id="a2d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">10埃里克·阿拉索、迭戈·奥尔特戈、保罗·艾伯特、诺埃尔·奥康纳和凯文·麦克吉尼斯。深度半监督学习中的伪标记和确认偏差。arXiv预印本arXiv:1908.02983，2019。</p><p id="cd3a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[11] Hieu Pham，Qizhe Xie，Zihang Dai和Quoc V Le .元伪标签。arXiv预印本:2003.10580，2020</p><p id="9b1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[12] Kihyuk Sohn，David，，Zhang，Nicholas Carlini，Ekin D Cubuk，Alex Kurakin，和Colin Raffel。Fixmatch:简化具有一致性和置信度的半监督学习。arXiv预印本arXiv:2001.07685，2020。</p><p id="931d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[13]Durk P . Kingma、Shakir Mohamed、Danilo Jimenez Rezende和Max Welling。具有深度生成模型的半监督学习。《神经信息处理系统进展》,第3581–3589页，2014年。</p><p id="2391" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[14]x .zhuѓ和z .ghahramaniѓн(2002年)。利用标签传播从有标签和无标签数据中学习。GTX 560 Ti</p></div></div>    
</body>
</html>
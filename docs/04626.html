<html>
<head>
<title>Ensemble Learning: Basics and Overview</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集成学习:基础和概述</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/ensemble-learning-basics-and-overview-316e9691fa35?source=collection_archive---------36-----------------------#2021-06-30">https://medium.com/geekculture/ensemble-learning-basics-and-overview-316e9691fa35?source=collection_archive---------36-----------------------#2021-06-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="ca86" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">什么是集成学习？它有哪些类型？</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/9df3d381a493323e0c8ea7b24fd4f72d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yx9e5bUhEpYBaAaKroWHYg.png"/></div></div></figure><p id="5e92" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi kf translated">想象一下你想在这个周末去看一部电影，但是你不能决定看哪一部。所以为了方便起见，你可以在网上浏览，看看哪部电影是热门。你向你的朋友或家人寻求建议，却很少阅读评论。所以现在你有来自多种不同来源的信息，因此，你只需要根据这些信息做出决定。</p><p id="6c50" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">你在这里做的是向随机的人提出一个问题，然后汇总他们的回答。你意识到聚合的答案比专家的答案更好。这就是所谓的<em class="ko">群体智慧</em>。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kp"><img src="../Images/17f5e07ecabef4d01a40c1b5488fc279.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*rdCYYjpzIwL-wh8yYG2cBQ.png"/></div></figure><p id="028f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">类似地，在机器学习的世界中，如果你聚集一组预测器(分类或回归)的值，你通常会得到比单个好的预测器提供的结果更好的结果。这些团体的组合被称为<strong class="jl hj">合奏</strong>。应用这些集成来获得答案是<strong class="jl hj">集成学习</strong>，使用这些技术的算法被称为<strong class="jl hj">集成方法</strong>。</p><h1 id="226d" class="kq kr hi bd ks kt ku kv kw kx ky kz la io lb ip lc ir ld is le iu lf iv lg lh bi translated">集成学习的类型</h1><ol class=""><li id="b683" class="li lj hi jl b jm lk jp ll js lm jw ln ka lo ke lp lq lr ls bi translated">投票集合:堆叠集合</li><li id="517b" class="li lj hi jl b jm lt jp lu js lv jw lw ka lx ke lp lq lr ls bi translated">Bagging或Bootstrap聚合:并行集成</li><li id="9d36" class="li lj hi jl b jm lt jp lu js lv jw lw ka lx ke lp lq lr ls bi translated">增强:顺序集成</li></ol><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ly"><img src="../Images/55cf42238abdfc0622e3210bfcbfa443.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*pkTtZQUh37alH8SMqoeKmw.png"/></div></figure><h1 id="8e23" class="kq kr hi bd ks kt ku kv kw kx ky kz la io lb ip lc ir ld is le iu lf iv lg lh bi translated">投票团。</h1><p id="0f75" class="pw-post-body-paragraph jj jk hi jl b jm lk ij jo jp ll im jr js lz ju jv jw ma jy jz ka mb kc kd ke hb bi translated"><em class="ko">投票</em>合奏，这个术语本身就定义了它做什么。<em class="ko">投票</em>集成是一种基于集成技术的机器学习模型，它对多个不同或相同的模型进行训练，并基于从这些模型中选择的最高概率来预测输出。简单地说，投票分类器要求集成模型说出输出的概率，并选择多数概率输出。</p><p id="df99" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">投票集合可以进一步分为两种类型:</p><ol class=""><li id="8fd2" class="li lj hi jl b jm jn jp jq js mc jw md ka me ke lp lq lr ls bi translated">硬投票:一个很简单的方法就是对类的多数预测进行投票，预测得到最多票数的值。这被称为硬投票分类器。假设您训练了四个模型，它们的预测如下(1，0，0，0)，假设是二进制分类。因此，在这里您可以看到0具有多数输出，因此0将是最终的预测。</li><li id="8b8c" class="li lj hi jl b jm lt jp lu js lv jw lw ka lx ke lp lq lr ls bi translated">软投票:使用投票分类器的另一种方式是基于特定类别的平均概率来获得预测。假设您为二元分类1和0训练一个模型。1的预测概率为(0.75，0.36，0.42)，0的预测概率为{0.5，0.6，0.8}。现在，1的预测平均值为0.51，0的预测平均值为0.63，因此最终输出将为0。</li></ol><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mf"><img src="../Images/f7c8788a8bad85871479e62c8f4d74fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*EvwoQlvSitU8dHbDE8MR-A.png"/></div><figcaption class="mg mh et er es mi mj bd b be z dx">Voting Classifier e.g. Hard Voting Classifier</figcaption></figure><p id="9054" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">投票集合的准确度高于最佳预测器的最高个体准确度。相反，如果每个分类器都是弱学习器(给出较低的准确度)，集成仍然可以是强学习器，只要有足够数量的弱学习器并且所有的都足够不同。</p><p id="8b64" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">对这些的解释可以用<a class="ae mk" href="https://www.investopedia.com/terms/l/lawoflargenumbers.asp" rel="noopener ugc nofollow" target="_blank"> <em class="ko">大数定律找到。</em>T3】</a></p><h1 id="94ce" class="kq kr hi bd ks kt ku kv kw kx ky kz la io lb ip lc ir ld is le iu lf iv lg lh bi translated">打包/引导聚合</h1><p id="5056" class="pw-post-body-paragraph jj jk hi jl b jm lk ij jo jp ll im jr js lz ju jv jw ma jy jz ka mb kc kd ke hb bi translated">Bagging指的是集成技术，其中我们对所有不同的预测器使用相同的训练算法，但是在训练集的不同随机子集上训练它们。当这种取样与替换一起进行时，称为装袋。</p><p id="d30d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这种技术不是绝对的，因为数据的变化会对模型产生非常严重的影响。当我们进行bagging时，每个个体的偏差变得更高，聚合过程减少了偏差和方差，最终的集成在原始数据上具有与单个预测器相同的偏差和更低的方差。所以<em class="ko">装袋减少了模型的方差</em>。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ml"><img src="../Images/8d29a8c7d9b3ccf7891949bf01535d4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*WVUqmWcTeg9KusH1GhcpUg.png"/></div><figcaption class="mg mh et er es mi mj bd b be z dx">Bagging Ensemble with random sampling</figcaption></figure><p id="93a1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">Bagging允许跨多个预测器对训练实例进行多次采样，或者对相同的预测器进行多次采样。</p><p id="4f48" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">一旦训练了所有的预测器，集成就可以通过简单地聚合所有预测器的预测来对新的实例进行预测。</p><p id="dfd3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">聚合通常是分类器的统计模式，是回归的平均值。</p><blockquote class="mm mn mo"><p id="193f" class="jj jk ko jl b jm jn ij jo jp jq im jr mp jt ju jv mq jx jy jz mr kb kc kd ke hb bi translated">同样在bagging中，预测是并行完成的，所以它们更快，更受欢迎。</p></blockquote><p id="d60d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">随机森林</strong></p><p id="8e8f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">随机森林是决策树的集合，通常通过bagging方法(有时是粘贴)进行训练，通常样本大小设置为训练集的大小。您可以使用Random Forest分类器类来代替构建Bagging分类器并将其传递给决策树分类器，这对于决策树来说更加方便和优化。</p><h1 id="c5a2" class="kq kr hi bd ks kt ku kv kw kx ky kz la io lb ip lc ir ld is le iu lf iv lg lh bi translated">助推</h1><p id="6ce6" class="pw-post-body-paragraph jj jk hi jl b jm lk ij jo jp ll im jr js lz ju jv jw ma jy jz ka mb kc kd ke hb bi translated">Boosting是机器学习中存在的另一种集成方法，它也像Bagging一样使用各种预测器来提高它们的准确性。与使用并行技术的Bagging不同，Boosting使用顺序技术。依次增强训练预测器，每个预测器都试图校正它的前一个预测器(先前训练的预测器)。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ms"><img src="../Images/c147c5bc90395799602e92966466a15b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*JI05N7kDjGLpxtOVQNfGbw.png"/></div><figcaption class="mg mh et er es mi mj bd b be z dx">Boosting Ensemble</figcaption></figure><blockquote class="mm mn mo"><p id="7902" class="jj jk ko jl b jm jn ij jo jp jq im jr mp jt ju jv mq jx jy jz mr kb kc kd ke hb bi translated">由于这种用于增强的顺序学习，它不像使用并行学习的bagging那样可扩展。</p></blockquote><h1 id="4ba3" class="kq kr hi bd ks kt ku kv kw kx ky kz la io lb ip lc ir ld is le iu lf iv lg lh bi translated">增压的类型</h1><p id="aae4" class="pw-post-body-paragraph jj jk hi jl b jm lk ij jo jp ll im jr js lz ju jv jw ma jy jz ka mb kc kd ke hb bi translated"><strong class="jl hj"> AdaBoost </strong></p><p id="317d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">AdaBoost更关注前一个不足的训练实例。这导致新的预测者更加关注先前预测者的困难情况。</p><blockquote class="mm mn mo"><p id="a82f" class="jj jk ko jl b jm jn ij jo jp jq im jr mp jt ju jv mq jx jy jz mr kb kc kd ke hb bi translated"><em class="hi">以</em>为例，训练第一个分类器，然后增加误分类训练实例的相对权重，根据来自先前预测器的更新权重训练第二个分类器。当它对训练集进行预测时，权重被更新等等。最终答案只是计算所有预测值的预测值及其权重，根据多数加权投票预测的类别就是预测的类别。</p></blockquote><p id="fdde" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">梯度增强</strong></p><p id="1b17" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">像AdaBoost一样，梯度增强也通过将先前训练预测器添加到集合中并校正其前身来顺序工作。然而，在梯度增强中，不是像AdaBoost那样在每次迭代时改变每个不正确的分类观测值的权重，梯度增强方法试图使新的预测器与前一个预测器产生的残差相适应。</p><blockquote class="mm mn mo"><p id="c974" class="jj jk ko jl b jm jn ij jo jp jq im jr mp jt ju jv mq jx jy jz mr kb kc kd ke hb bi translated">A <!-- --> daBoost在每个实例中调整前任权重，而梯度增强试图用先前预测器的误差来拟合新预测器。</p></blockquote><p id="bb41" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> XGBoost </strong></p><p id="22a2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">请注意，梯度增强的优化实现在XGBoost库中可用，XGBoost的字面意思是Xtreme Gradient Boost。XGBoost是梯度提升决策树的实现，旨在提高速度和性能。</p><p id="3cd3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">该库提供了一个用于各种计算环境的系统，尤其是:</p><ul class=""><li id="95c4" class="li lj hi jl b jm jn jp jq js mc jw md ka me ke mt lq lr ls bi translated"><strong class="jl hj">树构建的并行化</strong>在训练期间使用你所有的CPU核心。</li><li id="dc23" class="li lj hi jl b jm lt jp lu js lv jw lw ka lx ke mt lq lr ls bi translated"><strong class="jl hj">分布式计算</strong>用于使用机器集群训练非常大的模型。</li><li id="3089" class="li lj hi jl b jm lt jp lu js lv jw lw ka lx ke mt lq lr ls bi translated"><strong class="jl hj">核外计算</strong>适用于不适合内存的超大型数据集。</li><li id="4ca3" class="li lj hi jl b jm lt jp lu js lv jw lw ka lx ke mt lq lr ls bi translated"><strong class="jl hj">数据结构和算法的高速缓存优化</strong>，以充分利用硬件。</li></ul><p id="23b7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">XGBoost在分类和回归预测建模问题上主导结构化或表格化数据集。证据是，它是Kaggle竞争数据科学平台上竞赛获胜者的首选算法。</p></div><div class="ab cl mu mv gp mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="hb hc hd he hf"><p id="2230" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我希望现在你对什么是集成学习和不同类型的集成学习有了一个基本的概念。</p><p id="e14f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">感谢阅读！。</p><p id="4a98" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">关注更多更新。</p><blockquote class="mm mn mo"><p id="a741" class="jj jk ko jl b jm jn ij jo jp jq im jr mp jt ju jv mq jx jy jz mr kb kc kd ke hb bi translated"><strong class="jl hj">参考文献:</strong></p></blockquote><div class="nb nc ez fb nd ne"><a href="https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab dw"><div class="ng ab nh cl cj ni"><h2 class="bd hj fi z dy nj ea eb nk ed ef hh bi translated">应用机器学习XGBoost简介-机器学习掌握</h2><div class="nl l"><h3 class="bd b fi z dy nj ea eb nk ed ef dx translated">XGBoost是一种算法，最近一直主导着应用机器学习和Kaggle竞争…</h3></div><div class="nm l"><p class="bd b fp z dy nj ea eb nk ed ef dx translated">machinelearningmastery.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns jh ne"/></div></div></a></div><div class="nb nc ez fb nd ne"><a href="https://scikit-learn.org/stable/modules/ensemble.html" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab dw"><div class="ng ab nh cl cj ni"><h2 class="bd hj fi z dy nj ea eb nk ed ef hh bi translated">1.11.集合方法-sci kit-学习0.24.2文档</h2><div class="nl l"><h3 class="bd b fi z dy nj ea eb nk ed ef dx translated">在平均方法中，驱动的原则是独立地建立几个估计量，然后平均它们的估计量</h3></div><div class="nm l"><p class="bd b fp z dy nj ea eb nk ed ef dx translated">scikit-learn.org</p></div></div><div class="nn l"><div class="nt l np nq nr nn ns jh ne"/></div></div></a></div><p id="c015" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">通过Scikit-Learn和TensorFlow进行机器实践学习<em class="ko"> Aurelien Geron </em>。</p></div></div>    
</body>
</html>
# 新人工智能:文本到视频的下一个级别！超越文本到图像

> 原文：<https://medium.com/geekculture/new-ai-next-level-text-to-video-beyond-text-to-image-511bfd21d5ce?source=collection_archive---------6----------------------->

![](img/5f7bfac5db02c0ee7cddeed137bcc143.png)

Photo by [Denise Jans](https://unsplash.com/@dmjdenise?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

在我们深入研究本文所有有趣的部分之前，如果你想看视频版本，你可以点击上面的链接。

文本到图像的生成已经遍布互联网和新闻。一些人认为媒体的关注分散了人工智能社区的注意力，而另一些人则开始拟人化，给这些模型赋予创造力等特征。然而，不可否认的是，这一技术里程碑将永远改变视觉艺术。图像生成的一个自然发展是视频生成，许多人为此感到兴奋。在本文中，我们将讨论一些从文本提示生成视频的 AI 模型。想象一下，把《哈利·波特》系列的整个文本放入这个模型，或者可能是一部《权力的游戏》同人小说，显然重写了上两季，因为我们知道那是如何结束的。但是在我们开始之前，让我们讨论一下文本到视频生成的现状。

当前的模型在视频生成方面非常有限，主要是因为两个重要的约束。与图像不同，视频表示为每个时间单位的多个帧，以每秒帧数计算。这需要非常高的计算资源来训练视频数据，这是一个非常大的问题。另一个问题是缺乏准确的数据集。最大的多语言视频描述数据集 VATEX 仅包含约 41，250 个视频和 825，000 个中英文字幕。可用于视频合成的数据集要么是针对非常特定的领域定制的，要么是可用的字幕不能准确地表示特定时间的帧。现在我们知道了使用文本提示生成视频所面临的挑战，让我们深入研究目前已经发布的 AI 模型。

我们将从 CogVideo 开始。CogVideo 拥有 94 亿个参数，是最大的跨多个领域的文本到视频生成预训练模型。它使用了他们所谓的多帧速率分层训练技术，分辨率质量为 480×480，每秒 8 帧，总共 4 秒。我希望我没有因为太多技术细节而把你搞糊涂。如果你有任何问题，请在下面的评论区留下，我会很好地回答。你可以看出这个项目还处于起步阶段，有几个方向可以改进生成的视频。目前，该模型接受中文作为输入，因此必须首先翻译英文文本。我会在描述中分享互动链接，这样你就可以玩玩了。

接下来，我们有微软亚洲和北京大学合作的女娲-Infinity。该项目的目标是制作高分辨率的图像和更长的视频。该模型能够以非常高的质量执行几个视觉合成任务。我们来讨论其中的几个。该模型以一幅图像作为输入，可以生成一幅类似的图像，分辨率非常高，为 38，912 x 2，048！这大约是人工智能模型创建的原始输入大小的 19 倍。生成的图像的内容显示了仍然适合原始版本的上下文的新添加内容。该模型还可以从简单的输入图像制作高分辨率的动画。最后，它还可以产生质量惊人的文本到视频的任务。看看这个由模型生成的令人难以置信的粉红猪小妹视频。

https://[mobile.twitter.com/Buntworthy/status/1562447108237832195](https://www.google.com/url?q=http://mobile.twitter.com/Buntworthy/status/1562447108237832195&sa=D&source=docs&ust=1663624680819301&usg=AOvVaw1WC2k1xZRACW2dKBz6sRpT)

也许你可以开始为你的孩子或弟弟妹妹写不同的 Peppa Pig 剧本，这样他们就有源源不断的剧集了。然而，该模型的一个问题是，用于训练该模型的数据集范围狭窄，因此不能很好地推广到其他类型的视频。这肯定会很快改变，我们肯定会密切关注它。

还有其他视频合成项目，不一定是文本到视频，但仍然值得一提。第一个是来自 Deepmind 的 Transframer，它能够从提供的单个图像中生成 30 秒的视频。它非常适合视频预测和改变视图。然而，分辨率质量不是最高的，但在以后的迭代中可能会有所提高。另一个最先进的模型来自英伟达实验室。他们致力于一个视频生成模型，该模型可以精确复制对象运动、摄像机角度调整和不断变化的内容。该模型创建新的视频内容，并解决长期不一致的问题，其中场景可能在时间帧之间发生不真实的变化，例如云以不自然的方式来回移动。接下来，Runway 是一个视频编辑平台，它宣布计划通过使用文本提示来改变场景，从而扩展其编辑功能。例如，下面的链接是一个根据输入的描述更改背景的视频。

https://twitter.com/runwayml/status/1568220303808991232

你可以看到背景没有显示任何动态对象，文本也没有从头开始生成一个全新的视频。也许未来的更新会包含更复杂的编辑。但就目前而言，这仍然是一项非常引人注目的任务。最后，许多创意人员通过将几个文本插入图像帧来生成视频。通过添加多个帧并在图像之间切换，结果显示出迷人的动画效果。

你可以看到我们正处于创新的人工智能视频生成工具的尖端。你认为我们还需要几个月或几年才能看到高质量的文字或图像长视频？请在下面的评论区让我知道你的预测。尽管这对未来来说都是令人兴奋的消息，但我们不能忘记提到这些视频生成工具可能带来的负面社会影响。像其他类型的生成任务一样，错误信息和虚假信息的可能性仍然是人工智能社区的一个巨大障碍。版权问题将被带到这个问题的最前沿，法律将不得不更新，以反映时代的变化。我无法想象拉里和杰里看到人工智能生成的他们心爱的杰里·宋飞情景喜剧的视频会特别高兴。我们可能还需要几年时间才能实现这一场景，但开始考虑这些可能性并无大碍。另一方面，它开启了无限的创意机会，任何人都可以基于教育、营销或娱乐目的的脚本制作视频。广告代理商或像迪士尼这样的流媒体服务将如何适应这种情况？我想我们得等等看。

感谢阅读这篇文章。我希望你今天学到了一些东西。

谢谢大家！

**资源**:

**cog video**

github:[https://github.com/THUDM/CogVideo](https://github.com/THUDM/CogVideo)

演示网站:[https://models.aminer.cn/cogvideo/](https://models.aminer.cn/cogvideo/)

拥抱脸空间:[https://huggingface.co/spaces/THUDM/CogVideo](https://huggingface.co/spaces/THUDM/CogVideo)

论文:[https://arxiv.org/pdf/2205.15868.pdf](https://arxiv.org/pdf/2205.15868.pdf)

***女娲-Infinty***

网址:[https://nuwa-infinity.microsoft.com/#/](https://nuwa-infinity.microsoft.com/#/)

论文:[https://arxiv.org/abs/2207.09814](https://arxiv.org/abs/2207.09814)

***其他:***

变压器:[https://sites.google.com/view/transframer](https://sites.google.com/view/transframer)

英伟达实验室:[https://www.timothybrooks.com/tech/long-video-gan/](https://www.timothybrooks.com/tech/long-video-gan/)

跑道:[https://runwayml.com/](https://runwayml.com/)
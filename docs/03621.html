<html>
<head>
<title>Ridge and Lasso Regression :</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">山脊和套索回归；</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/ridge-and-lasso-regression-51705b608fb9?source=collection_archive---------3-----------------------#2021-06-12">https://medium.com/geekculture/ridge-and-lasso-regression-51705b608fb9?source=collection_archive---------3-----------------------#2021-06-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="045a" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">对正则化技术的见解</h2></div><p id="17d6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jt translated">线性回归是机器学习中最常用和最简单的监督学习技术之一。它建立了模型中的特征(自变量)和标签(因变量)之间的关系。在线性回归中，优化函数或损失函数被称为<em class="kc">【残差平方和(RSS)</em><strong class="iz hj"/>，用于定义和度量模型的误差<strong class="iz hj">。</strong>梯度下降算法用于通过多次迭代找到最佳成本函数。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/f4d5a3ecf1339e23030141f1b8a5dc26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YCT_kjVsoBnZptFp0at7Zw.jpeg"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Cost function for linear regression (image by author)</figcaption></figure><p id="39e9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="kc">这里，βj代表不同独立变量或预测值(x)的系数估计值，并分别描述每个特征的权重或大小。</em></p><p id="420a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在线性回归中，如果我们向模型添加更多的特征，其复杂性会增加，这又会导致方差增加和偏差减少，即<strong class="iz hj"> <em class="kc">过拟合</em> </strong>。在过度拟合的情况下，所创建的模型很好地拟合了训练数据，但是未能估计训练集之外的变量之间的真实关系。因此，该模型在测试数据上表现不佳。因此，为了克服过拟合情况，我们可以使用正则化技术，该技术将通过降低系数的幅度来降低模型复杂度。</p><blockquote class="kt ku kv"><p id="8548" class="ix iy kc iz b ja jb ij jc jd je im jf kw jh ji jj kx jl jm jn ky jp jq jr js hb bi translated">在正则化技术中，通过保持相同数量的变量来减少独立变量的大小。它的工作原理是在复杂模型的成本函数中增加一个惩罚或收缩项。它减少了模型的方差，代价是增加了少量的偏差。</p></blockquote><p id="3fa6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正则化技术用于提高模型的泛化能力。这是通过调整最佳拟合线的斜率来实现的。参考下图，其中直线非常适合训练数据集，而不适合测试数据集。使用正则化，通过使用超参数或罚项α来改变最佳拟合线的斜率或使线倾斜一点。在正则化之后，该模型在训练中可能表现不好，因为该线没有精确地通过训练数据点，但是它将在测试中给出相当好的结果。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kz"><img src="../Images/a41b4ec78251f685704208292676ff9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*n-fid04EiwcTZZPO0RXlMQ.gif"/></div><figcaption class="kp kq et er es kr ks bd b be z dx">Image by author</figcaption></figure><p id="6fab" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">主要有两种类型的正则化技术，下面给出:</p><ol class=""><li id="1334" class="la lb hi iz b ja jb jd je jg lc jk ld jo le js lf lg lh li bi translated">岭回归(L-2范数)</li><li id="57ef" class="la lb hi iz b ja lj jd lk jg ll jk lm jo ln js lf lg lh li bi translated">套索回归(L-1范数)</li></ol><h1 id="055a" class="lo lp hi bd lq lr ls lt lu lv lw lx ly io lz ip ma ir mb is mc iu md iv me mf bi translated"><strong class="ak">岭回归:</strong></h1><p id="75be" class="pw-post-body-paragraph ix iy hi iz b ja mg ij jc jd mh im jf jg mi ji jj jk mj jm jn jo mk jq jr js hb bi translated">在该技术中，通过添加罚项(收缩项)来改变线性回归的成本函数，罚项将λ(超参数)乘以每个要素的平方权重。岭回归的成本函数变为:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es ml"><img src="../Images/113d931881f834131330234139b5221a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gTQPvSVWV83YJToTZBgTrg.jpeg"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Cost function for Ridge Regression (Image by author)</figcaption></figure><p id="33a4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">根据上面的等式，罚项正则化了模型的系数或权重。因此，岭回归降低了系数的大小，这将有助于降低模型的复杂性。</p><h1 id="c0ee" class="lo lp hi bd lq lr ls lt lu lv lw lx ly io lz ip ma ir mb is mc iu md iv me mf bi translated"><strong class="ak">拉索回归:</strong></h1><p id="e933" class="pw-post-body-paragraph ix iy hi iz b ja mg ij jc jd mh im jf jg mi ji jj jk mj jm jn jo mk jq jr js hb bi translated">Lasso代表<em class="kc">最小绝对和选择算子</em>。这是另一种用于降低模型复杂性的正则化技术。它类似于岭回归，只是罚项包括绝对权重而不是权重的平方。成本函数lasso回归如下所示:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mm"><img src="../Images/93701a7f5cda939799f4bd79be59c15e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1dqxiyoM978s9ul96kfj-g.jpeg"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Cost function for Lasso Regression (Image by author)</figcaption></figure><blockquote class="kt ku kv"><p id="0fd8" class="ix iy kc iz b ja jb ij jc jd je im jf kw jh ji jj kx jl jm jn ky jp jq jr js hb bi translated">当λ等于零时，岭回归或套索回归的成本函数就等于RSS。随着λ值的增加，方差减小，偏差增大。最佳拟合线的斜率将减小，并且该线变为水平。随着该项的增加，模型对独立变量的响应变得越来越弱。因此，我们必须明智地选择lambda的值，在一系列值中进行迭代，选择一个误差最小的值。</p></blockquote><h2 id="1864" class="mn lp hi bd lq mo mp mq lu mr ms mt ly jg mu mv ma jk mw mx mc jo my mz me na bi translated">岭回归和套索回归的区别:</h2><ul class=""><li id="6729" class="la lb hi iz b ja mg jd mh jg nb jk nc jo nd js ne lg lh li bi translated">在岭回归中，通过降低系数的大小来降低模型的复杂性，但它从不将系数的值设置为绝对零。然而，套索回归往往使系数达到绝对零。</li></ul><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es nf"><img src="../Images/5ff60e0b2d3296a461d0a1412bde9627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1MppprewjrucYEpTVDujxg.jpeg"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Boston Housing Price Data set (Image by author)</figcaption></figure><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es ng"><img src="../Images/96119f2eca9b8ba520c789a520c577dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ezjn41SfDkWbfxnTm_kE4g.jpeg"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Magnitude of coefficients after Linear Regression (Image by author)</figcaption></figure><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es nh"><img src="../Images/89c56e0fbcc1f79515dc3c963d9f8ba2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IxeUb6Xoa_puFCVwUr5Xqg.jpeg"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Magnitude of coefficients after Ridge Regression (Image by author)</figcaption></figure><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es nh"><img src="../Images/ed33ea9a1fc105307536e909f4a6b8e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a5aYqdSMJCacGJm2ADPK4w.jpeg"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx">Magnitude of coefficients after Lasso Regression (Image by author)</figcaption></figure><p id="604a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们以波士顿房价数据集为例。数据集由506行和14列组成。“价格”列将是我们的因变量，所有其他列都是因变量。参考分别对数据集应用线性、岭型和套索回归后的三幅图。脊和套索回归的λ值取为0.5。从图中可以看出，岭回归后系数的大小有所收缩。但是在套索回归的情况下，除了一些特征之外，所有其他特征的系数都降低到绝对零。</p><ul class=""><li id="1ea0" class="la lb hi iz b ja jb jd je jg lc jk ld jo le js ne lg lh li bi translated">岭回归仅通过降低特征的系数并保持模型中存在的所有特征来帮助避免模型过度拟合。但是在套索回归的情况下，除了降低模型的复杂性，它还有助于自动特征选择。套索回归将系数值转换为零。系数变为零的特征在预测目标变量时不太重要，因此可以从模型中删除。</li><li id="ab75" class="la lb hi iz b ja lj jd lk jg ll jk lm jo ln js ne lg lh li bi translated">岭回归比套索回归快。</li></ul><h2 id="b0ee" class="mn lp hi bd lq mo mp mq lu mr ms mt ly jg mu mv ma jk mw mx mc jo my mz me na bi translated"><strong class="ak">岭和套索回归的局限性:</strong></h2><p id="2d89" class="pw-post-body-paragraph ix iy hi iz b ja mg ij jc jd mh im jf jg mi ji jj jk mj jm jn jo mk jq jr js hb bi translated">▹岭回归无助于特征选择。</p><p id="e642" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">▹岭回归用于缩小系数，但从不将它们的值设置为绝对零。该模型将保留所有特征，并且将保持复杂，这可能导致模型性能较差。</p><p id="81ac" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">▹当我们对一个具有高度相关变量的模型应用套索回归时，它将只保留几个变量，并将其他变量设置为零。这将导致一些信息的丢失以及模型精度的降低。</p></div></div>    
</body>
</html>
<html>
<head>
<title>XGBoost in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的XGBoost</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/xgboost-in-machine-learning-586f2c56e8d8?source=collection_archive---------9-----------------------#2022-11-10">https://medium.com/geekculture/xgboost-in-machine-learning-586f2c56e8d8?source=collection_archive---------9-----------------------#2022-11-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/c68c7bc3368d09d2ee5fb0c358189873.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WNZMv_674nTMLunX"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Photo by <a class="ae iu" href="https://unsplash.com/@markuswinkler?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Markus Winkler</a> on <a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="9510" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">本文将讨论XGBoost算法，并构建和优化模型以获得最佳结果。</p><p id="da08" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是上一篇<a class="ae iu" rel="noopener" href="/@abhi2652254/cross-validation-in-machine-learning-483ae08faf3f?source=your_stories_page-------------------------------------"> <strong class="ix hj">的延续部分</strong> </a>引自原<strong class="ix hj"/><a class="ae iu" href="https://www.kaggle.com/learn/intermediate-machine-learning" rel="noopener ugc nofollow" target="_blank"><strong class="ix hj">科目</strong> </a> <strong class="ix hj">。</strong></p><p id="63eb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">免责声明:</strong> XGBoost算法在Kaggle比赛中以将竞争对手带到<strong class="ix hj">排行榜</strong>而闻名。</p><p id="c34f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">梯度推进</strong>是一种迭代循环的方法，从而将模型添加到集合中。</p><p id="dbc8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">它从用一个模型初始化集合开始，这个模型的预测可能非常简单明了<strong class="ix hj">。(即使预测非常不准确，对系综的后续添加<strong class="ix hj">将解决这些错误。)</strong></strong></p><p id="a1e9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，循环开始如下:</p><ul class=""><li id="7fc1" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">首先，<strong class="ix hj">当前集合</strong>用于为数据集中的每个观察值生成预测。为了进行预测，将来自<strong class="ix hj">集合</strong>中所有模型的<strong class="ix hj">预测相加</strong>。</li><li id="bb22" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">这些预测然后用于计算一个<strong class="ix hj">损失函数</strong>(假设<a class="ae iu" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj">均方误差</strong> </a>)。</li><li id="022b" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">然后，损失函数用于拟合将被添加到集合的新模型<strong class="ix hj">。具体来说，确定<strong class="ix hj">模型参数</strong>，使得<strong class="ix hj">将这个新模型添加到集合</strong>将减少损失。(注:<strong class="ix hj">梯度提升</strong>中的<strong class="ix hj">梯度</strong>是指在<strong class="ix hj">损失函数</strong>上使用<a class="ae iu" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>来确定这个新模型中的<strong class="ix hj">参数</strong>。)</strong></li><li id="94a5" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">最后，将<strong class="ix hj">新型号</strong>添加到整体中，并重复该过程。</li></ul><p id="e916" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我会写代码来一步步走完对<a class="ae iu" href="https://stackabuse.com/bytes/end-to-end-xgboost-regression-pipeline-with-scikit-learn/" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj"> XGBoost </strong> </a> <strong class="ix hj">的理解过程。</strong></p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="6c19" class="kq kr hi km b fi ks kt l ku kv">import pandas as pd<br/>from sklearn.model_selection import train_test_split</span><span id="47d3" class="kq kr hi km b fi kw kt l ku kv"># Read the data<br/>X = pd.read_csv('../input/train.csv', index_col='Id')<br/>X_test_full = pd.read_csv('../input/test.csv', index_col='Id')</span><span id="5198" class="kq kr hi km b fi kw kt l ku kv"># Remove rows with missing target, separate target from predictors<br/>X.dropna(axis=0, subset=['SalePrice'], inplace=True)<br/>y = X.SalePrice    <br/>          <br/>X.drop(['SalePrice'], axis=1, inplace=True)</span><span id="734a" class="kq kr hi km b fi kw kt l ku kv"># Break off validation set from training data<br/>X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,random_state=0)<br/></span><span id="be15" class="kq kr hi km b fi kw kt l ku kv"># "Cardinality" means the number of unique values in a column<br/># Select categorical columns with relatively low cardinality (convenient but arbitrary)</span><span id="24ed" class="kq kr hi km b fi kw kt l ku kv">low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() &lt; 10 and X_train_full[cname].dtype == "object"]</span><span id="222e" class="kq kr hi km b fi kw kt l ku kv"># Select numeric columns<br/>numeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]</span><span id="48f1" class="kq kr hi km b fi kw kt l ku kv"># Keep selected columns only<br/>my_cols = low_cardinality_cols + numeric_cols<br/>X_train = X_train_full[my_cols].copy()<br/>X_valid = X_valid_full[my_cols].copy()<br/>X_test = X_test_full[my_cols].copy()</span><span id="769b" class="kq kr hi km b fi kw kt l ku kv"># One-hot encode the data (to shorten the code, we use pandas)<br/>X_train = pd.get_dummies(X_train)<br/>X_valid = pd.get_dummies(X_valid)<br/>X_test = pd.get_dummies(X_test)<br/>X_train, X_valid = X_train.align(X_valid, join='left', axis=1)<br/>X_train, X_test = X_train.align(X_test, join='left', axis=1)</span></pre><p id="fc09" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面使用的代码是从我以前使用的以前的模块中继承来的(请检查我以前的<a class="ae iu" rel="noopener" href="/@abhi2652254"> <strong class="ix hj">部分</strong> ) </a>。<strong class="ix hj">基数</strong>用于具有类别&lt; 10 <strong class="ix hj">的<strong class="ix hj">分类列</strong>。80%培训</strong>和<strong class="ix hj"> 20%测试数据</strong>准备完毕。<strong class="ix hj">数字列</strong>也被使用。最后，<strong class="ix hj"> X_train、X_test和X_valid </strong>被创建。它们是<strong class="ix hj">训练集、测试集和验证集。</strong></p><p id="d039" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们构建下面的XGBoost模型。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="11d2" class="kq kr hi km b fi ks kt l ku kv">from xgboost import XGBRegressor</span><span id="6098" class="kq kr hi km b fi kw kt l ku kv"># Define the model<br/>my_model_1 = XGBRegressor(random_state = 0)</span><span id="a177" class="kq kr hi km b fi kw kt l ku kv"># Fit the model<br/>my_model_1.fit(X_train, y_train)</span></pre><p id="92d2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这种情况下，<strong class="ix hj">随机种子</strong> ( <strong class="ix hj"> random_state </strong>)被设置为0。</p><p id="127b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">模型预测和平均绝对误差(<a class="ae iu" href="https://en.wikipedia.org/wiki/Mean_absolute_error" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj"> MAE </strong> </a>)为</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="91ca" class="kq kr hi km b fi ks kt l ku kv"><strong class="km hj">from</strong> sklearn.metrics <strong class="km hj">import</strong> mean_absolute_error​</span><span id="218a" class="kq kr hi km b fi kw kt l ku kv"><em class="kx"># Get predictions</em></span><span id="cdb1" class="kq kr hi km b fi kw kt l ku kv">predictions_1 <strong class="km hj">=</strong> my_model_1.predict(X_valid)</span><span id="3913" class="kq kr hi km b fi kw kt l ku kv"># Calculate MAE<br/>mae_1 = mean_absolute_error(predictions_1, y_valid) # Your code here</span><span id="d3d4" class="kq kr hi km b fi kw kt l ku kv"># Uncomment to print MAE<br/>print("Mean Absolute Error:" , mae_1)</span><span id="c6f6" class="kq kr hi km b fi kw kt l ku kv"><br/>Mean Absolute Error: 17662.73</span></pre><p id="b005" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们有了带有<strong class="ix hj">默认参数</strong>的<strong class="ix hj">基线</strong>模型结果。现在，让我们<strong class="ix hj">改进模型</strong>，看看MAE值是否可以降低。</p><p id="b12d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">修改后的模型的代码是</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="cbc5" class="kq kr hi km b fi ks kt l ku kv"># Define the model<br/>my_model_2 = XGBRegressor(n_estimators=600,learning_rate=0.05) </span><span id="aa10" class="kq kr hi km b fi kw kt l ku kv"># Fit the model<br/>my_model_2.fit(X_train, y_train) # Your code here</span><span id="9f75" class="kq kr hi km b fi kw kt l ku kv"># Get predictions<br/>predictions_2 = my_model_2.predict(X_valid) # Your code here</span><span id="4638" class="kq kr hi km b fi kw kt l ku kv"># Calculate MAE<br/>mae_2 = mean_absolute_error(predictions_2, y_valid) # Your code here</span><span id="2b9e" class="kq kr hi km b fi kw kt l ku kv"># Uncomment to print MAE<br/>print("Mean Absolute Error:" , mae_2)</span><span id="dc96" class="kq kr hi km b fi kw kt l ku kv">Mean Absolute Error: 16716.16</span></pre><p id="166c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面使用的参数是</p><h2 id="1345" class="kq kr hi bd ky kz la lb lc ld le lf lg jg lh li lj jk lk ll lm jo ln lo lp lq bi translated"><code class="du lr ls lt km b">n_estimators</code></h2><p id="a1f3" class="pw-post-body-paragraph iv iw hi ix b iy lu ja jb jc lv je jf jg lw ji jj jk lx jm jn jo ly jq jr js hb bi translated"><code class="du lr ls lt km b">n_estimators</code>指定建模循环的次数。它等于集合中包含的<strong class="ix hj">个型号</strong>的数量。</p><ul class=""><li id="5ca1" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">太低的<em class="kx">a值导致<em class="kx">欠拟合</em>，从而导致<strong class="ix hj">对训练数据和测试数据的预测</strong>不准确。</em></li><li id="171d" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><em class="kx">a值过高</em>导致<em class="kx">过拟合</em>，导致<strong class="ix hj">对训练数据</strong>预测准确，但对测试数据预测不准确。</li></ul><p id="ec4f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">典型值范围从100-1000，尽管这很大程度上取决于下面讨论的<code class="du lr ls lt km b">learning_rate</code>参数。</p><h2 id="9b66" class="kq kr hi bd ky kz la lb lc ld le lf lg jg lh li lj jk lk ll lm jo ln lo lp lq bi translated"><code class="du lr ls lt km b">learning_rate</code></h2><p id="4a09" class="pw-post-body-paragraph iv iw hi ix b iy lu ja jb jc lv je jf jg lw ji jj jk lx jm jn jo ly jq jr js hb bi translated">不是通过<strong class="ix hj">将来自每个组件模型的预测</strong>相加来获得预测，而是在将它们相加之前，可以将来自每个模型的预测<strong class="ix hj">乘以<strong class="ix hj">小数值</strong>(称为<strong class="ix hj">学习率</strong>)。</strong></p><p id="0309" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这意味着每增加一棵树，帮助就少一点。因此，可以在不过度拟合的情况下设置更高的值<code class="du lr ls lt km b">n_estimators</code>。</p><p id="96b9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一般来说，<strong class="ix hj">小的学习率</strong>和<strong class="ix hj">大数量的估计器</strong>将产生更<strong class="ix hj">精确的XGBoost模型</strong>，尽管它也将花费模型<strong class="ix hj">更长的时间来训练</strong>，因为它在周期中进行更多的迭代。默认情况下，XGBoost设置<code class="du lr ls lt km b">learning_rate=0.1</code>。上述MAE值为16716.16，比之前的MAE值(17662.73)有所提高。当然，可以迭代<strong class="ix hj">各种参数值</strong>以达到<strong class="ix hj">更低的MAE数</strong>。</p><p id="732c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">打破模式</strong></p><p id="d573" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">本教程的最后一步是打破模型，意思是使用<strong class="ix hj">某些参数值来增加MAE </strong>(大于17662.73)。虽然我们总是以降低MAE值为目标，但这种方式将提供一种利用各种参数并通过迭代得出合理的MAE值的思路。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="16f0" class="kq kr hi km b fi ks kt l ku kv"># Define the model<br/>my_model_3 = XGBRegressor(n_estimators=200,learning_rate=0.01)</span><span id="56f3" class="kq kr hi km b fi kw kt l ku kv"># Fit the model<br/>my_model_3.fit(X_train, y_train) # Your code here</span><span id="b21e" class="kq kr hi km b fi kw kt l ku kv"># Get predictions<br/>predictions_3 = my_model_3.predict(X_valid)</span><span id="1326" class="kq kr hi km b fi kw kt l ku kv"># Calculate MAE<br/>mae_3 = mean_absolute_error(predictions_3, y_valid)</span><span id="382a" class="kq kr hi km b fi kw kt l ku kv"># Uncomment to print MAE<br/>print("Mean Absolute Error:" , mae_3)</span><span id="91a7" class="kq kr hi km b fi kw kt l ku kv"><br/>Mean Absolute Error: 29111.51</span></pre><p id="f3d0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">MAE值在<strong class="ix hj"> 29111 </strong>左右，即&gt; 17662.73。</p><p id="d690" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">提前停止轮次</strong>的参数也被使用，定义如下</p><h2 id="381e" class="kq kr hi bd ky kz la lb lc ld le lf lg jg lh li lj jk lk ll lm jo ln lo lp lq bi translated"><code class="du lr ls lt km b">early_stopping_rounds</code></h2><p id="a452" class="pw-post-body-paragraph iv iw hi ix b iy lu ja jb jc lv je jf jg lw ji jj jk lx jm jn jo ly jq jr js hb bi translated"><code class="du lr ls lt km b">early_stopping_rounds</code>用于自动找到<code class="du lr ls lt km b">n_estimators</code>的理想值。早期停止会导致模型在验证分数停止提高时停止迭代，即使<code class="du lr ls lt km b">n_estimators</code>没有任何硬停止。最好为<code class="du lr ls lt km b">n_estimators</code>设置一个较高的值，然后使用<code class="du lr ls lt km b">early_stopping_rounds</code>来寻找停止迭代的最佳时间。</p><p id="8f6f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里，我将结束XGBoost教程，尽管从长远来看，它需要大量的实践。我将写下一篇关于<strong class="ix hj">数据泄露</strong>的文章，并结束课程，让即将到来的<strong class="ix hj">端到端项目</strong>的<strong class="ix hj">管道</strong>上线。</p><p id="3e9f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">敬请期待，在此之前，请查看我的其他<a class="ae iu" rel="noopener" href="/@abhi2652254"> <strong class="ix hj">文章</strong> </a>，说<a class="ae iu" href="https://www.linkedin.com/in/obhinaba17/" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj">你好</strong> </a>，<strong class="ix hj"> </strong>，并让我知道你是否想要一位技术作家为你写作，我们可以通过一个简短的电话进行讨论。</p></div></div>    
</body>
</html>
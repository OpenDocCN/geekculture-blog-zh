<html>
<head>
<title>Generating Haikus From Images</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从图像中生成俳句</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/generating-haikus-from-images-c0d35c2470ce?source=collection_archive---------13-----------------------#2021-03-10">https://medium.com/geekculture/generating-haikus-from-images-c0d35c2470ce?source=collection_archive---------13-----------------------#2021-03-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div class="er es hg"><img src="../Images/7540a3b49fa44733ebc4ee03266eaeeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*sHd4JSIjaMzRdNeyGO8_cg.jpeg"/></div></figure><div class=""/><p id="925e" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">作者:<a class="ae jk" href="https://www.linkedin.com/in/petermdavis/" rel="noopener ugc nofollow" target="_blank">皮特·戴维斯</a>，<a class="ae jk" href="https://www.linkedin.com/in/ramkapistalam/" rel="noopener ugc nofollow" target="_blank">拉姆·卡皮斯塔姆</a>，<a class="ae jk" href="https://www.linkedin.com/in/india-lindsay/" rel="noopener ugc nofollow" target="_blank">英迪亚·林赛</a>，<a class="ae jk" href="https://www.linkedin.com/in/josephalexmcgraw/" rel="noopener ugc nofollow" target="_blank">亚历克斯·麦格劳</a>，<a class="ae jk" href="https://www.linkedin.com/in/immanuel-p/" rel="noopener ugc nofollow" target="_blank">伊曼纽尔·彭米尼塞里</a>，<a class="ae jk" href="https://www.linkedin.com/in/matthew-streichler/" rel="noopener ugc nofollow" target="_blank">马修·斯特莱希勒</a></p><p id="a91b" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">俳句是一种受5-7-5音节模式限制的日本诗歌的流行形式。传统的俳句倾向于关注自然世界的元素。现代变体通常在坚持音节结构的同时融入独特的文体特征。</p><p id="5f44" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们感兴趣的是确定一种创造性的方法来结合几种NLP和机器学习技术，包括卷积神经网络和变压器，来编写俳句。我们开发了一个生成模型，它以一幅图像作为输入，并写出与图像中包含的对象相关的俳句。这个项目的主要挑战是创作与输入图像相关的诗歌，同时遵守英语语法规则和俳句的结构限制。</p><p id="87ea" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们的诗歌生成模型包括以下3个步骤:</p><ol class=""><li id="9c6a" class="jl jm hp io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated">将图像输入VGG16，生成代表图像的标签</li><li id="1b20" class="jl jm hp io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated">使用标签产生连贯的，有意义的俳句，遵循5-7-5音节结构</li><li id="3e20" class="jl jm hp io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated">通过使用创新的神经网络技术(CLIP)选择最合适的诗歌来提高相关性。</li></ol><h1 id="38cc" class="jz ka hp bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">相关著作</h1><p id="9b96" class="pw-post-body-paragraph im in hp io b ip kx ir is it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj hb bi translated">我们的项目受到了其他几位数据科学家工作的启发。pranav Vadrevu<a class="ae jk" href="https://towardsdatascience.com/generate-fresh-movie-stories-for-your-favorite-genre-with-deep-learning-143da14b29d6" rel="noopener" target="_blank">【1】</a>使用预先训练的GPT-2模型构建了一个电影故事生成器。他通过用电影剧本数据微调GPT-2模型来提高输出故事的连贯性。</p><p id="c929" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">丹尼斯·克里维茨基<a class="ae jk" href="https://avoncourt.de/generation-poems-recurrent-neural-network/" rel="noopener ugc nofollow" target="_blank">【2】</a>创造了一个发电机，输出基于莎士比亚和哥特的诗歌。他使用递归神经网络(RNN)架构来训练生成器。</p><p id="c6c9" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">marmik Pandya<a class="ae jk" href="https://www.researchgate.net/publication/313874773_NLP_based_Poetry_Analysis_and_Generation" rel="noopener ugc nofollow" target="_blank">【3】</a>发表了一份技术报告，该报告使用了一种基于简单词性(POS)标记的系统来生成诗歌。他们还尝试了一种遗传算法来提高模型的一致性。</p><h1 id="6bb8" class="jz ka hp bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">步骤1:从图像生成标签</h1><p id="b321" class="pw-post-body-paragraph im in hp io b ip kx ir is it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj hb bi translated">我们的俳句生成的第一步是允许用户上传他们想要的相关俳句的图片。为了识别俳句的主题，我们使用VGG16模型来识别图像中包含的对象。</p><p id="bbf7" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">首先，根据VGG16模型的要求对图像进行大小调整和归一化。第二，图像被输入VGG16模型，该模型从<em class="lc">火炬视觉</em>包装中导入，并带有预调整的重量。为了获得输入图像的单个标签，从所有可能的ImageNet标签中选择具有最高概率的标签。</p><h1 id="1456" class="jz ka hp bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">步骤2:使用GPT2生成俳句</h1><p id="0074" class="pw-post-body-paragraph im in hp io b ip kx ir is it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj hb bi translated">在VGG16模型返回分配给该图像的第一个标签后，我们使用NLTK英语停用词集随机生成第一个单词。然后，我们将这两个单词输入到GPT2模型中，以创建一个戏剧性的句子。使用python包Syllapy，我们计算了这个句子中的音节数，并将其限制在5个。如果添加由GPT2模型生成的下一个单词使音节数超过5，我们识别下一个单词的同义词或反义词，并随机选择其中一个符合必要音节数的选项。我们继续将第一个句子输入到GPT2模型中，以生成下一个7个音节的序列作为俳句的第二行。我们以同样的方式调整音节数。类似地，我们将第一行和第二行的组合输入到生成器中，以接收第三行，并限制其音节数为5。对于每张图片，我们生成了10种不同的俳句。</p><h1 id="f032" class="jz ka hp bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">步骤3:使用CLIP验证俳句</h1><p id="f47c" class="pw-post-body-paragraph im in hp io b ip kx ir is it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj hb bi translated">在Imagenet和GPT2基于一个图像输入共同制作了10个不同的俳句之后，我们使用CLIP从为特定图像生成的10个俳句中确定最佳俳句。CLIP将图像和俳句列表作为输入，并为每个俳句分配一个百分比，该百分比对应于与图像输入相关联的每个俳句的相对可能性。这些百分比的总和等于1，因为模型仅从给定的俳句中进行选择，以确定哪些俳句最有可能与图像相关联。我们的模型选择了从CLIP中获得最高概率的俳句作为每个图像输入的最终俳句。</p><h1 id="dbd2" class="jz ka hp bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">模型的简要背景</h1><h2 id="60a0" class="ld ka hp bd kb le lf lg kf lh li lj kj ix lk ll kn jb lm ln kr jf lo lp kv lq bi translated">VGG16</h2><p id="c1f1" class="pw-post-body-paragraph im in hp io b ip kx ir is it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj hb bi translated">用于为图像输入生成标签的算法是VGG16，这是一个提交给2014年ImageNet挑战赛的模型。VGG16是在ImageNet数据集上训练的卷积神经网络，可用于分类多达1000个不同的类别。模型的输入大小是224乘224，模型本身是16层深。该模型利用了几个3×3核，并且所有隐藏层利用了ReLU非线性。整个架构的示意图如下所示:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es lr"><img src="../Images/70818af88edf6543c8d4f04c8cdbea0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zqWIDMFyLfTd2IST"/></div></div></figure><h2 id="0803" class="ld ka hp bd kb le lf lg kf lh li lj kj ix lk ll kn jb lm ln kr jf lo lp kv lq bi translated">GPT2</h2><p id="5dd1" class="pw-post-body-paragraph im in hp io b ip kx ir is it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj hb bi translated">GPT2是最先进的大规模无监督语言生成模型。这是一个遵循多头自关注架构的转换器，具有编码器-解码器层堆栈，结合完全连接的前馈网络。这种类型的结构允许模型在给定文档中所有先前单词的情况下预测下一个单词，同时利用并行处理来提高计算速度。GPT2使用WebText进行预训练，WebText是来自人类创作的互联网页面的超过800万个文档的语料库。使用迁移学习，该模型可用于从可能的文本序列生成文本。它可以接收各种输入，从一个单词，到一个句子，到一个段落[4]。</p><p id="b3b2" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">GPT2已经可以通过huggingface.co[5]在线获得，这是一个开源的人工智能库。我们使用了gpt2-流派-故事生成器版本的GPT 2。这个特殊的变体是由用户pranavpsv开发的[1]。它用预先训练好的GPT2权重进行初始化，但根据不同类型的故事进行微调。给定一个类型，这个模型专门生成故事。</p><p id="5e58" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们最初尝试生成俳句时使用的是GPT2的原始形式。但是，输出的句子在主题中不一致，语法和标点符号使用不正确，导致句子不自然。我们选择使用故事生成器变体，使用戏剧流派输入选项，目的是生成更有意义和连贯的文本。虽然故事生成器GPT2并不完美，但它极大地改善了我们的结果。</p><h2 id="9ce1" class="ld ka hp bd kb le lf lg kf lh li lj kj ix lk ll kn jb lm ln kr jf lo lp kv lq bi translated">夹子</h2><p id="7225" class="pw-post-body-paragraph im in hp io b ip kx ir is it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj hb bi translated">对比语言-图像预训练(CLIP)是一种模型，它是一种预训练模型，用于找到哪个文本最适合特定图像。为了预训练剪辑，一组图像被馈送到图像编码器，该图像编码器生成n×1向量(n是图像的数量)，然后图像标签的列表被馈送到文本编码器，生成1×n向量(n是标签的数量)。计算两个向量中每个项目之间的点积的软最大值，以给出所有分类的概率。这个概率被解释为图像的最可能标签。在对模型进行预训练之后，可以将一幅单独的图像与一组标签一起输入到CLIP中。该模型将计算图像和所有标签之间的点积的softmax，以输出最能代表图像的文本。剪辑的直观表示如下图所示:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es ma"><img src="../Images/65b8ea014317b456cc853644f12eb04a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Z4Xs4tU26xmkIMme"/></div></div></figure><h2 id="e387" class="ld ka hp bd kb le lf lg kf lh li lj kj ix lk ll kn jb lm ln kr jf lo lp kv lq bi translated">结果！</h2><p id="4c99" class="pw-post-body-paragraph im in hp io b ip kx ir is it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj hb bi translated">这里是我们的模型生成的一些样本俳句。虽然远非完美，但它们相当有趣。</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mb"><img src="../Images/475a98166e9d31ca10d0f118015034ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SaNTpcvSyeVwW07MwQOUDg.jpeg"/></div></div><figcaption class="mc md et er es me mf bd b be z dx">ImageNet label: ‘suit’. Probability of association: 50.83%</figcaption></figure><figure class="ls lt lu lv fd hk er es paragraph-image"><div class="er es mg"><img src="../Images/c71312c6facc39f0d4d9b93f08a0184b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*3TpLYakL9tGQZsInYQ3ohw.png"/></div><figcaption class="mc md et er es me mf bd b be z dx">ImageNet label: ‘alp’. Probability of association: 29.17%</figcaption></figure><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mh"><img src="../Images/da30b725565363791a7159e07ec7fce0.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*w71HnrTUsAuTLp1HFLti9g.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx">ImageNet label: ‘racer’. Probability of association: 47.31%</figcaption></figure><h1 id="9783" class="jz ka hp bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">结论和未来工作</h1><p id="50f0" class="pw-post-body-paragraph im in hp io b ip kx ir is it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj hb bi translated">通过结合这三个模型，我们能够生成一些与感兴趣的图像相关的有趣的俳句。我们面临的主要挑战是创作有意义的俳句。由于我们的俳句是由GPT2的故事体裁变体产生的，它们往往缺乏诗意。更确切地说，它们读起来就像一个被必要音节所限制的故事。为了改善这一点，在俳句语料库上重新训练GPT2的最后几层可以帮助产生更多“诗意”的诗歌。</p><p id="7bc6" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">出现的其他问题与生产的标签和GPT2模型的质量有关。当描述图片的词不准确或太详细时，其他模型就很难产生和选择连贯的俳句。例如，当输入一只柯基犬的图像时，VGG16产生了“Pembroke”的标签——指的是一种特定品种的威尔士柯基犬。这个词对于GPT2模型来说太具体了，无法识别并产生无意义的文本。</p><p id="0cc1" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">此外，该模型产生了语法使用不准确的句子。如下面的输出所示，重复使用单词“is”是不自然的。第二，VGG16模型已经将这种仙人掌标记为图腾柱。</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div class="er es mi"><img src="../Images/2ee6fc268b0a4b59b9317952dafed616.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*5RUDNvCZneaCcUKuQb3zjw.png"/></div></figure><p id="1bea" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">由于沙漠很像一个岛屿，这个例子突出了CLIP将整个俳句连接到图像的能力。未来的改进可能与改进句子的语法结构有关。</p><p id="4ef1" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们面临的最后一个挑战与不自然的话题转换有关。俳句中的每一行都有意义，但是后面的行不会建立在前面的行之上。如前所述，在俳句上微调GPT2模型是解决这个问题的另一种方法。</p><p id="1c7f" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">总的来说，我们的模型写的俳句相当有趣。总结这篇文章的最好方式是展示一首不那么平庸的关于美味汉堡的俳句:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mj"><img src="../Images/f23014503925be93ec0ab8e3a1feb444.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XWaiYvZTK0hbZ0GWjcu9ng.png"/></div></div></figure><p id="ebd5" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hq">查看我们的代码:</strong> <a class="ae jk" href="https://github.com/indialindsay/Haiku_Generation" rel="noopener ugc nofollow" target="_blank">这里！</a></p><p id="9052" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">参考资料:</p><p id="d450" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">[1]普拉纳夫·瓦德雷沃。<a class="ae jk" href="https://towardsdatascience.com/generate-fresh-movie-stories-for-your-favorite-genre-with-deep-learning-143da14b29d6" rel="noopener" target="_blank">“用深度学习为你喜欢的类型生成新鲜的电影故事”</a></p><p id="d561" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">[2]丹尼斯·克里维茨基。<a class="ae jk" href="https://avoncourt.de/generation-poems-recurrent-neural-network/" rel="noopener ugc nofollow" target="_blank">“用递归神经网络生成诗歌”</a></p><p id="d07c" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">[3]马尔米克·潘迪亚。<a class="ae jk" href="https://www.researchgate.net/publication/313874773_NLP_based_Poetry_Analysis_and_Generation" rel="noopener ugc nofollow" target="_blank">《诗歌分析与生成》</a></p><p id="bb3b" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">[4] <a class="ae jk" href="https://towardsdatascience.com/attention-and-transformer-models-fe667f958378" rel="noopener" target="_blank">注意和变压器型号。“注意力是你所需要的全部”是Helene Kortschak |写给数据科学的一篇文章</a></p><p id="410b" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">【5】<a class="ae jk" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">拥抱脸——在解决NLP的任务中，一次一个承诺。</a></p></div></div>    
</body>
</html>
<html>
<head>
<title>How to use Bert-as-service on Google Colab</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在Google Colab上使用Bert-as-service</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/how-to-use-bert-as-service-on-google-colab-c6a6c32f434a?source=collection_archive---------12-----------------------#2021-07-27">https://medium.com/geekculture/how-to-use-bert-as-service-on-google-colab-c6a6c32f434a?source=collection_archive---------12-----------------------#2021-07-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="cea4" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">本文将带您完成完整的步骤，了解如何在Bert即服务的帮助下，使用预先训练好的Bert模型来使用迁移学习。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/5942cb0a42db6a28f7d4b4fdb9f6723b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dvSnQ4lsk9_C8gmY"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Photo by <a class="ae jn" href="https://unsplash.com/@element5digital?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Element5 Digital</a> on <a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="fc59" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">变形金刚在最近几年变得非常流行，并广泛用于各种NLP任务，如语言建模、语言翻译和问题回答。变形金刚可以理解句子中重要单词的上下文，从而给出最先进的结果。可以参考这篇<a class="ae jn" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj">文章</strong> </a>看看变压器是怎么工作的。</p><p id="f54c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">变形金刚擅长的领域之一是<strong class="jq hj">转移学习。</strong>人们可以使用预先训练好的变形金刚，如谷歌的Bert，并对其进行微调，以执行NLP任务。一个这样的任务是生成单词嵌入/句子嵌入，这是大多数NLP应用中的上游任务。这些嵌入保留了每个单词的上下文。例如，考虑这个句子:</p><p id="e3b4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">“这本书真牛逼。它会告诉你能以令人难以置信的低价预订的最佳旅游目的地。”</strong></p><p id="d7f3" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">使用转换器的单词嵌入在三个方面优于其他单词嵌入，如<strong class="jq hj"> ELMo </strong>、<strong class="jq hj"> Word2vec </strong>等。</p><p id="1b85" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">a)理解给定句子中每个单词与所有其他单词的关系。</p><p id="4ffd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">Transformer使用自我注意机制在给定句子中的两个或多个单词之间建立关系。例如在句子中，单词<strong class="jq hj">“it”</strong>“in”…..它会告诉……”指作者所说的<strong class="jq hj">书</strong>。</p><p id="e2a0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> b)为在句子中具有不同上下文的相同单词生成不同的单词嵌入。</strong></p><p id="3209" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">书</strong>这个词根据上下文有两种不同的意思。一个代表一个<strong class="jq hj">名词</strong>，另一个代表一个<strong class="jq hj">动词</strong>。转换器能够理解这一点，并为每个单词生成不同的单词嵌入。</p><p id="f27e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">与Elmo不同，变压器可以并行处理单词，而Elmo是以顺序方式处理单词，从而提高了效率。</p><p id="7eec" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在让我们看看如何在google colab中生成这些嵌入，好吗？！</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es kk"><img src="../Images/f966c629f9e0ed549932413a773004f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*l8yfBCJl2qIqikBe"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Photo by <a class="ae jn" href="https://unsplash.com/@adigold1?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Adi Goldstein</a> on <a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><p id="f528" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">首先，让我们从安装google drive开始，您将在那里存储您的文件。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ks kt l"/></div></figure><p id="cbdf" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">然后，我们加载我们的IMDB电影评论数据集，并预处理评论。这是一个二元分类数据集，带有正面或负面的评论。所以这是一个二元分类问题。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ks kt l"/></div></figure><p id="1270" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">首先，我们将数据分成训练集和验证集:X_train、X_test、y_train和y_test。一旦我们完成了这些，我们的数据就可以使用bert-as-service生成句子嵌入了。我们将为X_train和X_test生成句子嵌入。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ks kt l"/></div></figure><p id="9044" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">可选</strong> : <strong class="jq hj">为了更好的性能和准确性，训练和测试句子嵌入可以被标准化和缩放。</strong></p><p id="7425" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在是时候测试我们使用bert-as-service生成的句子嵌入了。为此，我们使用Keras创建了一个简单的神经网络，有四个隐藏层。并且我们在最终输出层使用了一个<strong class="jq hj"> sigmoid </strong>激活函数，损耗为<strong class="jq hj">“二元交叉熵”。</strong>输入的维度是<strong class="jq hj"> </strong>句子嵌入向量的长度。</p><pre class="iy iz ja jb fd ku kv kw kx aw ky bi"><span id="6371" class="kz la hi kv b fi lb lc l ld le"><strong class="kv hj">def</strong> create_model(X_train):<br/>    input_dim = X_train.shape[1]  <em class="lf"># Number of features</em><br/><br/>    model = Sequential()<br/>    model.add(layers.Dense(512, input_dim=input_dim, activation='relu'))<br/>    model.add(layers.Dropout(0.5))<br/>    model.add(layers.Dense(512, input_dim=input_dim, activation='relu'))<br/>    model.add(layers.Dropout(0.5))<br/>    model.add(layers.Dense(256, input_dim=input_dim, activation='relu'))<br/>    model.add(layers.Dropout(0.5))<br/>    model.add(layers.Dense(64, input_dim=input_dim, activation='relu'))<br/>    model.add(layers.Dense(1, activation='sigmoid')) <em class="lf"># activation = {sigmoid (for multi-label) ; softmax (for multi-class)} </em><br/><br/>    <em class="lf"># use binary_crossentropy for multi-label classification (classify more than one topics per document)</em><br/>    <em class="lf"># use categorical_crossentropy for multi-class classification (classify one topic per document)</em><br/>    model.compile(loss='binary_crossentropy', optimizer='adam', <br/>                  metrics=['accuracy'])<br/>    <br/>    <strong class="kv hj">return</strong> model</span></pre><p id="c5b0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在我们创建了模型之后，是时候训练模型了。我们将在验证集上保存具有最佳准确度分数的模型。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ks kt l"/></div></figure><p id="cc01" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">准确率达到了86%左右，相当不错。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lg"><img src="../Images/13f90fe3973761baa5d495a685082250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ePke5yuY7gJSMXcXbr_acw.png"/></div></div></figure><p id="5e16" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">结论:</strong></p><p id="d2b9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">转换器在处理作为NLP应用程序基础的上下文文本数据方面非常出色。Bert-as-service是一种非常简单而有效的方法，使用Bert作为服务，只需几行代码就可以生成预训练的句子和单词嵌入。</p><p id="26c0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">在这个github repo中找到完整的代码</strong>:<a class="ae jn" href="https://github.com/Sougotadayglo/Bert-serving-client.git" rel="noopener ugc nofollow" target="_blank">https://github.com/Sougotadayglo/Bert-serving-client.git</a></p><p id="3cf0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">参考文献:</strong></p><p id="59fb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">https://github.com/hanxiao/bert-as-service</p><p id="eb3d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">https://arxiv.org/abs/1706.03762</p><p id="2ff5" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">https://github.com/google-research/bert</p><p id="27e9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[<a class="ae jn" href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2017/08/transformer-novel-neural-network . html</a>]</p></div></div>    
</body>
</html>
<html>
<head>
<title>Support Vector Machines and Kernels</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机和核</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/support-vector-machines-and-kernels-8b064ee53fc3?source=collection_archive---------51-----------------------#2021-06-15">https://medium.com/geekculture/support-vector-machines-and-kernels-8b064ee53fc3?source=collection_archive---------51-----------------------#2021-06-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/7703341c0e6370e36b658d80a241f431.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gfaXJKO_EbFWnZA7"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx">Photo by <a class="ae hv" href="https://unsplash.com/@mockupgraphics?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Mockup Graphics</a> on <a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="f413" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是我正在工作的系列的第七部分，在其中我们将讨论和定义介绍性的机器学习算法和概念。在这篇文章的最后，你会找到这个系列的所有前几篇文章。我建议你按顺序读这些。原因很简单，因为我在那里介绍了一些概念，这些概念对于理解本文中讨论的概念至关重要，我将在许多场合引用它们。</p><p id="3af6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">今天我们来看看我们最后的监督学习算法，支持向量机(SVMs)。我们将研究直觉，以及如何使用内核的概念来提出复杂的决策边界。</p><p id="37a7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们开始吧。</p><h1 id="dd18" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">支持向量机</h1><p id="329f" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">支持向量机广泛应用于回归和分类问题。为了理解对支持向量机的需求，考虑以下具有两个特征和两个类别(Xs和Os)的分类问题:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kw"><img src="../Images/d19054f3df16f15a5c1f2811972c387e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rg6VJg8IcORYfzXuUZZEMQ.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 1: </strong>Classification Problem with Two Features and Two Classes</figcaption></figure><p id="5cac" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们两个班的分界线是什么？有许多选项:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lb"><img src="../Images/55c0d8b6559702f487bdd0d9859ac113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vatkw-VpjJSFzubiSvyrQA.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 2: </strong>Possible Lines Fitting the Classification Problem</figcaption></figure><p id="e1ae" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在决定使用哪条线最好之前，我们需要想出一些标准来帮助我们区分好的和不好的。在上一篇文章中，我们已经看到了其中的一些标准。以下是一些例子:</p><ul class=""><li id="fcaf" class="lc ld hy ix b iy iz jc jd jg le jk lf jo lg js lh li lj lk bi translated">不要吃太多</li><li id="748b" class="lc ld hy ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated">不是吃不饱</li><li id="748f" class="lc ld hy ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated">对新数据进行很好的概括</li></ul><p id="fb95" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在考虑以下两个例子:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lq"><img src="../Images/b3ea82dca65f7934cd4a1c8ba453671b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*38leGcK3-OzLTMCwbteIBQ.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 3:</strong> Different Possibilities of Fitting a Line Through the Same Points</figcaption></figure><p id="0862" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">并为每个添加新的数据点:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lr"><img src="../Images/2beb3043a9abc50f8a11daeb237b253c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IFXMkETN-MBQKd2m2tkXLQ.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 4:</strong> Demonstration of How Different Graphs Generalize To New Data</figcaption></figure><p id="fefb" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">哪个图比较好？肯定是右边的那个。左边的一个错误地将X分类为O，而右边的一个与训练点的距离足够大，可以推广到新数据。这就是SVM算法的目标:</p><blockquote class="ls lt lu"><p id="b116" class="iv iw lv ix b iy iz ja jb jc jd je jf lw jh ji jj lx jl jm jn ly jp jq jr js hb bi translated">在N维空间中找到一个超平面，该超平面清楚地分类数据点，其中N是特征的数量</p></blockquote><p id="499d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看看如何。</p><h2 id="db00" class="lz ju hy bd jv ma mb mc jz md me mf kd jg mg mh kh jk mi mj kl jo mk ml kp mm bi translated">直觉</h2><p id="22d9" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">像往常一样，我们理解任何机器学习算法的第一步将是看它的成本函数。在本系列的第三部分<a class="ae hv" rel="noopener" href="/geekculture/logistic-regression-using-gradient-descent-intuition-and-implementation-36a8498afdcb">使用梯度下降的逻辑回归:直觉和实现</a>中，我们看到了逻辑回归算法的成本函数:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mn"><img src="../Images/581debd8855359ff4eb12d006b7e8a40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xDD_XGQj1D_R6k77.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 1:</strong> Logistic Regression Cost Function</figcaption></figure><p id="9d28" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个等式告诉我们，每个训练示例贡献的成本为:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es mo"><img src="../Images/bdf9ecf5f27502bfb66a7b8298e3010e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*_Te19pZyJVSjIaZAhxIK2w.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 2: </strong>Cost of Every Individual Training example</figcaption></figure><p id="09e1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中<code class="du mp mq mr ms b">h</code>是逻辑函数:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es mt"><img src="../Images/72574dbb07607b636fc097bfc0694599.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*n79R2pJzl0Rjch2PQ_1ATg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 3:</strong> Logistic Function Used in Logistic Regression</figcaption></figure><p id="c514" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当<code class="du mp mq mr ms b">y^(i) = 1</code>时，<strong class="ix hz">等式2 </strong>的右边将等于零，留给我们:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es mu"><img src="../Images/609ebac5fc80b817b21cf228bfdc1820.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*oVWN1sBiqZwZX77420XrUw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 4:</strong> Cost When y=1</figcaption></figure><p id="8157" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以画出<strong class="ix hz">等式4 </strong>的图表，以便更直观地了解它的行为方式:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mv"><img src="../Images/9ebdd3cfb6250aa3a5d780720ceadecf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tjDnNL7ryh0gFsFazqR_Qg.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 5:</strong> Graph of Logistic Regression Cost Function When y=1</figcaption></figure><p id="a74c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们看到<code class="du mp mq mr ms b">Theta^T * X</code>必须远大于零，我们的模型才能产生值1。支持向量机的成本函数将以同样的方式运行。然而，SVM将增加更硬的约束，要求<code class="du mp mq mr ms b">Theta^T * X</code>大于1，而不是必须大于0。这是大概的样子:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mw"><img src="../Images/211d5c3282d8d84413d71c106bfce4b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DOP0KH_20cpsH1d04TS63w.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 6:</strong> Graph of SVM Cost Function When y=1</figcaption></figure><p id="9113" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们称之为<code class="du mp mq mr ms b">Cost_1(Theta^T * X)</code>。我们可以对等式2 中的<code class="du mp mq mr ms b">y=0</code>进行相同的精确分析:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mx"><img src="../Images/11d9c085f0ab47b62307a293de8b0bb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VA79aXE2VFapvJtUObOWRg.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 7:</strong> Graph of SVM Cost Function When y=0</figcaption></figure><p id="1d97" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将把这个案例称为<code class="du mp mq mr ms b">Cost_0(Theta^T * X)</code>。在我们把所有这些放在一起之前，我们需要做最后一个评论。</p><p id="bf00" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们在研究神经网络时发现，可以在成本函数中添加一个正则化参数来防止过度拟合。事实是，这个正则化参数可以并且应该用于所有的成本函数。例如，下面的<strong class="ix hz">等式1 </strong>添加了正则化参数:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es my"><img src="../Images/8c30a051b40c5cada3c6e50711d3baf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PvdrBSZ2yYhPntmtfwlJxg.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 4:</strong> Logistic Regression Cost Function With Regularization Parameter</figcaption></figure><p id="16ad" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当使用支持向量机时，正则化的概念再次被使用，但是约定是使用稍微不同的符号。该模型旨在最小化以下功能:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mz"><img src="../Images/8a1ab3d07453e5450c4586062a19ec2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6D28ytYtGbHiowI1IzDUiQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 5:</strong> SVM Cost Function With Regularization Parameter</figcaption></figure><p id="dc1e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注意，我们仍然对<code class="du mp mq mr ms b">Theta</code>向量的平方求和，只是不再乘以<code class="du mp mq mr ms b">lambda/2m</code>。相反，我们乘以一个常数<code class="du mp mq mr ms b">C</code>。我们不会讨论我们为什么这样做，但请记住，这仍然是为了实现同样的事情。</p><p id="4c69" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们到目前为止看到的所有例子中，我们能够用一条简单的直线来划分不同的类。当然，情况不会总是这样。例如，考虑下面的分类问题:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es na"><img src="../Images/2d8239572081c1a331b789999ec70ca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*odms9ZD-EFT4gMj8bMEy3g.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 8: </strong>Complex Arrangement of Training Points</figcaption></figure><p id="8ada" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们如何让我们的算法得出一个足够复杂的决策边界，能够将积极的类别与消极的类别分开，就像这样:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nb"><img src="../Images/4d7c0d6b12a123c84718b1e1758e022e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8rYseFofeW9cLKae3ohxlw.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 8: </strong>Decision Boundary</figcaption></figure><p id="ed49" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">答案？<strong class="ix hz">内核</strong>。</p><h1 id="a9fb" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">核</h1><p id="0400" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">当我们在本系列的第4.1部分研究神经网络时，我们已经研究过这个问题。当时，我们想要一个不涉及高度有序方程的解决方案。</p><p id="3c2a" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">SVM以不同的方式处理复杂的决策界限。通常，我们模型中考虑的函数的形式是<code class="du mp mq mr ms b">Thetas</code>乘以<code class="du mp mq mr ms b">Xs</code>的任意阶。例如:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es nc"><img src="../Images/1d87897c357fc68d5e3799d78810353c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/0*fXjyeyO3VNWRVbkL.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 5: </strong>Arbitrary Example of A Function To Be Optimized</figcaption></figure><p id="2204" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这一次，我们的<code class="du mp mq mr ms b">Xs</code>将以不同的方式计算。我们将有<code class="du mp mq mr ms b">f_1</code>、<code class="du mp mq mr ms b">f_2</code>、<code class="du mp mq mr ms b">f_3</code>等等，而不是<code class="du mp mq mr ms b">x_1</code>和<code class="du mp mq mr ms b">x_2</code>。这种符号上的变化用来演示我们函数的输入现在必须通过第二个函数。要理解这一点，请看下图:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nd"><img src="../Images/9025895365b3c5d60c3d6b12228f51ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q8mxDA1MYCILfCqsdalWVQ.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 9: </strong>Landmarks</figcaption></figure><p id="a483" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du mp mq mr ms b">l</code>这里被称为<strong class="ix hz">地标</strong>。这些地标充当“门槛”。每个标志和训练点之间的距离将使用我们选择的距离公式来计算。我们将这个距离方程称为<strong class="ix hz">相似性函数</strong>。相似性函数是计算任意两点之间的相似性(距离)的任何函数。我们也称之为<strong class="ix hz">内核</strong>。例如，我们可以使用<strong class="ix hz">高斯核</strong>来计算我们的训练点和我们在<strong class="ix hz">图9 </strong>中突出显示的地标之间的相似性:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lr"><img src="../Images/99f988ffc4290ae9c8024503a28a911c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XkYS9FiaXCBm7kN7e6NMug.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 10: </strong>Example of Gaussian Kernel Used to Calculate Similarity</figcaption></figure><p id="198c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们通过一个例子来全面了解这一点。我们仍然在使用<strong class="ix hz">图9 </strong>中的图表，但是让我们给它添加一些训练点:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ne"><img src="../Images/a33f13a85b49f585873cb86c85c283da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nMW4H3oe8FTsUcvVbh0Clw.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 10: </strong>Landmarks and Training points</figcaption></figure><p id="c132" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，假设我们想要使用以下等式来拟合此图:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es nf"><img src="../Images/6d1620d6136e6c4adba427ad26a31a33.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*mzXgfhHd8F-VqQGiZPPKJw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 6: </strong>Equation We Use to Fit Our Training Data</figcaption></figure><p id="1ec0" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在训练完这个模型后，我们得到了<code class="du mp mq mr ms b">Theta_0 = -0.5</code>、<code class="du mp mq mr ms b">Theta_1 = 1</code>、<code class="du mp mq mr ms b">Theta_2 = 1</code>和<code class="du mp mq mr ms b">Theta_3 = 0</code>。</p><p id="03af" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">训练点<code class="du mp mq mr ms b">1</code>靠近地标1，所以<code class="du mp mq mr ms b">f_1</code>将近似等于1(因为<code class="du mp mq mr ms b">e^0 = 1</code>并且我们在这里使用高斯核)。类似地，由于训练点<code class="du mp mq mr ms b">1</code>远离地标2和地标3，<code class="du mp mq mr ms b">f_2</code>和<code class="du mp mq mr ms b">f_3</code>将近似等于零。因此，<strong class="ix hz">等式6 </strong>将等于<code class="du mp mq mr ms b">-0.5 + 1 = 0.5 &gt;= 0</code>，因此我们预测训练点1的<code class="du mp mq mr ms b">one</code>。如果对第二点和第三点进行相同的计算，您将得出结论，即所有靠近界标1和界标2的点将产生预测值1，而所有靠近界标3的点将产生预测值0。因此，我们可以近似以下决策边界:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ng"><img src="../Images/33d885ae3abdbe2210d0a36285ae5a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rEauMplLIFgTaAvYPwlHcg.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 11: </strong>Landmarks, Training points and Decision Boundary</figcaption></figure><h1 id="12ce" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结论</h1><p id="87b1" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在本文中，我们介绍了支持向量机算法。在该算法中，目标是选择一个到训练点距离最大的决策边界。我们展示了当我们向我们的模型呈现新数据时，为什么需要大的距离来防止错误发生。</p><p id="f2e1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们还探讨了核的概念，以及在我们的训练数据不可线性分离的情况下，如何使用核来提出更复杂的决策边界。</p><p id="a4c8" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在下一篇文章中，我们开始讨论<strong class="ix hz">无监督学习算法</strong>。在此之前，我留给你以下几点思考:</p><ul class=""><li id="26a6" class="lc ld hy ix b iy iz jc jd jg le jk lf jo lg js lh li lj lk bi translated">我们看到了如何使用地标来提出更复杂的决策边界。我们如何选择地标的位置？这是一个很难自己想出来的概念，所以我建议你参考<a class="ae hv" href="https://www.youtube.com/watch?v=XfyR_49hfi8" rel="noopener ugc nofollow" target="_blank">吴恩达在这方面的经验</a>。</li><li id="ba0e" class="lc ld hy ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated">如果我们已经有了神经网络，为什么还需要支持向量机？</li><li id="ed64" class="lc ld hy ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated">有几种不同的方法来理解支持向量机背后的直觉。在本文中，我们通过首先查看我们的逻辑回归实现，然后以一种可以与SVM一起使用的方式操作它，从而获得了一种理解。另一种方式是用向量操作来看它。这里有一个<a class="ae hv" href="https://www.youtube.com/watch?v=_PwhiWxHK8o&amp;t=2592s&amp;ab_channel=ArtificialIntelligence-AllinOneArtificialIntelligence-AllinOne" rel="noopener ugc nofollow" target="_blank">的精彩演讲，从这个角度描述了支持向量机</a>。</li></ul><h1 id="31c6" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">过去的文章</h1><ol class=""><li id="5841" class="lc ld hy ix b iy kr jc ks jg nh jk ni jo nj js nk li lj lk bi translated"><strong class="ix hz">第一部分:</strong> <a class="ae hv" href="https://ali-h-khanafer.medium.com/data-pre-processing-ee81bbe5cc77" rel="noopener">数据预处理</a></li><li id="3cf8" class="lc ld hy ix b iy ll jc lm jg ln jk lo jo lp js nk li lj lk bi translated"><strong class="ix hz">第二部分:</strong> <a class="ae hv" href="https://ali-h-khanafer.medium.com/linear-regression-using-gradient-descent-intuition-and-implementation-522d43453fc3" rel="noopener">使用梯度下降的线性回归:直觉和实现</a></li><li id="9b24" class="lc ld hy ix b iy ll jc lm jg ln jk lo jo lp js nk li lj lk bi translated"><strong class="ix hz">第三部分:</strong> <a class="ae hv" rel="noopener" href="/geekculture/logistic-regression-using-gradient-descent-intuition-and-implementation-36a8498afdcb">梯度下降逻辑回归:直觉与实现</a></li><li id="a5c5" class="lc ld hy ix b iy ll jc lm jg ln jk lo jo lp js nk li lj lk bi translated"><strong class="ix hz">第四部分— 1: </strong> <a class="ae hv" rel="noopener" href="/geekculture/neural-networks-part-1-terminology-motivation-and-intuition-73675fc43947">神经网络第一部分:术语、动机和直觉</a></li><li id="6c99" class="lc ld hy ix b iy ll jc lm jg ln jk lo jo lp js nk li lj lk bi translated"><strong class="ix hz">第四部分— 2: </strong> <a class="ae hv" rel="noopener" href="/geekculture/neural-networks-part-2-backpropagation-and-gradient-checking-4f8d1350fb0b">神经网络第二部分:反向传播和梯度检测</a></li><li id="256f" class="lc ld hy ix b iy ll jc lm jg ln jk lo jo lp js nk li lj lk bi translated"><strong class="ix hz">第六部分:</strong> <a class="ae hv" rel="noopener" href="/geekculture/evaluating-your-hypothesis-and-understanding-bias-vs-variance-86512cce4253">评估你的假设，理解偏差与方差</a></li></ol><h1 id="9c0e" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">无耻的插头</h1><ul class=""><li id="43b9" class="lc ld hy ix b iy kr jc ks jg nh jk ni jo nj js lh li lj lk bi translated">【twitter.com/ali_khanafer2】推特: <a class="ae hv" href="https://twitter.com/ali_khanafer2" rel="noopener ugc nofollow" target="_blank">推特</a></li><li id="a00c" class="lc ld hy ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated"><strong class="ix hz">领英</strong>:<a class="ae hv" href="https://www.linkedin.com/in/ali-khanafer-319382152/" rel="noopener ugc nofollow" target="_blank">linkedin.com/in/ali-khanafer-319382152/</a></li></ul><h1 id="37bb" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">参考</h1><ol class=""><li id="d9f8" class="lc ld hy ix b iy kr jc ks jg nh jk ni jo nj js nk li lj lk bi translated"><a class="ae hv" href="https://www.coursera.org/learn/machine-learning?page=1" rel="noopener ugc nofollow" target="_blank">吴恩达的机器学习Coursera课程</a></li><li id="0dbd" class="lc ld hy ix b iy ll jc lm jg ln jk lo jo lp js nk li lj lk bi translated"><a class="ae hv" href="https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47" rel="noopener" target="_blank"> Rohith Gandhi的支持向量机——机器学习算法介绍</a></li><li id="5a1e" class="lc ld hy ix b iy ll jc lm jg ln jk lo jo lp js nk li lj lk bi translated"><a class="ae hv" href="https://www.youtube.com/watch?v=_PwhiWxHK8o&amp;t=2592s&amp;ab_channel=ArtificialIntelligence-AllinOneArtificialIntelligence-AllinOne" rel="noopener ugc nofollow" target="_blank">麻省理工学院开放课件的学习:支持向量机</a></li></ol></div></div>    
</body>
</html>
<html>
<head>
<title>Auto-code generation using GPT-2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用GPT-2的自动代码生成</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/auto-code-generation-using-gpt-2-4e81cb05430?source=collection_archive---------8-----------------------#2021-06-16">https://medium.com/geekculture/auto-code-generation-using-gpt-2-4e81cb05430?source=collection_archive---------8-----------------------#2021-06-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="3899" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">用人工智能更快地编码</h2><div class=""/><h1 id="889a" class="io ip hi bd iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl bi translated">关于GPT-2</h1><blockquote class="jm"><p id="e735" class="jn jo hi bd jp jq jr js jt ju jv jw dx translated">GPT-2代表“<strong class="ak">生成预测变换器</strong>”。这是一个开源模型，经过超过15亿个参数的训练，用于生成给定序列的下一个文本序列。</p><p id="1488" class="jn jo hi bd jp jq jr js jt ju jv jw dx translated">GPT-2具有生成文本的非凡能力，远远超出了传统语言模型的预期。</p></blockquote><h1 id="54da" class="io ip hi bd iq ir is it iu iv iw ix iy iz jx jb jc jd jy jf jg jh jz jj jk jl bi translated">“太危险了，不能释放。”</h1><h2 id="adcd" class="ka ip hi bd iq kb kc kd iu ke kf kg iy kh ki kj jc kk kl km jg kn ko kp jk ho bi translated">一个短语发布了OpenAI的新闻声明，以配合他们在2019年2月发布的GPT-2语言模型。</h2><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es kq"><img src="../Images/9bad599c2bd2a2cbd93f59879ff868e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*BQ5pUlbr_wYOrzVSdPbOTQ.jpeg"/></div></figure><blockquote class="ky kz la"><p id="3db9" class="lb lc ld le b lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly jw hb bi translated">在今天的后事实信息生态系统中，人工智能驱动的错误信息的威胁已经成为一个尚未解决的巨大问题，特别是最近发布了更强大的GPT-3。</p></blockquote><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es lz"><img src="../Images/a3f9fca0eadedd1d5d3db056c51495f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3DxtoxyhZZddjRVg.jpeg"/></div></div><figcaption class="me mf et er es mg mh bd b be z dx">Performance of GPT on different datasets</figcaption></figure><blockquote class="jm"><p id="7599" class="jn jo hi bd jp jq mi mj mk ml mm jw dx translated">"今天，联合国呼吁立即从世界上撤走所有核武器。"</p></blockquote><p id="f6b9" class="pw-post-body-paragraph lb lc hi le b lf mn lh li lj mo ll lm kh mp lp lq kk mq lt lu kn mr lx ly jw hb bi translated"><strong class="le hs"> <em class="ld">你刚才看的这句话既不是我写的，也不是编辑写的。这句话是GPT写的——2</em>T5】</strong></p><blockquote class="ky kz la"><p id="0624" class="lb lc ld le b lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly jw hb bi translated">GPT-2是一个基于1.5位转换器的语言模型，在800万网页的数据库中训练。它被训练来简单地预测40GB互联网文本中的下一个单词。由于一些问题，研究人员发布了一个非常小的模型进行实验。</p></blockquote><h1 id="9a4f" class="io ip hi bd iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl bi translated">GPT-2的工作机制</h1><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es lz"><img src="../Images/dd1453c43f951eab99009cfc96eab3bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UmykBKt7OHzTSUoG.png"/></div></div><figcaption class="me mf et er es mg mh bd b be z dx">Working og GPT-2 [<a class="ae ms" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank">Source</a>]</figcaption></figure><ul class=""><li id="b3f5" class="mt mu hi le b lf lg lj lk kh mv kk mw kn mx jw my mz na nb bi translated"><strong class="le hs">生成:</strong>这意味着模型被训练来预测给定记号序列中的下一个记号。该模型被给予大量原始文本，然后被要求使用统计特征生成更多文本，这当然涉及不同层和机制的实现，例如RNN-LSTM和注意力机制。</li><li id="5142" class="mt mu hi le b lf nc lj nd kh ne kk nf kn ng jw my mz na nb bi translated"><strong class="le hs">预训练:</strong> OpenAI训练了一个庞大而强大的语言转换器模型，将其用于摘要、神经机器翻译等任务。现在，这个模型在40 GB的文本上被训练，被称为WebText。</li></ul><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es lz"><img src="../Images/c905d1acfe803e82291eec135aadd858.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SaSwwhWtd04v0Ys0.png"/></div></div><figcaption class="me mf et er es mg mh bd b be z dx">Transformer architecture</figcaption></figure><ul class=""><li id="e130" class="mt mu hi le b lf lg lj lk kh mv kk mw kn mx jw my mz na nb bi translated"><strong class="le hs">变形金刚:</strong>GPT-2是用多层解码变形金刚建造的。</li></ul><figure class="kr ks kt ku fd kv er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es nh"><img src="../Images/8b3804cfe5dcc938cfcc5e1607aaa841.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PIFgY3d16zYH37k1.jpg"/></div></div><figcaption class="me mf et er es mg mh bd b be z dx">The text in blue is given as an input and the text in red is the predicted output</figcaption></figure><p id="38b1" class="pw-post-body-paragraph lb lc hi le b lf lg lh li lj lk ll lm kh lo lp lq kk ls lt lu kn lw lx ly jw hb bi translated"><strong class="le hs">GPT-2架构基于变形金刚的概念。</strong></p><p id="8c61" class="pw-post-body-paragraph lb lc hi le b lf lg lh li lj lk ll lm kh lo lp lq kk ls lt lu kn lw lx ly jw hb bi translated">GPT-2的工作机制涉及基于变换器的编码器解码器架构，以学习输入和输出相关性。</p><p id="ae14" class="pw-post-body-paragraph lb lc hi le b lf lg lh li lj lk ll lm kh lo lp lq kk ls lt lu kn lw lx ly jw hb bi translated">为了生成给定序列中的下一个输出，模型需要将之前生成的数据作为输入。</p><ul class=""><li id="7a97" class="mt mu hi le b lf lg lj lk kh mv kk mw kn mx jw my mz na nb bi translated">GPT-2有很强的适应文本环境的能力，从而产生现实和连贯的输出。</li><li id="5f6d" class="mt mu hi le b lf nc lj nd kh ne kk nf kn ng jw my mz na nb bi translated">该模型的工作原理是在创建时将每个令牌添加到输入序列中。在下一步中，新的序列成为模型的输入。这是一个叫做<strong class="le hs">“自动回归”</strong>的想法。这是使RNNs不合理地有效的想法之一。</li></ul><h1 id="2523" class="io ip hi bd iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl bi translated">代码自动生成是什么意思？</h1><h2 id="75f0" class="ka ip hi bd iq kb kc kd iu ke kf kg iy kh ki kj jc kk kl km jg kn ko kp jk ho bi translated">自动代码生成基本上包括基于用户先前的输入和过去的习惯完成一系列代码。</h2><ul class=""><li id="6b24" class="mt mu hi le b lf ni lj nj kh nk kk nl kn nm jw my mz na nb bi translated">许多商业平台，如<strong class="le hs"> TabNine和Kite </strong>已经可以在市场上完成这项任务。这两种方法都使用GPT-2根据用户提供的先前输入来预测下一个代码序列。</li><li id="db67" class="mt mu hi le b lf nc lj nd kh ne kk nf kn ng jw my mz na nb bi translated">下面是一个简短的视频，展示了自动化代码生成过程的惊人能力:</li></ul><figure class="kr ks kt ku fd kv"><div class="bz dy l di"><div class="nn no l"/></div><figcaption class="me mf et er es mg mh bd b be z dx">A video demonstration by Tabnine of automatic code generation</figcaption></figure><h1 id="369c" class="io ip hi bd iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl bi translated">为代码生成微调GPT-2的步骤:</h1><h2 id="9417" class="ka ip hi bd iq kb kc kd iu ke kf kg iy kh ki kj jc kk kl km jg kn ko kp jk ho bi translated">1.克隆所需的存储库</h2><ul class=""><li id="ab89" class="mt mu hi le b lf ni lj nj kh nk kk nl kn nm jw my mz na nb bi translated">在这里，我们克隆了auto_coding存储库，其中包含代码和脚本，用于微调GPT-2模型的自动代码生成。</li><li id="dd19" class="mt mu hi le b lf nc lj nd kh ne kk nf kn ng jw my mz na nb bi translated">我们需要以脚本的形式提供训练示例(示例:Python、C、C++、Java和Javascript)。</li><li id="6dbb" class="mt mu hi le b lf nc lj nd kh ne kk nf kn ng jw my mz na nb bi translated">为了微调我们的GPT-2模型，我们使用了scikit-learn示例中的脚本。</li></ul><pre class="kr ks kt ku fd np nq nr ns aw nt bi"><span id="075d" class="ka ip hi nq b fi nu nv l nw nx">!rm -rf auto_coding</span><span id="ee19" class="ka ip hi nq b fi ny nv l nw nx">!git clone <a class="ae ms" href="https://github.com/aasthaengg/auto_coding" rel="noopener ugc nofollow" target="_blank">https://github.com/aasthaengg/auto_coding</a> &amp;&amp; cd auto_coding &amp;&amp; pip install -r requirements.txt &amp;&amp; cd dataset &amp;&amp; git clone <a class="ae ms" href="http://github.com/pytorch/examples.git" rel="noopener ugc nofollow" target="_blank">http://github.com/scikit-learn/examples.git</a> &amp;&amp; python convert.py — segment_len 256 — stride 10 — dev_size 0.1</span></pre><h2 id="ffb9" class="ka ip hi bd iq kb kc kd iu ke kf kg iy kh ki kj jc kk kl km jg kn ko kp jk ho bi translated"><strong class="ak"> 2。下载所需脚本</strong></h2><p id="2589" class="pw-post-body-paragraph lb lc hi le b lf ni lh li lj nj ll lm kh nz lp lq kk oa lt lu kn ob lx ly jw hb bi translated">现在，我们正在下载<strong class="le hs">的convert.py脚本。</strong></p><p id="3c21" class="pw-post-body-paragraph lb lc hi le b lf lg lh li lj lk ll lm kh lo lp lq kk ls lt lu kn lw lx ly jw hb bi translated">该脚本包含将我们的示例转换为模型所期望格式的训练数据的代码。</p><pre class="kr ks kt ku fd np nq nr ns aw nt bi"><span id="df1b" class="ka ip hi nq b fi nu nv l nw nx">!wget <a class="ae ms" href="https://raw.githubusercontent.com/aasthaengg/auto_coding/master/dataset/convert.py" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/aasthaengg/auto_coding/master/dataset/convert.py</a> &amp;&amp; cp convert.py /content/auto_coding/dataset/</span></pre><h2 id="86a7" class="ka ip hi bd iq kb kc kd iu ke kf kg iy kh ki kj jc kk kl km jg kn ko kp jk ho bi translated">3.导航到所需的方向</h2><p id="e997" class="pw-post-body-paragraph lb lc hi le b lf ni lh li lj nj ll lm kh nz lp lq kk oa lt lu kn ob lx ly jw hb bi translated">这里，我们正在执行convert.py脚本，其中<strong class="le hs">段长度为256，步幅为10，开发规模为10%。</strong></p><p id="ecd1" class="pw-post-body-paragraph lb lc hi le b lf lg lh li lj lk ll lm kh lo lp lq kk ls lt lu kn lw lx ly jw hb bi translated">90%的数据将用于训练，剩下的10%将用于测试我们的模型。</p><pre class="kr ks kt ku fd np nq nr ns aw nt bi"><span id="a378" class="ka ip hi nq b fi nu nv l nw nx">!cd auto_coding &amp;&amp; cd dataset &amp;&amp; python convert.py — segment_len 256 — stride 10 — dev_size 0.1</span></pre><h2 id="72cf" class="ka ip hi bd iq kb kc kd iu ke kf kg iy kh ki kj jc kk kl km jg kn ko kp jk ho bi translated"><strong class="ak"> 4。执行训练脚本并选择模型为distilgpt2 </strong></h2><p id="fd0a" class="pw-post-body-paragraph lb lc hi le b lf ni lh li lj nj ll lm kh nz lp lq kk oa lt lu kn ob lx ly jw hb bi translated">我们有不同版本的模型可供微调。这里，选择<strong class="le hs"> distilgpt2 </strong>进行微调。如果一个人有足够的计算资源和庞大的数据集，那么他们可能会选择有更多参数的版本。</p><pre class="kr ks kt ku fd np nq nr ns aw nt bi"><span id="f8db" class="ka ip hi nq b fi nu nv l nw nx">!cd /content/auto_coding &amp;&amp; python train.py — model_select distilgpt2</span></pre><h2 id="eee7" class="ka ip hi bd iq kb kc kd iu ke kf kg iy kh ki kj jc kk kl km jg kn ko kp jk ho bi translated">5.模型现在已经训练好了。让我们看看这个模型</h2><p id="4299" class="pw-post-body-paragraph lb lc hi le b lf ni lh li lj nj ll lm kh nz lp lq kk oa lt lu kn ob lx ly jw hb bi translated">训练get完成后，我们需要执行<strong class="le hs"> interact.py脚本。</strong>该脚本将运行模型进行测试。需要提供输入，模型将预测序列。</p><pre class="kr ks kt ku fd np nq nr ns aw nt bi"><span id="851c" class="ka ip hi nq b fi nu nv l nw nx">!python interact.py</span></pre><h1 id="4b42" class="io ip hi bd iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl bi translated">这个用例如何帮助技术专业人员？</h1><ul class=""><li id="0919" class="mt mu hi le b lf ni lj nj kh nk kk nl kn nm jw my mz na nb bi translated">该解决方案已经在行业中使用。它允许开发者以47%的击键次数更快地编写代码。这有助于开发人员提高工作效率。</li><li id="7379" class="mt mu hi le b lf nc lj nd kh ne kk nf kn ng jw my mz na nb bi translated">此外，如果一个代码块已经提前写好了，那么用户只需要用几笔相同的单词，用户就会得到一个自动完成的代码块。</li><li id="b2a3" class="mt mu hi le b lf nc lj nd kh ne kk nf kn ng jw my mz na nb bi translated">根据一些评论，这帮助开发人员减少了大量时间，因为他们只编写了70%到80%的代码，剩下的20%到30%的代码是自动生成的。</li><li id="48e7" class="mt mu hi le b lf nc lj nd kh ne kk nf kn ng jw my mz na nb bi translated">凭借所有这些优势，该行业还节省了大量时间。</li></ul><h1 id="ea2e" class="io ip hi bd iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl bi translated">GPT新协议的局限性</h1><ul class=""><li id="ef6c" class="mt mu hi le b lf ni lj nj kh nk kk nl kn nm jw my mz na nb bi translated">GPT-2不能用于处理复杂和长的语言结构。</li></ul><p id="6b8c" class="pw-post-body-paragraph lb lc hi le b lf lg lh li lj lk ll lm kh lo lp lq kk ls lt lu kn lw lx ly jw hb bi translated">如果有人想生成一个与特定领域(如文学、金融或医学)相关的文本序列，那么它将无法很好地执行。</p><ul class=""><li id="75c1" class="mt mu hi le b lf lg lj lk kh mv kk mw kn mx jw my mz na nb bi translated">在计算资源方面存在某些限制。为了训练这样一个具有数十亿个参数的庞大模型，我们需要非常昂贵的计算资源来训练，以便模型能够更好地执行。</li></ul><h1 id="633a" class="io ip hi bd iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl bi translated">作者对GPT建筑的看法</h1><ul class=""><li id="1588" class="mt mu hi le b lf ni lj nj kh nk kk nl kn nm jw my mz na nb bi translated">GPT无疑是自然语言处理领域的一项惊人发明，但它的能力仍然未知，因为由于一些严重的威胁，OpenAI从未发布完整的预训练模型。</li><li id="4c65" class="mt mu hi le b lf nc lj nd kh ne kk nf kn ng jw my mz na nb bi translated">为了在自动代码生成上执行这个任务，我使用了distilt-GPT 2，因为它体积小，模型微调相对便宜。为了更有效和清晰的预测，我们将序列生成大小限制为15。</li><li id="2c82" class="mt mu hi le b lf nc lj nd kh ne kk nf kn ng jw my mz na nb bi translated">如果有人想到使用具有更多参数的GPT-2模型，那么序列的大小可以相应地增加，同时考虑到计算资源。</li></ul><figure class="kr ks kt ku fd kv er es paragraph-image"><div class="er es oc"><img src="../Images/6c6ddcc2746254113c2fdebb75bb2960.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/0*jzzbjTcYcSn6X2Xt"/></div><figcaption class="me mf et er es mg mh bd b be z dx">Here is a tweet from VP of AI at Facebook</figcaption></figure><p id="a374" class="pw-post-body-paragraph lb lc hi le b lf lg lh li lj lk ll lm kh lo lp lq kk ls lt lu kn lw lx ly jw hb bi translated">我很清楚额外的风险，例如GPT-3的类似人类的文本生成能力被用于网络钓鱼、诈骗、垃圾邮件、散布虚假新闻或其他欺诈行为的可能性。因此，在使用这些模型时，应该牢记道德规范。</p><p id="8865" class="pw-post-body-paragraph lb lc hi le b lf lg lh li lj lk ll lm kh lo lp lq kk ls lt lu kn lw lx ly jw hb bi translated">我们应该使用人工智能来改善我们的生活，而不是通过任何形式的犯罪活动。</p><h1 id="579e" class="io ip hi bd iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl bi translated">结论</h1><p id="9ffe" class="pw-post-body-paragraph lb lc hi le b lf ni lh li lj nj ll lm kh nz lp lq kk oa lt lu kn ob lx ly jw hb bi translated">祝贺你一直坚持到这篇博客的结尾！非常感谢你花时间阅读这篇文章。我希望它对你的起床和出行有帮助。</p><p id="2968" class="pw-post-body-paragraph lb lc hi le b lf lg lh li lj lk ll lm kh lo lp lq kk ls lt lu kn lw lx ly jw hb bi translated"><strong class="le hs">你喜欢GPT 2号的超能力吗？请在评论区让我知道，所有的想法和见解都是热切赞赏的。</strong></p></div></div>    
</body>
</html>
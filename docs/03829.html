<html>
<head>
<title>Neural Machine Translation Using seq2seq model with Attention.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">注意使用seq2seq模型的神经机器翻译。</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/neural-machine-translation-using-seq2seq-model-with-attention-9faea357d70b?source=collection_archive---------3-----------------------#2021-06-16">https://medium.com/geekculture/neural-machine-translation-using-seq2seq-model-with-attention-9faea357d70b?source=collection_archive---------3-----------------------#2021-06-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="ca24" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用具有注意机制的双向LSTM的单词级英语到马拉地语翻译。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/941b48725a409d2645f38fee74258583.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*BN2u78ce5Wfv5Za13IMGfg.gif"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">(Animation source: Author)</figcaption></figure></div><div class="ab cl jn jo gp jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="hb hc hd he hf"><h2 id="d922" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">简介:-</h2><p id="6697" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">在这篇文章中，我们将讨论一个非常有趣的自然语言处理(NLP)神经机器翻译(NMT)的注意模型。机器翻译只不过是将文本从一种语言自动翻译成另一种语言。</p><p id="2105" class="pw-post-body-paragraph ks kt hi ku b kv ll ij kx ky lm im la kf ln lc ld kj lo lf lg kn lp li lj lk hb bi translated">在这里，我们将学习如何将序列对序列架构(seq2seq)与Bahdanau的注意力机制一起用于NMT。</p><h2 id="b134" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">先决条件:-</h2><p id="2a18" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">本文假设您理解以下内容:-</p><ul class=""><li id="b4d5" class="lq lr hi ku b kv ll ky lm kf ls kj lt kn lu lk lv lw lx ly bi translated">递归神经网络(RNN)、长短期记忆(<a class="ae lz" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"/>)、<a class="ae lz" href="https://www.youtube.com/watch?v=D-a6dwXzJ6s&amp;t=188s" rel="noopener ugc nofollow" target="_blank">双向网络。</a></li><li id="fd0d" class="lq lr hi ku b kv ma ky mb kf mc kj md kn me lk lv lw lx ly bi translated">序列到序列架构<a class="ae lz" href="https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf" rel="noopener ugc nofollow" target="_blank">(编码器-解码器)</a>。</li><li id="6b4d" class="lq lr hi ku b kv ma ky mb kf mc kj md kn me lk lv lw lx ly bi translated"><a class="ae lz" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">注意机制。</a></li></ul><p id="d26a" class="pw-post-body-paragraph ks kt hi ku b kv ll ij kx ky lm im la kf ln lc ld kj lo lf lg kn lp li lj lk hb bi translated">在阅读代码之前，我们将简单地讨论一下双向LSTM和注意力机制。</p><h2 id="7f80" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">双向:-</h2><p id="e685" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">如果你理解LSTM，那么双向是相当简单的。在双向网络中，你可以使用简单RNN(循环神经网络)、GRU(门控循环单元)或LSTM(长短期记忆)。我将在这篇文章中使用LSTM。</p><ul class=""><li id="20f3" class="lq lr hi ku b kv ll ky lm kf ls kj lt kn lu lk lv lw lx ly bi translated">向前的层是我们通常的LSTM层，但是向后的LSTM是流动方向向后的层。</li><li id="7434" class="lq lr hi ku b kv ma ky mb kf mc kj md kn me lk lv lw lx ly bi translated">在每一个时间步长，输入在向前层和向后层传递。</li><li id="da0f" class="lq lr hi ku b kv ma ky mb kf mc kj md kn me lk lv lw lx ly bi translated">每个时间步长的输出是两个单元输出的组合(向前和向后层)。因此，for预测模型也将知道下一个单词。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mf"><img src="../Images/e4531a39286b16b2f8cf0f6d54a34e8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Anp9azV0BPgTxs5luu1-yg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Bidirectional LSTM (Image source: Author)</figcaption></figure><p id="6db5" class="pw-post-body-paragraph ks kt hi ku b kv ll ij kx ky lm im la kf ln lc ld kj lo lf lg kn lp li lj lk hb bi translated"><strong class="ku hj">为什么我们需要双向？</strong></p><ul class=""><li id="bfe7" class="lq lr hi ku b kv ll ky lm kf ls kj lt kn lu lk lv lw lx ly bi translated">在任何语言的句子中，下一个词都会对前一个词产生影响。<strong class="ku hj">例- </strong> (1)“哈利喜欢苹果，因为他工作他们的。”以及(2)“哈利喜欢苹果，它是健康的。”</li><li id="6699" class="lq lr hi ku b kv ma ky mb kf mc kj md kn me lk lv lw lx ly bi translated">在第一句中,“苹果”是公司的意思，在第二句中,“苹果”是水果的意思。我们可以这样说，因为我们知道接下来的话。首先，“苹果”取决于“工作”，其次取决于“健康”。现在在RNN/LSTM，我们只有前向层，所以他们不会有下一个序列单词的信息，因此没有适当的句子模型上下文可能无法预测正确的单词。</li><li id="90c9" class="lq lr hi ku b kv ma ky mb kf mc kj md kn me lk lv lw lx ly bi translated">在双向情况下，我们具有前向和后向层，该模型可以具有前一个和下一个单词的信息，因此具有适当上下文的句子模型将预测得更好。</li></ul><h2 id="a192" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">Bahdanau的注意:-</h2><p id="f71d" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">这里我假设你已经知道了注意力。如果不通过<a class="ae lz" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">这张纸</a>。如果你喜欢看视频，我推荐你在YouTube上看Krish naik的视频。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mg mh l"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Video from<a class="ae lz" href="https://www.youtube.com/channel/UCNU_lfiiWBdtULKOw6X0Dig" rel="noopener ugc nofollow" target="_blank"> Krish Naik’s </a>YouTube channel.</figcaption></figure><p id="a1aa" class="pw-post-body-paragraph ks kt hi ku b kv ll ij kx ky lm im la kf ln lc ld kj lo lf lg kn lp li lj lk hb bi translated"><strong class="ku hj">现在为什么要用注意力呢？</strong></p><p id="bb20" class="pw-post-body-paragraph ks kt hi ku b kv ll ij kx ky lm im la kf ln lc ld kj lo lf lg kn lp li lj lk hb bi translated">为了预测单词，一些前面或后面的单词是重要的。正如我们在双向讨论中所说的。但是哪个词更重要。发现我们用注意模型来赋予单词重要性。那么模型可以更集中于更重要的单词。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mi"><img src="../Images/cb4b34aa4f5af8ddee95865c67c5d9a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XTY2_xgVt24XuFhm_-A5Sw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Attention Model(Image source: Author)(Content source: <a class="ae lz" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">Bahdanu’s Attention</a>)</figcaption></figure><p id="3864" class="pw-post-body-paragraph ks kt hi ku b kv ll ij kx ky lm im la kf ln lc ld kj lo lf lg kn lp li lj lk hb bi translated">注意力是如何工作的</p><ul class=""><li id="6924" class="lq lr hi ku b kv ll ky lm kf ls kj lt kn lu lk lv lw lx ly bi translated">在获得输出Z(在注意力图像中)之后，第一步是计算注意力权重(α)，该输出Z是前向和后向隐藏状态[h，h`]的串联。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mj"><img src="../Images/00fbe6fba933712da370105c72d1ab27.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*IzRKo0eaZtv6olC9rgidVw.jpeg"/></div></figure><ul class=""><li id="c772" class="lq lr hi ku b kv ll ky lm kf ls kj lt kn lu lk lv lw lx ly bi translated">为了计算α，我们需要使用上述公式计算的分数(e)。分数基于解码器(LSTM)的隐藏状态(在预测y之前)和编码器z的输出</li><li id="b1d8" class="lq lr hi ku b kv ma ky mb kf mc kj md kn me lk lv lw lx ly bi translated">然后，上下文向量(Ct)被计算为注意力权重(α)和编码器输出(Z)的加权和。</li><li id="995b" class="lq lr hi ku b kv ma ky mb kf mc kj md kn me lk lv lw lx ly bi translated">解码器使用上下文向量(Ct)作为输入来生成输出y。</li></ul><h2 id="f9de" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">数据集:-</h2><p id="c7e6" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">从manythings.org<a class="ae lz" href="http://www.manythings.org/anki/" rel="noopener ugc nofollow" target="_blank">下载翻译数据集</a></p><p id="c124" class="pw-post-body-paragraph ks kt hi ku b kv ll ij kx ky lm im la kf ln lc ld kj lo lf lg kn lp li lj lk hb bi translated">我正在使用英语到马拉地语的翻译数据集。但是你可以下载任何，你只需要在代码上做一些改变。</p><h2 id="ee7b" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">预处理:-</h2><p id="c00f" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">首先，我们需要了解翻译需要什么。</p><ul class=""><li id="1511" class="lq lr hi ku b kv ll ky lm kf ls kj lt kn lu lk lv lw lx ly bi translated">我们需要一种语言的句子和另一种语言的句子。</li><li id="f52f" class="lq lr hi ku b kv ma ky mb kf mc kj md kn me lk lv lw lx ly bi translated">它们可能是一些英文单词的缩写，这可能会使我们的模型混淆。它会以不同的方式看到像“不能”和“不能”这样的单词，因此我们将扩展所有的缩写。</li><li id="cb3c" class="lq lr hi ku b kv ma ky mb kf mc kj md kn me lk lv lw lx ly bi translated">像彗差、点号和数字这样的字符在翻译中没有用，所以我们将把它们都去掉。</li></ul><p id="14a1" class="pw-post-body-paragraph ks kt hi ku b kv ll ij kx ky lm im la kf ln lc ld kj lo lf lg kn lp li lj lk hb bi translated">现在，在下面的代码中，我们将完成所有的清理过程并保存数据。</p><p id="f376" class="pw-post-body-paragraph ks kt hi ku b kv ll ij kx ky lm im la kf ln lc ld kj lo lf lg kn lp li lj lk hb bi translated"><em class="mk">注意:-要运行收缩功能，您需要收缩-扩展字典，您可以从</em> <a class="ae lz" href="https://github.com/AdiShirsath/Neural-Machine-Translation/tree/master/Data" rel="noopener ugc nofollow" target="_blank"> <em class="mk">这里</em> </a> <em class="mk">下载该文件包含125个以上的收缩。</em></p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mh l"/></div></figure><h2 id="8550" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">为模型准备数据:-</h2><ul class=""><li id="279c" class="lq lr hi ku b kv kw ky kz kf mm kj mn kn mo lk lv lw lx ly bi translated">在目标语言的句子中添加SOS(字符串开头)和EOS(字符串结尾)标记。由于这个记号，我们可以有彼此不同的目标句子和输入句子的长度。这也有助于解码器开始和停止预测。</li><li id="d2c5" class="lq lr hi ku b kv ma ky mb kf mc kj md kn me lk lv lw lx ly bi translated">注意:-输入(即英语句子)的长度需要相同，目标(即马拉地语)的长度应该相同，但它们可以彼此不同。</li><li id="20ab" class="lq lr hi ku b kv ma ky mb kf mc kj md kn me lk lv lw lx ly bi translated">神经网络不接受文本作为输入，所以我们必须将它们转换成数字。为此，我们将使用Tensorflow的<a class="ae lz" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer" rel="noopener ugc nofollow" target="_blank">标记器。</a></li><li id="2263" class="lq lr hi ku b kv ma ky mb kf mc kj md kn me lk lv lw lx ly bi translated">这个分词器非常有帮助，我们可以从中获得单词的频率、要索引的词典和单词的索引。这将用于将单词转换成数字(用于训练)以及将数字转换成单词(用于预测)</li><li id="9e24" class="lq lr hi ku b kv ma ky mb kf mc kj md kn me lk lv lw lx ly bi translated"><strong class="ku hj">填充:- </strong>神经网络也需要相同长度的输入(即句子),因此我们将用“0”填充英语和马拉地语的句子，以获得句子长度作为相应语言的最大长度句子。</li></ul><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mh l"/></div></figure><h2 id="97d1" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">构建模型:-</h2><p id="bf72" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">我们将首先建立编码器，然后解码器与关注层。</p><p id="536d" class="pw-post-body-paragraph ks kt hi ku b kv ll ij kx ky lm im la kf ln lc ld kj lo lf lg kn lp li lj lk hb bi translated">注意:-我用不同的参数做了实验，下面的参数对我来说是最好的，你可以用其他的参数做实验。尝试改变嵌入输出维度，LSTM单位，甚至添加更多的LSTM层。毕竟机器学习就是要做实验。</p><p id="0866" class="pw-post-body-paragraph ks kt hi ku b kv ll ij kx ky lm im la kf ln lc ld kj lo lf lg kn lp li lj lk hb bi translated"><strong class="ku hj">编码器:- </strong></p><ul class=""><li id="f9bc" class="lq lr hi ku b kv ll ky lm kf ls kj lt kn lu lk lv lw lx ly bi translated">正如我们之前讨论的，我们将在编码器中使用双向LSTM</li><li id="d0b4" class="lq lr hi ku b kv ma ky mb kf mc kj md kn me lk lv lw lx ly bi translated">它将学习输入语言(即英语)的模式</li><li id="29e2" class="lq lr hi ku b kv ma ky mb kf mc kj md kn me lk lv lw lx ly bi translated">我们将使用编码器的输出及其状态(上下文向量[h，c])。现在，在双向模式中，取状态有一点不同，因为它有前向和后向状态，所以我们必须同时考虑这两种状态。(也就是说，我们将连接它们。)</li></ul><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mh l"/></div></figure><p id="d742" class="pw-post-body-paragraph ks kt hi ku b kv ll ij kx ky lm im la kf ln lc ld kj lo lf lg kn lp li lj lk hb bi translated"><strong class="ku hj">解码器:- </strong></p><ul class=""><li id="c651" class="lq lr hi ku b kv ll ky lm kf ls kj lt kn lu lk lv lw lx ly bi translated">在解码器中，我们只使用LSTM。</li><li id="03f3" class="lq lr hi ku b kv ma ky mb kf mc kj md kn me lk lv lw lx ly bi translated">关注层:-我从<a class="ae lz" href="https://colab.research.google.com/drive/1XrjPL3O_szhahYZW0z9yhCl9qvIcJJYW" rel="noopener ugc nofollow" target="_blank">【此处】</a>借用了关注的代码。你可以把它放在BahdanauAttention.py文件中，或者直接从<a class="ae lz" href="https://github.com/AdiShirsath/Neural-Machine-Translation/tree/master/Attention_model" rel="noopener ugc nofollow" target="_blank">这里</a>下载文件。我们使用<a class="ae lz" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank"> bahdanau的注意力</a>作为注意力层。</li></ul><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mh l"/></div></figure><p id="40c6" class="pw-post-body-paragraph ks kt hi ku b kv ll ij kx ky lm im la kf ln lc ld kj lo lf lg kn lp li lj lk hb bi translated">这是我们的模型图-</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mp"><img src="../Images/7783716e38170bc1190b54b2b7eaee3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2A6k4EY5ITmwm3MJelrtow.png"/></div></div></figure><h2 id="7020" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">培训:-</h2><p id="a9c0" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">现在，让我们开始训练我们的注意力模型。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mh l"/></div></figure><p id="dd64" class="pw-post-body-paragraph ks kt hi ku b kv ll ij kx ky lm im la kf ln lc ld kj lo lf lg kn lp li lj lk hb bi translated">在这里，我在验证集上获得了95.04%的准确率，损失为0.32。</p><h2 id="40cc" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">推理模型:-</h2><p id="ce62" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">我们使用该模型通过使用预训练模型的权重来预测输出序列。</p><p id="6112" class="pw-post-body-paragraph ks kt hi ku b kv ll ij kx ky lm im la kf ln lc ld kj lo lf lg kn lp li lj lk hb bi translated">这里，我们不能像其他ML和DL模型一样应用<em class="mk"> model.predict() </em>，因为在我们的情况下，编码器模型学习输入句子中的特征，而解码器只是获取编码器状态，并使用解码器输入来逐字预测。因此，对于预测，我们必须做同样的过程。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mh l"/></div></figure><h2 id="94ef" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">预测:-</h2><p id="94fd" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">我们的模型将预测数字和单词，所以我们需要函数将它们转换成目标语言的句子。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mh l"/></div></figure><h2 id="b660" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">结果:-</h2><p id="93f6" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">最后，让我们把我们的句子从英语翻译成马拉地语。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mh l"/></div></figure><p id="dd95" class="pw-post-body-paragraph ks kt hi ku b kv ll ij kx ky lm im la kf ln lc ld kj lo lf lg kn lp li lj lk hb bi translated">以下是我的一些结果</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mq"><img src="../Images/9424ec2b53e505bf1e4a03478c3f5973.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*EG4Cnxyj1_j3JFXghbs3fQ.jpeg"/></div></figure><p id="055e" class="pw-post-body-paragraph ks kt hi ku b kv ll ij kx ky lm im la kf ln lc ld kj lo lf lg kn lp li lj lk hb bi translated"><strong class="ku hj">哇！！！！！</strong> → <strong class="ku hj"> <em class="mk">我们得到了一些惊人的结果。</em> </strong></p><h2 id="1f1d" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">结束注释:-</h2><p id="1c02" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">如果你还没有不加注意地使用过编码器-解码器模型，你可以看看我的文章。</p><div class="mr ms ez fb mt mu"><a rel="noopener follow" target="_blank" href="/geekculture/neural-machine-translation-using-sequence-to-sequence-model-164a5905bcd7"><div class="mv ab dw"><div class="mw ab mx cl cj my"><h2 class="bd hj fi z dy mz ea eb na ed ef hh bi translated">使用序列到序列模型的神经机器翻译</h2><div class="nb l"><h3 class="bd b fi z dy mz ea eb na ed ef dx translated">使用编码器-解码器LSTM模型的单词级英语到马拉地语翻译。</h3></div><div class="nc l"><p class="bd b fp z dy mz ea eb na ed ef dx translated">medium.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni jh mu"/></div></div></a></div><h2 id="44c7" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">参考文献:- </strong></h2><p id="40d8" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">Bahdanau的注意力研究论文<a class="ae lz" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><div class="mr ms ez fb mt mu"><a href="https://towardsdatascience.com/implementing-neural-machine-translation-with-attention-using-tensorflow-fc9c6f26155f" rel="noopener follow" target="_blank"><div class="mv ab dw"><div class="mw ab mx cl cj my"><h2 class="bd hj fi z dy mz ea eb na ed ef hh bi translated">使用Tensorflow实现注意力神经机器翻译</h2><div class="nb l"><h3 class="bd b fi z dy mz ea eb na ed ef dx translated">使用Bahdanau注意力的神经机器翻译(NMT)的Tensorflow实现的逐步解释。</h3></div><div class="nc l"><p class="bd b fp z dy mz ea eb na ed ef dx translated">towardsdatascience.com</p></div></div><div class="nd l"><div class="nj l nf ng nh nd ni jh mu"/></div></div></a></div><div class="mr ms ez fb mt mu"><a href="https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="mv ab dw"><div class="mw ab mx cl cj my"><h2 class="bd hj fi z dy mz ea eb na ed ef hh bi translated">面向所有人的深度学习注意力机制综合指南</h2><div class="nb l"><h3 class="bd b fi z dy mz ea eb na ed ef dx translated">概述注意力机制改变了我们处理深度学习算法领域的方式，如自然语言…</h3></div><div class="nc l"><p class="bd b fp z dy mz ea eb na ed ef dx translated">www.analyticsvidhya.com</p></div></div><div class="nd l"><div class="nk l nf ng nh nd ni jh mu"/></div></div></a></div><div class="mr ms ez fb mt mu"><a href="https://towardsdatascience.com/neural-machine-translation-nmt-with-attention-mechanism-5e59b57bd2ac" rel="noopener follow" target="_blank"><div class="mv ab dw"><div class="mw ab mx cl cj my"><h2 class="bd hj fi z dy mz ea eb na ed ef hh bi translated">具有注意机制的神经机器翻译(NMT)</h2><div class="nb l"><h3 class="bd b fi z dy mz ea eb na ed ef dx translated">深度学习语言翻译指南！</h3></div><div class="nc l"><p class="bd b fp z dy mz ea eb na ed ef dx translated">towardsdatascience.com</p></div></div><div class="nd l"><div class="nl l nf ng nh nd ni jh mu"/></div></div></a></div></div></div>    
</body>
</html>
<html>
<head>
<title>Different Transformer Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不同的变压器型号</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/different-transformer-models-f6f405999e4a?source=collection_archive---------5-----------------------#2022-01-17">https://medium.com/geekculture/different-transformer-models-f6f405999e4a?source=collection_archive---------5-----------------------#2022-01-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="a9b9" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="fca1" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">Transformer模型在NLP领域实现了一个重要的里程碑。不仅在NLP领域，而且在计算机视觉领域，我们可以看到Transformer模型优于所有所谓的最先进的模型。变压器也被证明可以改善长期依赖性，并可用于利用称为迁移学习的技术，这在我们的数据量较少时非常有用。</p><p id="518a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">如果你喜欢阅读更多关于变形金刚的内容，请参考我之前的博客。</p><div class="kg kh ez fb ki kj"><a rel="noopener follow" target="_blank" href="/geekculture/transformers-231c4a430746"><div class="kk ab dw"><div class="kl ab km cl cj kn"><h2 class="bd hj fi z dy ko ea eb kp ed ef hh bi translated">变形金刚(电影名)</h2><div class="kq l"><h3 class="bd b fi z dy ko ea eb kp ed ef dx translated">Transformer模型已经成为大多数NLP任务中的首选模型。许多基于变压器的模型，如伯特…</h3></div><div class="kr l"><p class="bd b fp z dy ko ea eb kp ed ef dx translated">medium.com</p></div></div><div class="ks l"><div class="kt l ku kv kw ks kx ky kj"/></div></div></a></div><p id="639e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">今天有如此多的模型可用，其灵感来自变压器架构。在这篇博客中，我们将讨论一些流行的基于变压器的模型。我们将看到每个模型的架构、组件、工作和培训过程。</p><h1 id="84d3" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">通用终端</h1><p id="1bdd" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">GPT代表生成预训练变压器。</p><p id="2db5" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">如果您记得变压器的架构，您可能知道变压器由编码器堆栈和解码器堆栈组成。GPT本质上是变压器中的解码器堆栈。变压器中的解码器堆栈负责预测输出序列的下一个字。给定前面的单词，解码器被训练来预测序列中的当前单词。前一阶段输入与当前预测的单词一起形成用于预测下一个单词的输入。这种模型被称为自回归模型。因此，我们可以将GPT模型理解为一个<em class="kz">下一个单词预测</em>模型，它可以用于句子完成、自然语言生成等等。</p><h2 id="ac54" class="la ig hi bd ih lb lc ld il le lf lg ip jo lh li it js lj lk ix jw ll lm jb ln bi translated">体系结构</h2><p id="14d9" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">GPT模型的架构将如下图所示。</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es lo"><img src="../Images/d333540202b2ca7cf7fc82043c5a3a00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eQr4bOpXCppRX3hr"/></div></div></figure><p id="7fa5" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">根据解码器堆栈的数量，GPT有不同的变体。</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es lz"><img src="../Images/92bff27ca59b3ba40f5c2ed8962ca043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3COnDk0OPmhBlan3sSmchg.png"/></div></div></figure><p id="508d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">GPT的主要目的是逐个符号地生成序列。输入单词通过嵌入层传递，嵌入层将把标记转换成嵌入或向量。这些向量进一步使用位置编码进行编码。位置编码是一种捕获序列中单词顺序的方法，是由Transformer架构本身引入的。然后，这些向量通过解码器堆栈中的第一层。一个解码器堆栈的输出作为输入传递到下一个单元，直到最后一级。来自最后一个解码器堆栈的输出将是一个概率分布，它指示词汇表中的每个单词成为序列中的下一个单词的概率。概率最高的单词将被认为是下一个单词。在每个时间戳，模型将使用所有前面的单词来生成下一个单词。这个过程将一直持续到序列结束。</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es lo"><img src="../Images/8829e22249fe5c72e07a3dc79510390d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cgG87Uid8W1LguIn"/></div></div></figure><p id="b349" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">GPT模型使用一种叫做掩蔽自我注意的概念来训练下一个单词的预测。掩蔽自我注意是一种我们在预测当前单词时掩蔽当前标记和未来标记的方法。因此，在预测第n个单词时，我们将考虑所有之前的单词，直到时间戳n-1，并屏蔽所有未来的单词。通过这种方式，模型将通过从过去的上下文中学习来预测任何单词。</p><h2 id="c15f" class="la ig hi bd ih lb lc ld il le lf lg ip jo lh li it js lj lk ix jw ll lm jb ln bi translated">培养</h2><ul class=""><li id="c2d1" class="ma mb hi jf b jg jh jk jl jo mc js md jw me ka mf mg mh mi bi translated">利用图书语料库数据对GPT1模型进行训练</li><li id="cb96" class="ma mb hi jf b jg mj jk mk jo ml js mm jw mn ka mf mg mh mi bi translated">使用大约40 GB的网络文本数据来训练GPT 2模型。这些文本数据是由研究人员从互联网上抓取的，它包含了大约800万个文档</li><li id="c790" class="ma mb hi jf b jg mj jk mk jo ml js mm jw mn ka mf mg mh mi bi translated">GPT 3是在混合语料库上训练的。总共有五个数据集，包括通用抓取、网络文本2、图书1、图书2和维基百科</li></ul><h1 id="c930" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">伯特</h1><p id="0c1b" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">BERT代表来自变压器的双向编码器表示。</p><p id="1643" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">转换器模型由两部分组成—编码器堆栈(多个编码器单元)和解码器堆栈(多个解码器单元)。我们已经看到，GPT模型不过是变压器的解码器堆栈。解码器堆栈主要用于预测句子中的未来单词，而编码器堆栈用于获得句子的有意义的编码表示，以便解码器单元可以更好地理解它。我们有没有办法也利用编码器堆栈？答案是肯定的。这就是伯特模型所做的。BERT是一种语言模型，它可以通过考虑单词的语义和上下文来生成嵌入。</p><h2 id="aa1a" class="la ig hi bd ih lb lc ld il le lf lg ip jo lh li it js lj lk ix jw ll lm jb ln bi translated">伯特建筑</h2><p id="ca38" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">如前所述，BERT只不过是转换器的编码器堆栈</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es lo"><img src="../Images/7a5c61023518f8ea105ae6f5d95685dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ac5178F3zb2WtftR"/></div></div></figure><p id="3628" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">对BERT模型的输入将是被编码的单词(一位热编码或任何其他编码形式),并且来自BERT模型的输出将是固定维度的嵌入。在上图中，O0是对应于第一个单词的嵌入，O2是第二个单词的嵌入，依此类推。然后，这些嵌入可以用于任何下游任务，如句子分类、NER、相似性检查等等。BERT模型的内部看起来像这样。</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es lo"><img src="../Images/b8e42dfe9dbfade97f73b15735201707.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*G8v1BR2q0nPASZkv"/></div></div></figure><h2 id="ada7" class="la ig hi bd ih lb lc ld il le lf lg ip jo lh li it js lj lk ix jw ll lm jb ln bi translated">培养</h2><p id="a354" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">BERT模型的训练分两步完成</p><ol class=""><li id="8a4b" class="ma mb hi jf b jg kb jk kc jo mo js mp jw mq ka mr mg mh mi bi translated">屏蔽字预测</li><li id="0f16" class="ma mb hi jf b jg mj jk mk jo ml js mm jw mn ka mr mg mh mi bi translated">下一句预测</li></ol><p id="4302" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">屏蔽词预测是通过随机屏蔽每个输入句子中的一些标记或词来完成的。该模型被训练来预测被屏蔽的单词。这一步最好的地方在于，它不需要任何标记数据，并且是完全自我监督的。我们训练所需要的只是一些文本数据，屏蔽语言模型将在此基础上进行训练，优化其权重。在训练结束时，模型将能够理解对其进行训练的语言，并且训练的模型可以用于各种下游任务。</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es lo"><img src="../Images/a7bbd826d63d3182e81a3a0b02df171b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*18eM7OJ4IEcnH5Lg"/></div></div></figure><p id="ca4b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">下一句预测用于理解两个句子之间是否有任何关系。该模型被训练来预测特定的句子是否跟随另一个句子。对于这个任务，我们将给出两个句子，用一个[SEP]标记分开。现在，将来自模型的第一输出O0给予分类器模型，以预测“<em class="kz">第二句跟随第一句吗？”。</em></p><p id="56bc" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><em class="kz">第一个输入令牌总是一个特殊的分类【CLS】令牌。因此，对应于该令牌的最终状态被用作分类任务的聚集序列表示，并被用于下一句预测，其中它被馈送到预测标签“是下一个”或“不是下一个”的概率的FFNN + SoftMax层。</em></p><p id="1d3f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">BERT是最先进的语言模型之一，可用于各种下游NLP任务。然而，由于该模型是基于非常通用的数据训练的，因此该模型可能不是特定于领域的任务的非常好的选择。有各种各样的模型受BERT的启发，并针对特定领域的数据进行了训练。医疗领域是广泛使用BERT模型的一个领域。有各种基于BERT的医学数据预处理模型，并广泛应用于各种下游任务，如NER、关系提取等。研究还发现了更强大、更快速、更轻便的BERT版本。</p><h1 id="3e85" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">比奥伯特</h1><p id="7c3f" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">Biobert是首个针对特定领域的模型，针对医疗领域进行了预先训练。该模型使用PubMed摘要和PubMed central全文文章在大规模医疗数据上进行训练。为了训练生物伯特模型，研究人员使用通用域伯特模型作为基础模型，并优化其权重，然后该模型再次根据医疗数据进行微调。这种类型的培训被称为混合领域方法。由于模型已经用最优的一般域权重初始化，预训练的成本更少。用于训练BioBert模型的语料库如下所示。</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div class="er es ms"><img src="../Images/4acb32f491aa66dffd67abde5c7e4931.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/0*6eqAzkprIQEWvcME"/></div><figcaption class="mt mu et er es mv mw bd b be z dx">Image is taken from the original paper</figcaption></figure><p id="da6e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">该模型在医学数据的NER、RE和QA等下游任务中取得了很好的效果。</p><h1 id="2015" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">西伯特</h1><p id="a840" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">Sci-Bert是另一个特定领域的模型，经过预先训练，可以在NLP中执行科学任务。与BioBert类似，该模型也利用了Transformer模型的无监督训练策略，以解决高质量标记数据的短缺问题。该模型在语义学者的114万篇科学论文上进行训练。该数据是混合领域数据，其中18%的论文来自计算机科学领域，82%来自生物医学领域。该语料库包含大约30亿个类似于BERT模型的单词。该模型在下游任务中进行评估，如NER、PICO抽取、句子分类、关系分类、依存句法分析等。</p><h1 id="5db6" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">考研伯特</h1><p id="5c1f" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">PubMed BERT是另一个基于特定领域BERT的模型，基于生物医学数据进行训练。这个模型是由微软研究院在2020年开发的。与上面两个遵循混合领域训练的模型不同，PubMed模型是在生物医学数据上从头开始训练的。这篇论文说，使用领域外文本(在这种情况下是维基百科和书籍语料库)进行训练是一种迁移学习，其中源数据是一般文本语料库，目标领域是医学。迁移学习是必需的，并且在目标数据较少以及源和目标数据几乎匹配时使用。在这种情况下，目标语料库非常大，并且与源数据非常不同。该论文证明了生物医学预训练不需要任何混合领域训练方法，并且可以从头开始进行。</p><p id="6975" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">用于训练的数据是包含大约30亿个单词的PubMed摘要。还有另一个版本的PubMed模型，它使用PubMed central上的完整文章进行训练，具有大约168亿个单词的训练语料库。</p><h1 id="47f8" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">临床伯特</h1><p id="a4e8" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这是另一个基于BERT的模型，针对临床数据进行了预训练——通用临床文本和出院总结。根据临床数据训练两个模型</p><ul class=""><li id="5fd6" class="ma mb hi jf b jg kb jk kc jo mo js mp jw mq ka mf mg mh mi bi translated">临床BERT——使用在一般领域训练的BERT模型作为基础模型</li><li id="55ec" class="ma mb hi jf b jg mj jk mk jo ml js mm jw mn ka mf mg mh mi bi translated">临床BioBERT——使用BioBERT模型作为基础模型</li></ul><h1 id="85e5" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">蒸馏啤酒</h1><p id="813c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">DistilBert是Bert模型的提炼版本。这是Bert模型的一个更小、更快、更便宜的版本，比BERT基本模型少40%的参数。该架构几乎与Bert模型相似，但层数减少了1/2。DistillBert通过动态屏蔽和删除下一句预测任务对大批量数据进行训练。该模型的性能与其他基准模型相当，并且该模型能够以更少的参数保留BERT基本模型95%以上的性能。</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es lo"><img src="../Images/5c79b8f4b487aadc12fc4eac9c718b87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TeWd2xvN4EawLxvj"/></div></div></figure><h1 id="952a" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">罗贝尔特</h1><p id="e1f7" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">Roberta遵循稳健优化的Bert预训练方法。本文介绍了一些设计优化，并根据附加数据进一步训练模型。模型架构与Bert模型的架构相同，但有一些变化。</p><ul class=""><li id="cbc7" class="ma mb hi jf b jg kb jk kc jo mo js mp jw mq ka mf mg mh mi bi translated">移除了下一句预测任务</li><li id="d8d9" class="ma mb hi jf b jg mj jk mk jo ml js mm jw mn ka mf mg mh mi bi translated">使用来自CC新闻、开放网络文本和故事的附加数据进行训练</li><li id="3c4f" class="ma mb hi jf b jg mj jk mk jo ml js mm jw mn ka mf mg mh mi bi translated">更大的训练批量，更大的学习率，以及更长序列的训练</li><li id="e4b7" class="ma mb hi jf b jg mj jk mk jo ml js mm jw mn ka mf mg mh mi bi translated">动态改变掩蔽模式</li></ul><h1 id="54af" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">XLNET</h1><p id="5215" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">BERT是一个自动编码器语言模型，它在构造当前单词时会考虑过去和未来的单词。BERT用于从使用“屏蔽”令牌破坏的数据中重建原始数据。然而，这种方法有一个缺点。这种类型的训练假设屏蔽的令牌是彼此独立的，并且忽略捕捉它们之间的关系(如果有的话)。</p><p id="0bf1" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">XLNET是一个自回归语言模型。通常，自回归(AR)模型在预测下一个单词时可以考虑过去的单词或未来的单词，但它不能像BERT模型那样同时考虑两者。AR回归模型的一个例子是GPT，如上所述，它在训练过程中只在一个方向上学习。但是XLNET将通过让AR模型从双向上下文中学习来修改这个概念。XLNET在预训练阶段引入了<strong class="jf hj">置换语言建模</strong>来实现这一点。这里，为了预测第n个位置的单词，我们将把该单词放在所有位置，并在目标单词之前使用其余单词的不同组合。这将从下面的例子中变得更加清楚。</p><p id="c3bf" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">假设，我们的序列长度是4，我们想预测第三个字。</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div class="er es mx"><img src="../Images/8deb388485bb3838ff9f8e0913c8d4ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/0*4VpWuclUyCfDB66k"/></div><figcaption class="mt mu et er es mv mw bd b be z dx">Image is taken from <a class="ae my" href="https://towardsdatascience.com/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335" rel="noopener" target="_blank">XLNET</a></figcaption></figure><p id="3fde" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在这里，为了预测X3，我们把这个词放在所有不同的位置。然后我们考虑所有剩余单词的组合，因此这实现了双向学习。经过训练后，一个特定的目标词将从两边学习它的上下文</p><h1 id="e862" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">最后的想法</h1><p id="7f73" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在这篇博客中，我试图介绍不同变压器模型的一些基础知识。请注意，我在这里只包括了几个非常流行和广泛使用的模型。你也可以随时阅读其他型号的产品。</p><p id="39c4" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">另外，请随时通过<a class="ae my" href="https://www.linkedin.com/in/vinitha-v-n-5a0560179/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae my" href="https://twitter.com/Vinitha_vn" rel="noopener ugc nofollow" target="_blank"> Twitter </a>联系我</p></div></div>    
</body>
</html>
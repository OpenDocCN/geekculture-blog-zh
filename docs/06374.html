<html>
<head>
<title>Actor-Critic: Off-Policy Actor-Critic Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">演员-评论家:非策略演员-评论家算法</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/actor-critic-off-policy-actor-critic-algorithm-cca654845558?source=collection_archive---------20-----------------------#2021-08-18">https://medium.com/geekculture/actor-critic-off-policy-actor-critic-algorithm-cca654845558?source=collection_archive---------20-----------------------#2021-08-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="728a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将探索提交给2012年ICML的论文<a class="ae jd" href="https://arxiv.org/pdf/1205.4839.pdf" rel="noopener ugc nofollow" target="_blank">背后的思想。本文提出了一种完全在线的行动者-批评家方法，使用行为策略来学习目标策略。这种偏离策略的学习方法利用了行动者-批评家方法中的行动选择，同时具有偏离策略的好处，以便更好地探索和使用经验重放。最后，我将尝试实现文中描述的算法。</a></p><h1 id="db46" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">评论家:政策评估</h1><p id="b477" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">首先，本文描述了政策评估的方法。类似于基线行动者-批评家方法，我们可以使用一组权重来学习价值函数的线性近似。在实施过程中，我们可以仅使用神经网络来学习非线性近似。该论文使用<a class="ae jd" href="https://sites.ualberta.ca/~szepesva/papers/ICML10_controlGQ.pdf" rel="noopener ugc nofollow" target="_blank">梯度-TD方法</a>来学习偏离策略设置中的价值函数的权重，并且还保证了与自举方法不同的收敛性。这些方法试图最小化λ加权投影贝尔曼方程的均方误差。本文中给出的最终算法使用了一种称为GTD(λ)的变体，该变体由<a class="ae jd" href="http://www.incompleteideas.net/papers/maei-thesis-2011.pdf" rel="noopener ugc nofollow" target="_blank"> Maei (2011) </a>引入。我不会详细介绍价值近似值方法，但它本质上是TD(λ)在非策略设置中的一种变体。</p><h1 id="e1ad" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">政策外政策梯度定理</h1><p id="8756" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">在我们看行动者如何学习策略函数之前，让我们看看策略梯度定理在非策略设置中是如何工作的。这里快速回顾一下政策梯度定理。首先，我们将策略定义为π <strong class="ih hj"> ᵤ </strong> (a|s)，其中输出是给定状态<em class="kh"> s </em>下动作<em class="kh"> a </em>的概率。该策略有一个权重向量<strong class="ih hj"> u </strong>，或者参数<strong class="ih hj"> u </strong>，如果你使用神经网络来学习的话。目标是最大化性能测量函数J的<strong class="ih hj"> u </strong>:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es ki"><img src="../Images/b521efe170e9f37fcfe64d7e539d26dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*dDn6Jg9YnsWnJmyp1JmxtA.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx">Taken from Off-Policy Actor-Critic (Degris et al. 2013)</figcaption></figure><p id="99c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里d <strong class="ih hj"> ᵇ </strong> (s)是在<strong class="ih hj"> b </strong>下的状态分布其中P(sₜ = s | s₀，<strong class="ih hj">b</strong>∀t∈ℕ，状态遵循一个行为分布<strong class="ih hj"> b </strong>。这与策略上的性能测量函数略有不同，因为我们是从单独的行为分布中取样的。回想一下我们是如何在加强中更新权重的:<strong class="ih hj">u</strong>ₜ₊₁=<strong class="ih hj">u</strong>ₜ+α∇j(<strong class="ih hj">u</strong>ₜ).我们可以通过计算∇J( <strong class="ih hj"> u </strong>的近似值来获得梯度∇J( <strong class="ih hj"> u </strong>)。关于这个近似值的完整推导，请参考我在<a class="ae jd" href="https://chengxi600.medium.com/?p=aa2ff134c1b" rel="noopener">政策梯度</a>上的帖子。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ku"><img src="../Images/3eb3caadb5b23f81225c32c98bd39b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R_pAwtwLQ53l7TXmsLy8hA.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx">Taken from Off-Policy Actor-Critic (Degris et al. 2013)</figcaption></figure><p id="784f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个从右到左的方程式的快速纲要。<em class="kh"> Q(s，a) </em>是评估在状态<em class="kh"> s. </em>中采取的动作<em class="kh"> a </em>的值函数。我们将其乘以动作<em class="kh"> a </em>给定状态<em class="kh"> s </em>遵循策略π相对于权重<strong class="ih hj"> u </strong>的概率梯度。然后，我们对动作空间中所有动作<em class="kh"> a </em>的乘积求和，给出状态<em class="kh"> s </em>的值的梯度的期望值。然后，我们将其乘以由状态<em class="kh"> s </em>的行为分布d <strong class="ih hj"> ᵇ </strong>给出的权重，并对我们的状态空间中所有状态<em class="kh"> s </em>的加权值求和。最终值将是我们的权重<strong class="ih hj"> u </strong>的性能测量的近似梯度。</p><p id="98a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文提供了两个定理来证明这种近似:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es kz"><img src="../Images/4ebd3f8dd4b460de75ff4b1c2ca666aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*1Tw3EOwBpi70bdF3yvPjQQ.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx">Taken from Off-Policy Actor-Critic (Degris et al. 2013)</figcaption></figure><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es la"><img src="../Images/2e3022110a108f8e127a83f30bfeed77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*TwzlrtKPsbKB_FEj9wz8vw.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx">Taken from Off-Policy Actor-Critic (Degris et al. 2013)</figcaption></figure><p id="cfff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">定理1表明，在逼近方向上任何政策参数的更新给我们一个更好的政策。在符合策略的设置中，策略梯度定理表明我们的近似值<strong class="ih hj"> g </strong> ( <strong class="ih hj"> u </strong>)等于性能测量函数j的梯度。虽然我们在不符合策略的设置中无法做到这一点，但定理2建立了使用近似值找到的解决方案与性能测量函数之间的关系。为了证明这两个定理，请查看<a class="ae jd" href="https://arxiv.org/pdf/1205.4839.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>的附录。</p><h1 id="5a12" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">参与者:带有资格跟踪的在线更新</h1><p id="f0bc" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">现在我们已经建立了∇J的近似值，我们可以得出演员的更新方程。就像增强的更新方程一样，我们希望找到近似值的期望值，而不是实际上对所有状态和动作求和。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lb"><img src="../Images/8c16fe55e7414c95f1f4fe186b28efee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z8qWGr9B0WDAheiOxCWjjA.png"/></div></div></figure><p id="e80a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们从(1)中的非策略策略梯度近似开始。然后在(2)中，我们可以通过从分布<em class="kh"> d </em> <strong class="ih hj"> <em class="kh"> ᵇ </em> </strong>中采样一个状态，将所有状态的求和写成一个期望值，从而消除求和。在(3)中，我们将期望值乘以两个等于1的常数，然后重新排列一些项。常数ρ就是<a class="ae jd" href="http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/" rel="noopener ugc nofollow" target="_blank">重要性权重</a>。这里，<em class="kh"> b(a|s) </em>是动作概率<em class="kh"> a </em>给定状态<em class="kh"> s </em>。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lc"><img src="../Images/ae9cee60eee12f9a24d5aaa99511b4e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*srNO8CKFHmYvLmN1hFprWw.png"/></div></div></figure><p id="e169" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过引入b(a|s ),我们能够通过从行为分布<strong class="ih hj"> b </strong>中采样一个动作来去除状态<em class="kh"> s </em>中所有动作的总和。这篇论文用一种新的符号简化了预期:</p><blockquote class="ld le lf"><p id="b4a4" class="if ig kh ih b ii ij ik il im in io ip lg ir is it lh iv iw ix li iz ja jb jc hb bi translated">我们引入了新的符号Eb [ ]，以表示在行为策略下，所有随机变量(按时间步长索引)从它们的极限平稳分布中抽取的隐含条件期望。</p></blockquote><p id="4921" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们已经将近似值转换成了可用于计算更新方程的期望值。然后，论文从中减去一个基线，然后通过使用非政策λ-收益来近似Q函数，定义为:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lj"><img src="../Images/33c3517f534a6889960da3b3a8b7d339.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*G4XuNAwnm_F2JH1cDf1rhA.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx">Taken from Off-Policy Actor-Critic (Degris et al. 2013)</figcaption></figure><p id="93bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以从Q函数中减去基线，因为基线的梯度是0。更详细的解释，请查看我的帖子<a class="ae jd" rel="noopener" href="/nerd-for-tech/policy-gradients-reinforce-with-baseline-6c871a3a068">用基线</a>加固。这里我们有一个近似为<strong class="ih hj"> g </strong> ( <strong class="ih hj"> u </strong>)的最终期望，因为λ收益与Q函数略有不同。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lk"><img src="../Images/bca14f7974fb08d135592c5f104fb729.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*8da9wD68nOCJx9uf0j-K3w.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx">Taken from Off-Policy Actor-Critic (Degris et al. 2013)</figcaption></figure><p id="bb12" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，期望值用于前视更新等式:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es ll"><img src="../Images/d035894a0742b853b52f6aae32fa504d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*hY_PH76dCG-DjfIbvOiXTg.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx">Taken from Off-Policy Actor-Critic (Degris et al. 2013)</figcaption></figure><p id="2694" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据这个等式，我们必须对整个轨迹进行采样，以计算λ收益。幸运的是，我们可以将这种前视更新转换为后视更新，以便算法完全在线工作。类似于后视TD(λ)算法，我们可以使用合格轨迹来更新参数<strong class="ih hj"> u </strong>来代替。对于这些痕迹的解释，你可以查看我在<a class="ae jd" rel="noopener" href="/geekculture/actor-critic-value-function-approximations-b8c118dbf723">价值函数近似</a>上的帖子。这是写有资格跟踪的期望，</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lm"><img src="../Images/c1f7a45c732c727406853c56d4f930a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*g_cTzOZ-xj95_O9AqWgpcA.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx">Taken from Off-Policy Actor-Critic (Degris et al. 2013)</figcaption></figure><p id="e394" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中，δ是TD误差:δₜ = Rₜ₊₁ + γV(sₜ₊₁) - V(sₜ)，并且资格轨迹用以下等式更新:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es ln"><img src="../Images/9cbf48fbf0007ff19fcdc83bcc93cdbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*HlEJFrhd6YDBGIIFR4pc5A.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx">Taken from Off-Policy Actor-Critic (Degris et al. 2013)</figcaption></figure><p id="1702" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们可以将此期望用于后视更新等式:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lo"><img src="../Images/90d4f371225e669ee043fcba2df80dc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*0yH45f7UKRDAoGQfZDsPvg.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx">Taken from Off-Policy Actor-Critic (Degris et al. 2013)</figcaption></figure><p id="ce7d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是论文中的Off-PAC算法的伪代码:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lp"><img src="../Images/5a83ef32de90010b882236d26f4fef9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*JKG7AYFi2UPOjzo2gR7eTg.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx">Taken from Off-Policy Actor-Critic (Degris et al. 2013)</figcaption></figure><p id="92e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">向量<strong class="ih hj"> e </strong> ᵥ，<strong class="ih hj"> e </strong> ᵤ分别是用于评论家和演员的轨迹，<strong class="ih hj"> w </strong>是用于GTD(λ)算法更新评论家的轨迹。向量<strong class="ih hj"> v </strong>和<strong class="ih hj"> w </strong>是评论家和演员的权重向量。注意，我们可以在Actor更新的实现中简化ψ(s，a) = ∇ ln π(a | s)。该算法中使用的超参数是:</p><ul class=""><li id="ab14" class="lq lr hi ih b ii ij im in iq ls iu lt iy lu jc lv lw lx ly bi translated">αʷαᵛ:批评家的步长</li><li id="cd06" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">αᵘ:演员的步长</li><li id="edd4" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">λ:轨迹的重量衰减</li><li id="b7a2" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">b(⋅|s):平稳行为策略分布</li></ul><h1 id="9075" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">履行</h1><p id="6048" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">对于这个算法的实现，我决定尝试按照本文中的方式来实现它。但首先，我想实现一个普通的非策略策略梯度方法，而不需要对价值函数进行λ回报和非策略学习的所有修改。</p><h1 id="fa11" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">政策外政策梯度实施</h1><p id="0d1a" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">回想一下我们导出的政策外政策梯度定理的期望:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es me"><img src="../Images/62fb1cdc67d412a2c18f1072b24082fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kCWw6SWKD6tmY_GERA34Ew.png"/></div></div></figure><p id="909e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于一个普通的保单外保单梯度，我们将使用贴现的累积报酬Gₜ作为q值。这是我输入的伪代码:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es mf"><img src="../Images/79e0b0f0f3e7292668e9a5bb2bfdd13c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PRfaK52XmR5-3V3v3TGYuQ.png"/></div></div></figure><p id="6ff5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如您所见，它基本上与加强相同，只是我们从静态行为策略中采样轨迹，并在更新步骤中乘以重要性权重ρ。</p><p id="8386" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是我在这个实现中使用的超参数:</p><ul class=""><li id="c1a0" class="lq lr hi ih b ii ij im in iq ls iu lt iy lu jc lv lw lx ly bi translated">α(学习率):0.01</li><li id="e292" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">γ(折扣系数):0.99</li><li id="4594" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">最大步数:5000</li><li id="a1ad" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">剧集数量:1000</li><li id="3327" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">行为策略:动作空间上的均匀分布</li></ul><p id="fc13" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于实验，我在每集生成两条轨迹:一条来自行为策略，用来训练我们的目标策略，另一条来自目标策略，这样我们就可以在那一集评估我们的目标策略。虽然这可能会使训练时间增加一倍，但每集的运行时间仍然是2n = O(n)。</p><p id="1555" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是我们的代理在CartPole环境下超过1000集的培训历史:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es mg"><img src="../Images/9dd7296339249ba0ff3512dbfaea200a.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*Sx8d0cxmTzG_fNmj4fbdPw.png"/></div></figure><p id="7078" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">灰线是我们稳定行为政策的回报历史，蓝线是我们目标政策的回报历史。正如预期的那样，灰线没有改善，因为我们是从一个稳定的行为政策中取样的。蓝线显示了我们的代理在1000集里的进步。</p><p id="528e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的代理人在3次试玩的50次游戏中取得了142.37的平均分数。这比我们的普通保单保单梯度(得分为79.6)要好得多。这里可以看到脱离政策的好处。由于我们从一个稳定的行为策略中采样我们的轨迹，我们的代理在整个试验中保持相同的探索水平，并且可以防止自己陷入次优的局部最大值。</p><h1 id="2b9b" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">政策外行动者-批评者实施</h1><p id="50cc" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">现在，让我们尝试实现本文中描述的Off-PAC算法。对于我的实现，我决定完全按照论文的实现，并对演员和评论家使用学习过的线性近似器。下面是我的代码片段的快速浏览:</p><figure class="kj kk kl km fd kn"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="6a2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，我们初始化演员和评论家权重向量u和v，以及轨迹向量ev、eu和w。这些向量的形状是(输入x输出)，其中输入是演员权重向量的状态空间，输出是动作空间，1是评论家权重向量。然后我们有一些辅助函数:</p><figure class="kj kk kl km fd kn"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="280f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于该论文使用演员和评论家的线性近似器，而不是通过神经网络的前向传递，我们简单地将权重向量乘以特征向量来获得输出。剩下的就很简单了。我们计算我们需要的常数并更新所有的向量:</p><figure class="kj kk kl km fd kn"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="8f0f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尝试在MountainCar环境中运行代理后，我没有得到我想要的结果。以下是遵循行为策略的奖励历史:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es mj"><img src="../Images/bbe5012a5fd2290f8e23747464074ecd.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*vStbU6W2JH9ZWetu6bdAzg.png"/></div></figure><p id="e4ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如预期的那样，行为策略基本上与随机策略相同，一些幸运的运行得分在-5000以上。然而，拥有50个playthroughs的代理的平均分数仍然是-5000。即使使用论文中描述的相同超参数，政策也没有改善。即使是在CartPole环境下，结果也和随机策略差不多。在研究学习的权重时，问题似乎来自梯度爆炸。然而，在降低了学习率之后，代理陷入了次优策略。</p><h1 id="ebf1" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">最后的想法</h1><p id="2471" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">虽然我对不能在论文中复制结果感到有点失望，但了解政策梯度和价值函数近似在非政策设置中如何工作仍然很有趣。对于论文中描述的Off-PAC算法的实现，我决定尝试精确地复制该算法，并使用矩阵作为我的权重，而不是使用神经网络来参数化学习到的函数。我在使用Pytorch张量方面确实有所进步，因为我必须手动完成所有的计算。</p><p id="dd84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我曾经遇到的一个问题是需要对目标策略的权重向量进行就地操作。首先，我需要为权重向量启用requires_grad标志，因为我需要计算目标策略概率相对于权重向量的梯度。为了能够获得权重向量的梯度，它需要是叶张量，因为Pytorch autograd从非叶张量到叶张量遍历DAG以执行链规则。注意，在Pytorch中，Autograd跟踪一个DAG，其中节点是张量，边是操作，箭头指向操作产生的张量。叶张量是没有对其执行操作的张量，因此就地操作也算。</p><p id="55a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">解决方案是在权重向量上使用detach()，以便将结果张量从DAG中分离出来。然后，我必须再次启用requires_grad标志，因为从技术上来说，我不是在做一个就地操作，而是在创建一个新的叶张量。</p><p id="68f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代码:</p><ul class=""><li id="aba3" class="lq lr hi ih b ii ij im in iq ls iu lt iy lu jc lv lw lx ly bi translated">脱离策略策略梯度:<a class="ae jd" href="https://github.com/chengxi600/RLStuff/blob/master/Policy%20Gradients/Off-Policy_Policy_Gradient.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/cheng i600/rl stuff/blob/master/Policy % 20 gradients/Off-Policy _ Policy _ gradient . ipynb</a></li><li id="869a" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">场外政策演员-评论家:<a class="ae jd" href="https://github.com/chengxi600/RLStuff/blob/master/Actor-Critic/Off-Policy_Actor_Critic.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/cheng i600/rl stuff/blob/master/演员-评论家/场外政策_演员_评论家. ipynb </a></li></ul><p id="0099" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><ul class=""><li id="4830" class="lq lr hi ih b ii ij im in iq ls iu lt iy lu jc lv lw lx ly bi translated"><a class="ae jd" href="https://arxiv.org/pdf/1205.4839.pdf" rel="noopener ugc nofollow" target="_blank">非政策演员评论家(德格里斯等人，2013年)</a></li><li id="9db0" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated"><a class="ae jd" href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#off-policy-policy-gradient" rel="noopener ugc nofollow" target="_blank">政策梯度算法(Lilian Weng的博客)</a></li><li id="e958" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated"><a class="ae jd" href="http://www.incompleteideas.net/papers/maei-thesis-2011.pdf" rel="noopener ugc nofollow" target="_blank">梯度时差学习算法(Maei 2011) </a></li></ul></div></div>    
</body>
</html>
<html>
<head>
<title>Explained with python: What Does The Linear Regression Equation Tell You?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用python解释:线性回归方程告诉你什么？</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/explained-with-python-what-does-the-linear-regression-equation-tell-you-49680b25b4f2?source=collection_archive---------12-----------------------#2022-12-26">https://medium.com/geekculture/explained-with-python-what-does-the-linear-regression-equation-tell-you-49680b25b4f2?source=collection_archive---------12-----------------------#2022-12-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="0add" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第一部分:机器学习算法系列</strong></p><p id="a37e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嘿，在这个系列中，我们将仔细研究机器学习算法，并研究每个算法的优缺点。我们将讨论算法以及算法背后的数学。</p><p id="e953" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，让我们弄清楚机器学习中使用的一些基本术语。</p><ul class=""><li id="c7b3" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><strong class="ih hj">监督ml算法:</strong>那些使用标记数据的算法称为监督ML算法。监督最大似然算法广泛用于两个任务:分类和回归。</li><li id="ceb6" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><strong class="ih hj">分类:</strong>当任务是将样本对象归入确定的类别(目标变量)时，则称为分类。例如，对电子邮件是否是垃圾邮件进行分类。</li><li id="7424" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><strong class="ih hj">回归:</strong>当任务是预测连续变量(目标变量)时，那么就称为回归。例如，预测房价。</li><li id="ec40" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><strong class="ih hj">无监督ml算法:</strong>那些使用未标记数据的算法被称为无监督ML算法。无监督算法用于聚类。</li><li id="3af3" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><strong class="ih hj">聚类:</strong>在给定的未标记数据中寻找组的任务被称为聚类。</li><li id="7bc0" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><strong class="ih hj">误差:</strong>实际值与预测值之差</li><li id="264d" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><strong class="ih hj">梯度下降:</strong>以产生误差函数最小值的方式更新模型参数的机制。</li></ul><h2 id="d82a" class="jr js hi bd jt ju jv jw jx jy jz ka kb iq kc kd ke iu kf kg kh iy ki kj kk kl bi translated"><strong class="ak">什么是机器学习中的线性回归？</strong></h2><p id="ea76" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">线性回归是一种受监督的机器学习算法，用于预测称为目标的连续数值变量。这是最简单的机器学习算法之一。之所以称之为“线性”，是因为该算法假设输入特征(也称为自变量)和输出变量(也称为因变量或目标变量)之间的关系是线性的。换句话说，该算法试图找到最符合数据的直线(或多个输入要素情况下的超平面)。</p><h2 id="f45f" class="jr js hi bd jt ju jv jw jx jy jz ka kb iq kc kd ke iu kf kg kh iy ki kj kk kl bi translated"><strong class="ak">线性回归的类型:</strong></h2><ol class=""><li id="3cd1" class="jd je hi ih b ii km im kn iq kr iu ks iy kt jc ku jj jk jl bi translated"><strong class="ih hj">简单线性回归:</strong></li></ol><p id="5f57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当使用单个输入要素预测输出值时，线性回归称为简单线性回归。当给定单个输入特征时，我们可以在2D空间中的因变量和自变量之间画一条线。这里，<code class="du kv kw kx ky b">b0</code>是截距，<code class="du kv kw kx ky b">b1</code>是系数，<code class="du kv kw kx ky b">x1, x2,…, xn</code>被称为输入特征，y是输出变量。</p><figure class="la lb lc ld fd le er es paragraph-image"><div class="er es kz"><img src="../Images/8c2ad102f46bcc87fb69118ebe870e6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/format:webp/0*eCytPQifh9Vs0Y5q.png"/></div></figure><p id="ac10" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。多元线性回归:</strong></p><p id="a84c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当使用多个输入要素预测输出变量时，线性回归称为多元线性回归。当只给定两个输入特征时，我们可以在3D空间中绘制因变量和自变量之间的平面。在更高维度中，可视化变得困难，但是直觉是找到更高维度中的超平面。这里，<code class="du kv kw kx ky b">b0</code>是截距，<code class="du kv kw kx ky b">b1, b2, b3, ……., bn-1, bn </code>是系数，<code class="du kv kw kx ky b">x1, x2,…, xn</code>是输入特征，y是输出变量。</p><figure class="la lb lc ld fd le er es paragraph-image"><div class="er es lh"><img src="../Images/026daf2095e7ab480bb9f3c1e3b4960a.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*9hCb6Dhu-F5YOviJ.png"/></div></figure><h2 id="4d18" class="jr js hi bd jt ju jv jw jx jy jz ka kb iq kc kd ke iu kf kg kh iy ki kj kk kl bi translated">至此，我们已经理解了线性回归试图绘制线性边界，但它是如何做到的呢？</h2><h2 id="2b51" class="jr js hi bd jt ju jv jw jx jy jz ka kb iq kc kd ke iu kf kg kh iy ki kj kk kl bi translated">它如何找到一条完美的线来分隔给定的两个类？</h2><p id="538f" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">如公式所示，b0称为截距，b1，b2，…，bn称为线性回归系数，现在的目标是找到最小化误差函数的线性边界。误差函数是目标变量的预测值和实际值之差的平方。如果我们不平方误差，那么正负点就会互相抵消。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es li"><img src="../Images/db60daf27fd35b50b62c272001ded8f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gYQPDB0ZfM0t4fWFCraE0Q.png"/></div></div></figure><p id="c720" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们需要找到线性回归的系数和截距，使误差平方和(SSE)最小。梯度下降是最流行的技术之一，用于寻找ml和深度学习算法的最佳系数。</p></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><p id="af48" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下一节中，我们将在保险数据集上训练模型，在给定输入特征年龄、性别、身体质量指数、医院支出、过去咨询次数等的情况下，我们必须预测费用。</p><h2 id="7b04" class="jr js hi bd jt ju jv jw jx jy jz ka kb iq kc kd ke iu kf kg kh iy ki kj kk kl bi translated">Python实现:</h2><p id="7411" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">可以使用python中的<code class="du kv kw kx ky b">sklearn </code>库对线性回归模型进行训练和测试。我们将使用insurance.csv数据集来训练线性回归模型。执行一些预处理步骤来描述数据、处理缺失值和检查线性回归的假设。</p><p id="edc5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤1:使用</strong> <code class="du kv kw kx ky b"><strong class="ih hj">pandas</strong></code> <strong class="ih hj">库加载所有必要的库和数据集。</strong></p><pre class="la lb lc ld fd lu ky lv bn lw lx bi"><span id="62e5" class="ly js hi ky b be lz ma l mb mc">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.model_selection import train_test_split<br/>from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF<br/>from sklearn.metrics import classification_report<br/>insurance=pd.read_csv('new_insurance_data.csv') <br/>insurance.head()</span></pre><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es md"><img src="../Images/544318419db826170c801e040663f76f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xUpya6E32ETuE7qnqVnOjA.png"/></div></div><figcaption class="me mf et er es mg mh bd b be z dx">Randomly selected 5 records from the dataset</figcaption></figure><p id="2e4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤2:检查变量的空值、形状和特征数据类型:</strong></p><pre class="la lb lc ld fd lu ky lv bn lw lx bi"><span id="3600" class="ly js hi ky b be lz ma l mb mc"># checks for non-null entries, size and datatype<br/>insurance.info()</span></pre><figure class="la lb lc ld fd le er es paragraph-image"><div class="er es mi"><img src="../Images/790b93982fef52dff106db84c16b7ed1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*xRuYibx2xvWWpeVpTEDWLA.png"/></div></figure><p id="125d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以使用<code class="du kv kw kx ky b">df.isna().sum() </code> <strong class="ih hj"> : </strong>分别检查每个特征的空计数</p><pre class="la lb lc ld fd lu ky lv bn lw lx bi"><span id="b99f" class="ly js hi ky b be lz ma l mb mc">insurance.isnull().sum()<br/># helps me to check for null values</span></pre><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mj"><img src="../Images/52fefe30bc4380b108fb8ad9db55c582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fNhMcq3WFdYoePDHWMTJ_Q.png"/></div></div></figure><p id="a4df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第三步。填充缺失值:</strong>我们可以使用模式填充对象类型特征的缺失值，使用均值或中值填充整数类型特征的缺失值。</p><pre class="la lb lc ld fd lu ky lv bn lw lx bi"><span id="dbd7" class="ly js hi ky b be lz ma l mb mc"># calculating mode for object data type features which will be used to fill missing values.<br/># We have 3 features which are of object type<br/>print(f"mode of sex feature: {insurance['sex'].mode()[0]}")<br/>print(f"mode of region feature: {insurance['region'].mode()[0]}")<br/>print(f"mode of smoker feature: {insurance['smoker'].mode()[0]}")</span></pre><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mk"><img src="../Images/288402ff33cd0a8a70e99e755ec66935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W4iJgfPJRkMP3-uSRKxjNQ.png"/></div></div></figure><pre class="la lb lc ld fd lu ky lv bn lw lx bi"><span id="d2ee" class="ly js hi ky b be lz ma l mb mc"># describe() function will give the descriptive statistics for all numerical features<br/>insurance.describe().transpose()</span></pre><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ml"><img src="../Images/dcecd20536616f02df5d0e2f9addb9d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ah1MrR5jJRTO_5HXmFlL7A.png"/></div></div></figure><p id="33c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们看到，对于数字特征，均值和中值几乎相同。因此，现在我们将把数值特征的空值替换为它们的中值，把分类变量的空值替换为它们的模式。</p><pre class="la lb lc ld fd lu ky lv bn lw lx bi"><span id="0e9c" class="ly js hi ky b be lz ma l mb mc">for col_name in list(insurance.columns):<br/>    if insurance[col_name].dtypes=='object':<br/>        # filling null values with mode for object type features<br/>        insurance[col_name] = insurance[col_name].fillna(insurance[col_name].mode()[0])<br/>    else:<br/>        # filling null values with mean for numeric type features<br/>        insurance[col_name] = insurance[col_name].fillna(insurance[col_name].median())<br/># Now the null count for each feature is zero<br/>print("After filling null values:")<br/>print(insurance.isna().sum())</span></pre><figure class="la lb lc ld fd le er es paragraph-image"><div class="er es mm"><img src="../Images/bd6bcdad7cef5b2069622a7d283a2a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*iRj2xQLLdVlSKd2DjZMrqg.png"/></div></figure><p id="c061" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤4:异常值分析</strong></p><p id="a2a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将为除目标可变电荷之外的所有数字特征绘制一个箱线图。</p><pre class="la lb lc ld fd lu ky lv bn lw lx bi"><span id="fa80" class="ly js hi ky b be lz ma l mb mc">i = 1<br/>plt.figure(figsize=(16,15))<br/>for col_name in list(insurance.columns):<br/>    # total 9 box plots will be plotted, therefore 3*3 grid is taken<br/>    if((insurance[col_name].dtypes=='int64' or insurance[col_name].dtypes=='float64') and col_name != 'charges'):<br/>        plt.subplot(3,3, i)<br/>        plt.boxplot(insurance[col_name])<br/>        plt.xlabel(col_name)<br/>        plt.ylabel('count')<br/>        plt.title(f"Box plot for {col_name}")<br/>        i += 1<br/>plt.show()</span></pre><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mn"><img src="../Images/9cde4c0683b03bedc4689f3ea2ae1025.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kkvqpaIpd6HUuQDrAl9efw.png"/></div></div></figure><p id="4b7d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们看到特征<code class="du kv kw kx ky b">‘bmi’, ‘Hospital_expenditure’</code>和<code class="du kv kw kx ky b">‘Number_of_past_hospitalizations’ </code>有异常值。我们将移除这些异常值:</p><pre class="la lb lc ld fd lu ky lv bn lw lx bi"><span id="4970" class="ly js hi ky b be lz ma l mb mc">outliers_features = ['bmi', 'Hospital_expenditure', 'Anual_Salary', 'past_consultations']<br/>for col_name in outliers_features:<br/>    Q3 = insurance[col_name].quantile(0.75)<br/>    Q1 = insurance[col_name].quantile(0.25)<br/>    IQR = Q3 - Q1<br/>    upper_limit = Q3 + 1.5*IQR<br/>    lower_limit = Q1 - 1.5*IQR<br/>    prev_size = len(insurance)<br/>    insurance = insurance[(insurance[col_name] &gt;= lower_limit) &amp; (insurance[col_name] &lt;= upper_limit)]<br/>    cur_size = len(insurance)<br/>    print(f"dropped {prev_size - cur_size} rows for {col_name}  due to presence of outliers")</span></pre><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mo"><img src="../Images/ee06c565b9091be7d5d3eebaacee5794.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UGWiP_TOXtdKysj1m7d-DQ.png"/></div></div></figure><p id="ba49" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第五步:</strong> <strong class="ih hj">检查相关性:</strong></p><p id="92a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">年龄与收费、年龄与年薪等之间存在相关性。因为它们的相关性大于0.5。</p><pre class="la lb lc ld fd lu ky lv bn lw lx bi"><span id="dc4d" class="ly js hi ky b be lz ma l mb mc">import seaborn as sns<br/>sns.heatmap(insurance.corr(),cmap='gist_rainbow',annot=True)<br/>plt.show()</span></pre><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mp"><img src="../Images/4c1baccd990fc3caa9bd345e111b0acc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zhHPzyyGyueGOszzsGlFZQ.png"/></div></div></figure><p id="57e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">我们将检查特征之间的多重共线性:</strong></p><pre class="la lb lc ld fd lu ky lv bn lw lx bi"><span id="e2c6" class="ly js hi ky b be lz ma l mb mc">from statsmodels.stats.outliers_influence import variance_inflation_factor <br/>col_list = []<br/>for col in insurance.columns:<br/>    if ((insurance[col].dtype != 'object') &amp; (col != 'charges') ):#only num cols except for the charges column<br/>        col_list.append(col)<br/><br/>X = insurance[col_list]<br/>vif_data = pd.DataFrame() <br/>vif_data["feature"] = X.columns <br/>vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))] <br/>print(vif_data)</span></pre><figure class="la lb lc ld fd le er es paragraph-image"><div class="er es mq"><img src="../Images/eadff73ca0b674688a9caf5c2835fa0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*0QxGLqnVbKBXvSYukLleag.png"/></div></figure><p id="d6d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们看到步骤数要素具有最高的共线性，等于61.43，因此我们将删除步骤数要素，并再次检查VIF评分。</p><pre class="la lb lc ld fd lu ky lv bn lw lx bi"><span id="ec6e" class="ly js hi ky b be lz ma l mb mc"># deleting num_of_steps feature<br/>insurance.drop('num_of_steps', axis = 1, inplace= True)<br/>from statsmodels.stats.outliers_influence import variance_inflation_factor <br/>col_list = []<br/>for col in insurance.columns:<br/>    if ((insurance[col].dtype != 'object') &amp; (col != 'charges') ):#only num cols except for the charges column<br/>        col_list.append(col)<br/>X = insurance[col_list]<br/><br/>X = insurance[col_list]<br/>vif_data = pd.DataFrame() <br/>vif_data["feature"] = X.columns <br/>vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))] <br/>print(vif_data)</span></pre><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mr"><img src="../Images/f116fbe8161d516cf84e3fae3caff0f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l3daGLmHyiDwk_EiJvMu2g.png"/></div></div></figure><p id="3b13" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">删除步骤数要素后，年龄具有等于14.63的最高共线性，因此我们将删除年龄要素并再次检查VIF评分。</p><pre class="la lb lc ld fd lu ky lv bn lw lx bi"><span id="2918" class="ly js hi ky b be lz ma l mb mc"># deleting age feature<br/>insurance.drop('age', axis = 1, inplace= True)<br/>from statsmodels.stats.outliers_influence import variance_inflation_factor <br/>col_list = []<br/>for col in insurance.columns:<br/>    if ((insurance[col].dtype != 'object') &amp; (col != 'charges') ):#only num cols except for the charges column<br/>        col_list.append(col)<br/>X = insurance[col_list]<br/><br/>X = insurance[col_list]<br/>vif_data = pd.DataFrame() <br/>vif_data["feature"] = X.columns <br/>vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))] <br/>print(vif_data)</span></pre><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ms"><img src="../Images/ccb2a152dcb8173e15b629a1b9551779.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z2E8I4Nw30HbjmJNyQ2uug.png"/></div></div></figure><p id="d6a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">删除年龄要素后，身体质量指数的最高共线性等于10.36，因此我们将删除身体质量指数要素并再次检查VIF得分。</p><pre class="la lb lc ld fd lu ky lv bn lw lx bi"><span id="8fd1" class="ly js hi ky b be lz ma l mb mc"># deleting bmi feature<br/>insurance.drop('bmi', axis = 1, inplace= True)<br/>from statsmodels.stats.outliers_influence import variance_inflation_factor <br/>col_list = []<br/>for col in insurance.columns:<br/>    if ((insurance[col].dtype != 'object') &amp; (col != 'charges') ):#only num cols except for the charges column<br/>        col_list.append(col)<br/>X = insurance[col_list]<br/><br/>X = insurance[col_list]<br/>vif_data = pd.DataFrame() <br/>vif_data["feature"] = X.columns <br/>vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))] <br/>print(vif_data)</span></pre><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mt"><img src="../Images/32f725a892338c9f6a9a9250a202d53b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*spZuxb01oI1pDQ-bay2qcw.png"/></div></div></figure><p id="8578" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第五步:分离出输入特征和目标变量:</strong></p><pre class="la lb lc ld fd lu ky lv bn lw lx bi"><span id="3410" class="ly js hi ky b be lz ma l mb mc">x=insurance.loc[:,['children','Claim_Amount','past_consultations','Hospital_expenditure','NUmber_of_past_hospitalizations','Anual_Salary']]<br/>y=insurance.loc[:,'charges']<br/>x_train, x_test, y_train, y_test=train_test_split(x,y,train_size=0.8, random_state=0)<br/>print("length of train dataset: ",len(x_train) )<br/>print("length of test dataset: ",len(x_test) )</span></pre><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mu"><img src="../Images/ce2f7c7380d702ee6eba0ce26241acee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gjX5lgEHvfo6lJVMhdTVyQ.png"/></div></div></figure><p id="c67f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤6:在训练集上训练线性回归模型，并在测试数据集上对其进行评估:</strong></p><pre class="la lb lc ld fd lu ky lv bn lw lx bi"><span id="798f" class="ly js hi ky b be lz ma l mb mc">from sklearn.linear_model import LinearRegression<br/>from sklearn.metrics import classification_report, recall_score, r2_score, f1_score, accuracy_score<br/><br/>model = LinearRegression()<br/># train the model<br/>model.fit(x_train, y_train)<br/>print("trained model coefficients:", model.coef_, " and intercept is: ", model.intercept_)<br/># model.intercept_ is b0 term in linear boundary equation, and model.coef_ is<br/>#  the array of weights assigned to ['children','Claim_Amount','past_consultations','Hospital_expenditure',<br/>#                    'NUmber_of_past_hospitalizations','Anual_Salary'] respectively</span></pre><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mv"><img src="../Images/9a4b8059372b1c38547db2c781d698bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rfEo4BGPJ17YWTkYw9lOuQ.png"/></div></div></figure><pre class="la lb lc ld fd lu ky lv bn lw lx bi"><span id="79ec" class="ly js hi ky b be lz ma l mb mc">y_pred = model.predict(x_test)<br/>error_pred=pd.DataFrame(columns={'Actual_data','Prediction_data'})<br/>error_pred['Prediction_data'] = y_pred<br/>error_pred['Actual_data'] = y_test<br/>error_pred["error"] = y_test - y_pred<br/>sns.distplot(error_pred['error'])<br/>plt.show()</span></pre><figure class="la lb lc ld fd le er es paragraph-image"><div class="er es mw"><img src="../Images/fde2d2ce7e8e89e9c7d0c3829406ff0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*VNwyDFAFTlPBO3wcB4MZiA.png"/></div></figure><p id="6a94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以绘制实际目标和残差或误差之间的残差图:</p><pre class="la lb lc ld fd lu ky lv bn lw lx bi"><span id="78cc" class="ly js hi ky b be lz ma l mb mc">sns.scatterplot(x = y_test,y =  (y_test - y_pred), c = 'g', s = 40)<br/>plt.hlines(y = 0, xmin = 0, xmax=20000)<br/>plt.title("residual plot")<br/>plt.xlabel("actural target")<br/>plt.ylabel("residula error")</span></pre><figure class="la lb lc ld fd le er es paragraph-image"><div class="er es mx"><img src="../Images/e50774fa0fac6e61bf9ae2c760b9994e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*S7I2dcPvQu6VorlNeU7ckg.png"/></div></figure><p id="3bf0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> R平方得分:</strong></p><p id="91d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">r的平方被称为<strong class="ih hj">决定系数。</strong> R平方是一种统计度量，表示回归中自变量所解释的因变量中方差的比例。<strong class="ih hj"> </strong>该值范围从0到1。值“1”表示预测器完美地解释了y中的所有变化。值“0”表示预测器“x”没有解释“y”中的任何变化。r平方值包含三项SSE、SSR和SST。</p><p id="62d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> SSE </strong>是误差平方和。它也被称为残差平方和(RSS)</p><p id="4949" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SSR 是回归平方和。</p><p id="2baa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">平方和(SST) </strong>是观察到的因变量与其均值之差的平方。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es my"><img src="../Images/686b38295d1ebf16fc07bd5276ea4e0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_94ntNygm16cu4bCDCyedw.png"/></div></div></figure><figure class="la lb lc ld fd le er es paragraph-image"><div class="er es mz"><img src="../Images/02e94258c46762041cb9e751ddcca885.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*H7yLaQo2BW462IwVcn-sSA.png"/></div></figure><pre class="la lb lc ld fd lu ky lv bn lw lx bi"><span id="89db" class="ly js hi ky b be lz ma l mb mc"># check for model performance<br/>print(f'r2 score of trained model: {r2_score(y_pred=y_pred, y_true= y_test)}')</span></pre><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es na"><img src="../Images/69c8e2b2f2308fd56f066a007938781c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p0rwOBcAeXRnwoDJBB0sAw.png"/></div></div></figure></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><p id="e0c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">线性回归的假设:</strong></p><ol class=""><li id="0a29" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ku jj jk jl bi translated"><strong class="ih hj">线性关系:</strong>线性回归假设预测变量与自变量之间的线性关系。您可以使用散点图来显示2D空间中自变量和因变量之间的关系。</li><li id="aea0" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ku jj jk jl bi translated"><strong class="ih hj">特征之间的多重共线性很小或没有:</strong>线性回归假设特征应该是相互独立的，即特征之间没有相关性。您可以使用VIF函数来查找要素的多重共线性值。一般假设是，如果VIF特征值大于5，那么这些特征高度相关。</li><li id="f986" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ku jj jk jl bi translated"><strong class="ih hj">同质性:</strong>线性回归假设误差项具有恒定的方差，即误差项的扩散应该是恒定的。这个假设可以通过绘制残差图来检验。如果这个假设被违反，那么这些点将形成一个漏斗形状，否则它们将保持不变。</li><li id="4b2d" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ku jj jk jl bi translated"><strong class="ih hj">正态:</strong>线性回归假设给定数据集的每个要素都遵循正态分布。您可以为每个要素绘制直方图和KDE图，以检查它们是否呈正态分布。</li><li id="c133" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ku jj jk jl bi translated"><strong class="ih hj">误差:</strong>线性回归假设误差项也应该是正态分布。您可以绘制误差项的直方图和KDE图，以检查它们是否呈正态分布。</li></ol><p id="eb7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里是代码和数据集的GitHub l <a class="ae nb" href="https://github.com/grgupta13/machine-learning/tree/main/ML" rel="noopener ugc nofollow" target="_blank">墨水。</a></p></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><p id="f661" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">希望你喜欢这篇文章，非常感谢你的宝贵时间。如果你喜欢这篇文章，那么别忘了鼓掌、评论和关注。敬请关注更多文章。</p></div></div>    
</body>
</html>
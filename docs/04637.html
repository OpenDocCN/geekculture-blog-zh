<html>
<head>
<title>A Comprehensive Guide to KNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">KNN综合指南</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/a-comprehensive-guide-to-knn-5b0d4cd90947?source=collection_archive---------47-----------------------#2021-06-30">https://medium.com/geekculture/a-comprehensive-guide-to-knn-5b0d4cd90947?source=collection_archive---------47-----------------------#2021-06-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1bb2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在当今高度互联的互联网时代，全球各地的人们以前所未有的规模日复一日地以不同的方式消费信息，如浏览在线商店的产品、在YouTube/网飞上观看视频、寻找推荐、新闻、搜索信息，所有这些都在以指数速度增长。</p><p id="10db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这样的环境中，提供有效的搜索、有意义的推荐、相关和相似的新闻/文章/产品等对于商业组织来说是非常关键的。在最短的时间内。</p><p id="da3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，当涉及到开发能够在最短时间内提供用户正在寻找的信息的可扩展解决方案时，最近邻算法已经成为一种选择的武器。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/d2889b8475dac91bed9036d21fc1249e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3vjC6IvOYTP1D_VVIN9gjQ.jpeg"/></div></div></figure><p id="8a70" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">什么是KNN算法？</strong></p><p id="4483" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">KNN算法基于一个简单的假设，即相似的事物(属于同一类别的数据点)往往彼此接近。在KNN算法中，给定一个查询点，我们找到它的最近邻，并观察它们是如何被分类的。基于查询点周围的大多数类别，确定查询点的类别。<br/>就这么简单。</p><p id="4517" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">KNN的几个属性:</strong></p><p id="e5e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一种受监督的机器学习算法。<br/>有监督的机器学习是指我们为训练数据集中的数据点提供标签。在我们没有对应于训练数据中的数据点的标签的情况下，这样的问题通过无监督的机器学习算法来解决。</p><p id="75bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">KNN算法可以用来解决二元分类问题以及多类分类问题。在二元分类中，我们只有两个类，其中所有的数据点都被划分，而在多类分类中，我们可以有多个类。</p><p id="fdcb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">KNN也可以用来解决回归问题。</p><p id="f86b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">KNN是一种非参数算法:非参数算法是对训练数据集的底层数据分布不做任何假设的算法。非参数算法有助于解决许多现实世界的问题，因为现实世界的数据集很少遵循任何理论上的数据分布。</p><p id="8236" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">KNN是一个懒惰的算法。术语“懒惰算法”意味着模型实际上不经历训练数据集上的任何训练，即模型不使用训练数据来实现任何泛化。这些模型在测试或预测阶段使用训练数据本身来对查询数据点进行预测。<br/>例如，KNN使用其训练数据点来寻找查询数据点的最近邻居，以便预测其类别标签。</p></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><p id="6845" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">KNN是如何工作的？</p><p id="e716" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上所述，KNN是基于这样一个简单的假设，即相似的事物彼此接近地存在，即在空间/平面中彼此接近的数据点应该属于同一类。使用这个概念，为了预测查询数据点的类标签，KNN遵循以下步骤:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jw"><img src="../Images/38f697de1ddaea8cc39bf0d53c592e6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*WeqfO16UAkKJPGl8IgTl7A.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx">Query point in KNN</figcaption></figure><p id="2554" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第一步:</strong>计算查询点周围邻居的距离。<br/>为了计算距离，我们可以使用像欧几里德距离、曼哈顿距离、汉明距离或闵可夫斯基距离这样的距离度量。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jw"><img src="../Images/ac4fdbd8f6299d2a11eff7cd8e0b5a4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*-cHQxiia_aJAfemtqVZ2gg.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx">Calculate the distances of neighbors around the query point</figcaption></figure><p id="3f2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第二步:</strong>找到最近的K个邻居。<br/> K是您在评估查询数据点的类别标签时要考虑的相邻数据点的数量。至少对于二进制分类而言，K值优于奇数(原因将在下一点解释)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kb"><img src="../Images/082a0646b49213c8f1b291c7bdf36215.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*-IItSEUstuzYBzdNi1ooRQ.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx">K = 3</figcaption></figure><p id="91ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤3: </strong>投票:<br/>每个最近的K个邻居(最近的K个数据点)为他们自己的类标签投票。统计所有的选票，并计算出哪个班级获得了多数票。这就是我们倾向于K取奇数的原因，这样投票就不会出现平局。<strong class="ih hj">获得最高票数的类别被认为是查询数据点的预测类别标签。</strong></p></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><p id="eb74" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">如何选择合适的K值？</strong></p><p id="ff82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在KNN选择k值时，请记住以下几点:</p><p id="4922" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上所述，您应该更喜欢将奇数设置为k。</p><p id="e7a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">小K值导致不稳定的决策边界。为了获得平滑的决策边界，使用K的实际值。<br/>随着K的增加，决策表面的平滑度也增加。</p><p id="34d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，你可以选择一个K的初始值，观察增加或减少K值时的误差率。</p><p id="6d17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了找到K的最佳值，绘制“误差率与K值”图，并选择误差率最小的K值。</p><p id="ed86" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">等于训练数据中数据点总数的K值将导致拟合不足，因为多数类将倾向于支配超空间上的所有点。</p><p id="bd09" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">K变化对决策边界的影响:</strong></p><p id="feff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于K = 1:决策边界不稳定且不平滑。这会导致过度拟合。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kc"><img src="../Images/8bf4754e970b041eec783e74ba2e63b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*HG8XaPSLv_HP13btbMV98w.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx">K = 1</figcaption></figure><p id="ff19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于K = 5:判定边界是平滑的。随着K的增加，决策曲面的平滑度也增加。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kd"><img src="../Images/081c4b419ea9ebda725e83c76f55bd40.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*ZdnGy4cj7YVLM5nw-7tFug.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx">K = 5</figcaption></figure></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><p id="2dec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">KNN的局限性:</strong></p><p id="1a92" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.KNN不适用于非常大的数据集。如果数据集太大，KNN将需要更大的内存，速度会相当慢。你可以尝试一些降维技术，如主成分分析，t-SNE等。在将KNN应用于大型数据集之前。</p><p id="2fb8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.万一，如果一个查询点离训练点太远，我们就不能确定KNN所做的分类预测。</p><p id="5c24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.如果数据传播过于随机，KNN就表现不佳。</p></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><p id="a925" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">加权KNN: </strong></p><p id="a301" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们到目前为止所了解的，特定类的邻居的数量在决定查询数据点的类标签中起着至关重要的作用。此外，为了使预测更准确，我们还可以考虑最近邻居的距离的大小，即给最接近的邻居较高的权重，给较远的最近的邻居较低的权重。这种技术被称为加权KNN。</p><p id="e1b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，我们根据最近的邻居与查询数据点的距离为它们分配权重。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ke"><img src="../Images/60a67fb5efb03ea6a9d2aaa4adbf95b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*o288V8i30jSZIyfUAawxwg.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx">Wi = Weights</figcaption></figure><p id="f2c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">权重与查询点和最近邻点之间的距离成反比。因此，随着距离的增加，权重降低，因此，与更远的最近邻居相比，更近的最近邻居将获得更高的权重。</p><p id="56eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，为了预测查询点的类别标签，代替简单KNN情况下的基于普通数字的投票，这里我们将进行如下操作:</p><p id="4205" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.首先，计算每个最近邻的“距离X权重”(距离乘以权重)。我们就叫它DiWi吧。(距离。数据点Xi的权重)。</p><p id="dd97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.现在，计算所有DiWi的类别和，即，对属于特定类别标签的所有数据点的DiWi求和。K个最近邻中所有数据点的DiWi总和最高的类将成为查询点的预测类。</p><p id="1b07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以进一步探索KNN的各种变体，如KNN使用KD树或LSH。我将在下一篇文章中介绍更多关于KNN的内容。</p><p id="00e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读。</p></div></div>    
</body>
</html>
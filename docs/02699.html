<html>
<head>
<title>Quick Guide: Gradient Descent(Batch Vs Stochastic Vs Mini-Batch)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">快速指南:梯度下降(批量对比随机对比小批量)</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/quick-guide-gradient-descent-batch-vs-stochastic-vs-mini-batch-f657f48a3a0?source=collection_archive---------14-----------------------#2021-05-24">https://medium.com/geekculture/quick-guide-gradient-descent-batch-vs-stochastic-vs-mini-batch-f657f48a3a0?source=collection_archive---------14-----------------------#2021-05-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/6cf40e1bf7659afc21fa6401d4559a54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pv2AJGKWmqbFC9NjAWXuCg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Photo by <a class="ae iu" href="https://unsplash.com/@isaacmsmith?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Isaac Smith</a> on <a class="ae iu" href="https://unsplash.com/s/photos/data?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="fa85" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jt translated"><span class="l ju jv jw bm jx jy jz ka kb di">在</span>这个故事中，我们将看看不同的梯度下降方法。你可能会有一些问题，比如“什么是梯度下降？”，“为什么我们需要梯度下降？”，“梯度下降法有多少种？”等等。到这个故事结束时，你将能够回答所有这些问题。但首先，我们将从线性回归模型开始。为什么？你很快就会知道了。</p><h1 id="8182" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">线性回归</h1><p id="dd3f" class="pw-post-body-paragraph iv iw hi ix b iy la ja jb jc lb je jf jg lc ji jj jk ld jm jn jo le jq jr js hb bi translated">您可能知道，线性回归是一种线性模型，这意味着它是一种发现或定义输入变量(x)和单个输出变量(y)之间的线性关系的模型。更一般地说，线性模型通过计算输入要素的加权和进行预测。数学上，线性回归模型表示为</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/dd058ec117cdf5013c150b5290553907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_nxwlQGQlDoJ5CxIWeOL2w.jpeg"/></div></div></figure><p id="8995" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上述等式可以以矢量化形式表示为</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lk"><img src="../Images/fae00b0360cd133dbf422615d2ae057d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BkJCWxIObIzoAnqM8YSWpA.jpeg"/></div></div></figure><p id="9a6e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，为了训练这个线性回归模型，我们需要找到参数值，以便模型最适合训练数据。为了衡量模型与数据的吻合程度，我们将使用最常用的性能指标，均方根误差(RMSE)。使用以下成本函数计算线性回归模型的RMSE</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ll"><img src="../Images/74b06b4665198f1660564ade3bb9733d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5SfKV9OBbbLYY05tKsz5yw.png"/></div></div></figure><p id="d12f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里，</p><ol class=""><li id="1e57" class="lm ln hi ix b iy iz jc jd jg lo jk lp jo lq js lr ls lt lu bi translated">x⁽ <em class="lv"> ⁱ </em> ⁾是包含iᵗʰ实例所有特征值的向量。</li><li id="a9dc" class="lm ln hi ix b iy lw jc lx jg ly jk lz jo ma js lr ls lt lu bi translated"><em class="lv"> y </em> ⁽ <em class="lv"> ⁱ </em> ⁾是iᵗʰ实例的标签值。</li></ol><p id="729a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以我们需要做的是找到使RMSE最小的<strong class="ix hj"> <em class="lv"> W </em> </strong>的值。我们可以通过正规方程或者梯度下降来实现。</p><h1 id="4b1b" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">正规方程</h1><p id="9d06" class="pw-post-body-paragraph iv iw hi ix b iy la ja jb jc lb je jf jg lc ji jj jk ld jm jn jo le jq jr js hb bi translated">可以使用数学等式来获得使成本函数最小化的<strong class="ix hj"> <em class="lv"> W </em> </strong> <em class="lv"> </em>的值。这个方程称为标准方程，给出如下</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mb"><img src="../Images/7e1880464923314347d87556088b3ae5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VDfbefNxZmw4mgVIILZLsw.jpeg"/></div></div></figure><p id="f5fb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你可以注意到，在正规方程中，我们需要计算Xᵀ.的倒数x，它可以是一个相当大的有序矩阵(n+1)✕(n+1).这种矩阵的计算复杂度大约为O(n)。这意味着如果特征的数量增加，那么计算时间将急剧增加。例如，如果要素数量增加一倍，计算时间将增加大约八倍。此外，当Xᵀ.x是奇异矩阵。</p><p id="d094" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">还有一种类似于正规方程的方法，SVD方法。Scikit-Learn的线性回归类使用SVD方法。在SVD方法中，不是计算逆，而是计算伪逆。奇异值分解方法的计算复杂度大约为O(n)。因此，当特征数量增加一倍时，计算时间会增加四倍。由于伪逆总是为矩阵定义的，SVD的时间复杂度优于前者，因此，SVD方法优于正规方程方法。但是，这种方法仍然不够好。还有一些优化的余地。因此，我们来看另一种方法“梯度下降”。</p><h1 id="8648" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">梯度下降</h1><p id="f96a" class="pw-post-body-paragraph iv iw hi ix b iy la ja jb jc lb je jf jg lc ji jj jk ld jm jn jo le jq jr js hb bi translated">梯度下降是一种寻找最优解的迭代优化算法。梯度下降可用于寻找使可微函数最小化的参数值。该算法背后的简单思想是迭代地调整参数以最小化成本函数。</p><p id="4c2a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在梯度下降法中，我们从参数的随机值开始。在每次迭代之后，我们朝着减少函数值的值移动，或者我们可以说，我们朝着函数的斜率下降或为负的方向移动(这就是为什么它被称为梯度下降)。</p><p id="ee76" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于我们的例子，我们从随机值<em class="lv"> W. </em>开始，随着我们一步一步地向前推进，<em class="lv"> W </em>的值逐渐提高，也就是说，我们一步一步地降低成本函数(RMSE)的值。每一步都被称为学习步骤。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/aa0e9fdce61c92eee273c8fd2fec8991.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*ojGj0ylkumr2EjXd2bW6SQ.png"/></div></div></figure><p id="d3cc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里要注意的一点是，我们需要学习的步骤的大小非常重要。我们需要确定步长的大小，以便在更少的步骤中获得最佳值<em class="lv"> W </em>步长的大小由超参数调用<em class="lv">的学习速率决定。</em>如果学习率太小，则该过程将花费更多时间，因为算法将经历大量迭代才能收敛。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es md"><img src="../Images/41b26fd4599869510890d473c07aa207.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*XIuVmU3x4Xu-MF02mdh5Pg.png"/></div></figure><p id="c8cc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">另一方面，如果学习率太大，与前面的步骤相比，您可能会增加成本函数值。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es me"><img src="../Images/f2019fc213f5824e5d00e7cfb97e8c70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*szBzk39GaNEtZD3QRscDQg.png"/></div></figure><p id="aa9e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在函数具有不同的局部极小值和一个全局极小值的情况下，找到一个合适的学习率是一项相当艰巨的任务，因为你可能最终会在局部极小值处结束。但是，RMSE成本函数是一个凸函数，这意味着它只有一个全局最小值。此外，函数是连续和平滑的，这给了我们优势。</p><p id="371b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在实现梯度下降时，我们需要计算代价函数相对于每个模型参数<strong class="ix hj"><em class="lv">【w】</em></strong><a class="ae iu" href="https://www.compart.com/en/unicode/U+2C7C" rel="noopener ugc nofollow" target="_blank"><strong class="ix hj">【ⱼ】</strong></a>的梯度，也就是说，我们需要计算代价函数对模型参数的偏导数。由于对均方误差(均方根)的求导比RMSE更简单，并且它也解决了我们的目的，因为最小化均方误差也将最小化RMSE(因为RMSE只是均方误差的平方根)，所以我们将使用均方误差作为成本函数。</p><p id="4550" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">关于模型参数<strong class="ix hj"><em class="lv">w</em></strong><a class="ae iu" href="https://www.compart.com/en/unicode/U+2C7C" rel="noopener ugc nofollow" target="_blank"><strong class="ix hj">ⱼ</strong></a>的成本函数(MSE)的偏导数如下</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/1c27203eef50c58ce574539e071b2845.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oenQIn7SUxC9f6zo9Eoq-Q.jpeg"/></div></div></figure><p id="285a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">或者我们可以使用下面的等式直接计算梯度向量，该梯度向量包含每个模型参数的成本函数的所有偏导数。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/a6305b648b455e2966c8619b919d32ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uIIYuroFTipBVgJEJrLgpg.png"/></div></div></figure><p id="8790" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们得到了梯度向量，我们需要从参数向量<em class="lv"> W. </em>中减去这个乘以学习率(用η表示)的梯度向量</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/8a40a0f649c22aa19d848ac452c8b26b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QV9ZVmPXn_AxgIKt9QOSxw.png"/></div></div></figure><p id="7925" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们要决定迭代的次数，也就是重复上述过程的次数，之后我们就会有解了。一个简单的方法是选择大量的迭代。迭代时，当梯度向量的范数变得非常小，小于一个叫做容差(用ϵ表示)的极小值时停止，或者你也可以在代价函数开始增加时停止。</p><blockquote class="mi mj mk"><p id="7b20" class="iv iw lv ix b iy iz ja jb jc jd je jf ml jh ji jj mm jl jm jn mn jp jq jr js hb bi translated">注意:我们还需要在梯度下降之前执行特征缩放，否则将需要更长的时间来收敛。</p></blockquote><p id="26d9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">梯度下降的类型</p><ol class=""><li id="056a" class="lm ln hi ix b iy iz jc jd jg lo jk lp jo lq js lr ls lt lu bi translated">批量梯度下降。</li><li id="5ce4" class="lm ln hi ix b iy lw jc lx jg ly jk lz jo ma js lr ls lt lu bi translated">随机梯度下降。</li><li id="7f0b" class="lm ln hi ix b iy lw jc lx jg ly jk lz jo ma js lr ls lt lu bi translated">小批量梯度下降。</li></ol><h1 id="abca" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">批量梯度下降</h1><p id="9888" class="pw-post-body-paragraph iv iw hi ix b iy la ja jb jc lb je jf jg lc ji jj jk ld jm jn jo le jq jr js hb bi translated">上面我们看到的方法是分批梯度下降。正如你可能已经注意到的，在计算梯度向量∇ <em class="lv"> w，</em>时，每一步都涉及对完整训练集<strong class="ix hj"> X </strong>的计算。由于该算法使用整批训练集，因此称为批梯度下降。</p><p id="e290" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在大量特征的情况下，批量梯度下降比正常方程方法或SVD方法表现得更好。但是在非常大的训练集的情况下，还是相当慢的。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mo"><img src="../Images/b9f805bff6c1c621a6edf3aec5655caf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BsrN93X17ufvhTaU5wlUNA.png"/></div></div></figure><h1 id="404f" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">随机梯度下降</h1><p id="d91e" class="pw-post-body-paragraph iv iw hi ix b iy la ja jb jc lb je jf jg lc ji jj jk ld jm jn jo le jq jr js hb bi translated">对于大型训练集，批量梯度下降变得非常慢，因为它使用整个训练数据来计算每一步的梯度。但随机梯度下降(SGD)的情况并非如此，因为在该算法中，从训练集中选择随机实例，并且仅使用该单个实例来计算梯度。这使得算法更快，因为它在每一步都必须处理非常少的数据。</p><p id="341b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于SGD的随机性质，成本函数上下跳跃，仅平均下降。因此，很有可能最终的参数值是好的但不是最好的。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mo"><img src="../Images/d7bc0fcd9de6a68682c77e356ffcebe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1AOm9ZhCUvRaEPNG1mbhWQ.png"/></div></div></figure><h1 id="8d5e" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">小批量梯度下降</h1><p id="1542" class="pw-post-body-paragraph iv iw hi ix b iy la ja jb jc lb je jf jg lc ji jj jk ld jm jn jo le jq jr js hb bi translated">这是我们要看的最后一个梯度下降算法。您可以将该算法称为批处理和随机梯度下降之间的中间地带。在该算法中，使用来自训练集的随机实例集来计算梯度。这些随机组被称为<em class="lv">小批量。</em></p><p id="9417" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">小批量GD比SGD稳定得多，因此该算法将给出比SGD更接近最小值的参数值。此外，在使用小批量GD时，我们可以从矩阵运算的硬件(尤其是GPU)优化中获得性能提升。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mo"><img src="../Images/b04b74b792d922edb8025840e76a0421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JEK19kVPv4JDdN5HBWPrBQ.png"/></div></div></figure></div><div class="ab cl mp mq gp mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="hb hc hd he hf"><p id="b232" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，小批量GD和随机GD将在最小值附近结束，批量GD将恰好在最小值处停止。但是Batch GD每走一步都要花很多时间。此外，如果我们使用一个好的学习时间表，随机GD和小批量GD将达到最小值。</p><p id="3959" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以现在，我认为你能够回答我在本文开始时提到的问题。如果你想看一个简单的python实现上述方法，这里是<a class="ae iu" href="https://github.com/tombro27/Gradient-Descent/blob/main/Gradient_Descent.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj"> <em class="lv">链接</em> </strong> </a>。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mw"><img src="../Images/9daa1fa2027f700852239f3636b23fa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VVbccmTR155jKLj25eRhQQ.png"/></div></div></figure></div></div>    
</body>
</html>
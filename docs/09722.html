<html>
<head>
<title>Google AI and Princeton discover this about Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">谷歌人工智能和普林斯顿发现了深度学习</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/google-ai-collaborated-with-princeton-to-discover-this-about-deep-learning-e6a0c6f7cae5?source=collection_archive---------2-----------------------#2021-12-25">https://medium.com/geekculture/google-ai-collaborated-with-princeton-to-discover-this-about-deep-learning-e6a0c6f7cae5?source=collection_archive---------2-----------------------#2021-12-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="8d33" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">他们的发现改变了我们对深度学习行为的认识</h2></div><p id="cbfb" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了帮助我了解您<a class="ae jt" href="https://forms.gle/7MfQmKhEhyBTMDUD7" rel="noopener ugc nofollow" target="_blank">请填写此调查(匿名)</a></p><p id="2e3c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ML模型的学习结果很大程度上取决于模型的学习速率。学习率是优化算法中的调整参数，该优化算法确定每次迭代的步长，同时向损失函数的最小值移动。由于它影响了新获得的信息覆盖旧信息的程度，因此它隐喻性地代表了机器学习模型“学习”的速度。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es ju"><img src="../Images/24023867f8085817494e413687e22b43.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*2y-mJ8bSUzhfNhW7oueVpA.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx">Learning Rate can make or break your model. This is why there are so many variants in practice.</figcaption></figure><p id="3bdc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">学习率的重要性不可小觑。这就是为什么有很多研究致力于发现新的学习速率表(LR应该如何随时间变化)和比较现有的学习速率表。谷歌人工智能、特拉维夫大学和普林斯顿大学的研究人员合作编写了<a class="ae jt" href="https://arxiv.org/abs/2002.11803" rel="noopener ugc nofollow" target="_blank"> <strong class="iz hj">，将自适应梯度方法与学习速率</strong> </a>分开。本文着眼于"<em class="kg">自适应梯度方法如何与学习速率表相互作用。在这篇文章中，我将分享一些有趣的文章，也许会对你的ML之旅有所帮助。通过评论或DMs分享你对论文的想法和你觉得最有趣的想法。这是我们大家互相学习/获得新思想的好方法。如果你希望从事机器学习方面的工作，看看这个视频，它解释了什么样的机器学习项目将最有助于加速你的成长和找到工作。</em></p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es kh"><img src="../Images/f77a1f82b686e18797899f57f836d6f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*b04b6pyB7JXstMCM5N3dXg.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx">Conversations like this are always interesting. Use IG/LinkedIn to reach out.</figcaption></figure><h1 id="4eb5" class="ki kj hi bd kk kl km kn ko kp kq kr ks io kt ip ku ir kv is kw iu kx iv ky kz bi translated">理解上下文</h1><p id="2f49" class="pw-post-body-paragraph ix iy hi iz b ja la ij jc jd lb im jf jg lc ji jj jk ld jm jn jo le jq jr js hb bi translated">为了理解这篇论文，理解他们所运用的理论的基础是很重要的。通常认为像Adam这样的学习率调度器很棒，因为它们能够计算两个方面，幅度和方向。可以把幅度想象成步长的绝对值，把方向想象成步长的方向。请记住，由于机器学习中的大量输入数据是高维的，因此选择正确的方向进行遍历不是一件小事。下图很好地概括了二阶矩优化器。如果你不能理解所有的东西，不要担心，只要注意我们是如何根据我们计算的梯度来改变不同的值。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es lf"><img src="../Images/eac16cb9a02e0e0a279bd7ae07a1d521.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TKJN3n6mKj5FwmSUH7jIDA.png"/></div></div><figcaption class="kc kd et er es ke kf bd b be z dx">Notice how we always tweak the parameters by a function of the gradient</figcaption></figure><p id="e7fa" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">本文作者试图说明选择正确的步长在学习行为中有多么重要。他们通过提出一个嫁接实验来做到这一点。我们不是从同一个优化器获取幅度和方向，而是从两个不同的优化器获取。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lk"><img src="../Images/19923990650a78da9853eca31bc7ffd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*TQdFHgt_lhIOALpnPhhHqw.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx">Notice how we are computing the magnitude and direction seperately.</figcaption></figure><p id="eb4b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，通过比较嫁接优化器在各种任务中的行为，我们可以检查步长对模型学习的整体性能有多重要。如果我们在使用相同优化协议的任务中看到相对一致的性能(尽管方向不同)，我们可以得出结论，步长是最重要的。反之亦然。</p><h1 id="a5f0" class="ki kj hi bd kk kl km kn ko kp kq kr ks io kt ip ku ir kv is kw iu kx iv ky kz bi translated">计算机视觉性能</h1><p id="6414" class="pw-post-body-paragraph ix iy hi iz b ja la ij jc jd lb im jf jg lc ji jj jk ld jm jn jo le jq jr js hb bi translated">作为谷歌人工智能的一个宠儿，没有一篇未经计算机视觉任务测试的谷歌论文是完整的。设置细节如下，"<em class="kg">我们在具有26M参数的50层剩余网络[HZRS16]上运行所有嫁接优化器对，并在ImageNet分类[DDS+09]上进行训练。我们使用大规模训练基础设施支持的4096的批量大小，以及由线性预热和逐步指数衰减组成的学习率计划。</em></p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es ll"><img src="../Images/10966a394f1d82dacabd06b11ecd94f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iUdr-FI0H5b3suSE8YPG1A.png"/></div></div><figcaption class="kc kd et er es ke kf bd b be z dx">They tested all pairs for grafting. All the rows control for implicit step size.</figcaption></figure><p id="9901" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">结果很有意思。我们看到每行的值在各行之间相对一致(前1名和前5名精度)(步长来自同一个优化器)。各列之间有相当多的差异(来自同一个优化器的方向)。这导致了一个非常有趣的结论。步长似乎是模型学习行为的主要因素。作者通过陈述“<em class="kg">阐明了这一点。图1简要显示了我们的主要经验观察结果:训练曲线的形状通过选择M进行聚类，M是提供步长的优化器。</em></p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es lm"><img src="../Images/bef0bffd69b913431ea59cc6e0d2b618.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2M_jkQZNU-UyCiAVSSarAA.png"/></div></div></figure><p id="286d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这本身就足够有趣了。但是仔细想想，这就引出了一个有趣的应用。想象一个数据集，其中我们的优化器比另一个更差。然而，优化器1的运行成本更低。因此，我们使用优化器2来计算步长，并将其移植到1上。这将提高1的性能，同时比优化器2更便宜。看看AdaGrad的研究结果来证明这个想法的概念。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es ln"><img src="../Images/af0ed8824adf583d5864c136c3c97990.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EpmemMIqwX1rfpM49LbsiA.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx">The appendix has a lot of the detail, if you are looking to implement this yourself.</figcaption></figure><p id="ecea" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于那些对移植的实现感兴趣的人，这在附录中。查看它的许多小配置/技术细节。如果你想实现类似的东西，这将是有帮助的。</p><h1 id="d0ba" class="ki kj hi bd kk kl km kn ko kp kq kr ks io kt ip ku ir kv is kw iu kx iv ky kz bi translated">NLP性能</h1><p id="63a4" class="pw-post-body-paragraph ix iy hi iz b ja la ij jc jd lb im jf jg lc ji jj jk ld jm jn jo le jq jr js hb bi translated">接下来，我们继续进行自然语言处理任务。根据作者，“<em class="kg">对于现实的大规模NLP设置，我们在具有375M参数的6层变压器网络[VSP+17]上训练所有嫁接的优化器，在WMT14英语-法语翻译任务上，该任务具有3630万个句子对。同样，我们使用大批量(384个序列)，实现非常强大的训练设置。更多详情请参见附录C.3. </em>“性能如下所示</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lo"><img src="../Images/74247c830b5e658f995ab5b257b39d0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*vGleL_7-nTVUjKj57AHnzg.png"/></div></figure><p id="1548" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">查看下表(在一个单独的任务中进行)，我们再次看到跨行的值比跨列的值稳定得多(尽管该任务的整体性能更稳定)。这再一次暗示了这样一个观点，即大小是比方向更重要的因素。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lp"><img src="../Images/e6ec20c02742ecd238609f58cdc5a3ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*bRUQFj1pSkJ03q8_nG8prg.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx">AdaGrad grafted with Adam is the best performer</figcaption></figure><p id="186a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">事实上，我们看到最好的结果来自嫁接的表演者。作者甚至在他们的评论中强调了这一点，“<em class="kg">有趣的是，除了通过选择M来展示相同的性能指标聚类之外，这些实验还表明，嫁接优化器有可能优于基本方法M和D；损失曲线见图2，下游BLEU指标见表3.3，我们的结果与之一致。同样，我们并不是在谨慎的调整下宣称绝对的优势，而只是自举的力量；我们强调，我们甚至没有调整全局学习率标量。</em></p><p id="1ce0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">综合计算机视觉和自然语言处理的结果对我来说相当有说服力。令人惊讶的是，我们可以如此全面地展示步长的优势。升压陷阱的潜力也很吸引人，如果我们能够建立评估协议来识别特定问题的最佳组合，那将会很有趣。</p><p id="3f92" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你喜欢这篇文章，看看我的其他内容。我定期在Medium、YouTube、Twitter和Substack上发帖(所有链接都在下面)。我专注于人工智能、机器学习、技术和软件开发。如果你正在准备编码面试，看看:<a class="ae jt" href="https://codinginterviewsmadesimple.substack.com/" rel="noopener ugc nofollow" target="_blank">编码面试变得简单</a>。</p><p id="b317" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为一次性支持我的工作，以下是我的Venmo和Paypal。任何数额都值得赞赏，并有很大帮助:</p><p id="b57d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">https://account.venmo.com/u/FNU-Devansh<a class="ae jt" href="https://account.venmo.com/u/FNU-Devansh" rel="noopener ugc nofollow" target="_blank"/></p><p id="d1c2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">贝宝:<a class="ae jt" href="https://www.paypal.com/paypalme/ISeeThings" rel="noopener ugc nofollow" target="_blank">paypal.me/ISeeThings</a></p><h1 id="5210" class="ki kj hi bd kk kl km kn ko kp kq kr ks io kt ip ku ir kv is kw iu kx iv ky kz bi translated">向我伸出手</h1><p id="abef" class="pw-post-body-paragraph ix iy hi iz b ja la ij jc jd lb im jf jg lc ji jj jk ld jm jn jo le jq jr js hb bi translated">如果那篇文章让你对联系我感兴趣，那么这一部分就是为你准备的。你可以在任何平台上联系我，或者查看我的其他内容。如果你想讨论家教，发短信给我。如果你想支持我的工作，使用我的免费罗宾汉推荐链接。我们都有免费的股票，对你没有风险。<strong class="iz hj">所以不使用它只是失去免费的钱。</strong></p><p id="854d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">查看我在Medium上的其他文章。:<a class="ae jt" href="https://rb.gy/oaojch" rel="noopener ugc nofollow" target="_blank">https://rb.gy/zn1aiu</a></p><p id="59e0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我的YouTube:<a class="ae jt" href="https://rb.gy/88iwdd" rel="noopener ugc nofollow" target="_blank">https://rb.gy/88iwdd</a></p><p id="0c48" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在LinkedIn上联系我。我们来连线:<a class="ae jt" href="https://rb.gy/f7ltuj" rel="noopener ugc nofollow" target="_blank">https://rb.gy/m5ok2y</a></p><p id="e2a4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我的insta gram:【https://rb.gy/gmvuy9 T2】</p><p id="4136" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我的推特:【https://twitter.com/Machine01776819 T4】</p><p id="9c66" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我的子任务:<a class="ae jt" href="https://codinginterviewsmadesimple.substack.com/" rel="noopener ugc nofollow" target="_blank">https://codinginterviewsmadesimple.substack.com/</a></p><p id="6958" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">获得罗宾汉的免费股票:<a class="ae jt" href="https://www.youtube.com/redirect?redir_token=QUFFLUhqa0xDdC1jTW9nSU91WXlCSFhEVkJ0emJvN1FaUXxBQ3Jtc0ttWkRObUdfem1DZzIyZElfcXVZNGlVNE1xSUc4aVhSVkxBVGtHMWpmei1lWWVKNzlDUXVJR24ydHBtWG1PSXNaMlBMWDQycnlIVXNMYjJZWjdXcHNZQWNnaFBnQUhCV2dNVERQajFLTTVNMV9NVnA3UQ%3D%3D&amp;q=https%3A%2F%2Fjoin.robinhood.com%2Ffnud75&amp;v=WAYRtSj0ces&amp;event=video_description" rel="noopener ugc nofollow" target="_blank">https://join.robinhood.com/fnud75</a></p></div></div>    
</body>
</html>
<html>
<head>
<title>SinglePose and MultiPose Estimation papers Collection and Summary</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单项和多项评估论文收集和总结</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/collection-and-summary-of-singlepose-and-multipose-estimation-papers-bb807613c32a?source=collection_archive---------72-----------------------#2021-06-21">https://medium.com/geekculture/collection-and-summary-of-singlepose-and-multipose-estimation-papers-bb807613c32a?source=collection_archive---------72-----------------------#2021-06-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="d8ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将分享关于人体姿态估计的优秀论文的摘要。本文主要关注单姿态和多姿态估计。希望你会发现它内容丰富。</p><p id="4c18" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">单姿态估计</strong></p><p id="6745" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 1。DeepPose:通过深度神经网络进行人体姿态估计</strong> : Toshev et。艾尔。还有赛格迪等人。艾尔。将DeepPose表示为用于人体姿势估计的应用深度学习方法之一。本文将姿态估计方法设计为基于CNN的回归模型，用于估计人体关节的关键点。这里使用基于级联CNN的回归方法来重新调整身体关节的关键点。基于CNN的回归模型被有效地使用，因为它也可以估计某些隐藏的身体关节。</p><p id="75b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">模型</em> : </strong>深度姿态模型的功能分别如图5和图6所示。图5示出了使用CNN的第一阶段回归模型。图6示出了使用CNN的第二阶段回归模型。该模型的一个重要特征是，它使用级联回归和反馈系统来提炼关键点。在模型的第一阶段，特定的关键点被估计为粗调。关键点估计图像在预测关节周围被裁剪，并作为下一阶段的输入给出，该下一阶段用于估计姿态的微调。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/3e21010ba1dd9dfba5a0991412f5fdd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*fz9dkk_c5ERE8h1NiZtqJw.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx"><strong class="bd jq">Fig. 1.</strong> First Stage Regression Model using CNN</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/452668b0838137155be7e45fb9a019c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*29tTX_XicShPkZ8Sd7bk9Q.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx"><strong class="bd jq">Fig. 2.</strong> Second Stage Regression Model using CNN</figcaption></figure><p id="731f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">评价参数</em> : </strong>正确零件百分比(PCP)作为评价参数。</p><p id="1475" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">使用的数据集</em> : </strong> LSP和FLIC</p><p id="351d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">测试结果讨论</em> : </strong>三级深蹲给出了上臂和下臂正确部位的百分比分别达到0.56和0.38。最大PCP在上下支腿分别达到0.78和&amp; 0.71。平均正确率达到0.61%。</p><p id="d69b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。具有迭代误差反馈的人体姿态估计:</strong> Joao等，Pulkit等。艾尔。、卡特琳娜等人。艾尔。和Jitendra等人。艾尔。表示这种ConvNet迭代误差反馈模型。卷积网络(ConvNets)已经被用于使用前馈处理顺序地提取其特征用于多样化的分类任务。多层层次用于通过ConvNets表示图像。为输入和输出的结构2D建模提供了遗传框架。使用自上而下的反馈方法。网络不会直接预测结果，而是首先估计误差，并通过迭代的方式重新训练误差，直到达到预期的阈值。该模型已标注了17个关键点。</p><p id="41d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">模型的功能</em> : </strong>迭代反馈模型及其机制如图7和图8所示。当图像I被输入网络时，它将估计2D点集中的一些关键点y0。根据将函数f视为ConvNet的图表，该模型对几个方程起作用。函数g将把每个关键点位置转换成一个高斯热图。该模型可以学习身体关节空间配置和图像。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jr"><img src="../Images/ea3c3ecc4f3a6bc0843d55c184a088cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*SANI063kOHIiW4BVJgo3nw.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx"><strong class="bd jq">Fig. 3.</strong> An implementation of Iterative Error Feedback</figcaption></figure><p id="cfdd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ConvNet架构在ImageNet上进行了预训练。过滤器被修改为第一conv层的17个关键点。Conv-1层具有20个不同的输入通道，其中3个是具有预训练权重的ImageNet，而剩余的17个通道由方差为0.1的高斯噪声初始化。该模型在遮挡图像和可见图像上进行训练。为了获得遮挡点，反向传播梯度为零。然后，通过ConvNet和高斯滤波器，这些点变得可见。</p><p id="4dc5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">评估参数</em> </strong>:正确关键点百分比(PCKh)作为评估参数。</p><p id="6f94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">使用的数据集</em> : </strong> LSP和MP-II</p><p id="6a06" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">测试结果讨论:</em> </strong> IEF在头部检测和肩部检测中达到最大准确度95.5和91.6，上身和全身预测分别为81.9和81.3。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es js"><img src="../Images/e382865ba9a97950573cb4ea5f9c8161.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*2SHtaxc7W06OYgW_I9gAkA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx"><strong class="bd jq">Fig. 4.</strong> Result of Iterative Error Feedback</figcaption></figure><p id="d0cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。用于人体姿态估计的堆叠沙漏网络。艾尔。、余凯等人。艾尔。还有简等人。艾尔。代表这种独特的模型，它是一种扩展的跳网，称为堆栈沙漏网络。单个沙漏形网络由多个沙漏形网络首尾相连地连续放置而成。自上而下和自下而上的方法同时应用。在与完全连接的CNN和其他设计层紧密堆叠之前，该模块处理各种尺度的空间信息，以进行密集预测。</strong></p><p id="9a00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">型号</em>的功能:</strong>单管道与跳过层一起使用，以在每个单一分辨率下保存空间信息。<strong class="ih hj"> </strong>通过Conv层和最大池，当特征达到最低分辨率时，网络开始自上而下的方法，这是一系列上采样和不同尺度下特征的不同组合。当标准卷积层应用大型过滤器时，网络性能有所提高。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jt"><img src="../Images/47698077d587ffde011a0320b0e4fc8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*q279SNq_BDaPcF_8SByxzA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx"><strong class="bd jq">Fig. 5.</strong> Stack Hourglass key point annotations using heatmap</figcaption></figure><p id="4072" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">评价参数</em> </strong>:正确关键点百分比(PCKh)作为评价参数。</p><p id="d1d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">数据集使用了</em> : </strong> FLIC和MP-II</p><p id="1a79" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">测试结果讨论:</em> </strong>该模型对头部和肩部的预测率最高，分别为98.2和96.3，对脚踝的预测率最低，在MP-II人体姿态下为83.6。在FLIC数据集上，肘部和手腕的准确率分别达到了99%和97%。</p><p id="7c0c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4。人体姿态估计和跟踪的简单基线:</strong> Bin et。艾尔。、平海等人。艾尔。和陈一等人。艾尔。表示一个模型，它是一个网络，末端有ResNet和几个反卷积层。均方差用作预测和目标热图之间的损失。使用2D高斯滤波器生成目标热图。他们的模型结合了去卷积层中的上采样和卷积参数，而没有使用跳过层连接。</p><p id="676d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">型号</em>功能:</strong>该网络参照沙漏型和级联金字塔型网络构成。正常的卷积发生在起始层，当压缩到一定程度的数据通过反卷积进行采样处理时，与密度较低的架构相比，反卷积听起来更有效。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es js"><img src="../Images/b24f9e9afbdd9deab092de7b1ee7e027.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*n8YgZ67peBDpEqOuU9x_Xw.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx"><strong class="bd jq">Fig. 6.</strong> Comparison of different architectures</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ju"><img src="../Images/86e3d85660cf9512b44f210517631b1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*i2eVusrHJ7Yp4f0czP78Ag.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx"><strong class="bd jq">Fig. 7.</strong> Training of ResNet + DeConv layers</figcaption></figure><p id="f8ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">评估参数</em> </strong>:平均精度(AP)作为评估参数。</p><p id="8df4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">使用的数据集</em> : </strong> COCO</p><p id="68f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">测试结果讨论:</em> </strong>用ResNet-152架构训练的COCO测试开发上AP为73.7，AP50为91.9。</p><p id="2c9a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 5。用于人体姿态估计的深度高分辨率表示学习:</strong>孙克等。艾尔。、斌等人。艾尔。、董等人。艾尔。还有京东等。艾尔。表示基于自上而下方法的从高到低的分辨率网络，一个接一个地并行步进。它不使用中间热图监督。热图通过MSE损失进行分析。</p><p id="41ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">模型的功能</em> : </strong>该网络并联连接高到低分辨率的子网络，以获得更精确的空间热图。它在整个过程中保持高分辨率，多分辨率表示有助于通过热图进行图像分割和关键点注释。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jv"><img src="../Images/d9fd497d8953b83175d36b0faee2e5e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*1askrKC7QSHljPqu8WKGdw.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx"><strong class="bd jq">Fig. 8.</strong> HRNet</figcaption></figure><p id="6fbd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">评价参数</em> </strong>:平均精度(AP)作为评价参数。</p><p id="74d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">使用的数据集</em> : </strong> COCO和MP-II</p><p id="dbe4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">测试结果讨论:</em> </strong>用HRNet-W48架构训练的AP为75.5，AP50为92.5。用HRNet-W48 +额外数据架构训练的AP是77.0，AP50是92.7。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es jw"><img src="../Images/37f0dc6e7dcf2f1a0835395ad156c3a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WwmN9Th9EWHCoaAc"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx">Photo by <a class="ae kb" href="https://unsplash.com/@jon_chng?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jonathan Chng</a> on <a class="ae kb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="f218" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">多状态估计</strong></p><p id="0408" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 1。OpenPose:使用局部亲和场的实时多人2D姿态估计。艾尔。、吉内斯等人。艾尔。、托马斯等人。艾尔。，施恩等人。艾尔。和亚塞尔等人。艾尔。提出了一种实时的多人2D姿态估计方法，该方法使用了局部相似场，使得网络可以学习每幅图像上的身体部位。</strong></p><p id="3f19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">模型的功能</em> : </strong>该模型基于如图13所示的相似性方法工作。每一部分都应用了相似度模型和置信度图方法。在这个网络中，感受野被保留，计算量减少。对图像的每个像素点进行局部相似性滤波，以获得正确的关键点方向。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kc"><img src="../Images/7f534216731b7f94d0af393bb9d58e54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*j3tYpdJo-t2C0t4OtOu9BQ.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx"><strong class="bd jq">Fig. 9.</strong> Multi pose estimation and part affinity fields</figcaption></figure><p id="7123" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">评价参数</em> </strong>:正确关键点百分比(PCKh)作为评价参数。</p><p id="2c25" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">使用的数据集</em> : </strong> COCO、MP-II和COCO + Foot</p><p id="61ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">测试结果讨论:</em></strong>MP-II的测试结果是头部最高，为91.2，脚踝最低，为61.7。</p><p id="5a83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。DeepCut:用于多人姿势估计的联合子集划分和标记。艾尔。、埃尔达等人。艾尔。、于斯等人。艾尔。、Bjoern等人、Mykhaylo等人。艾尔。、彼得等人。艾尔。还有Bernt等人。艾尔。表示多尺度下的关节姿态估计方法。它能够调整被遮挡的图像。基于CNN的部分检测器用于分割和标记身体部分集合的形成。</strong></p><p id="81b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">模型的功能</em> : </strong>用于身体部位探测器快速R-CNN方法。采用了基于VGG模型的密集CNN模型。该公式提出了联合子集划分和标记问题。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kd"><img src="../Images/9a555391798c1e827df8d6e90af3bc76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*uuAiXD1400ldxFU6Ohf9Yg.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx"><strong class="bd jq">Fig. 10.</strong> DeepCut keypoint procedure</figcaption></figure><p id="362a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">评价参数</em> </strong>:正确关键点和零件的百分比(PCKh)和(PCP)作为评价参数。</p><p id="0d69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">数据集使用了</em> : </strong> LSP(单姿态)和We Are Family (WAF-多姿态)</p><p id="99e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">测试结果讨论:</em></strong>LSP上的姿态估计是基于PCP标度的，在AFR CNN和Dense CNN上躯干最大，分别为92.9和96.0，前臂最小，分别为64.2和71.7。</p><p id="db97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。RMPE:区域多人姿态估计:</strong>郝舒等。艾尔。、秦书等人。艾尔。、余荣等人。艾尔。、吴策等人。艾尔。表示自顶向下的多姿态估计方法。对称空间变换网络用于从不合适的包围盒中提取高质量的单人区域。单人姿态估计器用于预测人体姿态骨架的调整区域。对于重映射，使用空间去变换器网络来将人体姿态映射到实际的图像坐标系。为了处理冗余姿态推导的问题，使用了参数化姿态非最大值抑制方法。</p><p id="e7df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">模型的功能</em> : </strong>该方法使用基于VGG的SSD-512作为人体检测器，因为它对于基于识别的方法是有效的。堆栈沙漏网络也用于单人姿势估计。对于STN，使用了ResNet-18。基于ResNet — 152的快速RCNN用于姿态估计，用PyraNet代替姿态网络。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jt"><img src="../Images/3f31b3b41ea7531646f913a35aa56844.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*AUwvWLpu75xbMvqvLms_mA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx"><strong class="bd jq">Fig. 11.</strong> RMPE keypoint procedure</figcaption></figure><p id="2794" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">评估参数</em> </strong>:平均精度(AP)作为评估参数。</p><p id="5b54" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用的<strong class="ih hj"> <em class="jd">数据集</em> : </strong> MP-II和MSCOCO</p><p id="312b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">测试结果讨论:</em></strong>MP-II上的平均准确率为72 mAP。当网络使用基于更快RCNN的ResNet时，观察到最大准确度，头部为91.3，手腕最低为76.4。</p><p id="6d52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4。面具R-CNN: </strong>明凯等人。艾尔。、乔治亚等人。艾尔。彼得·多乐等人。艾尔。还有罗斯等人。艾尔。表示用于执行语义和实例分段的lionize架构。它是扩展的快速RCNN模型。在需要提取数据或特征的情况下，将图像调整到掩模尺寸。</p><p id="3e73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">模型的功能</em> : </strong>基本架构首先使用CNN从图像中提取特征图。这也是一种在目标图像上生成蒙版的自顶向下的方法。ResNeXt用于训练图像。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jt"><img src="../Images/59bc00b74ff6c10ebf457b8f058cd36c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*zEvbxLYo_YU6WAiZPpnn6Q.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx"><strong class="bd jq">Fig. 12.</strong> Mask RCNN architecture flowchart</figcaption></figure><p id="cb76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">评价参数</em> </strong>:正确关键点百分比(PCKh)作为评价参数。</p><p id="083a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">数据集使用了</em> : </strong> COCO</p><p id="148c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">测试结果讨论:</em> </strong>使用ResNeXt-50-FPN的结果对于净深度特征是36.7 AP和59.5 AP 50。</p></div></div>    
</body>
</html>
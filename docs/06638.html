<html>
<head>
<title>Different Models for Object Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对象检测的不同模型</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/different-models-for-object-detection-9c5cda7863c1?source=collection_archive---------7-----------------------#2021-08-25">https://medium.com/geekculture/different-models-for-object-detection-9c5cda7863c1?source=collection_archive---------7-----------------------#2021-08-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="4b99" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">更快的R-CNN、YOLOv5、RetinaNet和EfficientDet</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/f45628f1a71c9039b0146dabf7c32806.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*RSQpoKlcBYjtqTrpHGGVyw.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx">* Image courtesy of Unsplash/ Matt Nelson</figcaption></figure><p id="aec0" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">深度学习在过去十年中经历了非常快速的改进。一些架构建立在旧版本的基础上，因此产生了一系列算法。我们将使用四个算法家族的代表来说明对象检测的建模:</p><ol class=""><li id="e07a" class="kf kg hi jl b jm jn jp jq js kh jw ki ka kj ke kk kl km kn bi translated">更快的R-CNN (2级检测器，相对较慢但准确)，</li><li id="1e04" class="kf kg hi jl b jm ko jp kp js kq jw kr ka ks ke kk kl km kn bi translated">YOLOv5 (1级检测器，速度快，但精度较低)，</li><li id="fb86" class="kf kg hi jl b jm ko jp kp js kq jw kr ka ks ke kk kl km kn bi translated">RetinaNet(一级检测器，快速准确)，以及</li><li id="9394" class="kf kg hi jl b jm ko jp kp js kq jw kr ka ks ke kk kl km kn bi translated">EfficientDet(主要是1级检测器设计，快速准确)。</li></ol><p id="d00f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> <em class="kt">概要:</em> </strong></p><p id="4685" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">A.设置</p><p id="abca" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">B.数据准备</p><p id="2f85" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">C.系统模型化</p><p id="009e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">C.1 .更快的R-CNN</p><p id="674a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">约洛夫5</p><p id="82d7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">C.3. RetinaNet</p><p id="bdc8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">效率检测</p><p id="bb25" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">D.最终建模</p><p id="0b5f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">E.推理</p><p id="b8d6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><em class="kt">警告</em>:有趣的图片会让人分心！</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ku"><img src="../Images/3f29b3dff8c64156a1b5331b795acf30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JCZmcemck6-np0vIUqgTiw.png"/></div></div></figure><p id="31b1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">打开笔记本，我们开始吧！</p><p id="09a8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">答:设置</strong></p><p id="c6bf" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我用的是带GPU运行时和高RAM设置的Colab Pro。</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="f30f" class="le lf hi la b fi lg lh l li lj">!wget <a class="ae lk" href="https://raw.githubusercontent.com/airctic/icevision/master/install_colab.sh" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/airctic/icevision/master/install_colab.sh</a><br/>!bash install_colab.sh<br/>from icevision.all import *<br/>import icedata<br/>#import icevision</span></pre><p id="6aca" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">关于IceVision对象检测框架的介绍/复习，<a class="ae lk" rel="noopener" href="/@yrodriguezmd/object-detection-using-a-deep-neural-network-213ec8ac2da8">请参考此处的</a>。</p><p id="7a37" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> B .数据准备</strong></p><p id="10c8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">IceVision利用<a class="ae lk" href="https://www.robots.ox.ac.uk/~vgg/data/pets/" rel="noopener ugc nofollow" target="_blank">牛津-IIIT Pet数据集</a>来促进学习。</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="4e5f" class="le lf hi la b fi lg lh l li lj">path = icedata.pets.load_data()</span><span id="269a" class="le lf hi la b fi ll lh l li lj">class_map = icedata.pets.class_map()</span><span id="5c78" class="le lf hi la b fi ll lh l li lj">data_splitter = RandomSplitter([0.8, 0.2])</span><span id="42a4" class="le lf hi la b fi ll lh l li lj">parser = icedata.pets.parser(data_dir=path)</span><span id="4eed" class="le lf hi la b fi ll lh l li lj">train_records, valid_records = parser.parse(data_splitter)</span></pre><p id="c363" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">关于数据加载和准备的详细说明在<a class="ae lk" rel="noopener" href="/@yrodriguezmd/object-detection-using-a-deep-neural-network-213ec8ac2da8">B节和C节</a>中完成。</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="6531" class="le lf hi la b fi lg lh l li lj">show_records(train_records[3:9], ncols=3, <br/>             class_map=class_map, show=True,<br/>             label_color = '#ffff00', <br/>             font_size = 30, prettify = False)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lm"><img src="../Images/15f071922f7d7e2aebeea535a30cbc16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*GGTOUdYn17yvEdTFZw0opQ.png"/></div></figure><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="81ac" class="le lf hi la b fi lg lh l li lj">image_size = 384</span><span id="4f29" class="le lf hi la b fi ll lh l li lj">train_tfms = tfms.A.Adapter(           [*tfms.A.aug_tfms(size=image_size, presize=512),tfms.A.Normalize()])</span><span id="2502" class="le lf hi la b fi ll lh l li lj">valid_tfms = tfms.A.Adapter(<br/>           [*tfms.A.resize_and_pad(image_size), tfms.A.Normalize()])</span><span id="fed8" class="le lf hi la b fi ll lh l li lj">train_ds = Dataset(train_records, train_tfms)<br/>valid_ds = Dataset(valid_records, valid_tfms)</span></pre><p id="6e87" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> C .建模</strong></p><p id="d70a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><em class="kt"> C.1 .更快的基于区域的卷积神经网络(更快的R-CNN): </em> 2级检测器</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="3899" class="le lf hi la b fi lg lh l li lj">model_type_frcnn = models.torchvision.faster_rcnn </span></pre><p id="3e0f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">更快的R-CNN 物体探测方法分两个主要阶段。在第一阶段，网络查看整个图像，然后生成锚框(提议框)。这些锚框具有作为/包括对象(“对象性”)相对于作为背景的相关概率。锚框的客观性使得模型能够将学习集中在局部区域。提案箱使用CNN生成，因此该阶段的名称为“区域提案网络”(RPN)。</p><p id="526f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">第二阶段涉及分类过程(识别对象的标签)和回归过程(指定指示对象在图像中的位置的边界框)。</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="bacf" class="le lf hi la b fi lg lh l li lj">model_frcnn = model_type_frcnn.model( <br/>                      num_classes=len(parser.class_map))<br/>train_dl_frcnn = model_type_frcnn.train_dl(train_ds, <br/>                      batch_size=16, num_workers=4, shuffle=True)<br/>valid_dl_frcnn = model_type_frcnn.valid_dl(valid_ds, <br/>                      batch_size=16, num_workers=4, shuffle=False)</span><span id="ff93" class="le lf hi la b fi ll lh l li lj">metrics = [COCOMetric(metric_type=COCOMetricType.bbox)]</span></pre><p id="f89e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">有关数据加载器和指标的详细信息，请参考<a class="ae lk" rel="noopener" href="/@yrodriguezmd/object-detection-using-a-deep-neural-network-213ec8ac2da8">步骤D2–3，此处为</a>。</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="437c" class="le lf hi la b fi lg lh l li lj">learn_frcnn = model_type_frcnn.fastai.learner(<br/>                 dls=[train_dl_frcnn, valid_dl_frcnn], <br/>                 model=model_frcnn, metrics=metrics)</span></pre><p id="7076" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><a class="ae lk" href="https://docs.fast.ai/learner.html#Learner" rel="noopener ugc nofollow" target="_blank">学习器</a>有助于训练模型、数据和度量的协调。</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="75b1" class="le lf hi la b fi lg lh l li lj">learn_frcnn.<a class="ae lk" href="https://docs.fast.ai/callback.schedule.html#Learner.lr_find" rel="noopener ugc nofollow" target="_blank">lr_find()</a></span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ln"><img src="../Images/7fec3eea542e0412713be391a31d7907.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*taYZM3M3Ivg5ijf3iL74_g.png"/></div></figure><p id="5f50" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">建议的lr_min对应于绝对最小值之前的一个数量级。这被认为是一个好建议，将被采用。</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="9eb0" class="le lf hi la b fi lg lh l li lj">learn_frcnn.<a class="ae lk" rel="noopener" href="/@yrodriguezmd/developing-a-taste-for-deep-learning-241cabb43277">fine_tune</a>(10, 0.00021, freeze_epochs=1) </span><span id="8847" class="le lf hi la b fi ll lh l li lj">import matplotlib.pyplot as plt</span><span id="69f0" class="le lf hi la b fi ll lh l li lj">def plot_metrics(learn, title, x, y):<br/>  plt.plot(L(learn.recorder.values).itemgot())<br/>  plt.xlabel('epoch')<br/>  plt.ylabel('mAP (green), Loss (blue, orange)')<br/>  plt.title(title)<br/>  plt.text(x, y, <br/>         'Legend: mAP(green), train_loss(blue), valid_loss(orange');</span><span id="8600" class="le lf hi la b fi ll lh l li lj">plot_metrics(learn_frcnn, <br/>       'Mean Average Precision and Losses for Faster_rcnn', 0,-0.05)</span></pre><div class="iy iz ja jb fd ab cb"><figure class="lo jc lp lq lr ls lt paragraph-image"><img src="../Images/bca56028a7d78afc8fa8ab43e00e3ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*6eniosw2T3r1YopVnAlHnQ.png"/></figure><figure class="lo jc lu lq lr ls lt paragraph-image"><img src="../Images/c11772fa1293458135d56f9fe71fb24d.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*P6P4flN8W19tQA4vq9GUng.png"/></figure></div><p id="8633" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在学习率为2.1e-4的情况下，更快的R-CNN模型在10个时期的训练后达到了0.605的平均精度。验证损失显示出平稳的逐步改善。</p><p id="a64b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><em class="kt"> C.2. YOLOv5 </em> : 1级检测器</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="a41b" class="le lf hi la b fi lg lh l li lj">model_type_yolo = models.ultralytics.yolov5 <br/>backbone_yolo = model_type_yolo.backbones.small <br/>model_yolo = model_type_yolo.model(<br/>                      backbone = backbone_yolo(pretrained=True), <br/>                      num_classes=len(parser.class_map),<br/>                      img_size = image_size)</span></pre><p id="694b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">“你只看一次”(<a class="ae lk" href="https://blog.roboflow.com/yolov5-improvements-and-evaluation/" rel="noopener ugc nofollow" target="_blank"> YOLO </a>)是一个在单一阶段执行检测的家族。该模型不使用rpn。相反，它将图像分成网格框，每个网格框负责检测其边界内的对象。对于bboxes和类概率，信息被处理为一个<a class="ae lk" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank">回归问题</a>。它保留空间关系，并能够编码和识别上下文信息。</p><p id="b0bf" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">可用的主干配置之一是“小型”，即<a class="ae lk" href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data" rel="noopener ugc nofollow" target="_blank"> YOLOv5s </a>。体积小，具有处理速度快的优点。</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="929f" class="le lf hi la b fi lg lh l li lj">train_dl_yolo = model_type_yolo.train_dl(train_ds, <br/>                      batch_size=16, num_workers=4, shuffle=True)<br/>valid_dl_yolo = model_type_yolo.valid_dl(valid_ds, <br/>                      batch_size=16, num_workers=4, shuffle=False)<br/>                                       <br/>learn_yolo = model_type_yolo.fastai.learner(<br/>                          dls=[train_dl_yolo, valid_dl_yolo], <br/>                          model=model_yolo, metrics=metrics)<br/>learn_yolo.lr_find()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lv"><img src="../Images/6489d9753cf3ca4febda1e16104c29a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*OOrYrgdnoGL3uDQAZvS4Dg.png"/></div></figure><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="2eff" class="le lf hi la b fi lg lh l li lj">learn_yolo.fine_tune(10, 0.00063, freeze_epochs=1) </span><span id="b0ab" class="le lf hi la b fi ll lh l li lj">plot_metrics(learn_yolo, <br/>           'Mean Average Precision and Losses for YOLOv5', 0,-0.12)</span></pre><div class="iy iz ja jb fd ab cb"><figure class="lo jc lw lq lr ls lt paragraph-image"><img src="../Images/e20256c54f6016d3330cb0b5cc7565fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*76dXcHPSYT_J7PvWKZto6g.png"/></figure><figure class="lo jc lx lq lr ls lt paragraph-image"><img src="../Images/b82fc09c202f233234cdca42dcdbda51.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*raqIGFmRTTtxZx-7V6vMKQ.png"/></figure></div><p id="4aef" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">YOLOv5在10个周期后达到0.311的mAP，学习率为6.3e-4。处理每个时期的时间(00:31)比更快的R-CNN处理时间(02:36)要快。</p><p id="028b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><em class="kt"> C.3. RetinaNet </em> : 1级，快速准确</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="e937" class="le lf hi la b fi lg lh l li lj">model_type_ret = models.mmdet.retinanet </span><span id="712f" class="le lf hi la b fi ll lh l li lj">backbone_r50 = model_type_ret.backbones.resnet50_fpn_1x(pretrained=True)</span><span id="70cb" class="le lf hi la b fi ll lh l li lj">model_ret = model_type_ret.model(backbone=backbone_r50(pretrained=True), <br/>                      num_classes=len(parser.class_map))</span></pre><p id="742c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><a class="ae lk" href="https://arxiv.org/abs/1708.02002" rel="noopener ugc nofollow" target="_blank"> RetinaNet </a>是一个1级检测器，有两个子网络用于分类和回归。它使用焦点损失来解决前景和背景之间的不平衡。简单的例子(如背景)权重较低。这使得模型能够将训练集中在较难的例子上。</p><p id="a032" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">RetinaNet主干网配置之一是resnet_50_fpn。在<a class="ae lk" href="https://pytorch.org/vision/0.8/_modules/torchvision/models/resnet.html" rel="noopener ugc nofollow" target="_blank"> Resnet </a>设计中，身份函数的间歇同化使网络能够更深入，从而增加学习能力。</p><p id="365e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><a class="ae lk" href="https://arxiv.org/abs/1612.03144" rel="noopener ugc nofollow" target="_blank">特征金字塔网络</a> (FPN)设计连接架构中的不同阶段，使其能够检测不同比例的对象。</p><p id="b759" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这些变化使得RetinaNet/ Resnet_50/ FPN组合模型既准确又快速。</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="b641" class="le lf hi la b fi lg lh l li lj">train_dl_ret = model_type_ret.train_dl(<br/>             train_ds, batch_size=16, num_workers=4, shuffle=True)<br/>valid_dl_ret = model_type_ret.valid_dl(<br/>             valid_ds, batch_size=16, num_workers=4, shuffle=False)</span><span id="967d" class="le lf hi la b fi ll lh l li lj">learn_ret = model_type_ret.fastai.learner(<br/>             dls=[train_dl_ret, valid_dl_ret], <br/>             model=model_ret, metrics=metrics)<br/>learn_ret.lr_find()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ly"><img src="../Images/0d120f0b502fe95dcb0cca679f78cbb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*bDXkByYHoOvrd86NAinRgQ.png"/></div></figure><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="8fdb" class="le lf hi la b fi lg lh l li lj">learn_ret.fine_tune(10, 0.00012, freeze_epochs=1) </span><span id="55b1" class="le lf hi la b fi ll lh l li lj">plot_metrics(learn_ret, <br/>'Mean Average Precision and Losses for Retinanet/Resnet50', 0, 0.01)</span></pre><div class="iy iz ja jb fd ab cb"><figure class="lo jc lz lq lr ls lt paragraph-image"><img src="../Images/1b5a024c46ce1eaeea460cca5a1b09fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*X82hN1tF_eT3P0mwRnCXrQ.png"/></figure><figure class="lo jc ma lq lr ls lt paragraph-image"><img src="../Images/d708036903369e5235ad95f0ea9e5aa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*xf67QpOeoiE6lepfySmRfA.png"/></figure></div><p id="12f2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">RetinaNet模型在lr 1.2e-4下10个时期后达到0.684的mAP。有效损失呈持续下降趋势。01:34的时间介于“慢”快R-CNN (02:36)和快YOLOv5 (00:31)之间。</p><p id="8eb4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><em class="kt"> C.4. EfficientDet: </em> 1级，可扩展</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="4bc2" class="le lf hi la b fi lg lh l li lj">model_type_eff = models.ross.efficientdet <br/>backbone_eff = model_type_eff.backbones.tf_lite0</span><span id="3a46" class="le lf hi la b fi ll lh l li lj">model_eff = model_type_eff.model(<br/>                      backbone = backbone_eff(pretrained=True), <br/>                      num_classes=len(parser.class_map),<br/>                      img_size = image_size)</span></pre><p id="4869" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><a class="ae lk" href="https://arxiv.org/abs/1911.09070" rel="noopener ugc nofollow" target="_blank"> EfficientDet </a>主要是一个1阶段设计模型。它使用多个双向fpn来优化深度、宽度和分辨率特性的可扩展性。</p><p id="41eb" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">它的配置之一是<a class="ae lk" href="https://github.com/yrodriguezmd/icevision/blob/master/icevision/models/ross/efficientdet/backbones.py" rel="noopener ugc nofollow" target="_blank"> tf_lite0 </a>，从<a class="ae lk" href="https://github.com/rwightman/efficientdet-pytorch" rel="noopener ugc nofollow" target="_blank"> tf_efficientdet_lite0 </a>派生而来，反过来又从<a class="ae lk" href="https://tfhub.dev/tensorflow/lite-model/efficientnet/lite0/fp32/2" rel="noopener ugc nofollow" target="_blank"> efficientnet/lite0 </a>派生而来。这些配置是为移动应用设计的。</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="9ddc" class="le lf hi la b fi lg lh l li lj">train_dl_eff = model_type_eff.train_dl(train_ds,<br/>                      batch_size=16, num_workers=4, shuffle=True)<br/>valid_dl_eff = model_type_eff.valid_dl(valid_ds, <br/>                      batch_size=16, num_workers=4, shuffle=False)<br/>                                       <br/>learn_eff = model_type_eff.fastai.learner(<br/>                      dls=[train_dl_eff, valid_dl_eff], <br/>                      model=model_eff, metrics=metrics)<br/>learn_eff.lr_find()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ln"><img src="../Images/2e1e18154091b19c6c64023b095ab12e.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*A4FCMY194SszGSBpWEDsRA.png"/></div></figure><p id="5248" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">与之前学习率在e-4范围内的模型相比，efficientdet的建议LR在e-2处高出2个数量级。</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="1f80" class="le lf hi la b fi lg lh l li lj">learn_eff.fine_tune(10, 0.036, freeze_epochs=1) </span><span id="1bd5" class="le lf hi la b fi ll lh l li lj">plot_metrics(learn_eff, <br/>      'Mean Average Precision and Losses for EfficientDet',0,-0.9)</span></pre><div class="iy iz ja jb fd ab cb"><figure class="lo jc mb lq lr ls lt paragraph-image"><img src="../Images/d3388756020ad0cbbd76654fc2f0b3c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*GiAga2nhNNFCm3CjSPyh_A.png"/></figure><figure class="lo jc mc lq lr ls lt paragraph-image"><img src="../Images/d6131dd3ea94010b86572b01a338c5d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*PGD1f6S2ozmIwIoulDdcyw.png"/></figure></div><p id="ffbb" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">EfficientDet/ TF-Lite0模型以3.6e-2的学习速率在10个时期后达到0.467的mAP。valid_loss在早期出现突然增加，然后逐渐下降。1:10的时间比RetinaNet的1:34稍快。</p><p id="db10" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> D .最终建模</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es md"><img src="../Images/d8caa16ec919d53c8cc427a4a4d6ae30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*K66piIuQ6xBPwE10gbzk8A.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx">* Note: y-axes NOT to-scale</figcaption></figure><p id="04d0" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在这四个模型中，FPN retina net/resnet 50提供了最好的地图和损失趋势。下一个最佳映射性能来自更快的R-CNN，其次是EfficientDet/TF_Lite0。YOLOv5在这一盘表现不佳。</p><p id="6369" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">从最快到最慢的速度(在恒定的n_epoch和基于建议的lr_min变化的LR)如下:YOLOv5，EfficientDet，RetinaNet，更快的R-CNN。</p><p id="457f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">可以观察到，稳定期开始于时期8。</p><p id="c4fe" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">因此，对于最终的模型，我们将选择RetinaNet(好图，中速)，保留建议的LR，并将训练持续时间减少到8个历元。</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="c21a" class="le lf hi la b fi lg lh l li lj">model_type = models.mmdet.retinanet </span><span id="325f" class="le lf hi la b fi ll lh l li lj">backbone = model_type.backbones.resnet50_fpn_1x(pretrained=True)</span><span id="9e48" class="le lf hi la b fi ll lh l li lj">model = model_type.model(<br/>                      backbone=backbone(pretrained=True), <br/>                      num_classes=len(parser.class_map))</span><span id="a062" class="le lf hi la b fi ll lh l li lj">train_dl = model_type.train_dl(train_ds, <br/>                     batch_size=16, num_workers=4, shuffle=True)<br/>valid_dl = model_type.valid_dl(valid_ds, <br/>                     batch_size=16, num_workers=4, shuffle=False)</span><span id="12a7" class="le lf hi la b fi ll lh l li lj">learn = model_type.fastai.learner(<br/>            dls=[train_dl, valid_dl], model=model, metrics=metrics)</span><span id="5f6c" class="le lf hi la b fi ll lh l li lj">learn.fine_tune(8, 0.00012, freeze_epochs=1) </span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es me"><img src="../Images/a74cb710d18ae2c1313de05fcba436b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*iribz3Coo6iDi88N7ts6jQ.png"/></div></figure><p id="6d80" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">使用n_epochs =8，在每个epoch的01:34训练时间获得的mAP是0.652。对于检测猫和狗的品种来说，这是一个合理的模型。</p><p id="615e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> E .推论</strong></p><p id="7a0d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们将通过使用不属于数据集的狗图像来测试该模型。</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="72c5" class="le lf hi la b fi lg lh l li lj">!pip install bing-image-downloader<br/>from bing_image_downloader import downloader<br/>query_string = 'wiki samoyed' #<br/>samo = downloader.download(query_string, limit = 3, <br/>                            output_dir = 'dataset',<br/>                            adult_filter_off=True,<br/>                            force_replace=False,<br/>                            timeout=60,<br/>                            verbose=True)<br/>samo</span></pre><p id="c4c6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们将使用上面代码中的一个url输出。</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="4fe7" class="le lf hi la b fi lg lh l li lj">url = ['<a class="ae lk" href="https://upload.wikimedia.org/wikipedia/commons/c/c4/Samoyed-sweetjedysamoyeds.jpg'" rel="noopener ugc nofollow" target="_blank">https://upload.wikimedia.org/wikipedia/commons/c/c4/Samoyed-sweetjedysamoyeds.jpg'</a>]<br/>dest = 'Desktop'<br/>download_url(url[0],dest) </span><span id="53b0" class="le lf hi la b fi ll lh l li lj">from PIL import Image</span><span id="82aa" class="le lf hi la b fi ll lh l li lj">image = Image.open(dest)<br/>#image.to_thumb(128)</span><span id="8021" class="le lf hi la b fi ll lh l li lj">img = np.array(image)</span><span id="0ff7" class="le lf hi la b fi ll lh l li lj">infer_tfms = tfms.A.Adapter(<br/>          [*tfms.A.resize_and_pad(size=384), tfms.A.Normalize()])</span><span id="d76c" class="le lf hi la b fi ll lh l li lj">infer = Dataset.from_images(<br/>          [img], infer_tfms, class_map=class_map )</span><span id="e269" class="le lf hi la b fi ll lh l li lj">preds = model_type.predict(model, infer, keep_images=True)</span><span id="eccd" class="le lf hi la b fi ll lh l li lj">show_preds(preds=preds, font_size=25, label_color = '#3050ff')</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mf"><img src="../Images/a350bc20f1a20be129a1168805241ee6.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*fCaJzZLIiOHEFsMmhuvSww.png"/></div></figure><p id="a551" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">RetinaNet/Resnet50/ FPN模型能够预测狗的分类(基于我们的字符串查询的“真相”)，以及它在图像中的位置。</p><p id="48e0" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> <em class="kt">总结:</em> </strong></p><p id="cf20" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">不同的模式有不同的特点和优势。对于宠物数据集，以宠物识别和定位为目标，RetinaNet/Resnet_50/FPN在合理的时间内提供了最佳的检测性能。</p><p id="c57d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> <em class="kt">未来打法:</em> </strong></p><p id="def2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">评估模型在其他数据集上的性能。</p><p id="75a0" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我希望你喜欢和我一起走代码！:)</p><p id="cdb6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">玛利亚</p><p id="4ef4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">此小型项目的Github资源库:<a class="ae lk" href="https://github.com/yrodriguezmd/IceVision_miniprojects/blob/main/IV_OD_fasterrcnn%2C_yolo5%2C_retinanet%2C_effdet_2021_8_23.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/yrodriguezmd/ice vision _ mini projects/blob/main/IV _ OD _ faster rcnn % 2C _ yolo 5% 2C _ retina net % 2C _ eff det _ 2021 _ 8 _ 23 . ipynb</a></p><p id="ab42" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">领英:【https://www.linkedin.com/in/rodriguez-maria/ T2】</p><p id="5d91" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">推特:【https://twitter.com/Maria_Rod_Data T4】</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mg"><img src="../Images/297a73e54c3c326d2e4f6cdfa9edde5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*kBC7OhGY2RD1iVtDDE7meA.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx">* Image courtesy of Unsplash/ Alvan Nee</figcaption></figure></div></div>    
</body>
</html>
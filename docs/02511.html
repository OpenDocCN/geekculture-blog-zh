<html>
<head>
<title>Linear Regression: Gentle Introduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归:温和介绍</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/linear-regression-gentle-introduction-cc04ffda4267?source=collection_archive---------35-----------------------#2021-05-17">https://medium.com/geekculture/linear-regression-gentle-introduction-cc04ffda4267?source=collection_archive---------35-----------------------#2021-05-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e6d138216cf5f58a4565ad156f495c0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jmI550AcTgXRfU04c4wvJA.png"/></div></div></figure><p id="9edb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">回归分析是一套统计方法，用于找出因变量(y)和自变量(X)之间的关系。自变量不受其他变量的影响，但随着自变量的变化，它会直接影响因变量，这意味着我们必须研究因变量，以找到数据集中的模式。为了找到这种模式或预测产量，我们使用回归模型。</p><p id="7408" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我们已经有了一些关于回归的基本概念，让我们深入到线性回归的解释中去，以便更好地理解。</p><h1 id="e545" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">线性回归</h1><p id="b451" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">线性回归试图通过拟合线性方程来找出两个变量(即X和y)之间的关系:</p><blockquote class="kr"><p id="7bb6" class="ks kt hi bd ku kv kw kx ky kz la jn dx translated"><strong class="ak"><em class="lb">y</em></strong>=<strong class="ak">T5】β0</strong><strong class="ak">+<em class="lb">β</em>1X<em class="lb"/>+ε</strong></p></blockquote><p id="cdbf" class="pw-post-body-paragraph iq ir hi is b it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">在哪里，</p><blockquote class="lh li lj"><p id="7b77" class="iq ir lk is b it iu iv iw ix iy iz ja ll jc jd je lm jg jh ji ln jk jl jm jn hb bi translated">x是独立变量，沿x轴绘制</p><p id="ca48" class="iq ir lk is b it iu iv iw ix iy iz ja ll jc jd je lm jg jh ji ln jk jl jm jn hb bi translated">y是因变量，沿y轴绘制</p><p id="b9c2" class="iq ir lk is b it iu iv iw ix iy iz ja ll jc jd je lm jg jh ji ln jk jl jm jn hb bi translated">β0是截距</p><p id="d454" class="iq ir lk is b it iu iv iw ix iy iz ja ll jc jd je lm jg jh ji ln jk jl jm jn hb bi translated">β1是X的系数(斜率)</p><p id="e4fa" class="iq ir lk is b it iu iv iw ix iy iz ja ll jc jd je lm jg jh ji ln jk jl jm jn hb bi translated">ε是残差(误差)的和</p></blockquote><p id="f3ff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">误差项(即ε)用于说明因变量和自变量之间的线性关系无法解释的y的可变性。</p><p id="0905" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面的等式是从我们上学时在数学中用过的线的等式推导出来的，即</p><blockquote class="kr"><p id="cf88" class="ks kt hi bd ku kv kw kx ky kz la jn dx translated">y = mx + c</p></blockquote><p id="fa83" class="pw-post-body-paragraph iq ir hi is b it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">在哪里，</p><blockquote class="lh li lj"><p id="eb75" class="iq ir lk is b it iu iv iw ix iy iz ja ll jc jd je lm jg jh ji ln jk jl jm jn hb bi translated">m是直线的斜率</p><p id="5afb" class="iq ir lk is b it iu iv iw ix iy iz ja ll jc jd je lm jg jh ji ln jk jl jm jn hb bi translated">c是常数</p></blockquote><p id="4522" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在线性回归中，等式的书写方式发生了变化，但目标保持不变，即找到数据的最佳拟合线。</p><p id="415f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我们需要找到截距和系数，这样我们就可以找到我们的线，为此，我们使用</p><figure class="lp lq lr ls fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lo"><img src="../Images/28232789c8b28e4995cd0b8e9201916c.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*a08zUcfvxVVmzsKTSbte6Q.png"/></div></div></figure><figure class="lp lq lr ls fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/de1a32a87b27ab368900cc53dd060269.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*zCsiV0OYklP91ebBM0Mx0g.png"/></div></figure><blockquote class="lh li lj"><p id="5618" class="iq ir lk is b it iu iv iw ix iy iz ja ll jc jd je lm jg jh ji ln jk jl jm jn hb bi translated">x̄是x的意思</p><p id="7f7e" class="iq ir lk is b it iu iv iw ix iy iz ja ll jc jd je lm jg jh ji ln jk jl jm jn hb bi translated">ȳ是y的意思</p></blockquote><h1 id="ee61" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">简单线性回归</h1><figure class="lp lq lr ls fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/bcce9a998554926d3881a2b420047489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IAAGg4Qp7rgSiurk0s7ylQ.png"/></div></div></figure><p id="4a5e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当只有一个自变量(X)和一个因变量时，这个问题称为简单线性回归。这种回归的方程式表示为:</p><blockquote class="kr"><p id="ee32" class="ks kt hi bd ku kv kw kx ky kz la jn dx translated">y = β0 + β1X</p></blockquote><h1 id="85ad" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz lv kb kc kd lw kf kg kh lx kj kk kl bi translated">多元线性回归</h1><p id="26a6" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">当一个因变量的数据集中有多个要素时，该类型的回归属于多元线性回归。这种回归的方程式表示为:</p><blockquote class="kr"><p id="a613" class="ks kt hi bd ku kv kw kx ky kz la jn dx translated">y = β0 + β1X + β2X2 + β3X3 + ………..+ βnXn</p></blockquote><p id="fc2a" class="pw-post-body-paragraph iq ir hi is b it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">在多元线性回归中，方程会根据数据集中找到的变量数量进行扩展。对于每一个增加的变量，我们都要计算系数。</p><figure class="lp lq lr ls fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/c4dc943a8b4232537e21028a0d7140a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oofPi8EVgzGmONxgMMLJdg.png"/></div></div></figure><p id="d630" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">随着变量数量的增加，这意味着我们的任务无法通过应用或找到一条线来完成，因此，为了完成您的任务，我们必须在适当的位置拟合一个平面或一条线，以便每个特征都获得相同的偏好，并且没有特征会陷入困境，因为每个特征在预测时都会给出一些权重。</p><p id="5be1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我们知道了如何找到最佳拟合线，让我们看看线性回归中的关系。</p><h2 id="f91d" class="ly jp hi bd jq lz ma mb ju mc md me jy jb mf mg kc jf mh mi kg jj mj mk kk ml bi translated">线性回归中的关系</h2><ul class=""><li id="caa0" class="mm mn hi is b it km ix kn jb mo jf mp jj mq jn mr ms mt mu bi translated"><strong class="is hj">正相关:</strong>当一个变量的值随着另一个变量的值的增加而增加时，这意味着它是正相关的。</li></ul><figure class="lp lq lr ls fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mv"><img src="../Images/2391f488aa0c1d30d5eec3b7b9b0e96e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bnDDsDUPRQTISrT8mN1low.png"/></div></div></figure><p id="2814" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如上图所示，当x轴的值增加时，y轴的值也会增加。这种关系被称为积极关系。</p><ul class=""><li id="168f" class="mm mn hi is b it iu ix iy jb mw jf mx jj my jn mr ms mt mu bi translated"><strong class="is hj">负相关:</strong>当一个变量减少，而另一个变量增加时，则为负相关。</li></ul><figure class="lp lq lr ls fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mz"><img src="../Images/b4d12afaa17cd071a80314501a86fc91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eYtCQ0BrijCQLl0O8orSNQ.png"/></div></div></figure><p id="f679" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上图显示了负相关，因为一个变量随着另一个变量的增加而减少。</p><ul class=""><li id="c3d5" class="mm mn hi is b it iu ix iy jb mw jf mx jj my jn mr ms mt mu bi translated"><strong class="is hj">无关系:</strong>当回归曲线中的曲线平坦时，该关系称为无关系。</li></ul><figure class="lp lq lr ls fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/efdf2c5c39942473ffa7dbd2fc524c75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_GzMNmfFLRf8PDwkgh5gkQ.png"/></div></div></figure><p id="088d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上图中，对于x轴的每个新变量，y值都是常数，它形成了一条平坦的线(斜率值为0)。</p><h1 id="7bdd" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">线性回归线</h1><p id="eee1" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">寻找最佳拟合线最常用的方法是<strong class="is hj">最小二乘法。</strong>该方法减少了每个数据点到直线的垂直偏差的平方和，如果点位于拟合直线上，则偏差为0。简而言之，它减少了实际点和预测点之间的距离，并给出了最佳拟合线。</p><h1 id="cc1d" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">价值函数</h1><p id="a30c" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">成本函数用于衡量预测的y值与实际y值的接近程度。线性回归的成本函数是均方误差，它取y的实际值和预测值之间的平均误差。如果预测的直线距离实际数据点很远，则意味着误差更大，此时我们必须更新β0和β1的值，直到误差最小，我们尝试不断更新我们的值。</p><p id="2c99" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以发现成本函数可以由下式计算:</p><figure class="lp lq lr ls fd ij er es paragraph-image"><div class="er es na"><img src="../Images/af0a55d6e6eb89eb8617c5cab62ce472.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*KQ0cxaPdDjCQdVXo0c3Lfg.png"/></div></figure><p id="ff76" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在哪里，</p><blockquote class="lh li lj"><p id="7afd" class="iq ir lk is b it iu iv iw ix iy iz ja ll jc jd je lm jg jh ji ln jk jl jm jn hb bi translated"><em class="hi"> n是数据点的总数</em></p><p id="9523" class="iq ir lk is b it iu iv iw ix iy iz ja ll jc jd je lm jg jh ji ln jk jl jm jn hb bi translated">h <em class="hi"> θ(X)是我们的实际数据</em></p><p id="5ede" class="iq ir lk is b it iu iv iw ix iy iz ja ll jc jd je lm jg jh ji ln jk jl jm jn hb bi translated">ŷ是我们的预测值</p></blockquote><p id="f2fe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这就是为什么代价函数在每个机器学习问题中起着重要作用。</p><h1 id="ba84" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结论</h1><p id="4ccc" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">这都是关于线性回归。我希望你得到了一些知识，随时分享你对这篇文章的看法。感谢任何形式的反馈。谢谢！</p></div></div>    
</body>
</html>
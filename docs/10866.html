<html>
<head>
<title>Grab-and-go series: n-gram model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">抓起就走系列:n元模型</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/grab-and-go-series-n-gram-model-8703279c70cb?source=collection_archive---------13-----------------------#2022-02-19">https://medium.com/geekculture/grab-and-go-series-n-gram-model-8703279c70cb?source=collection_archive---------13-----------------------#2022-02-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="df27" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">在一张幻灯片中实现n元模型。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/f354a605c9b69a2e123789989a21cda2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6zvYQSZ2OX-McL6_OIpYTg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Obviously, people were very confused about everything around 1860:)</figcaption></figure><p id="e239" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">首先，Google n-gram是在1500年到2019年的所有书籍上训练的n-gram模型。看看历史上人们是怎么说的。</p><h2 id="0fc4" class="kj kk hi bd kl km kn ko kp kq kr ks kt jw ku kv kw ka kx ky kz ke la lb lc ld bi translated">背景</h2><p id="71c0" class="pw-post-body-paragraph jn jo hi jp b jq le ij js jt lf im jv jw lg jy jz ka lh kc kd ke li kg kh ki hb bi translated">为单词序列分配概率的模型被称为语言模型或LMs。本文介绍了最简单的LM: n-gram模型，包括概念和实现。</p><p id="9e93" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我制作了一张幻灯片来回答以下问题:</p><ol class=""><li id="8888" class="lj lk hi jp b jq jr jt ju jw ll ka lm ke ln ki lo lp lq lr bi translated">训练一个N元模型需要什么？</li><li id="ac87" class="lj lk hi jp b jq ls jt lt jw lu ka lv ke lw ki lo lp lq lr bi translated">N元模型的最终结果是什么？</li><li id="d160" class="lj lk hi jp b jq ls jt lt jw lu ka lv ke lw ki lo lp lq lr bi translated">用什么概念来近似下一个单词的概率？</li></ol><p id="9753" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">要看我在幻灯片中的演示，请点击<strong class="jp hj"> youtube </strong>链接<a class="ae lx" href="https://www.youtube.com/watch?v=lQi3lGWzC4g" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="299a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">此外，在我的<strong class="jp hj"> Github </strong>上抓取jupyter笔记本，自己尝试一下:<a class="ae lx" href="https://github.com/jink1994/NLPforAll" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ly"><img src="../Images/dc0d700d7307e70a470871402a7f1d83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*puR4Wf01AibLKZ_GvLbVrg.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">One slide to understand N-gram models.</figcaption></figure><h2 id="ed08" class="kj kk hi bd kl km kn ko kp kq kr ks kt jw ku kv kw ka kx ky kz ke la lb lc ld bi translated">n-gram的优点</h2><ul class=""><li id="12f3" class="lj lk hi jp b jq le jt lf jw lz ka ma ke mb ki mc lp lq lr bi translated">对于短句建模，它仍然非常强大。比如在<a class="ae lx" href="https://en.wikipedia.org/wiki/Query_expansion" rel="noopener ugc nofollow" target="_blank">查询扩展</a>的应用中就得心应手。</li><li id="fa6f" class="lj lk hi jp b jq ls jt lt jw lu ka lv ke lw ki mc lp lq lr bi translated">这是可以解释的:你直接从你的训练语料中得到条件概率。</li><li id="bbd8" class="lj lk hi jp b jq ls jt lt jw lu ka lv ke lw ki mc lp lq lr bi translated">重新训练模型并不会花你很多钱。</li></ul><h2 id="eb11" class="kj kk hi bd kl km kn ko kp kq kr ks kt jw ku kv kw ka kx ky kz ke la lb lc ld bi translated">主流程</h2><ul class=""><li id="4648" class="lj lk hi jp b jq le jt lf jw lz ka ma ke mb ki mc lp lq lr bi translated">得到一个训练语料库，是句子的列表。</li><li id="f5c7" class="lj lk hi jp b jq ls jt lt jw lu ka lv ke lw ki mc lp lq lr bi translated">基于训练语料库生成词典和n元语法元组的集合。</li><li id="53f3" class="lj lk hi jp b jq ls jt lt jw lu ka lv ke lw ki mc lp lq lr bi translated">用最大似然估计计算n元概率。</li><li id="c213" class="lj lk hi jp b jq ls jt lt jw lu ka lv ke lw ki mc lp lq lr bi translated">困惑地评估并生成您的句子！</li></ul><h2 id="cfb2" class="kj kk hi bd kl km kn ko kp kq kr ks kt jw ku kv kw ka kx ky kz ke la lb lc ld bi translated">参考</h2><ul class=""><li id="1d21" class="lj lk hi jp b jq le jt lf jw lz ka ma ke mb ki mc lp lq lr bi translated">我的Github—<a class="ae lx" href="https://github.com/jink1994/NLPforAll" rel="noopener ugc nofollow" target="_blank">https://github.com/jink1994/NLPforAll</a></li><li id="d8f1" class="lj lk hi jp b jq ls jt lt jw lu ka lv ke lw ki mc lp lq lr bi translated">展示这张幻灯片的youtube视频—<a class="ae lx" href="https://www.youtube.com/watch?v=lQi3lGWzC4g" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=lQi3lGWzC4g</a></li><li id="71b6" class="lj lk hi jp b jq ls jt lt jw lu ka lv ke lw ki mc lp lq lr bi translated">NLP I.M.O .最佳教程——<a class="ae lx" href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" rel="noopener ugc nofollow" target="_blank">https://web.stanford.edu/~jurafsky/slp3/3.pdf</a></li></ul></div></div>    
</body>
</html>
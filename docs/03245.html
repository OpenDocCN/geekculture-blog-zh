<html>
<head>
<title>Scikit-Learn 0.24: Top 5 New Features You Need To Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">sci kit-学习0.24:您需要了解的5大新功能</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/scikit-learn-0-24-top-5-new-features-you-need-to-know-7af15d8cdeac?source=collection_archive---------8-----------------------#2021-06-04">https://medium.com/geekculture/scikit-learn-0-24-top-5-new-features-you-need-to-know-7af15d8cdeac?source=collection_archive---------8-----------------------#2021-06-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/5d6c3c864b2aed9ed36f1253ac3a3fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*osadNSUIUZkwDqBC-ozxtg.jpeg"/></div></div></figure><p id="703d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Scikit-learn仍然是最受欢迎的Python开源免费机器学习库之一。scikit-learn库包含许多用于机器学习和统计建模的有效工具，包括分类、回归、聚类和降维。</p><p id="ba45" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">许多数据科学家、机器学习工程师和研究人员依靠这个库进行他们的<a class="ae jo" href="https://hackernoon.com/machine-learning-as-a-service-mlaas-with-sklearn-and-algorithmia-7299fbaed584?ref=hackernoon.com" rel="noopener ugc nofollow" target="_blank">机器学习</a>项目。我个人喜欢使用scikit-learn库，因为它提供了大量的灵活性，并且很容易理解它的文档和许多例子。</p><p id="bab9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这篇文章中，我很高兴与您分享scikit中的5个最佳新功能-learn 0.24。</p><h1 id="9424" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">安装最新版本的Scikit-Learn库</h1><p id="9385" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">首先，确保您安装了最新版本(带pip):</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="a811" class="lb jq hi kx b fi lc ld l le lf">pip install --upgrade scikit-learn</span></pre><p id="9ba9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果您使用conda，请使用以下命令:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="f0b1" class="lb jq hi kx b fi lc ld l le lf">conda install -c conda-forge scikit-learn</span></pre><p id="e051" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注:</strong>该版本支持Python版本<strong class="is hj"> 3.6 </strong>到<strong class="is hj"> 3.9 </strong>。</p><p id="cf11" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，让我们看看新的功能！</p><h1 id="f560" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">1.平均绝对百分比误差(MAPE)</h1><p id="f81e" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">scikit-learn的新版本为回归问题引入了一种新的评估指标，称为平均绝对百分比误差(MAPE)。以前你可以用一段代码来计算MAPE。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="bdb1" class="lb jq hi kx b fi lc ld l le lf">np.mean(np.abs((y_test — preds)/y_test))</span></pre><p id="b123" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是现在你可以从<strong class="is hj"> sklearn.metrics </strong>模块调用一个名为<strong class="is hj">mean _ absolute _ percentage _ error</strong>的函数来评估你的回归模型的性能。</p><p id="2079" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">示例:</strong></p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="3f47" class="lb jq hi kx b fi lc ld l le lf">from sklearn.metrics import mean_absolute_percentage_error<br/>y_true = [3, -0.5, 2, 7]<br/>y_pred = [2.5, 0.0, 2, 8]<br/>print(mean_absolute_percentage_error(y_true, y_pred))</span></pre><p id="fcc9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi">0.3273809523809524</p><p id="9e90" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注:</strong>记住，该函数并不表示输出为范围[0，100]内的百分比。相反，我们用范围[0，1/eps]来表示它。最佳值为<strong class="is hj"> 0.0。</strong></p><h1 id="7eb0" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">2.OneHotEncoder支持缺失值</h1><p id="c973" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated"><a class="ae jo" href="https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f?ref=hackernoon.com" rel="noopener ugc nofollow" target="_blank"> OneHotEncoder </a>现在可以处理数据集中出现的缺失值。它将缺失值视为一个类别。让我们在下面的例子中更多地了解它是如何工作的。</p><p id="3a10" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先导入重要的包。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="6903" class="lb jq hi kx b fi lc ld l le lf">import pandas as pd <br/>import numpy as np<br/>from sklearn.preprocessing import OneHotEncoder</span></pre><p id="a7b4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">创建一个包含缺失值的分类要素的简单数据框。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="13c7" class="lb jq hi kx b fi lc ld l le lf"># intialise data of lists.<br/>data = {'education_level':['primary', 'secondary', 'bachelor', np.nan,'masters',np.nan]}<br/>  <br/># Create DataFrame<br/>df = pd.DataFrame(data)<br/>  <br/># Print the output.<br/>print(df)</span></pre><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/76243b0abc38433008ef834de7451bfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*dxu_XdOryz8jqzhC-bSgbA.jpeg"/></div></figure><p id="1071" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如您所见，在我们的<strong class="is hj">教育水平</strong>列中，我们缺少了两个值。</p><p id="5646" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">创建OneHotEncoder的实例。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="4e31" class="lb jq hi kx b fi lc ld l le lf">enc = OneHotEncoder()</span></pre><p id="1654" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后拟合和转换我们的数据。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="cec9" class="lb jq hi kx b fi lc ld l le lf">enc.fit_transform(df).toarray()</span></pre><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/8a385b22143577e29d3c31b6eb58d0a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*oRONE2OMhiAKop9jl0ALrg.jpeg"/></div></figure><p id="989c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们的education_level列已经被转换，所有缺失的值都被视为一个新的类别(检查上面数组的最后一列)。</p><h1 id="9b42" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">3.一种新的特征选择方法</h1><p id="2fbd" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated"><strong class="is hj">SequentialFeatureSelector</strong>是scikit-learn中一种新的特征选择方法。它可以是向前选择，也可以是向后选择。</p><p id="e195" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">(一)前进选择</strong></p><p id="adfe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它迭代地找到最佳新特征，然后将其添加到所选特征集中。这意味着我们从零个特征开始，然后找到一个使估计量的交叉验证分数最大化的特征。所选特征被添加到集合中，并且重复该过程，直到我们达到所选特征的期望数量。</p><p id="9d8e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> (b)反向选择</strong></p><p id="edb2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第二次选择遵循相同的想法，但方向不同。这里，我们从所有特征开始，然后从集合中删除一个特征，直到我们达到所选特征的期望数量。</p><p id="c186" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">例子</strong></p><p id="c8dc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">导入重要的包。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="a71f" class="lb jq hi kx b fi lc ld l le lf">from sklearn.feature_selection import SequentialFeatureSelector<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.datasets import load_iris</span></pre><p id="7b96" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">加载虹膜数据集及其特征名称。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="ab06" class="lb jq hi kx b fi lc ld l le lf">X, y = load_iris(return_X_y=True, as_frame=True)<br/>feature_names = X.columns</span></pre><p id="c92d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">创建估计器的实例。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="50c0" class="lb jq hi kx b fi lc ld l le lf">knn = KNeighborsClassifier(n_neighbors=3)</span></pre><p id="d236" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">创建SequentialFeatureSelector的实例，设置要选择的特征数量为<strong class="is hj"> 2 </strong>，设置方向为“<strong class="is hj">向后</strong>”。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="e55c" class="lb jq hi kx b fi lc ld l le lf">sfs = SequentialFeatureSelector(knn, n_features_to_select=2,direction='backward')</span></pre><p id="207e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后学习要选择的特性。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="dcd1" class="lb jq hi kx b fi lc ld l le lf">sfs.fit(X,y)</span></pre><p id="16d2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">显示选定的功能。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="3fdb" class="lb jq hi kx b fi lc ld l le lf">print("Features selected by backward sequential selection: "<br/>      f"{feature_names[sfs.get_support()].tolist()}")</span></pre><p id="caa4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过反向顺序选择选择的特征:['花瓣长度(cm)'，'花瓣宽度(cm)']</p><p id="15a0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这种新的特征选择方法的唯一缺点是，它可能比你已经知道的其他方法(SelectFromModel &amp; RFE)慢，因为它使用交叉验证来评估模型。</p><h1 id="bb60" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">4.超参数调谐的新方法</h1><p id="bf5a" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">谈到超参数调优，Scikit-learn的GridSearchCV和RandomizedSearchCv一直是许多数据科学家的首选。但在新版本中，我们有两个新的超参数调整类，称为<strong class="is hj">halvinggridsearccv</strong>和<strong class="is hj"> HalvingRandomSearchCV </strong>。</p><p id="f8b2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">HalvingGridSearchCV和HalvingRandomSearchCV使用一种称为<strong class="is hj">连续减半</strong>的新方法来寻找最佳超参数。连续减半就像所有超参数组合之间的竞争或比赛。</p><p id="166e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">连续减半是如何工作的？</strong></p><p id="f80f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在第一次迭代中，他们在一个观察子集(训练数据)上训练超参数的组合。然后在下一次迭代中，仅选择在第一次迭代中具有良好性能的超参数组合，并且它们将在大量观察中被训练以竞争。</p><p id="8f64" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，在每次迭代中重复这个选择过程，直到在最终迭代中选择出超参数的最佳组合。</p><p id="4f4e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意:这些课程仍然是实验性的:</p><p id="6174" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">例如:</strong></p><p id="a24e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">导入重要的包。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="957a" class="lb jq hi kx b fi lc ld l le lf">from sklearn.datasets import make_classification<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.experimental import enable_halving_search_cv  <br/>from sklearn.model_selection import HalvingRandomSearchCV<br/>from scipy.stats import randint</span></pre><p id="5377" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于这些新类仍然是实验性的，为了使用它们，我们显式地导入了<strong class="is hj">enable _ halving _ search _ cv</strong>:</p><p id="4a14" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用make_classification方法创建分类数据集。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="b3c9" class="lb jq hi kx b fi lc ld l le lf">X, y = make_classification(n_samples=1000)</span></pre><p id="ebec" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">创建估计器的实例。这里我们使用随机森林分类器。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="c083" class="lb jq hi kx b fi lc ld l le lf">clf = RandomForestClassifier(n_estimators=20)</span></pre><p id="ea40" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为调整创建参数分布。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="02c8" class="lb jq hi kx b fi lc ld l le lf">param_dist = {"max_depth": [3, None],<br/>              "max_features": randint(1, 11),<br/>              "min_samples_split": randint(2, 11),<br/>              "bootstrap": [True, False],<br/>              "criterion": ["gini", "entropy"]}</span></pre><p id="6d7d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后我们用RandomForestClassifier作为估计器和参数分布列表实例化HalvingGridSearchCV类。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="6485" class="lb jq hi kx b fi lc ld l le lf">rsh = HalvingRandomSearchCV(<br/>    estimator=clf,<br/>    param_distributions=param_dist,<br/>    cv = 5,<br/>    factor=2,<br/>    min_resources = 20)</span></pre><p id="b732" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">HalvingRandomSearchCV中有两个重要的参数你需要知道。</p><p id="175f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(a) <strong class="is hj">因子</strong> —这决定了为每个后续迭代选择的超参数组合的比例。例如，<strong class="is hj"> <em class="li">因子=3 </em> </strong>表示只选择三分之一的候选进行下一次迭代。</p><p id="f4b3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(b) <strong class="is hj"> min_resources </strong>是在第一次迭代中为每个超参数组合分配的资源量(观察值的数量)。</p><p id="4d8d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，我们可以用数据集来拟合我们创建的搜索对象。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="f801" class="lb jq hi kx b fi lc ld l le lf">rsh.fit(X,y)</span></pre><p id="7c00" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">训练后，我们可以看到不同的输出，如:-</p><p id="4ee4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(a)迭代次数。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="4912" class="lb jq hi kx b fi lc ld l le lf">print(rsh.n_iterations_ )</span></pre><p id="d2fa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi">6</p><p id="609d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(b)在每次迭代中评估的候选参数的数量。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="76a8" class="lb jq hi kx b fi lc ld l le lf">print(rsh.n_candidates_ )</span></pre><p id="fcc8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">【50，25，13，7，4，2】</strong></p><p id="3b62" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(c)每次迭代使用的资源数量。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="fc0a" class="lb jq hi kx b fi lc ld l le lf">print(rsh.n_resources_)</span></pre><p id="31b3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">【20，40，80，160，320，640】</strong></p><p id="65dd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(d)对拒绝数据给出最佳结果的参数设置。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="50e8" class="lb jq hi kx b fi lc ld l le lf">print(rsh.best_params_)</span></pre><p id="12f3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> {'bootstrap': False，<br/> 'criterion': 'entropy '，<br/> 'max_depth': None，<br/> 'max_features': 5，<br/> 'min_samples_split': 2} </strong></p><h1 id="2ccf" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">5.新的半监督学习自训练元估计器</h1><p id="32bb" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">Scikit-learn 0.24为半监督学习引入了一个新的自我训练实现，称为<strong class="is hj">自我训练分类器</strong>。SelfTrainingClassifier可以与任何可以返回每个类的概率估计值的监督分类器一起使用。</p><p id="c417" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这意味着任何监督分类器都可以作为半监督分类器，允许它从数据集中的未标记观察值中学习。</p><p id="901d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意:</strong>目标列中未标记的值必须为-1。</p><p id="ca9a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们在下面的例子中更多地了解它是如何工作的。</p><p id="8ef8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">导入重要的包</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="d1d8" class="lb jq hi kx b fi lc ld l le lf">import numpy as np<br/>from sklearn import datasets<br/>from sklearn.semi_supervised import SelfTrainingClassifier<br/>from sklearn.svm import SVC</span></pre><p id="364b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这个例子中，我们将使用iris数据集和超级向量机算法作为监督分类器(它可以实现<strong class="is hj"> fit </strong>和<strong class="is hj"> predict_proba </strong>)。</p><p id="26d3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，我们加载数据集，并随机选择一些未标记的观察值。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="5c8f" class="lb jq hi kx b fi lc ld l le lf">rng = np.random.RandomState(42)<br/>iris = datasets.load_iris()<br/>random_unlabeled_points = rng.rand(iris.target.shape[0]) &lt; 0.3<br/>iris.target[random_unlabeled_points] = -1</span></pre><p id="cc29" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如您所见，目标列中未标记的值为-1。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lj"><img src="../Images/cc45c48dbc2a01b8ebb595372cd3d709.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zk5SMWgEzOi_4Vys3R4-oA.jpeg"/></div></div></figure><p id="dd24" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">创建监督估计器的实例。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="15b3" class="lb jq hi kx b fi lc ld l le lf">svc = SVC(probability=True, gamma="auto")</span></pre><p id="3acb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">创建一个自训练元估计器的实例，并添加svc作为base_estimator。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="ab59" class="lb jq hi kx b fi lc ld l le lf">self_training_model = SelfTrainingClassifier(base_estimator=svc)</span></pre><p id="5bf1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，我们可以在具有一些未标记观察值的虹膜数据集上训练自训练模型。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="f29c" class="lb jq hi kx b fi lc ld l le lf">self_training_model.fit(iris.data, iris.target)</span></pre><p id="512c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">自我训练分类器(base_estimator=SVC(gamma='auto '，probability=True))</p><h1 id="2ed6" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">关于Scikit的最终想法-学习0.24</h1><p id="d28a" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">正如我所说的，scikit-learn仍然是最受欢迎的开源机器学习库之一，所有的<a class="ae jo" href="https://towardsdatascience.com/14-lesser-known-impressive-features-of-scikit-learn-library-e7ea36f1149a?ref=hackernoon.com" rel="noopener" target="_blank">特性</a>都可供您进行端到端的机器学习项目。您还可以在您的机器学习项目中实现本文中介绍的令人印象深刻的新功能。</p><p id="e3c4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可以在scikit-learn 0.24 <a class="ae jo" href="https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_0_24_0.html?ref=hackernoon.com" rel="noopener ugc nofollow" target="_blank">这里</a>找到发布的其他功能的亮点。</p><p id="992c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">恭喜👏👏，你已经做到这篇文章的结尾了！我希望你学到了一些新的东西，对你的下一个机器学习或数据科学项目有所帮助。</p><p id="f989" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你学到了新的东西或者喜欢阅读这篇文章，请分享给其他人看。在那之前，下期帖子再见！</p><p id="b69f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你也可以在推特上找到我<a class="ae jo" href="https://twitter.com/Davis_McDavid?ref=hackernoon.com" rel="noopener ugc nofollow" target="_blank"> @Davis_McDavid。</a></p><p id="d9b7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="li">最后一件事:</em> </strong> <em class="li">在下面的链接里多看看类似这样的文章。</em></p><div class="lk ll ez fb lm ln"><a href="https://towardsdatascience.com/improve-ml-model-performance-by-combining-categorical-features-a23efbb6a215" rel="noopener follow" target="_blank"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hj fi z dy ls ea eb lt ed ef hh bi translated">通过组合分类特征提高ML模型性能</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">提高机器学习模型性能的简单技巧。</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">towardsdatascience.com</p></div></div><div class="lw l"><div class="lx l ly lz ma lw mb io ln"/></div></div></a></div><div class="lk ll ez fb lm ln"><a rel="noopener follow" target="_blank" href="/geekculture/machine-learning-tutorial-feature-engineering-and-feature-selection-for-beginners-dd15b9d354"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hj fi z dy ls ea eb lt ed ef hh bi translated">机器学习教程—面向初学者的特征工程和特征选择</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">他们说数据是新的石油，但我们并不直接使用石油的来源。它必须经过处理和清洗…</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">medium.com</p></div></div><div class="lw l"><div class="mc l ly lz ma lw mb io ln"/></div></div></a></div><div class="lk ll ez fb lm ln"><a href="https://medium.datadriveninvestor.com/how-to-transform-machine-learning-models-into-native-code-with-zero-dependencies-597d01684a9f" rel="noopener  ugc nofollow" target="_blank"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hj fi z dy ls ea eb lt ed ef hh bi translated">如何将机器学习模型转换成零依赖的本机代码</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">将训练好的ML模型转换成你选择的编程语言。</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">medium.datadriveninvestor.com</p></div></div><div class="lw l"><div class="md l ly lz ma lw mb io ln"/></div></div></a></div></div></div>    
</body>
</html>
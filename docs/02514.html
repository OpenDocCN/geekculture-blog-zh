<html>
<head>
<title>Linear Regression From Scratch in Python WITHOUT Scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不使用Scikit在Python中从头开始线性回归-学习</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/linear-regression-from-scratch-in-python-without-scikit-learn-a06efe5dedb6?source=collection_archive---------0-----------------------#2021-05-18">https://medium.com/geekculture/linear-regression-from-scratch-in-python-without-scikit-learn-a06efe5dedb6?source=collection_archive---------0-----------------------#2021-05-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7d60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本教程中，我将简要介绍一种最常用的机器学习算法，线性回归，然后我们将学习如何在没有sci-kit-learn的情况下使用python从头开始使用最小二乘法来实现它。我们还将看看回归分析中R平方的解释，以及如何用它来衡量回归模型的好坏。</p><p id="95ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归是一种预测分析算法，显示因变量(x)和自变量(y)之间的线性关系。</p><p id="0860" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于给定的数据点，我们试图绘制一条最符合这些点的直线。直线方程如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/838d9705871fe44b408bcb1e07fa5341.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/1*Hxj819xh5skxf8P5LpZ_CA.gif"/></div></figure><p id="2cff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中，<br/> <strong class="ih hj"> x: </strong>输入数据点<br/> <strong class="ih hj"> y: </strong>预测值，因变量(监督学习)</p><p id="29fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">模型通过寻找最佳m，c值获得最佳拟合回归线</strong>。<br/> <strong class="ih hj"> m: </strong>回归线的偏差或斜率<br/> <strong class="ih hj"> c </strong>:截距，显示估计回归线穿过𝑦轴的点</p><h2 id="4fac" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">成本函数(J)</h2><p id="8cf4" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">如上所述，我们的目标是找到预测值和实际值之间差异(误差/残差)最小的回归线或最佳拟合线。这就是成本函数发挥作用的地方，因为我们广泛使用成本函数来计算(c，m)的值，以达到使预测y值(y^)和真实y值(y)之间的误差最小化的最佳值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kl"><img src="../Images/81daff2b753f80fb730492f6a77522c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g-mFpRq-irsYcz6QJg2Jxw.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx">Image Source: <a class="ae ku" href="https://realpython.com/linear-regression-in-python/" rel="noopener ugc nofollow" target="_blank">Linear Regression By Real Python</a></figcaption></figure><blockquote class="kv kw kx"><p id="ca2e" class="if ig ky ih b ii ij ik il im in io ip kz ir is it la iv iw ix lb iz ja jb jc hb bi translated">线性回归的成本函数(j)是预测y值(y^)和真实y值(y)之间的<strong class="ih hj">均方根误差(RMSE) </strong>。</p></blockquote><h2 id="3490" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">均方差(MSE)</h2><p id="b854" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">给定我们简单的线性方程y = c + m*x，我们可以计算MSE为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lc"><img src="../Images/3a5d5330699b5afd31eb239d21c6e2a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*3qXCkc5Ps_ihE0FKXiBAQA.png"/></div></figure><p id="4c0d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在哪里，</p><ul class=""><li id="1d1b" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated">𝑁是观测值的总数(数据点)</li><li id="9744" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">yᵢ是观测值的实际值，y^是预测值</li><li id="7cf2" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">j是成本函数，在这种情况下是均方误差</li></ul><h2 id="63cc" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">从头开始Python实现:</h2><p id="0d84" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">是时候学习算法的数学实现了。对于本教程，我将使用一个简单的x和相应的y值的数据集，如下所示。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/b5c8889d7490c658270240fdb6081795.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*DGyKp10ugKUzvvPSldrUQA.png"/></div></figure><p id="ed29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们计算x和y的平均值，我们将它们表示为x̅ <strong class="ih hj"> </strong> &amp; <strong class="ih hj"> </strong> y̅.</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/eabb56a072b227dbe199a560766847f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*l3O-IoGfURQ2ZEqOdcE0GA.png"/></div></figure><p id="a63f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看，我们的目标是使用最小二乘法预测最佳回归直线。所以要找到它，我们必须先找到这条线的方程。因此，如果y = c+ m*x，其中“m”是斜率/偏差，用x的变化除以y的变化表示。</p><blockquote class="kv kw kx"><p id="c6fb" class="if ig ky ih b ii ij ik il im in io ip kz ir is it la iv iw ix lb iz ja jb jc hb bi translated">x的变化是实际输入值xᵢ和x̅之间的差异，类似地，y的变化是标签yᵢ和y̅.之间的差异</p></blockquote><p id="246c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是<strong class="ih hj"> m </strong>的数学表示，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lt"><img src="../Images/cb70f10b456099b6c52e21e067840d64.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/1*TLC7sUmnMCzCLz1qB5xfFg.gif"/></div></figure><p id="ca5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，根据“m”的公式，我们要做的是为我们非常简单的数据集中的每个数据点计算(x-x̅)和(y-y̅)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lu"><img src="../Images/daf04c801dd839ca59510004f90de271.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iWbSjhM9tZj28QfWYe-V0w.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx">Image Source: <a class="ae ku" href="https://www.youtube.com/watch?v=E5RjzSK0fvY&amp;ab_channel=edureka%21" rel="noopener ugc nofollow" target="_blank">Linear Regression by Edureka</a></figcaption></figure><p id="0913" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们已经准备好了公式中的所有元素，我们将计算分子和分母的总和，并找到“m”的最终值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lv"><img src="../Images/33b1d30dbf671a65a8db4baf37e01833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ePO8tviyklxYvT4BSXRezQ.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx">Image Source: <a class="ae ku" href="https://www.youtube.com/watch?v=E5RjzSK0fvY&amp;ab_channel=edureka%21" rel="noopener ugc nofollow" target="_blank">Linear Regression by Edureka</a></figcaption></figure><p id="276f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们有了最后一个等式:</p><p id="5fc9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.6 = 0.4 * 3 + c</p><p id="05cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">c = 2.4</p><p id="fe7b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，对于给定的m = 0.4 &amp; c=2.4，让我们预测所有输入值x ={1，2，3，4，5}的y</p><p id="07ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">y = 0.4*1 + 2.4 = 2.8</p><p id="15de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">y = 0.4*2 + 2.4 = 3.2</p><p id="0bfe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">y = 0.4*3 + 2.4 = 3.6</p><p id="776b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">y = 0.4*4 + 2.4 = 4.0</p><p id="760f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">y = 0.4*5 + 2.4 = 4.4</p><p id="0757" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，如果我们绘制它们，穿过所有这些预测y值并在2.4处切割y轴的线就是我们的回归线。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lw"><img src="../Images/5d6b497e2956a856f293478033be1bb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G5ZGQyPd21dhe9a2pkL9-g.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx">Image by Author</figcaption></figure><p id="02dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们的工作是计算实际值和预测值之间的距离，并缩小这个距离。或者换句话说，我们必须减少实际值和预测值之间的误差。误差最小的线就是线性回归的线。</p><h2 id="335d" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated"><strong class="ak">这就是它的内部工作方式:</strong></h2><ol class=""><li id="4bf8" class="ld le hi ih b ii kg im kh iq lx iu ly iy lz jc ma lj lk ll bi translated"><strong class="ih hj"> <em class="ky">对不同的‘m’值做多次迭代，</em> </strong></li><li id="afa3" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc ma lj lk ll bi translated"><strong class="ih hj"> <em class="ky">然后计算直线y = c + m*x的方程，随着m值的变化，直线也会变化。</em>T9】</strong></li><li id="1ba9" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc ma lj lk ll bi translated"><strong class="ih hj"> <em class="ky">每次迭代后，根据直线计算预测值，并比较实际&amp;预测值之间的距离。</em>T13】</strong></li><li id="ccec" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc ma lj lk ll bi translated"><strong class="ih hj"> <em class="ky">预测值与实际值距离最小的线将被确定为回归线。</em> </strong></li></ol><h1 id="a747" class="mb jm hi bd jn mc md me jr mf mg mh jv mi mj mk jy ml mm mn kb mo mp mq ke mr bi translated">r平方值:</h1><p id="97f9" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">既然我们已经找到了最佳拟合的回归线，那么是时候来衡量它的好坏或检查我们的模型表现得有多好了。</p><blockquote class="kv kw kx"><p id="2d3b" class="if ig ky ih b ii ij ik il im in io ip kz ir is it la iv iw ix lb iz ja jb jc hb bi translated">r平方值是数据与拟合回归线接近程度的统计度量。</p></blockquote><p id="b682" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它也被称为决定系数或多重决定系数。</p><p id="47e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是我们计算R平方值的方法，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ms"><img src="../Images/3bc74fdeab520267eaa4728fbde8e00a.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/1*xMDP6Hw9tgJc88JomDNnsA.gif"/></div></figure><p id="aa70" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中，yₚᵣₑ𝒹是预测的y值，y̅是平均值，y是实际值</p><p id="67f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基本上，我们是在计算预测值和平均值之间的差值，然后除以实际值和平均值之间的差值。</p><p id="1cb3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是将预测值和实际值之间的所有差异相加后的最终R平方值</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mt"><img src="../Images/3711208769358982dc5309ed71792efe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ieGtqRjcfBec9Ws-i6HDEA.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx">Image Source: <a class="ae ku" href="https://www.youtube.com/watch?v=E5RjzSK0fvY&amp;ab_channel=edureka%21" rel="noopener ugc nofollow" target="_blank">Linear Regression by Edureka</a></figcaption></figure><p id="d6a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是个近似值，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mu"><img src="../Images/b3c81dfab13476105af61a99e60becda.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*0GWLvEfPiFurZjixtR73dQ.png"/></div></figure><p id="e22b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">意味着我们的数据点离回归线很远。嗯，那可不好，是吧？</p><p id="5bbd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基本上，R平方值越高，我们的模型性能就越好。因此，随着R平方值逐渐增大，实际点与回归线的距离会减小，模型的性能会提高。</p></div><div class="ab cl mv mw gp mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="hb hc hd he hf"><h2 id="cc66" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">Python中的实现:</h2><p id="66d7" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">既然我们已经学习了线性回归背后的理论&amp; R平方值，让我们继续学习编码部分。我将使用python和Google Colab。</p><p id="64ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将使用一个来自Kaggle的名为head brain <a class="ae ku" href="https://www.kaggle.com/saarthaksangam/headbrain" rel="noopener ugc nofollow" target="_blank">的简单数据集。</a>它有237行和4列，这意味着237个观察值和4个标签。我们必须根据给定的头部尺寸(厘米)来预测一个人的大脑重量。</p><p id="1899" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一步:</p><p id="54e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">导入必要的库，如pandas、NumPy和matplotlib</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="nc nd l"/></div></figure><p id="b6a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二步:</p><p id="2174" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将CSV文件导入为熊猫数据框。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="nc nd l"/></div></figure><p id="4bed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是我们的样本数据集:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ne"><img src="../Images/0cc9ba487e831019864d4dc561df8437.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xA2zVv4TYnNDLjKTavfKhA.png"/></div></div></figure><p id="9ba8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第三步:</p><p id="74f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以发现头部大小和大脑重量之间的线性关系。下一步是收集我们的X和Y值。x由头部尺寸值组成，Y由大脑重量值组成。我们还需要找到' m '和' c '的值，因此，我们需要找到X &amp; Y值的平均值。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="nc nd l"/></div></figure><p id="1b45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第四步:</p><p id="1581" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用公式计算m &amp; c，如果你记得我们在本文前面讨论过，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/838d9705871fe44b408bcb1e07fa5341.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/1*Hxj819xh5skxf8P5LpZ_CA.gif"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lt"><img src="../Images/cb70f10b456099b6c52e21e067840d64.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/1*TLC7sUmnMCzCLz1qB5xfFg.gif"/></div></figure><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="nc nd l"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nf"><img src="../Images/1fd7fb70091616729616eec86cadaf2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*DITG6RZH_bhdHevxZwBdCg.png"/></div></figure><p id="df54" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第五步:</p><p id="3b83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们有了m &amp; c，让我们画出输入点和回归线。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="nc nd l"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ng"><img src="../Images/2244efdd4f1a49642b779a0cb2b6091d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ztbqUrCIw6Cx1wVLKV0LcA.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx">Image by Author</figcaption></figure><p id="0d82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第六步:</p><p id="d56c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在是时候衡量我们的模型有多好了。为此，如上所述，我们将计算R平方值并评估我们的线性回归模型。如果你需要复习R平方的公式:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ms"><img src="../Images/3bc74fdeab520267eaa4728fbde8e00a.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/1*xMDP6Hw9tgJc88JomDNnsA.gif"/></div></figure><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="nc nd l"/></div></figure><p id="64d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的代码生成R平方值，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nh"><img src="../Images/614b06b7664fffb62dc3852ea66cc6ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*TqzrVzc87vi4YepHpfURnQ.png"/></div></figure></div><div class="ab cl mv mw gp mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="hb hc hd he hf"><p id="1a22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是不使用sklearn库从头实现的线性回归。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ni"><img src="../Images/c0c31a03e9a57cad31ba04e76488af86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*kw5EeGmjn2wM4razh1A7QA.gif"/></div><figcaption class="kq kr et er es ks kt bd b be z dx">Image Source: Google</figcaption></figure><p id="2e2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你不想被所有这些数学和理论所困扰，并且非常想寻找一种更简洁的方法，sklearn库有一个惊人的内置线性回归函数供你使用。下面是相关的代码片段:</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="nc nd l"/></div></figure><h2 id="bf7b" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">结束注释:</h2><p id="25bd" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">在本教程中，我们已经学习了线性回归算法背后的理论，以及在不使用sklearn内置线性模型的情况下从头开始实现该算法。</p><p id="7d39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在我的GitHub 上找到数据和iPython笔记本<a class="ae ku" href="https://github.com/sindhuseelam/linear-regression-implementation" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="b661" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">参考资料:</h2><p id="0f73" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">[1]<a class="ae ku" href="https://www.youtube.com/watch?v=E5RjzSK0fvY&amp;ab_channel=edureka%21" rel="noopener ugc nofollow" target="_blank">YouTube上Edureka的线性回归</a></p><p id="b88a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2]<a class="ae ku" href="https://www.geeksforgeeks.org/ml-linear-regression/" rel="noopener ugc nofollow" target="_blank">https://www.geeksforgeeks.org/ml-linear-regression/</a></p></div><div class="ab cl mv mw gp mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="hb hc hd he hf"><p id="e31f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<a class="ae ku" href="https://www.linkedin.com/in/sindhuseelam/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae ku" href="https://twitter.com/SindhuSeelam_" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上与我联系，获取更多关于机器学习、统计学和深度学习的教程和文章。</p></div></div>    
</body>
</html>
<html>
<head>
<title>Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/principal-component-analysis-3d2b3a0bb93e?source=collection_archive---------3-----------------------#2021-05-28">https://medium.com/geekculture/principal-component-analysis-3d2b3a0bb93e?source=collection_archive---------3-----------------------#2021-05-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="881e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">一种选择最重要特征的方法</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/3fa812b37de15a4d66a80f33b2774ba9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F2f1whiAvABj6le6mrLkBg.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Photo by <a class="ae jn" href="https://unsplash.com/@jcoudriet?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Jason Coudriet</a> on <a class="ae jn" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="e0e8" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">介绍</h2><p id="9135" class="pw-post-body-paragraph km kn hi ko b kp kq ij kr ks kt im ku jz kv kw kx kd ky kz la kh lb lc ld le hb bi translated">主成分分析是机器学习中广泛使用的降维技术之一。在庞大的数据集中，以最小的信息损失降低维度。减小尺寸将减少所需的时间和存储空间。</p><p id="fbb7" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">因此，主要思想是在低维空间中寻找精确的数据表示。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lk"><img src="../Images/dcc173741660b909cb70db3f100fa145.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*cRB88Z_VLicYm_OkWg2yjw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Two Features Data Points (Image by author)</figcaption></figure><p id="bb26" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">假设我们有一个包含两个特征(列)的数据集，我们希望将数据集的维度降低到一维。现在简单地删除任何特征都会导致信息丢失。在上图中，我们可以观察到两个特性的方差都很高，删除任何一个特性都会导致几乎50%的信息丢失。为了克服这个问题，我们使用PCA，因为它保留了数据中最大的方差。</p><p id="86ef" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">现在来看一组不同的数据点</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ll"><img src="../Images/ee194cce61253f1af5f4d5b518d04b47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*gw-vW0nGfLsYbf8y1NbpNA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Two Features Data Points-2 (Image by author)</figcaption></figure><p id="38fa" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">这里，我们可以观察到特征F1上的数据点的方差或分布非常小。因此，如果我们想在这种情况下跳过一个特性，那么就是F1。</p><p id="ceab" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">我们有没有办法在第一组数据点中做同样的事情？当你画了上面的两个图，并试着用同样的数据点做第三个图，但是在轴上有一些旋转，你就会明白这个概念了</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lm"><img src="../Images/6b97688bd5c1459ae69f6954ea735d1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*g2Woa1eYE5qz6fwApeeB-A.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Two NEW Features Data Points-3 (Image by author)</figcaption></figure><p id="eccb" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">轴旋转后，我们制作了两个新特性，这里NewF2中的扩散较小，我们可以跳过该特性。这些新功能是主要组件。主成分捕捉数据中最大的方差。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ln"><img src="../Images/4a9264452e3191358b0e2d45b1948619.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*HVctBq-Q0XaLKlvUx3Eskw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Two Features Data Points with PC-4 (Image by author)</figcaption></figure><p id="d8ea" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">记住，如果我们在一个数据集中有n个特征(列),我们将有n个主成分。但是我们只保留那些保存更多信息的。</p><h1 id="9c23" class="lo jp hi bd jq lp lq lr ju ls lt lu jy io lv ip kc ir lw is kg iu lx iv kk ly bi translated">PCA步骤</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lz"><img src="../Images/38c537eafa5adf5b96b67480573f5ce6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JnRmiXk5nj83OC50KIHXUg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Steps in PCA (Image by author)</figcaption></figure><h2 id="aa7a" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">1.标准化</h2><p id="c196" class="pw-post-body-paragraph km kn hi ko b kp kq ij kr ks kt im ku jz kv kw kx kd ky kz la kh lb lc ld le hb bi translated">标准化是一种扩展数据的方法。如果给定数据集中的不同特征处于不同的尺度，则相应的方差将会不同，并且所有特征的贡献将不会相同。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ma"><img src="../Images/ea16af0c9151694b15d3984a0708f238.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*zDFb6ZtMJJXNhe6F1q46LQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">z value (Image by author)</figcaption></figure><p id="c53e" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">具有高方差的变量将支配具有低方差的变量，导致有偏差的输出。</p><p id="4bed" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">注意:标准化是一个重要的步骤，在盲目进行标准化之前，理解数据和使用什么类型的模型是很重要的。当使用基于距离的算法时，标准化就完成了。</p><h2 id="5978" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">2.协方差矩阵</h2><p id="94b4" class="pw-post-body-paragraph km kn hi ko b kp kq ij kr ks kt im ku jz kv kw kx kd ky kz la kh lb lc ld le hb bi translated">协方差矩阵的计算将有助于我们理解变量是如何相互变化的。此外，协方差矩阵用于计算特征向量和特征值。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mb"><img src="../Images/4e7bca303a06bad5e9d72f37b61f5743.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*dP2ebco6ICFl7X9BxGp4FQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">covariance matrix (Image by author)</figcaption></figure><p id="5d65" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">变量与其自身的协方差就是方差。</p><p id="2499" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">COV(X，X)=VAR(X)，主对角线上的其他元素也是如此。</p><h2 id="95f4" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">3.计算特征向量和特征值</h2><p id="2386" class="pw-post-body-paragraph km kn hi ko b kp kq ij kr ks kt im ku jz kv kw kx kd ky kz la kh lb lc ld le hb bi translated">特征向量是主成分，特征值告诉我们解释的信息(方差)的百分比。</p><p id="3f4f" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">特征向量和特征值的计算公式</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mc"><img src="../Images/514d3310bc7ebdec9dba9369dbe97359.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*s5uaJHfDmsB4puw6R5El1w.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Eigenvector and Eigenvalue computation (Image by author)</figcaption></figure><p id="1e73" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">上述公式用于计算特征向量和特征值。这里A是任何矩阵，在我们的例子中，我们将把我们的协方差矩阵代替A .特征值是一个<em class="md">标量。</em></p><p id="5ecb" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">我们希望计算v和λ，以满足上述等式。让我们用一个给定的矩阵a来试试。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es me"><img src="../Images/4931edc19dd55c9c0dcf9f8582b74fc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*o52ZQbeoX1xPQhTQYRpiAw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">An X a vector (Image by author)</figcaption></figure><p id="f571" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">在这种情况下，我们选择了一个向量(1，1，1)，但结果与该向量无关。我不能取任何满足上述方程的标量。</p><p id="2d32" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">让我们试试另一个向量(1，1，0)</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mf"><img src="../Images/5245c6bc41e51931e88470148371e934.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F4RQBLKER7gOpX4XUROsPg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by author</figcaption></figure><p id="4aa4" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">在这种情况下，我们的向量和标量值3满足上面的等式。这就是v和λ的计算方法。</p><p id="00c8" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">协方差矩阵的特征向量是解释的方差最大的方向，特征值是每个特征向量的系数，给出了每个主分量中的方差量。</p><p id="b177" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">10维数据给出了10个主成分，但是PCA试图在第一个成分中保留尽可能多的信息，然后在第二个成分中保留尽可能多的信息，依此类推。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mg"><img src="../Images/726daa086e9f63ec3d51dd484d086811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vQyP6oxNAi9nwyQR9RPVGA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by author</figcaption></figure><p id="1d26" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">第一台电脑的解释差异百分比最高(信息)。如果我们想把10维减少到5维，我们可以把主成分从6维减少到10维，因为它们存储的信息最少。</p><p id="58f7" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">主成分或特征向量表示最大信息存在的方向，所有主成分相互垂直。这一独特的特征使得所有的PCs不相关，这就是为什么PCA用于消除多重共线性。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mh"><img src="../Images/2ab44bec7339e1ab9c4a361abad8ef53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*1-luiBfQB182Uz5kacJ0eg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by author</figcaption></figure><p id="e583" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">现在，所有的特征值都是标量，那么，如何计算解释的方差百分比呢？</p><p id="8137" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">为了计算每个分量保留的%方差，我们将每个分量的特征值除以所有特征值的总和。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mi"><img src="../Images/5047172edba4e75d7a2cd825b1d41245.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ja8XM8YsmTYtd6plamnMKg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by author</figcaption></figure><p id="b319" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">在2d数据中</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mj"><img src="../Images/2a6b915886fc9cc336fc3873cbc61d9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GNBOnEasZRwLfWqV48OY_w.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by author</figcaption></figure><p id="8e61" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">PC1解释了96 %的方差，而PC2仅解释了4 %。</p><h2 id="4481" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">4.特征向量</h2><p id="70ba" class="pw-post-body-paragraph km kn hi ko b kp kq ij kr ks kt im ku jz kv kw kx kd ky kz la kh lb lc ld le hb bi translated">特征向量是所有特征向量的矩阵，我们决定按列保存这些特征向量。</p><p id="5644" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">如果我们选择从<strong class="ko hj"><em class="md">【n】</em></strong>中保留<strong class="ko hj"> <em class="md"> m个</em> </strong>特征向量，那么最终的数据将有m个维度。</p><h2 id="1d0b" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">5.沿着主成分对齐/重铸数据</h2><p id="2c92" class="pw-post-body-paragraph km kn hi ko b kp kq ij kr ks kt im ku jz kv kw kx kd ky kz la kh lb lc ld le hb bi translated">到目前为止，除了标准化之外，我们没有对原始数据进行任何操作。具有特征向量(方向)的特征向量用于将数据从原始轴对准/重新定向到新的主分量轴。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mk"><img src="../Images/f58af43b9a610599ed3dcc36ccbc92a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RLQX_d3qKHdJj-Q2zpqB8A.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by author</figcaption></figure><h2 id="eab1" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">PCA的优势</h2><ul class=""><li id="bf63" class="ml mm hi ko b kp kq ks kt jz mn kd mo kh mp le mq mr ms mt bi translated">PCA消除了多重共线性，因为所有PC都是相互垂直的，因此是独立的。</li><li id="442b" class="ml mm hi ko b kp mu ks mv jz mw kd mx kh my le mq mr ms mt bi translated">更多的特征导致过度拟合，PCA移除不必要的信息减少过度拟合。</li></ul><h2 id="3820" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">PCA的局限性</h2><ul class=""><li id="6052" class="ml mm hi ko b kp kq ks kt jz mn kd mo kh mp le mq mr ms mt bi translated">PCA后形成的新特征是不可解释的，即它们是某些方向上的数字。</li><li id="1820" class="ml mm hi ko b kp mu ks mv jz mw kd mx kh my le mq mr ms mt bi translated">PCA试图保留数据的全局形状，而不保留局部结构。</li></ul><p id="e47c" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated">爱情！活下去！笑！干杯</p><p id="e194" class="pw-post-body-paragraph km kn hi ko b kp lf ij kr ks lg im ku jz lh kw kx kd li kz la kh lj lc ld le hb bi translated"><strong class="ko hj">参考文献</strong>:</p><ul class=""><li id="e75d" class="ml mm hi ko b kp lf ks lg jz mz kd na kh nb le mq mr ms mt bi translated">[Steven M. Holland，佐治亚大学]:主成分分析</li><li id="98e2" class="ml mm hi ko b kp mu ks mv jz mw kd mx kh my le mq mr ms mt bi translated">【skymind.ai】:特征向量、特征值、PCA、协方差、熵</li><li id="70d2" class="ml mm hi ko b kp mu ks mv jz mw kd mx kh my le mq mr ms mt bi translated">[Lindsay I. Smith]:主成分分析教程</li><li id="a9c0" class="ml mm hi ko b kp mu ks mv jz mw kd mx kh my le mq mr ms mt bi translated">greatlearning.com</li></ul></div></div>    
</body>
</html>
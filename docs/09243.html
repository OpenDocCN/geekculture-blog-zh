<html>
<head>
<title>Part 1: Crawling a website using BeautifulSoup and Requests</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">第1部分:使用BeautifulSoup和请求爬行网站</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/part-1-crawling-a-website-using-beautifulsoup-and-requests-413ffb29dd8e?source=collection_archive---------9-----------------------#2021-12-02">https://medium.com/geekculture/part-1-crawling-a-website-using-beautifulsoup-and-requests-413ffb29dd8e?source=collection_archive---------9-----------------------#2021-12-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/c0f41a14cc618c3efd1617b8fb803337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M1vcPlCK0U7Optpaz9kdKA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Used with permission from Pixabay</figcaption></figure><p id="8c70" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">你有没有做过一个项目，需要抓取一个子页面数量未知的网站？有哪些选择，我们能以多快的速度编写一个基本脚本来完成这项工作？在这个项目中，我正是这样做的，我从一个开发人员网站上搜索著名的报价，然后用streamlit将这些数据重新格式化成一个有用的web应用程序。</p></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><p id="ac97" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这个版本中，我们将和www.quotes.toscrape.com一起玩，这是一个专门设计来让开发者学习网络报废的网站。</p><p id="9846" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们将使用Requests和BeautifulSoup来展示即使使用一个相对简单的scrapping库也可以抓取多个页面。对于更大或更复杂的项目，我建议使用Scrapy或Selenium。</p></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><p id="52f3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">一如既往，我喜欢最小化依赖性。在这里，我将只依靠三个:熊猫，请求和美丽的声音</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="e363" class="kj kk hi kf b fi kl km l kn ko">import requests<br/>from bs4 import BeautifulSoup<br/>import pandas as pd</span></pre><p id="13c2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里我们用一个函数requests.get()导入<a class="ae jz" href="https://docs.python-requests.org/en/latest/" rel="noopener ugc nofollow" target="_blank">请求</a>来发布HTML/1.1请求。如果您想更好地理解requests在做什么，我建议您快速浏览一下官方文档。</p><blockquote class="kp kq kr"><p id="62eb" class="iu iv ks iw b ix iy iz ja jb jc jd je kt jg jh ji ku jk jl jm kv jo jp jq jr hb bi translated">" Requests允许您非常容易地发送HTTP/1.1请求。没有必要手动添加查询字符串到你的URL，或者对你的文章数据进行格式编码。多亏了<a class="ae jz" href="https://github.com/urllib3/urllib3" rel="noopener ugc nofollow" target="_blank"> urllib3 </a>，保活和HTTP连接池是100%自动的</p></blockquote><p id="b256" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在我们要创建一些全局变量来使用。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="1205" class="kj kk hi kf b fi kl km l kn ko"># Globals<br/>url = '<a class="ae jz" href="http://quotes.toscrape.com'" rel="noopener ugc nofollow" target="_blank">http://quotes.toscrape.com'</a><br/>url_list = [url,]<br/>pages = []<br/>soup_list = []<br/>not_last_page = True</span></pre><p id="37ff" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里，我们创建了几个要填充的列表(url_list、pages、soup_list ),并将not_last_page设置为True。我们一会儿就会明白为什么。</p><p id="9874" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">3.接下来，我们采取3步的方法来解析我们所有的页面。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="262f" class="kj kk hi kf b fi kl km l kn ko">#1: Pull the requests<br/>def pullUrl(func):<br/>    def inner(*args, **kwargs):<br/>        page = requests.get(url_list[-1])<br/>        if page.status_code == 200:<br/>            pages.append(page)<br/>            func(*args, **kwargs)<br/>        else:<br/>            print(f'The url {url} returned a status of {page.status_code}')<br/>    return inner</span><span id="cde4" class="kj kk hi kf b fi kw km l kn ko">#2: Make some soup<br/>def makeSoup(func):<br/>    def inner(*args, **kwargs):<br/>        soup = BeautifulSoup(pages[-1].content, 'html.parser')<br/>        soup_list.append(soup)<br/>        func(*args, **kwargs)<br/>    return inner</span><span id="3826" class="kj kk hi kf b fi kw km l kn ko">#3: Parse the URLs<br/><a class="ae jz" href="http://twitter.com/pullUrl" rel="noopener ugc nofollow" target="_blank">@pullUrl</a><br/><a class="ae jz" href="http://twitter.com/makeSoup" rel="noopener ugc nofollow" target="_blank">@makeSoup</a><br/>def getURLs():<br/>    global not_last_page<br/>    try:<br/>        next_page = url+soup_list[-1].find('li', {'class': 'next'}).find('a')['href']<br/>        print(next_page)<br/>        url_list.append(next_page)<br/>    except:<br/>        not_last_page = False</span></pre><p id="5705" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里发生了一些事情，所以让我们浏览一下代码。首先，我用谷歌Chrome的DevTools查看了我们正在抓取的网站的HTML。</p><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kx"><img src="../Images/0e498fa9ab68bca032d33f1987fd19c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c1Yq9Sc3ReHOP-xk8AHxgg.png"/></div></div></figure><p id="6179" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">请注意,“下一步”按钮位于一个html有序列表<ol>中，该列表包含一个“下一步”类。还要注意，url是与“/page/n/”连接的索引页面。有了这些信息和我们的三个导入，我们可以创建一些简单的函数来查找和抓取整个网站。</ol></p><p id="c79e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">第一个函数是一个简单的pull请求，它最终将通过使用负索引“[-1]”查找列表中的最后一项来迭代我们的url_list。为了确保我们收到一个好的请求，我们在。status_code属性来确保我们收到了200状态。请注意，我们将在这里使用python解译器来使代码更加python化。为了更好地回顾如何使用装饰者，请查看<a class="ae jz" href="https://www.geeksforgeeks.org/decorators-in-python/" rel="noopener ugc nofollow" target="_blank">GeeksForGeeks.org</a>。</p><p id="b0af" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在第二个函数中，我们解析从第一个函数获得的请求返回的每个页面，并将其保存到soup_list中。我们再次使用内部函数语法来创建python包装器。</p><p id="85aa" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在我们的第三个函数中，我们遍历我们的HTML模式来提取“下一个”href url，并将其与我们的索引url连接起来，并将其附加到我们的url_list中。我们准备使用try:except通过一个循环来运行它，以捕捉当我们试图解析最终的web页面时会出现的属性错误，因为它没有“next”类。当这种情况发生时，我们将not_last_page更改为false以退出循环。第1页的语法和示例输出如下所示。</p><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ky"><img src="../Images/df7532f44c1e5c4f620a928865a2bfb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BKiXba-yzvxqTejtzq0Fzw.png"/></div></div></figure><p id="419b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">就是这样。有了这三个导入和我们的三个函数，我们就可以解析所有的html，在这个例子中最终得到10页的引号。</p><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kz"><img src="../Images/9b14f1398c1255dc81085b384f1ddbc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8amUFO3u734KcgOjlAIJRA.png"/></div></div></figure><p id="3f08" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">剩下的工作就是取出实际的数据，并将其放入数据帧中。</p><pre class="ka kb kc kd fd ke kf kg kh aw ki bi"><span id="2259" class="kj kk hi kf b fi kl km l kn ko"># Start with an empty Data Frame:<br/>quotes_df = pd.DataFrame(columns=['Author', 'Quote'])</span><span id="3bdf" class="kj kk hi kf b fi kw km l kn ko"># Add in the quotes dictionary:<br/>for k,v in quotes.items():<br/>    quotes_df = pd.concat([quotes_df, pd.DataFrame({'Author': k, 'Quote': v})], sort=True)</span></pre><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es la"><img src="../Images/74d39deda93dbe5bf8982bcdaf9d66be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g_Ampf-RRfHp-IkVsUkIJA.png"/></div></div></figure></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><p id="d558" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">那么，我们取得了什么成就？我们确定了一个站点，它有未知数量的内容页面，我们希望将这些页面整合到一个数据框架中，以便在未来的项目中使用。我们依靠最少数量的依赖，开发了一些功能，我们不仅能够抓取，而且实际上爬行了一个网站。最后，我们利用Python decorators使我们的代码更加Python化。在本文的第二部分中，我们将获取web爬行的输出，并快速开发一个web应用程序，以允许用户随机接收报价，或者根据作者姓名专门选择报价。</p></div></div>    
</body>
</html>
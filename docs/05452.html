<html>
<head>
<title>Implement BERT Using PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PyTorch实现BERT</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/implement-bert-using-pytorch-40e3068639e6?source=collection_archive---------13-----------------------#2021-07-21">https://medium.com/geekculture/implement-bert-using-pytorch-40e3068639e6?source=collection_archive---------13-----------------------#2021-07-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/74394f15751a52570a7648c411330efc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V6-JM6qz7FFjCXjLQLXNlw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Photo by <a class="ae iu" href="https://unsplash.com/@osmanrana?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Osman Rana</a> on <a class="ae iu" href="https://unsplash.com/s/photos/light?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4144" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你对NLP感兴趣，你可能听说过BERT。在这篇文章中，我们将看看伯特，看看它是什么。它是如何工作的？，以及如何用PyTorch来写。</p><p id="0f0a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">谷歌在2018年发布了一篇名为“<a class="ae iu" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">深度双向转换器语言解释</a>的预训练”的论文。在这项研究中，他们描述了BERT(带转换器的双向编码器表示)，这是一种语言模型，在问答、自然语言推理、分类和一般语言理解评估等任务中实现了最先进的性能。</p><p id="e371" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">BERT版本是在三个架构发布之后发布的，这三个架构也实现了最先进的性能。这些模型是:</p><ul class=""><li id="d0a2" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">乌尔姆-菲特(1月)</li><li id="0f21" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">埃尔莫(二月)，</li><li id="547f" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">GPT公开赛(6月)</li><li id="88ee" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">伯特(十月)。</li></ul><p id="6298" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">开放的GPT和伯特使用变压器设计，不使用递归神经网络。这使得该架构能够通过自我关注机制来考虑长期关系，这自然改变了我们对顺序数据建模的方式。它介绍了一种用于包括图片制作在内的计算机视觉应用的编码器-解码器架构。</p><p id="929f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">那么，是什么让BERT从2018款的其余车型中脱颖而出呢？为了回答这个问题，我们必须首先理解什么是BERT以及它是如何工作的。</p><p id="6068" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">那么，我们开始吧。</p><h1 id="621c" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">伯特是什么？</h1><p id="133a" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">“带变压器的双向编码器表示”或BERT是“带变压器的双向编码器表示”的首字母缩写换句话说，通过编码器运行数据或单词嵌入，BERT提取模式或表示。编码器由许多层叠的变压器组成。这是一个双向转换器，这意味着它在训练过程中同时检查左右上下文。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/ae8cb6a8701605cc01f102375f0d57df.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/0*w48eEJOTQxCSafNg"/></div></figure><p id="614d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">BERT使用两种训练范式:<strong class="ix hj">预训练</strong>和<strong class="ix hj">微调</strong>。</p><p id="ecf8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该模型在一个巨大的数据集上进行训练，以在预训练期间提取模式。这通常是一个无监督的学习任务，其中模型是在无标签的数据集上训练的，如来自维基百科等大规模语料库的数据。</p><p id="7126" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在微调期间，该模型为下游任务(如分类、文本生成、语言翻译、问题回答等)进行了训练。你基本上可以下载一个预先训练好的模型，然后在你的数据上进行迁移学习。</p><h1 id="56f5" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">BERT的核心组件</h1><p id="e566" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">伯特借用了以前发布的SOTA模型的想法。让我们详细阐述一下那句话。</p><h2 id="5980" class="lp ki hi bd kj lq lr ls kn lt lu lv kr jg lw lx kv jk ly lz kz jo ma mb ld mc bi translated">变形金刚</h2><p id="8351" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">变压器架构是BERT最重要的部分。编码器和解码器是构成转换器的两个组件。自注意层和前馈神经网络是编码器的两个组成部分。</p><p id="c688" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">自我注意层接受输入，将每个单词转换成中间编码表示，随后输入到前馈神经网络。自我注意层、编码器-解码器注意和前馈神经网络都是前馈网络的一部分，前馈网络将这些表示发送给解码器。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/b19fbb88723d61cdbbc05620f2d9e0ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Q0UTt7qa2yl_t-Ce"/></div></div></figure><p id="86ec" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">转换器设计的优点是允许模型保留无限长的序列，这在以前用常规的rnn、LSTMs和gru是不可能的。然而，尽管它可以建立长期依赖关系，但它缺乏上下文意识。</p><h2 id="cc78" class="lp ki hi bd kj lq lr ls kn lt lu lv kr jg lw lx kv jk ly lz kz jo ma mb ld mc bi translated">工程与后勤管理局</h2><p id="17cc" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">BERT的灵感来自ELMo(来自语言模型的嵌入)，它代表来自语言模型的嵌入。2017年，Peters等人发表了ELMo，其中涉及了语境理解的概念。</p><p id="90f5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">BERT借用了ELMo的另一个概念，即语言模型嵌入。埃尔莫是由<a class="ae iu" href="https://arxiv.org/abs/1802.05365" rel="noopener ugc nofollow" target="_blank">彼得斯等人介绍的。艾尔。</a>2017年，涉及语境理解的思想。ELMo的工作方式是使用双向LSTM来理解这种情况。因为它从两个方向考虑单词，所以它可以对拼写相同但具有不同含义的单词进行各种单词嵌入。</p><p id="fdec" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">例如，“你们这些孩子应该在黑暗中团结在一起”，并不等同于“把那根棍子递给我。”尽管事实上同一个词在两个句子中使用，但其含义因上下文而异。</p><p id="b21e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，ELMo通过考虑来自左右两个方向的单词来分配嵌入，而不是像早期的模型那样只包括来自左边的单词。使用了RNNs、LSTMs和其他单向模型。</p><p id="4794" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这允许ELMo从序列中提取上下文信息，但与transformers不同，ELMo并不长期依赖于序列，因为它使用了LTSM。</p><p id="1ab6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">到目前为止，我们已经看到，由于transformers中存在的注意机制，BERT可以访问文档中的序列，即使序列中当前单词后面的单词也是如此，即它可以保持长期依赖性，并且由于ELMo中存在的双向机制，它还可以实现对句子的上下文理解。</p><h2 id="4c0a" class="lp ki hi bd kj lq lr ls kn lt lu lv kr jg lw lx kv jk ly lz kz jo ma mb ld mc bi translated">乌尔姆拟合</h2><p id="8577" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">杰瑞米·霍华德和塞巴斯蒂安·鲁德在2018年发表了一项名为通用语言模型微调或ULM-FiT的研究，他们在研究中表示，迁移学习可能会在自然语言处理中使用，就像它在计算机视觉中一样。</p><p id="42e6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以前，对于单词嵌入，我们使用的预训练模型只针对整个模型的第一层，即嵌入层，整个模型都是从头开始训练的，这非常耗时，而且收效甚微。另一方面，Howard和Ruder提出了三种文本分类方法:</p><ul class=""><li id="967b" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">第一步包括在更大的数据集上训练模型，以便模型学习表示。</li><li id="8993" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">在第二步中，使用特定于任务的数据集对模型进行微调以进行分类，在此期间，他们包括两种额外的方法:判别微调和倾斜三角学习率(STLR)。在网络的传输层，前一种方法旨在微调或优化每个参数，而后一种方法控制每个优化步骤中的学习速率。</li><li id="7c2a" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">分类器在特定于任务的数据集上进行微调，用于第三阶段的分类。</li></ul><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/75d9bb855c5ffd4f0b46193868d1d15d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JL8lZ2Hy8csHcJ_b"/></div></div></figure><p id="db1a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于ULM-FiT的引入，NLP从业者现在可以在他们的NLP挑战中练习迁移学习方法。ULM-FiT迁移学习方法的主要问题是，它需要微调网络的所有层，这非常耗时。</p><h2 id="cc06" class="lp ki hi bd kj lq lr ls kn lt lu lv kr jg lw lx kv jk ly lz kz jo ma mb ld mc bi translated">GPT开放大学</h2><p id="497d" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">OpenAI的拉德福德、纳拉西姆汉、萨利曼斯和苏茨基弗介绍了GPT(预训练生成变压器)。在单向方法中，他们提供了一个只使用转换器中的解码器而不是编码器的模型。因此，它在各种任务中的表现优于所有以前的型号，例如:</p><ul class=""><li id="83dd" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">分类</li><li id="5bb8" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">自然语言推理</li><li id="1a4a" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">语义相似度</li><li id="1deb" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">问题回答</li><li id="8874" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">多项选择。</li></ul><p id="6d12" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">尽管GPT只使用了解码器，但它仍然可能具有长期依赖性。此外，与ULM-FiT相比，它将微调减少到最低限度。</p><p id="7b7f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下表从预培训、下游任务以及最重要的微调方面比较了各种模型。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/c1d7cd0b42ae3747172c408b88e2ecbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*izKi9jk77_F4l5sR"/></div></div></figure><p id="e519" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下是GPT论文的摘录:“与循环网络等替代方案相比，这种模型选择为我们提供了更好的结构化记忆，以解决文本中的长期依赖性，从而在各种任务中实现稳定的传输性能。”在转换过程中，我们利用从遍历式方法中派生出来的特定于任务的输入适应，这种方法将结构化的文本输入作为一个连续的记号序列来处理。正如我们在实验中所展示的，这些适应使我们能够有效地进行微调，对预训练模型的架构进行最小的改变。</p><h1 id="0b45" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">为什么是伯特？</h1><p id="9469" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">伯特是一个自我监督的典范。也就是说，在没有人类明确编程的情况下，它可以从原始语料库中产生输入和标签。请记住，它所训练的数据是非结构化的。</p><p id="1e37" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">掩蔽语言模型和下一句预测是BERT预先训练的两项任务，前者使用掩蔽输入，如“那个人[掩蔽]到商店”，而不是“那个人去了商店”。这限制了BERT看到它旁边的单词，这允许它尽可能多地学习双向表示，使它对几个下游任务更加灵活和可靠。后者预测这两个句子是否在上下文中相互分配。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/85ed4c8e0130fe305347f6ee7295d49a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VdAQt4Xw5VpmAC1C"/></div></div></figure><p id="1dee" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">例如，如果句子A是“[CLS]那个人[面具]去商店”，句子B是“企鹅[面具]是不会飞的鸟[SEP]”，那么BERT将能够区分这两个句子是否是连续的。</p><p id="cb20" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在训练过程中，BERT使用特殊类型的标记，如[CLS]、[屏蔽]、[分离]等，这些标记允许BERT区分一个句子何时开始，哪个单词被屏蔽，以及两个句子何时分开。我已经在<strong class="ix hj">预处理</strong>部分以表格的形式解释了这些令牌。</p><p id="6cff" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">由于我们之前讨论过的特性，BERT还可以用于特征提取</strong>，并将这些提取提供给你现有的模型。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/4e12b4b18654199920e7c749954fe11e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AwQCO4j4siEOdrAI"/></div></div></figure><p id="682a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在最初的BERT论文中，它与GPT在<a class="ae iu" href="https://gluebenchmark.com/" rel="noopener ugc nofollow" target="_blank">通用语言理解评测基准</a>上进行了比较，下面是结果。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/b0bc8e1d02488e4bb787972f34436bf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kmA5oGe_sEZZjR-P"/></div></div></figure><p id="5b8a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如你所看到的，伯特在所有的任务中都超过了GPT，平均比GPT高出7%。</p><h1 id="ddec" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">建筑模型</h1><p id="1c61" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">BERT是一个复杂的模型，如果你接近它的速度足够慢，你就会失去逻辑。因此，一个接一个地检查每个组件及其功能是有意义的。</p><p id="f2e4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">BERT具有以下组件:</p><ol class=""><li id="fc5e" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js mg jz ka kb bi translated">嵌入层</li><li id="9841" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mg jz ka kb bi translated">注意力屏蔽</li><li id="c866" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mg jz ka kb bi translated">编码器层</li><li id="dc98" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mg jz ka kb bi translated">多头注意力</li><li id="6c77" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mg jz ka kb bi translated">比例点产品关注度</li><li id="ef8b" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mg jz ka kb bi translated">位置式前馈网络</li><li id="2c49" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mg jz ka kb bi translated">伯特(组装所有部件)</li></ol><p id="34e7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了便于学习，你可以随时参考这个图表。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/2c17f09f06504ed2fbf91f309bcef8fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/0*wYaAI6IRl4v15nI3"/></div></figure><h2 id="3dff" class="lp ki hi bd kj lq lr ls kn lt lu lv kr jg lw lx kv jk ly lz kz jo ma mb ld mc bi translated">嵌入层</h2><p id="8bd8" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">BERT中的第一层是嵌入，它接受输入并构建一个查找表。嵌入层参数是可学习的，这意味着一旦学习过程完成，嵌入将把相关单词聚集在一起..</p><p id="0a3f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">嵌入层还保留单词之间的不同关系，如语义、句法和线性联系，以及上下文交互，因为BERT是双向的。</p><p id="c05b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在BERT的例子中，它为</p><ul class=""><li id="9515" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">令牌，</li><li id="f39b" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">分段和</li><li id="2534" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">位置。</li></ul><p id="bf8a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们还没有编写一个函数来接收输入并为位置嵌入进行格式化，但是标记和段的格式化已经完成了。因此，我们将获取数据，并为序列中的每个单词分配一个位置。它看起来像这样:</p><pre class="ll lm ln lo fd mi mj mk ml aw mm bi"><span id="5b0b" class="lp ki hi mj b fi mn mo l mp mq">print(torch.arange(30, dtype=torch.long).expand_as(input_ids))</span></pre><h1 id="74d9" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">损失和优化</h1><p id="a850" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">虽然原始论文计算了所有词汇的概率分布，但我们可以使用softmax近似值。但是一个简洁的方法是使用<strong class="ix hj"> <em class="mf">交叉熵损失</em> </strong>。它是<em class="mf"> softmax </em>和<em class="mf">负对数似然</em>的组合。</p><p id="1fe7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，在构建模型时，您不必包括softmax，而是从没有softmax归一化的前馈神经网络获得清晰的输出。</p><p id="942d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">说到优化，我们将使用<strong class="ix hj"> Adam </strong>优化器。</p><pre class="ll lm ln lo fd mi mj mk ml aw mm bi"><span id="ffbb" class="lp ki hi mj b fi mn mo l mp mq">criterion = nn.CrossEntropyLoss()<br/>optimizer = optim.Adam(model.parameters(), lr=0.001)</span></pre><h1 id="31b2" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">结论</h1><p id="31b7" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">BERT是一种前沿的自然语言处理模型。预训练模型是在大型语料库上训练的，您可以根据您的需求和任务在较小的数据集上对其进行微调。关于微调最好的部分是你不必做1000个纪元；根据参数和数据集处理的效率，它可以在3到10个时期内复制SOTA性能。</p><p id="9ac7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">快乐阅读！！</p></div></div>    
</body>
</html>
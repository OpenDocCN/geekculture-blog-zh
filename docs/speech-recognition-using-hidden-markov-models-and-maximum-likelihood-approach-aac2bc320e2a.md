# 基于隐马尔可夫模型和最大似然法的语音识别

> 原文：<https://medium.com/geekculture/speech-recognition-using-hidden-markov-models-and-maximum-likelihood-approach-aac2bc320e2a?source=collection_archive---------33----------------------->

![](img/d576968624f8a3a1772299e38d7a7c3d.png)

Photo by [Leon Dewiwje](https://unsplash.com/@elodiso?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

接收到的信号是数据和噪声的融合，因此分析语音信号的隐马尔可夫模型是基于维特比算法，从卷积码或网格图导出的，该卷积码或网格图是从卷积编码器转换的数据获得的。HMM 和 ML 使用联合概率和独立事件概念来增强接收信号。当使用 HMM 解码该信号时，那么在解码之后，可以预测未来的结果。通过应用概率 HMM 方法获得的启发式特征可以帮助您决定是否存在什么情绪或者哪个单词将被进一步说出，从而可以接受关于当前状况的心态，并且可以实现富有成效的输出。这是文章的概述，希望你会觉得有用。

**简介**

使用 HMM- Viterbi 算法进行语音信号增强和预测。概率方法因此被用于从接收的信号中预测启发式结果。卷积码编码器被送入维特比解码器，它主要利用联合概率和独立事件概念，预示着未来丰富的输出，它可以帮助你在先前输入的基础上决定下一个词是什么。深度学习太强大了，无法轻松解决现实问题。语音识别是可以预见的，未来的问题可以很容易地解决。

**最大似然估计**

最大似然法是一种用于通信的概率方法。在现实世界中，通信信道的计算过于复杂，因此为了计算这种复杂性，我们需要一个概率模型解码器，它可以帮助解码具有错误概率的信号。解码模型应该能够计算复杂的通信信道。基于所传输信号的经验数据，优先考虑用于解决复杂性的解码模型。这里，语音信号是要被接收的预兆，因此，最大似然方法在这里被预先选择。每个模型都有自己的一组参数来表征它们。最大似然法是基于联合概率和独立事件的概念，使用概率密度函数来绘制信号，强调连续数据。在分析数据的机器学习中，要考虑在实际测试中准备的数据集。为了训练具有连续数据的模型，回归方法是优选的。这里，语音信号将被训练，并且其未来的启发式预测将被做出，因此使用隐马尔可夫模型。

**格子图**

网格是一种图形，其节点被排序成垂直切片，每个节点每次都连接到至少一个较早的节点和至少一个较晚的节点。网格中的最早和最晚时间只有一个节点。

网格用于通信理论和加密的编码器和解码器。它们也是在 Baum-Welch 算法或 Viterbi 算法或隐马尔可夫模型中使用的中心数据类型。

**维特比算法**

维特比算法是借助于格子图或卷积码来解决隐藏状态的动态解码编程算法。它最常用于对约束长度 k≤3 的卷积码进行解码，但实际中使用 k=15 的值。

卷积码与分组码的不同之处在于编码方案中存在记忆。也就是说，卷积编码器通过向输入流符号添加冗余来实现低误码率传输。因此，它产生的输出位多于移入其存储器的输入位。卷积码是通过将待传输的信息序列通过线性有限状态移位寄存器来产生的。卷积编码器的码率定义为输入位数与输出位数之比。一般而言，码率定义为 Rc = k/n，编码器以(K，K，n)格式表示，其中

n:由编码器产生的代码符号的数量

k:进入编码器的信息位数

k:代码的约束长度

编码器的约束长度是代码内存的度量。(K，K，n)卷积编码器的结构如下图所示

(K，K，n)卷积码可以(例如)通过使用一组 n 个向量来指定，n 个模 2 加法器中的每一个都有一个向量。每个向量具有 Kk 个维度，并且包含编码器到模 2 加法器的连接。向量的第 I 个位置中的 1 表示移位寄存器中的相应级连接到模 2 加法器，0 表示该级和模 2 加法器之间没有连接。具体来说，让我们考虑约束长度为 K = 3、k = 1 和 n = 2 的二进制卷积编码器，如下图所示。

**隐马尔可夫模型**

HMM 基于扩充马尔可夫链。马尔可夫链是一个模型马尔可夫链，它告诉我们一些关于随机变量序列的概率，状态，每一个都可以取某个集合中的值。这些集合可以是单词、标签或代表任何事物的符号，比如天气。马尔可夫链做了一个很强的假设，如果我们想预测序列中的未来，唯一重要的是当前状态。当前状态之前的状态对未来没有影响，除非通过当前状态。

更正式地，考虑一系列状态变量 q1，q2，…，qi。马尔可夫模型体现了关于这个序列概率的马尔可夫假设:在预测未来时，过去并不重要，只有现在才重要。马尔可夫假设:

P(qi = a | Q1…qi 1)= P(qi = a | qi 1)

状态在图中被表示为节点，而带有概率的转换被表示为边。

当我们需要计算一系列可观察事件的概率时，马尔可夫链是很有用的。然而，在许多情况下，我们感兴趣的事件是隐藏的:我们不会直接观察它们。例如，我们通常不会观察到文本中隐藏的词性标签。相反，我们看到单词，必须从单词序列中推断出标签。我们称标签为隐藏的，因为它们没有被观察到。隐马尔可夫模型(HMM)允许我们谈论观察到的事件隐马尔可夫模型(如我们在输入中看到的单词)和我们认为是概率模型中的因果因素的隐藏事件(如词性标记)。HMM 由以下组件指定:

Q = q1q2…qN 一组 N 个状态

A = a11… aij … aNN 一个转移概率矩阵 a，每个 aij 代表从状态 I 转移到状态 j 的概率，s.t.PN j=1aij = 1 ∀i

O = o1，o2… oT 一系列的 T 个观察值，每一个都来自词汇 V = v1，v2，…，vV

B = bi (ot)一系列观察似然性，也称为发射概率，每一个都表示从一个状态 i π =π1，π2，…，πN 产生一个观察 ot 的概率。πi 是马氏链从状态 I 开始的概率，有些状态 j 可能有πj = 0，意味着它们不能是初始状态。此外，Pn i=1，πi = 1

一阶隐马尔可夫模型实例化了两个简化的假设。首先，与一阶马尔可夫链一样，特定状态的概率仅取决于前一状态:马尔可夫假设:

P(qi | Q1…qi 1)= P(qi | qi 1)

第二，输出观察 oi 的概率仅取决于产生观察 qi 的状态，而不取决于任何其他状态或任何其他观察:

输出独立性:

P (oi|q1…qi，…，qT，o1，…，oi，…，oT) = P(oi|qi)

**隐马尔可夫模型最大似然法**

隐马尔可夫模型假设一个随机变量序列是条件独立的，给定一个形成马尔可夫链的状态变量序列。这些模型的最大似然估计可以使用 EM 算法来执行。本文证明了极大似然估计序列的相合性。隐马尔可夫模型形成了一大类有用的随机过程模型，其中一系列计数、比例或多变量观察值同样容易描述。这些模型基于描述系统状态演变的马尔可夫链{Xi}。给定状态变量{xi}的实现序列，观察变量{Yi}是条件独立的，每个 Yi 的分布取决于相应的状态 xi。在估计问题中，假设 Y 的分布属于一个参数族，并且假设状态空间是有限的。隐马尔可夫模型的特殊情况，其中观察变量只有有限多个值，被称为马尔可夫链的概率函数；这个模型是由鲍姆和佩特里(1966)提出的。隐马尔可夫模型和状态空间模型之间有明显的相似性，例如线性状态空间模型:

xi = Fxi-1+ x，

yi = HXi + Wi，

由未观测的状态变量{Xi}、观测值{Y}和噪声变量{ V；}和 Wi}。在状态空间模型的许多应用中，目标是基于观察集 Y1，.。。Y，即，如果 i = n 则滤波，如果 i < n, or prediction if i > n 则平滑。在具有正态误差的经典模型中，使用卡尔曼滤波器来执行重建。

条件 1:随机矩阵[ajk

(φ0)]是不可约的。

条件 2:{ f(y，θ))的至多 m 个元素的混合族是可识别的。

条件 3:对于每个 y，f (y，.)是连续的，在无穷远处消失。

条件 4:对于每个 j，k，ajk(。)和θj(。)都是连续的。

条件 5:Eφ0[| log f(y1，θj(φ0)I]

Condition 6: For every EΦ0[sup ||θ’ -θ||< δ (logf (y1, θ’))] 0，(||。||)是欧氏距离，x+ = max{x，0))。

**HMM 和语音识别**

语音识别是将语音信号转换成单词序列的过程。各种方法已经用于语音识别，包括动态编程和神经网络。HMM 具有非常丰富的数学结构，因此可以形成广泛应用的理论基础。HMM 模型，当应用得当时，在实践中可以很好地用于几个重要的应用。

语音识别包括两个主要模块，特征提取和特征匹配。特征提取模块的目的是将语音波形转换成某种类型的表示，以便进一步分析和处理，这种提取的信息称为特征向量。信号处理前端模块完成语音信号到特征向量的转换过程。如上面的框图所示，前端的输入是无噪声的语音样本，其输出是特征向量。在特征匹配中，将从未知语音样本中提取的特征向量与声学模型进行评分，得分最高的模型获胜，其输出被视为已识别单词。以下是实现前端(提取特征因子)的几种方法

MFCC(梅尔频率倒谱系数)LPC(线性预测编码)

一旦获得特征向量，我们就建立声学模型。声学模型用于对未知语音样本进行评分。如框图所示，前端的输出作为声学模型的输入。不同类型的声学模型有

VQ 法典

GMM-高斯混合模型

声学模型表示:在语音识别中，声音的基本单位是音素。音位是用来区分词义的最小单位。对于“CAT”音素序列，K 是 A 和 t。在英语中，大约有 46 个音素。我们可以使用这个音素的适当连接从英语词典中构造任何单词。为了识别一个给定的单词，我们需要从语音样本中提取音素。由于语音信号的慢时变特性，短时频谱分析是表征语音信号最常用的方法。当在足够短的时间内(10 到 25 毫秒之间)检查时，它的特性相当稳定。然而，随着时间的推移，信号特征会发生变化，以反映所发出的不同语音。利用这种观察，我们发现在 10 到 25 毫秒内提取的特征向量对应于单个音素。对于 HMM 中的语音识别，我们为每个基本单元(音素)分配一个唯一的 HMM。研究表明，每个音素 HMM 可以用三种状态来表示，即开始、中间和结束状态。

为了理解这一点，以孤立词识别器为例，词汇表中的每个词都有不同的 HMM。当未知单词出现时，它相对于所有 HMM 模型被评分，并且具有最高评分的 HMM 被认为是被识别的单词。如框图所示，声学模型的输出是音素序列。通过反向查找字典(音素—单词),我们可以找到相应的单词。但是通常有许多单词具有相同音素序列，例如 car key 和 Khakee 具有相同的音素序列。在这种情况下，语言结构就进入了画面。语言结构使用上下文信息来缩小已识别单词的范围，以类似于给定的语法结构。这种类型的模型被称为单音或上下文无关模型，以下是不同类型的 HMM

1.上下文无关音素 HMM

状态数:每个音素的 d 状态 HMM 通常等于 3)

准确性:在连续语音识别中不准确

紧凑:d 状态 HMM 导致需要计算的参数更少

概述:是的，我们可以使用现有的音素 HMM 为新单词构建 HMM

2.上下文相关三音素 HMM

状态数:每个音素的 d 状态 HMM

准确性:准确，因为它具有左右音素关系 11

紧凑:每个音素具有直接的左右关系，需要计算更多的参数

概述:是

3.全词 HMM

状态数:无音素生成

分配状态数以模拟整个单词

准确:是的，需要大量的训练数据和少量的词汇

紧凑:不，随着词汇量的增加，需要太多的状态

常规:否，无法使用此表示法构建新单词

在实践中，三音素模型被广泛应用于使用隐马尔可夫模型的语音识别中。现在我们不讨论如何从语音信号中提取特征向量。我们假设使用 MFCC 或线性预测编码我们得到所需的特征向量。在 MFCC 的情况下，它是 39 维向量。问题是如何将特征向量映射到 HMM 状态。为此，我们使用称为向量量化的技术。

创建特征向量的训练集

将他们分成几个小组

用符号表示每个类别

对于每个类别 Vk，计算其由给定 HMM 状态生成的概率。

在矢量量化中，我们定义了包含每个符号条目的码本，该码本也称为原型矢量或码字。例如，如果我们有 256 个类别(即 8 比特 VQ)，我们在码本中有 256 个条目。如图 8 所示，将输入向量与每个原型向量进行比较，选择具有最小距离的向量，并将其索引值给予输入向量。

如何生成码书(K-means 聚类)？

从 L 个训练向量中选择 M 个向量——其中 M = 2B 作为初始码字(选择它们之间距离最大的 M)

对于每个训练向量，找到最近的码字(最小距离)。将该训练向量分配给该单元

对于每个单元，计算该单元的质心。新的码字是质心

重复最后两步，直到平均距离低于阈值

正如我们看到的评价问题即给定观测序列 O = O1，O2，O3，，OT 如何计算 P(O|λ)。在那里我们应用链式法则计算 P(O|λ)如图 10:链式法则—难解算法 P(O|λ) = X 整体状态πQ1 bq1(O1)aq1q 2 bq2(O2)aqT 1 Qt bqT(OT)这是 O(NT)的一个阶。如图 10 所示，P(o1，o2，O3)在所有状态序列上的概率为 P(O1，o2，o3|q0，q0) + P(o1，o2，o3|q0，q0，q1) + P(o1，o2，o3|q0，q1，q0) + P(o1，O2，o3|q0，q1，Q0)…。为了提高效率，我们使用前向变量，该变量使用动态规划技术将阶数降低到 N2T。在第 2.5.2 节中，我们定义了前向变量，使得αt(i) = P(O1，O2，，Oi，qt = Si |λ),并通过对所有 I 的所有先前值αt 1(I)求和来计算αt(j)。图 11 显示了音素序列为 W AH 和 n 的单词“ONE”的计算。在前向计算中，我们考虑对所有传入节点求和，以找到 P(O|λ)。因为我们音素模型是二元模型，所以只有它的前身状态输入被考虑来计算概率和。为了更好地理解，参考图 11，在时间 t=3 时，状态 N 的输入仅来自状态 AH 和 N，而不是来自 w。这样，给定观察序列和模型λ，我们可以找到 P(O|λ)。

这样，语音信号的解码和增强可以借助于 HMM 维特比算法来完成。

**参考文献**

[1]，，董，，郭伟斌，“无线信号识别中的深度学习技术综述”，载《无线通信与移动计算》2019 年第一期

[2] Brian G. Leroux，“隐马尔可夫模型的最大似然估计”，随机过程及其应用 40(1992)127–143，1990

[3]丹尼尔·茹拉夫斯基(Daniel Jurafsky)和詹姆斯·马丁(James H. Martin)，《隐马尔可夫模型》，语音和语言处理，2019 年

[4] Iead Rezek，Peter Sykacek，Stephen J. Roberts，“耦合隐马尔可夫模型的贝叶斯和最大似然学习的比较”，研究之门，2000 年

[5]李冰，巴保明，“多用户 CPM 系统简化最大似然接收机的设计”，科学世界杂志，2014 年第一卷，2014

[6] M. J. F. Gales，“基于 HMM 的语音识别的最大似然线性变换”，计算机语音和语言，1998 年

[7] Johanna Tuominen 和 Juha Plosila，“行动系统中的异步维特比解码器”，研究之门，2005 年
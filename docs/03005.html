<html>
<head>
<title>Random Forest</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/random-forest-bdf35283f11a?source=collection_archive---------23-----------------------#2021-05-29">https://medium.com/geekculture/random-forest-bdf35283f11a?source=collection_archive---------23-----------------------#2021-05-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="d269" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">了解森林。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/d7af641e4f71f761e96f4aa3f5c46323.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vuO2_Kw3aKJ3r6Yl"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx"><strong class="bd jt">Let the forest make the decision</strong></figcaption></figure><ul class=""><li id="2eb5" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">如果你不熟悉决策树  <strong class="ih hj">的概念，请阅读我写的关于</strong> <a class="ae kd" rel="noopener" href="/geekculture/decision-trees-51d4195a58d2"> <strong class="ih hj">决策树的文章。</strong></a></li></ul><p id="9add" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机森林算法，顾名思义，就是<strong class="ih hj">个不同的不相关决策树</strong>的集合。这个算法基本上是基于集体良知的。</p><ul class=""><li id="e229" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated"><strong class="ih hj">分类用例</strong>:对于每一个输入，所有的树投票，最高票数的类获胜(民主)。</li><li id="0a62" class="ju jv hi ih b ii ke im kf iq kg iu kh iy ki jc jz ka kb kc bi translated"><strong class="ih hj">回归用例</strong>:通常所有树的结果的平均值就是森林的输出。</li></ul><h1 id="1f3b" class="kj kk hi bd jt kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">如何创建不同的不相关的树？</h1><p id="6293" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">这分两步进行:</p><ul class=""><li id="ca44" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">首先，<strong class="ih hj">从原始数据集创建不同的数据集(允许替换行)</strong>。让我们通过一个例子来理解它:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ll"><img src="../Images/8413758176d5dde5e8baa410d257808b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RUVYGD9gHmLQWW4j4hfllQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Original dataset</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lm"><img src="../Images/7821891a29ab457a9007cedf279be708.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nh7knAFHf_NJcVZLYTACWA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Child dataset #1</figcaption></figure><p id="c404" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在子数据集#1中，可以注意到第一行已经重复，最后一行已经从原始数据集中删除。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ln"><img src="../Images/6c90a894bdaaf687e601aab383a5d1b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fK3O9boaJ3waK988RlqfCg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Child dataset #2</figcaption></figure><p id="f792" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在子数据集#2中，第一行已被原始数据集的第三行替换。类似地，准备各种数据集。随机数据集的数量将等于我们在训练时想要创建的树的数量。每棵树被称为一个估计量。</p><p id="6edd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从单个数据集创建大量数据集的整个过程被称为<strong class="ih hj">打包或引导聚合。</strong></p><ul class=""><li id="1a5f" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">其次，为了创建一棵树，<strong class="ih hj">随机森林算法在每个节点分裂处从特征空间中提取随机特征子集</strong>。</li></ul><p id="8e08" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了理解这一点，让我们考虑我们的示例数据集中的特征A是用于分类的强特征，即，它将总是在第一次分割中使用的特征，因为如果所有的特征被一次结合在一起，它会产生高信息增益(IG)。在这种情况下，生成树将彼此相似。因此，为了避免这种情况，在创建不同的树时，从样本空间中选择特征子集，因此在初始分割时不会总是考虑A。</p><h1 id="4d5b" class="kj kk hi bd jt kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated"><strong class="ak">一个系综过程如何优于单个树？</strong></h1><p id="0730" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">套袋法中每棵树的期望与随机森林的平均期望相同。因此，随机森林侧重于减少方差。如果树彼此之间不是完全不相关的，并且具有相关系数ρ“rho ”,那么方差由下式给出:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lo"><img src="../Images/7f26f6c513dca3e5efce3a2bc50e365e.png" data-original-src="https://miro.medium.com/v2/resize:fit:288/format:webp/1*-pc08tXGpaAIfxoecAuXcw.png"/></div></figure><p id="e0dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，通过平均大量的树，我们减少了第二项，从而减少了方差。当随机森林从特征集中选择特征的子集时，它进一步减小ρ的值，即理想地减小到0。</p><p id="96ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于分类，要素子集长度的默认值为√P，最小结点大小为1。</p><p id="ade3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于回归，要素子集长度的默认值为P/3，最小结点大小为5。</p><p id="1db0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">特征子集的这些参数长度也可以被调整为超参数。</p><h1 id="3635" class="kj kk hi bd jt kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">随机森林的其他功能:</h1><ul class=""><li id="2cfb" class="ju jv hi ih b ii lg im lh iq lp iu lq iy lr jc jz ka kb kc bi translated"><strong class="ih hj"> OOB </strong>:出袋样品。这些是从子数据集中移除的样本，并且最初存在于数据集中，假设这样的样本是“A”。然后，使用“A”来测试从缺少“A”的数据集创建的树，即树将尝试预测“A”的标签，然后对所有预测取平均值。OOB分数不应与验证分数相混淆。样本是OOB的几率约为37%。</li></ul><p id="cca6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中N是数据集中的样本数</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/b6cc167c0eb6a15532f1d46f780a3d33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*OmazXFgEANcjNxTxFCn0Ug.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Source: — <a class="ae kd" href="https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710" rel="noopener" target="_blank">https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710</a></figcaption></figure><ul class=""><li id="d5c5" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated"><strong class="ih hj">随机森林测量每个变量的强度</strong>:当树被创建时，对应于树的OOB样本被用来知道树的性能。在这样做的时候，随机森林算法<strong class="ih hj">随机置换OOB样本中的特征值</strong>，并计算所有树的平均精确度。这个尺度用来判断一个变量的强弱。</li></ul><h1 id="5104" class="kj kk hi bd jt kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated"><strong class="ak"> Sklearn随机森林算法的python实现:</strong></h1><pre class="je jf jg jh fd lt lu lv lw aw lx bi"><span id="4412" class="ly kk hi lu b fi lz ma l mb mc">import matplotlib.pyplot as plt<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import r2_score, roc_auc_score,classification_report<br/>from sklearn.decomposition import PCA<br/>import pandas as pd<br/>from utils.utils_scores import scores<br/><br/>flag_pca = False<br/><br/>data, target = load_breast_cancer(return_X_y=True)<br/>data = pd.DataFrame(data)<br/>target = pd.DataFrame(target)<br/><br/>if flag_pca:<br/>    pca = PCA()<br/>    data = pca.fit_transform(data)<br/><br/>data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.1,random_state=0)<br/><br/>RandomForestClassifierObject = RandomForestClassifier()<br/>RandomForestClassifierObject.fit(data_train, target_train)<br/><br/>target_test_predict = RandomForestClassifierObject.predict(data_test)<br/>scores(target_test,target_test_predict)</span></pre><p id="e592" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这段代码也可以在github <a class="ae kd" href="https://github.com/Shubham-Saket/binary_classification_example_codes/blob/master/RandomForest_Classification.py" rel="noopener ugc nofollow" target="_blank">这里找到。</a></p><p id="222c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望这篇文章能帮助你理解随机森林算法背后的直觉。</p><p id="965e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">来源:- </strong></p><ul class=""><li id="4537" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated"><a class="ae kd" href="https://www.math.mcgill.ca/yyang/resources/doc/randomforest.pdf" rel="noopener ugc nofollow" target="_blank">https://www . math . mcgill . ca/yyang/resources/doc/random forest . pdf</a></li></ul><p id="34ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">仅此而已。</p></div></div>    
</body>
</html>
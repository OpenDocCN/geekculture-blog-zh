<html>
<head>
<title>A Simple Introduction to Decision Tree and Support Vector Machines (SVM)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树和支持向量机的简单介绍(SVM)</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/a-simple-introduction-to-decision-tree-and-support-vector-machines-svm-3035f5bd50f?source=collection_archive---------5-----------------------#2021-02-01">https://medium.com/geekculture/a-simple-introduction-to-decision-tree-and-support-vector-machines-svm-3035f5bd50f?source=collection_archive---------5-----------------------#2021-02-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/bf526aeb4ed212731f1b95eba2981eff.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/0*x0HT2uwysdla5aFd.png"/></div></figure><p id="0b32" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">决策树和支持向量机是机器学习中用来进行预测的常用工具。这两种算法都可以用于分类和回归问题。不再拖延，让我们对他们做一个简短的介绍…</p><h1 id="c159" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">决策树制作</strong></h1><p id="a4ec" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">决策树是一种受监督的机器学习，其中数据根据某个参数被连续分割。该树可以用两个实体来解释，即决策节点和树叶。树叶是决定或最终结果。决策节点是数据被拆分的地方。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es kn"><img src="../Images/a22c7b0527b81ae89ef0a95e7cf74e08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wizgrd1WnGnmXHMxSgCXLA.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx">Image by Author</figcaption></figure><p id="88a2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">决策树的一个例子可以用上面的二叉树来解释。比方说，我们想利用给定的信息来预测一个人是否需要新冠肺炎测试。这里的决策节点是像“我暴露了吗”这样的问题？我有症状吗？叶节点是像“参加考试”、“不，不要参加考试”这样的结果。在这种情况下，这是一个二元分类问题(是，不是类型问题)。决策树主要有两种类型:</p><p id="82db" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 1。分类树(是/否类型)</strong></p><p id="d37d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们上面看到的是一个分类树的例子，其中的结果是一个变量，如“参加考试”或“不，不要参加考试”。这里的决策变量是绝对的。</p><p id="af6f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 2。回归树(连续数据类型)</strong></p><p id="6a36" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">回归树是指目标变量所在的算法，该算法用于预测其值。作为回归类型问题的一个示例，您可能希望预测汽车的销售价格，这是一个连续的因变量。这既取决于里程等连续因素，也取决于制造年份、事故历史等分类因素。</p><h2 id="ca7d" class="la jl hi bd jm lb lc ld jq le lf lg ju ix lh li jy jb lj lk kc jf ll lm kg ln bi translated">决策树的优点和缺点决策树方法的优点是:</h2><p id="1055" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated"><strong class="io hj">决策树的优势</strong></p><ul class=""><li id="b3f0" class="lo lp hi io b ip iq it iu ix lq jb lr jf ls jj lt lu lv lw bi translated">决策树很容易理解。它的结果在一组规则中</li><li id="be6d" class="lo lp hi io b ip lx it ly ix lz jb ma jf mb jj lt lu lv lw bi translated">它执行分类不需要太多的计算。</li><li id="e7db" class="lo lp hi io b ip lx it ly ix lz jb ma jf mb jj lt lu lv lw bi translated">它能够处理连续变量和分类变量。</li><li id="3cc9" class="lo lp hi io b ip lx it ly ix lz jb ma jf mb jj lt lu lv lw bi translated">它清楚地表明哪些字段对预测或分类最重要。</li></ul><p id="cccc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">决策树方法的弱点:</strong></p><ul class=""><li id="b1d4" class="lo lp hi io b ip iq it iu ix lq jb lr jf ls jj lt lu lv lw bi translated">决策树不太适合目标是预测连续属性的值的评估任务。</li><li id="9241" class="lo lp hi io b ip lx it ly ix lz jb ma jf mb jj lt lu lv lw bi translated">在许多类别和相对少量的训练样本的分类问题中，它们容易出错。</li><li id="818c" class="lo lp hi io b ip lx it ly ix lz jb ma jf mb jj lt lu lv lw bi translated">决策树中有很高的过拟合概率。</li></ul><h1 id="4541" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">支持向量机(SVM):</h1><p id="9a33" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">支持向量机(SVM)是一种受监督的机器学习算法，可用于分类或回归问题。它使用一种称为内核技巧的技术来转换您的数据，然后基于这些转换，它会在可能的输出之间找到一个最佳边界。SVM的目标是确定一个最优的分离超平面，它最大化不同类别的训练数据之间的间隔。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/112a61090e59ee8e621d2c5ec88e01c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*PtTM4-xEVHl8rrBvzL6BoA.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx">Image Data Camp</figcaption></figure><p id="e827" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">支持向量:</strong></p><p id="449e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">支持向量是最接近超平面的数据点。通过计算边距，这些点将更好地定义分隔线。</p><p id="ce0e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">超平面:</strong></p><p id="f4d1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">超平面是在具有不同类别的一组对象之间分开的决策平面。</p><p id="41a1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">保证金:</strong></p><p id="bb11" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">边距是最近的类点上的两条线之间的间隙。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es md"><img src="../Images/fcd90f65c3155c920f965fcab0bd9760.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*ygv74JrxdkWdn9TVtcnVaA.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx">Image Data Camp</figcaption></figure><p id="b6ac" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了区分这两类，有许多可能的超平面可供选择。我们的目标是找到给出最大余量的最佳超平面。</p><p id="eab2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">非线性数据:</strong></p><p id="9d32" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在上面的例子中，画一条线把两个类分开是很简单的，但是对于所有类型的问题，这种方法是不一样的。在这种情况下，SVM使用内核技巧将不可分数据转换为可分数据。为此，我们将增加第三维度。我们创建了一个新的z维，它可以通过某种方式计算:z = x + y</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es me"><img src="../Images/8ecce7a2bb83fdad40db69d7525cd293.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*2AZ7LRNvciiQUmBMhH4BvQ.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx">Image Data Camp</figcaption></figure><p id="c439" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> SVM内核:</strong></p><p id="6816" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">内核将输入数据空间转换成所需的形式。SVM使用了一种叫做核心技巧的技术，通过增加更多的维度将不可分离的问题转化为可分离的问题。它在非线性分离问题中最有用。内核技巧帮助你建立一个更准确的分类器。</p><p id="db8c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">支持向量机有助于文本和超文本分类、人脸检测、图像分类、使用SVM可以识别手写字符、癌症诊断和预后。</p><h1 id="d949" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">SVM的优势和劣势:</h1><p id="c96d" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated"><strong class="io hj">SVM的实力:</strong></p><p id="ec86" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">1.它在维数大于样本数的情况下是有效的。</p><p id="6830" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">2.它工作得很好，有一个清晰的分离边界</p><p id="d0c3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">3.它在决策函数中使用训练点的子集(称为支持向量)，因此它也是内存高效的。</p><p id="082d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">4.在高维空间是有效的。</p><p id="60bc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">SVM的弱点:</strong></p><p id="919b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">1.当我们有大数据集时，它不能很好地执行，因为所需的训练时间更长</p><p id="a202" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">2.SVM没有直接提供概率估计</p><p id="6519" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">3.当数据集有更多噪声时，它无法执行。</p><p id="c961" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">结论:</strong></p><p id="6943" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">没有完美的模型，每个模型都是工具。这意味着它需要根据每种情况进行适当的调整和优化。</p></div><div class="ab cl mf mg gp mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="hb hc hd he hf"><p id="e4db" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="mm">原载于2021年2月1日</em><a class="ae mn" href="https://www.numpyninja.com/post/a-simple-introduction-to-decision-tree-and-support-vector-machines-svm" rel="noopener ugc nofollow" target="_blank"><em class="mm">【https://www.numpyninja.com】</em></a><em class="mm">。</em></p></div></div>    
</body>
</html>
<html>
<head>
<title>AI Compilers Demystified</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能编译器去神秘化</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/ai-compilers-ae28afbc4907?source=collection_archive---------1-----------------------#2022-11-17">https://medium.com/geekculture/ai-compilers-ae28afbc4907?source=collection_archive---------1-----------------------#2022-11-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="b6e6" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">人工智能工程</h2><div class=""/><div class=""><h2 id="832e" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">通过编译加速AI/ML:NVIDIA TensorRT，ONNX Runtime，Apache TVM等</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/869461b7131ed443545b04521f8d01ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yXx5joYvjPEbi-xoyS0mOw.jpeg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Photo by <a class="ae jw" href="https://unsplash.com/@deepmind?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">DeepMind</a> on <a class="ae jw" href="https://unsplash.com/@deepmind?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="65ff" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">深度学习在过去十年一直是AI/ML的主流。它几乎在任何地方改变着我们的日常生活。我们不太会数，从Alexa设备到广告推荐、仓库机器人、自动驾驶汽车等等。</p><p id="57cc" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">但是随着深度学习和大规模模型的发展，训练和服务变得越来越具有挑战性。人工智能编译器的出现提高了训练和服务的效率。这里将揭示如何通过机器学习编译来加速分布式训练和服务，这是人工智能工程的一种基本方法。</p><h2 id="a56a" class="kt ku hi bd kv kw kx ky kz la lb lc ld kg le lf lg kk lh li lj ko lk ll lm ho bi translated">面向人工智能工程培训和服务的编译器</h2><p id="3f23" class="pw-post-body-paragraph jx jy hi jz b ka ln is kc kd lo iv kf kg lp ki kj kk lq km kn ko lr kq kr ks hb bi translated">大比例模型有<strong class="jz hs">个</strong>主拐点。第一个<strong class="jz hs">是基于变形金刚的大型模型似乎按下了“加速”按钮，以指数方式增加模型的大小。如武道2.0以<a class="ae jw" href="https://towardsdatascience.com/distributed-parallel-training-data-parallelism-and-model-parallelism-ec2d234e3214" rel="noopener" target="_blank"> 1.75万亿</a>参数登场，十倍于GPT-3参数。</strong></p><p id="3125" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">第二个挑战是对训练和发球效率的挑战。例如，在亚马逊SageMaker训练平台的240ml . p4d . 24x大型实例上训练GPT-3花费了大约<a class="ae jw" href="https://towardsdatascience.com/distributed-parallel-training-model-parallel-training-a768058aa02a" rel="noopener" target="_blank"> 25天</a>。</p><p id="55ac" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">MLOps可以自动化ML模型部署和服务的过程。最近，它还扩展到特征工程和特征商店。但它仅限于人工智能工作流的过程，由于运营的性质，无法从根本上改善培训和服务。</p><p id="b87d" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">人工智能工程，远远超出了MLOps，可以全面地设计ML工作流程以及培训和服务的架构。此外，它可以通过机器学习编译来加速服务和训练。</p><p id="ab46" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">ML编译对大规模模型服务和训练效率起着至关重要的作用。与传统的ML流程(从培训到验证和服务的三个主要步骤)不同，它在大规模ML工作流中变得必不可少。</p><p id="0f43" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">我们可以通过在将大规模模型部署到生产中之前编译它来加速服务。我们还可以通过编辑培训档案来加快培训速度。但是前者的效率似乎比后者更重要，因为在生产中服务是面向客户的，并且更关键。</p><p id="af97" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">此外，服务的总计算时间可能比训练的时间长得多，因为模型可能在一段时间内每秒处理数百万个请求。</p><p id="ad8f" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">除了ML编译，<a class="ae jw" href="https://pub.towardsai.net/5-types-of-ml-accelerators-767d26a643de" rel="noopener ugc nofollow" target="_blank">其他机制</a>可以加速训练和服务，比如AI计算平台、AI框架、云服务。但是我们将把重点放在编译器上，这里不涉及这些。</p><h2 id="6b03" class="kt ku hi bd kv kw kx ky kz la lb lc ld kg le lf lg kk lh li lj ko lk ll lm ho bi translated">人工智能编译器概述</h2><p id="e08d" class="pw-post-body-paragraph jx jy hi jz b ka ln is kc kd lo iv kf kg lp ki kj kk lq km kn ko lr kq kr ks hb bi translated">AI编译器出现了，并在加速训练和服务方面发挥了至关重要的作用，而我们几乎用尽了其他方法，如优化架构和硬件。</p><p id="133c" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">好消息是ML编译器可以显著提高大规模模型服务的效率。然后出现了很多ML编译器:Apache TVM，NVIDIA TensorRT，ONNX Runtime，LLVM，Google MLIR，TensorFlow XLA，Meta Glow，PyTorch nvFuser，以及Intel PlaidML和OpenVINO。</p><p id="d35a" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">让我们仔细看看，以便全面把握。</p><p id="ae66" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">Apache TVM</strong>:Apache<a class="ae jw" href="https://tvm.apache.org" rel="noopener ugc nofollow" target="_blank">TVM</a>是一个开源的ML编译器框架，用于CPU、GPU和其他ML硬件加速器。它旨在使ML工程师能够在任何硬件后端优化和有效运行计算。TVM提供了两个主要特性:1)。将深度学习模型编译成最小可部署模块；2).在更多后端上自动生成和优化模型的基础设施，性能更好。</p><p id="7cc2" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">NVIDIA</strong><a class="ae jw" href="https://developer.nvidia.com/tensorrt" rel="noopener ugc nofollow" target="_blank"><strong class="jz hs">TensorRT</strong></a>:这是一款针对NVIDIA GPUs的高性能深度学习推理优化器和运行时库。它可用于优化和部署以TensorFlow、PyTorch或ONNX格式开发的模型。TensorRT可以通过优化计算图、使用降低精度的算法和应用其他技术来显著提高LLMs的推理速度。</p><p id="148e" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><a class="ae jw" href="https://onnxruntime.ai" rel="noopener ugc nofollow" target="_blank"> <strong class="jz hs"> ONNX运行时</strong> </a> : ONNX(开放神经网络交换)是一种开源的表示深度学习模型的格式。它由微软、脸书和其他合作者创建，旨在提供一种标准格式，允许不同深度学习框架之间的互操作性。ONNX运行时是一个专注于性能的引擎，用于运行ONNX模型。它支持广泛的硬件平台，包括CPU、GPU和边缘设备。ONNX运行时旨在优化机器学习模型的执行，与直接在原生框架中运行模型相比，提供了更好的性能。</p><p id="9100" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">LLVM:  <a class="ae jw" href="https://llvm.org" rel="noopener ugc nofollow" target="_blank"> LLVM </a>最初是作为<a class="ae jw" href="https://llvm.org/pubs/2004-01-30-CGO-LLVM.html" rel="noopener ugc nofollow" target="_blank">的一个研究项目</a>在<a class="ae jw" href="https://cs.illinois.edu/" rel="noopener ugc nofollow" target="_blank"> UIUC </a>提供一个现代的、基于SSA的编译策略，支持任意编程语言的静态和动态编译。从那时起，LLVM已经发展成为一个包含几个子项目的伞式项目。</p><p id="327b" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">谷歌MLIR </strong> : <a class="ae jw" href="https://www.tensorflow.org/mlir/overview" rel="noopener ugc nofollow" target="_blank"> MLIR </a>(多级中间表示)是一种表示格式和编译器实用程序库，位于模型表示和生成硬件特定代码的低级编译器/执行器之间。它是现代优化编译器的灵活基础设施。这意味着它由一个中间表示(IR)规范和一个对该表示执行转换的编码工具包组成。用编译器的话来说，就是从高级表示转换到低级表示。</p><p id="5028" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">TensorFlow XLA</strong>:<a class="ae jw" href="https://www.tensorflow.org/xla" rel="noopener ugc nofollow" target="_blank">XLA</a>(加速线性代数)是一个针对线性代数的特定领域编译器，可以加速tensor flow模型，而可能不会更改源代码。XLA采用HLO(高级操作)定义的图形(“计算”)，并将它们编译成各种架构的机器指令。XLA是模块化的，因为它可以轻松地插入一个替代后端，以针对一些新颖的硬件架构。</p><p id="a2e9" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">Meta Glow</strong>:<a class="ae jw" href="https://ai.facebook.com/tools/glow/" rel="noopener ugc nofollow" target="_blank">Glow</a>从PyTorch等深度学习框架接受计算图，并为机器学习加速器生成高度优化的代码。它包含许多机器学习和硬件优化，如内核融合，以加速模型开发。</p><p id="62d7" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">py torch NV fuser</strong>:<a class="ae jw" href="https://pytorch.org/tutorials/intermediate/nvfuser_intro_tutorial.html" rel="noopener ugc nofollow" target="_blank">NV fuser</a>是一款DL编译器，能够实时编译快速灵活的GPU专用代码，以可靠地自动加速用户网络，通过在运行时生成快速定制“融合”内核，为运行在Volta和后来的CUDA加速器上的DL网络提供加速。它是专门为满足PyTorch社区的独特需求而设计的，支持不同的网络架构和程序，具有不同形状和跨度的动态输入。</p><p id="8d78" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">Intel PlaidML</strong>:<a class="ae jw" href="https://www.intel.com/content/www/us/en/artificial-intelligence/plaidml.html" rel="noopener ugc nofollow" target="_blank">PlaidML</a>是一款开源张量编译器。借助英特尔的nGraph图形编译器，它可以在各种CPU、GPU和其他加速器处理器架构之间实现流行的DL框架的性能可移植性。</p><p id="db98" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">Open vino</strong>:<a class="ae jw" href="https://docs.openvino.ai" rel="noopener ugc nofollow" target="_blank">Open vino</a>(开放视觉推理和神经网络优化)是一个开源的工具包，也是由Intel开发的。OpenVINO主要是在英特尔硬件上实现快速、高性能的深度学习推理，比如CPU、集成GPU、FPGAs、VPUs(视觉处理单元)。它提供了一套工具和库，旨在优化和加速计算机视觉和其他人工智能应用的深度学习模型。OpenVINO支持各种深度学习框架，包括TensorFlow、Caffe、ONNX、Kaldi等。总之，OpenVINO是为优化和加速英特尔硬件上的深度学习推理而定制的，而PlaidML是一种更通用的、与硬件无关的深度学习编译器，允许更广泛的设备兼容性。</p><p id="bb40" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">以上可能错过了其他有趣的AI编译器，但我们可以看到它们的受欢迎程度和重要性。</p><h2 id="2cd7" class="kt ku hi bd kv kw kx ky kz la lb lc ld kg le lf lg kk lh li lj ko lk ll lm ho bi translated">人工智能编译器如何工作</h2><p id="56be" class="pw-post-body-paragraph jx jy hi jz b ka ln is kc kd lo iv kf kg lp ki kj kk lq km kn ko lr kq kr ks hb bi translated">一个AI编译器内部是如何工作的？让我们深入了解它的架构。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ls"><img src="../Images/f1d440a577faac5367fdddf125e966d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TmYUdDB-iCp7bc7fqBQ-Aw.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">High-level design architecture of DL compilers (Source: <a class="ae jw" href="https://arxiv.org/abs/2002.03794" rel="noopener ugc nofollow" target="_blank">The DL Compiler</a>)</figcaption></figure><p id="2070" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">以上是大部分AI编译器的高层设计架构。它有两层:与框架相关的上层和与硬件相关的下层，分别对应于图中的编译器前端和后端。</p><p id="aebf" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><a class="ae jw" href="https://en.wikipedia.org/wiki/Intermediate_representation" rel="noopener ugc nofollow" target="_blank">中间表示</a> (IR)是程序的数据结构或抽象，是程序优化的关键。IR被设计成有助于进一步的处理，例如优化和翻译。IRs分布在上层和下层。</p><p id="d0e4" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">人工智能编译器将ML模型翻译成上层和下层的多级IRs。上层侧重于独立于硬件但与框架相关的转换和优化。下层负责硬件相关的优化、代码生成和编译。</p><p id="3b3d" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">但是为了支持跨设备(例如，不同的GPU或混合的GPU和CPU)，较低层可能不优化硬件专用但仍然与硬件相关的低级IRs。同样，为了支持跨框架(例如PyTorch和TensorFlow)，上层可以不限于特定的框架，而是仍然关注于与框架相关的高级IRs。</p><p id="5aea" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">一般而言，上层采用由ML框架(例如PyTorch)生成的ML模型作为输入，并将该模型转换成计算图形表示(即图形IR)。下层将来自上层的高级IR转换为低级IR，并执行与硬件相关的优化。</p><h2 id="8cfd" class="kt ku hi bd kv kw kx ky kz la lb lc ld kg le lf lg kk lh li lj ko lk ll lm ho bi translated">下一步是什么</h2><p id="ab18" class="pw-post-body-paragraph jx jy hi jz b ka ln is kc kd lo iv kf kg lp ki kj kk lq km kn ko lr kq kr ks hb bi translated">像其他ML技术一样，AI编译器不断进化以优化和扩展。如上所述，AI编译器通常可以在上层和下层进行优化:模型架构相关的优化和硬件特定的优化。</p><p id="b2c8" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">模型架构相关的优化</strong>:这种优化在构建计算图时在编译器的上层处理。它可以是特定于模型架构的(例如，变换器域)。它也可以在图节点和张量、数据流和块级别。</p><p id="dc4f" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">特定于硬件的优化</strong>:这发生在编译器的底层。优化主要基于硬件架构和属性，如内在硬件映射、内存分配和获取、内存延迟隐藏、面向循环的优化和并行化。此外，自动调谐机制在特定于硬件的优化中利用参数调谐。</p><p id="44c2" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs"> AI编译器扩展</strong> : AI编译器在设计、转换和优化方面类似于编程语言编译器。一些类似的想法使AI编译器成为一个广阔的领域。然后他们的优化和设计可以更加复杂、灵活、可扩展和高效。例如，Google在MLIR引入了可组合和模块化的代码生成，采用结构化和可重定向的方法来构建张量编译器。</p><h2 id="0928" class="kt ku hi bd kv kw kx ky kz la lb lc ld kg le lf lg kk lh li lj ko lk ll lm ho bi translated">最后的评论</h2><p id="f199" class="pw-post-body-paragraph jx jy hi jz b ka ln is kc kd lo iv kf kg lp ki kj kk lq km kn ko lr kq kr ks hb bi translated">这是我最近的第三篇关于深度学习分布式训练的文章。如果还没有，你可能想看看另外两个。<a class="ae jw" href="https://towardsdatascience.com/distributed-parallel-training-model-parallel-training-a768058aa02a" rel="noopener" target="_blank">第一个</a>开始介绍分布式模型并行培训。<a class="ae jw" href="https://towardsdatascience.com/distributed-parallel-training-data-parallelism-and-model-parallelism-ec2d234e3214" rel="noopener" target="_blank">第二个</a>全面覆盖关于数据并行和模型并行的分布式培训。</p><p id="d0ff" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">随着训练和服务规模的扩大，深度学习变得越来越具有挑战性。我们已经通过人工智能框架、计算平台和云服务进行了长期的努力来优化效率。AI/ML编译对于加速训练和服务是非常重要的。所有这些都可以由人工智能工程用工程原理统筹安排，实现综合效率。</p><p id="f53e" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><em class="lt">想了解更多关于人工智能工程的知识或获得对人工智能编译器的见解？关注我或者在LinkedIn上接触</em> <a class="ae jw" href="https://www.linkedin.com/in/luhui" rel="noopener ugc nofollow" target="_blank"> <strong class="jz hs"> <em class="lt">鲁晖胡</em></strong></a><strong class="jz hs"><em class="lt"/></strong><em class="lt">！</em></p></div><div class="ab cl lu lv gp lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="hb hc hd he hf"><h2 id="721f" class="kt ku hi bd kv kw kx ky kz la lb lc ld kg le lf lg kk lh li lj ko lk ll lm ho bi translated">参考</h2><ol class=""><li id="e8f8" class="mb mc hi jz b ka ln kd lo kg md kk me ko mf ks mg mh mi mj bi translated">TVM:用于深度学习的自动化端到端优化编译器:<a class="ae jw" href="https://arxiv.org/abs/1802.04799" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1802.04799</a></li><li id="74d7" class="mb mc hi jz b ka mk kd ml kg mm kk mn ko mo ks mg mh mi mj bi translated">https://arxiv.org/abs/2202.03293<a class="ae jw" href="https://arxiv.org/abs/2202.03293" rel="noopener ugc nofollow" target="_blank">MLIR的可组合和模块化代码生成:构造张量编译器的结构化和可重定向方法</a></li><li id="9e43" class="mb mc hi jz b ka mk kd ml kg mm kk mn ko mo ks mg mh mi mj bi translated">Glow:神经网络的图形降低编译技术:<a class="ae jw" href="https://arxiv.org/abs/1805.00907" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1805.00907</a></li><li id="1887" class="mb mc hi jz b ka mk kd ml kg mm kk mn ko mo ks mg mh mi mj bi translated">深度学习编译器:综合调查:<a class="ae jw" href="https://arxiv.org/abs/2002.03794" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2002.03794</a></li><li id="183f" class="mb mc hi jz b ka mk kd ml kg mm kk mn ko mo ks mg mh mi mj bi translated">5种类型的ML加速器:<a class="ae jw" href="https://pub.towardsai.net/5-types-of-ml-accelerators-767d26a643de" rel="noopener ugc nofollow" target="_blank">https://pub . toward sai . net/5-type-of-ML-Accelerators-767 d26a 643 de</a></li></ol></div></div>    
</body>
</html>
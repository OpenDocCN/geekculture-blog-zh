<html>
<head>
<title>Curse of dimensionality</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">维度的诅咒</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/curse-of-dimensionality-e97ba916cb8f?source=collection_archive---------22-----------------------#2021-09-06">https://medium.com/geekculture/curse-of-dimensionality-e97ba916cb8f?source=collection_archive---------22-----------------------#2021-09-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/de84c7fe95faa15023422c3b400c8787.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q7h4CnnoLPr_U6iLTbL-EA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Photo by <a class="ae iu" href="https://unsplash.com/@sigmund?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Sigmund</a> on <a class="ae iu" href="https://unsplash.com/s/photos/glasses-3d?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="786d" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated"><strong class="ak">简介</strong></h1><p id="141e" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">如果你已经在数据科学和机器学习方面积累了一些经验，你可能听说过有人担心可怕的“<strong class="jv hj">维数灾难</strong>”。如果没有，不要慌，我会尽量澄清的！</p><p id="44c4" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">大多数时候，维数灾难的定义与数据的<strong class="jv hj">稀疏度</strong>严格相关。事实上，随着维度的增加，空间的体积也急剧增加，数据变得越来越稀疏。</p><p id="5615" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">数据集稀疏性的一个结果是，我们的机器学习算法将需要更多的数据，以便<strong class="jv hj">推广</strong>！然而，收集标记数据并不总是那么容易。因此，从一开始就解决这个问题比寻找新数据更有效。</p><h1 id="3012" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">从另一个角度来看</h1><p id="f02b" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在我的机器学习数学课程中，教授给了我们一个这种现象的实际表示，我只在少数在线资源中看到过。</p><p id="783a" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">让我们考虑一个2D特征空间，由一个<strong class="jv hj">单位正方形</strong>表示，并且还考虑一个<strong class="jv hj">内接在这个正方形</strong>上的圆。如您所见，大多数数据点位于圆圈内，而稀疏数据位于角落。</p><figure class="ky kz la lb fd ij er es paragraph-image"><div class="er es kx"><img src="../Images/a5869575cbb82190231b46043fcd8584.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*NDMF-KQjcgRoi7qD46_X2Q.jpeg"/></div></figure><p id="4e7c" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">如果我们将特征空间的维度增加1，我们获得一个<strong class="jv hj">单位立方体</strong>和一个<strong class="jv hj">内接球。</strong>直观上，比以前更多的点是稀疏的(即在角上)。</p><figure class="ky kz la lb fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/4a636d34d5dc04703b00980ab553509a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*gJMIBBqpC5YLDTGuMi-fuQ.jpeg"/></div></figure><blockquote class="ld le lf"><p id="0c0e" class="jt ju kw jv b jw kr jy jz ka ks kc kd lg kt kg kh lh ku kk kl li kv ko kp kq hb bi translated">请闭上一只眼睛，想象这个立方体的所有边都一样长！</p></blockquote><p id="4e57" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在，如果我们继续一段时间，我们将在一个非常高维的空间中结束。会发生什么？嗯，超立方体的体积将永远是1，而超球体(也称为n球，具有n维)的体积将呈指数下降。所以，总是越来越多的点会躺在角落里！</p><p id="416a" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">你还困惑吗？让我们用数学方法来试试。在<strong class="jv hj">二维</strong>中，给定一个单位正方形，其面积等于1 whreas，内切圆的面积等于0.785</p><figure class="ky kz la lb fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/0ef856861c15933a04025cbcb82b1c47.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/1*mUx5R0-Xo76DwqTwYib5tQ.gif"/></div></figure><p id="c1de" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">有了<strong class="jv hj">三维</strong>，我们有了</p><figure class="ky kz la lb fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/7be8b2935a1346d2493b1aa9007bc464.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/1*ivLZTF44GWmOY73u_1d3UA.gif"/></div></figure><figure class="ky kz la lb fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/c628fe6a83d4c67f2c78a7b48bb1e71e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*QqWolf7Hxk3Zh9z9pYFAMA.jpeg"/></div></figure><p id="f504" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">最后，有了n个维度，我们就有了</p><figure class="ky kz la lb fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/7f1d90cbd54c46bd0364cfe742c60c0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/1*M4QuTe4FA8cghjd-NcVFTQ.gif"/></div></figure><blockquote class="ld le lf"><p id="c154" class="jt ju kw jv b jw kr jy jz ka ks kc kd lg kt kg kh lh ku kk kl li kv ko kp kq hb bi translated">参见<a class="ae iu" href="https://en.wikipedia.org/wiki/Volume_of_an_n-ball" rel="noopener ugc nofollow" target="_blank">维基百科</a>了解“n球”的体积</p></blockquote><p id="73e2" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">希望现在，你应该能够“看到”并理解维数和数据稀疏度之间的关系！</p><h1 id="84fa" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">距离度量呢？</h1><p id="8803" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">嗯，一些距离度量可能会受到这种现象的强烈影响。特别地，随着维数的增加，欧几里德距离将慢慢失去其相关性。</p><p id="7bea" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">为什么？首先，回忆一下维为<em class="kw"> d </em>的欧几里德距离的定义:</p><figure class="ky kz la lb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ln"><img src="../Images/fa1facfdbecbed3546c46e9e141712ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/1*vwxdS9QFlzGXfe3vGGO7-g.gif"/></div></div></figure><p id="4772" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">你可以想象，如果你开始增加新的维度，这个总数也会增加。因此，对于足够大的维数，最近点和最远点之间的比率接近1。因此，谈论“K”最近点(例如在KNN、LOF等地)没有任何意义。</p><h1 id="d222" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated"><strong class="ak">结论</strong></h1><p id="9943" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">综上所述，我们已经试图对<strong class="jv hj">维数灾难</strong>给出一个更正式的解释，这是机器学习中一个非常有争议且被低估的话题。</p><p id="30e7" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">每当我们有非常多的功能时，我们都有几个选项可以考虑，例如:</p><ul class=""><li id="8504" class="lo lp hi jv b jw kr ka ks ke lq ki lr km ls kq lt lu lv lw bi translated">移除高度<a class="ae iu" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" rel="noopener ugc nofollow" target="_blank">相关</a>或不相关的特征</li><li id="ac6b" class="lo lp hi jv b jw lx ka ly ke lz ki ma km mb kq lt lu lv lw bi translated">相似特征分组</li><li id="67cb" class="lo lp hi jv b jw lx ka ly ke lz ki ma km mb kq lt lu lv lw bi translated">使用<a class="ae iu" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">主成分分析</a>和类似技术</li></ul><h1 id="f239" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">参考</h1><ul class=""><li id="aa89" class="lo lp hi jv b jw jx ka kb ke mc ki md km me kq lt lu lv lw bi translated">维度的诅咒:<a class="ae iu" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank">维基百科</a></li><li id="9fac" class="lo lp hi jv b jw lx ka ly ke lz ki ma km mb kq lt lu lv lw bi translated">维度的历程:<a class="ae iu" href="https://mathematical-coffees.github.io/slides/mc08-delon.pdf" rel="noopener ugc nofollow" target="_blank">朱莉·德隆</a>，</li><li id="89a6" class="lo lp hi jv b jw lx ka ly ke lz ki ma km mb kq lt lu lv lw bi translated">维度的诅咒:<a class="ae iu" href="https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e" rel="noopener" target="_blank">托尼·姚</a></li></ul></div></div>    
</body>
</html>
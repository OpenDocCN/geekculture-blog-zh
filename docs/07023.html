<html>
<head>
<title>Review — Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述——裂脑自动编码器:跨通道预测的无监督学习</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/review-split-brain-autoencoders-unsupervised-learning-by-cross-channel-prediction-cebeba644a3c?source=collection_archive---------10-----------------------#2021-09-05">https://medium.com/geekculture/review-split-brain-autoencoders-unsupervised-learning-by-cross-channel-prediction-cebeba644a3c?source=collection_archive---------10-----------------------#2021-09-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="f5c7" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">用于自我监督学习的裂脑自动机，胜过拼图游戏、<a class="ae ix" href="https://sh-tsang.medium.com/review-unsupervised-visual-representation-learning-by-context-prediction-self-supervised-51a1d7ce6aff" rel="noopener">上下文预测</a>、<a class="ae ix" rel="noopener" href="/nerd-for-tech/review-bigan-adversarial-feature-learning-gan-535eb76be2ca">阿里/甘比</a>、<a class="ae ix" href="https://sh-tsang.medium.com/review-look-listen-and-learn-self-supervised-learning-ff89a7dee980" rel="noopener"> L -Net </a>、<a class="ae ix" href="https://sh-tsang.medium.com/review-context-encoders-feature-learning-by-inpainting-bd181e48997" rel="noopener">上下文编码器</a>等。</h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es iy"><img src="../Images/72e16ef36c0a0eb43af2deb3438df46d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*IAGtz6stIz-E2wgRyqS3_A.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx"><strong class="bd jk">Proposed Split-Brain Auto (Bottom) vs Traditional Autoencoder, e.g. Stacked Denoising Autoencoder (Top)</strong></figcaption></figure><p id="37c9" class="pw-post-body-paragraph jl jm hi jn b jo jp ij jq jr js im jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi kh translated"><span class="l ki kj kk bm kl km kn ko kp di">本文对加州大学柏克莱人工智能研究(BAIR)实验室的<strong class="jn hj">裂脑自动编码器:跨通道预测无监督学习</strong>(裂脑自动)进行了综述。在本文中:</span></p><ul class=""><li id="5743" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">一个网络被分成<strong class="jn hj">两个子网络</strong>，每个子网络被训练来执行一个困难的任务——从另一个预测数据信道的一个子集<strong class="jn hj">。</strong></li><li id="d093" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">通过强制网络解决<strong class="jn hj">跨通道预测</strong>任务，在不使用任何标签的情况下实现<strong class="jn hj">特征学习。</strong></li></ul><p id="9a47" class="pw-post-body-paragraph jl jm hi jn b jo jp ij jq jr js im jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi translated">这是一篇发表在<strong class="jn hj"> 2017 CVPR </strong>的论文，引用超过<strong class="jn hj"> 400次</strong>。(<a class="le lf ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----cebeba644a3c--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="f880" class="ln lo hi bd jk lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">概述</h1><ol class=""><li id="0ba5" class="kq kr hi jn b jo me jr mf ju mg jy mh kc mi kg mj kw kx ky bi translated"><strong class="jn hj">裂脑自动编码器(Split-Brain Auto) </strong></li><li id="a02b" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg mj kw kx ky bi translated"><strong class="jn hj">实验结果</strong></li></ol></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="4020" class="ln lo hi bd jk lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated">1.裂脑自动编码器(裂脑自动)</h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="er es mk"><img src="../Images/709a3af0b2b7a1ce233ef14ed816f9f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1LyZc61rB-mXcfIllqu68A.png"/></div></div><figcaption class="jg jh et er es ji jj bd b be z dx"><strong class="bd jk">Split-Brain Autoencoders applied to various domains</strong></figcaption></figure><h2 id="fc10" class="mp lo hi bd jk mq mr ms ls mt mu mv lw ju mw mx ly jy my mz ma kc na nb mc nc bi translated">1.1.跨通道编码器</h2><ul class=""><li id="e2dd" class="kq kr hi jn b jo me jr mf ju mg jy mh kc mi kg kv kw kx ky bi translated">首先，<strong class="jn hj">输入数据<em class="nd"> X </em>分为<em class="nd"> X </em> 1和<em class="nd"> X </em> 2。</strong></li><li id="e5d8" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">然后，<strong class="jn hj">X1通过网络<em class="nd"> F </em> 1预测<em class="nd"> X </em> 2 </strong>:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ne"><img src="../Images/13c16c23dc304c4a4cd92a5e453a7304.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/format:webp/1*sW-9w1EWt4lhZd-COe4hTQ.png"/></div></figure><blockquote class="nf ng nh"><p id="ebee" class="jl jm nd jn b jo jp ij jq jr js im jt ni jv jw jx nj jz ka kb nk kd ke kf kg hb bi translated">通过执行这个从X1预测X2的<strong class="jn hj">借口任务</strong>，我们希望<strong class="jn hj">获得包含高级抽象或语义的表示F(X1)</strong>。</p></blockquote><ul class=""><li id="e9fe" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">与<em class="nd"> F </em> 2类似的是<strong class="jn hj">T53】X2通过网络<em class="nd"> F </em> 2预测<em class="nd"> X </em> 1 </strong>。</li><li id="041a" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated"><strong class="jn hj">左</strong>:对于Lab色彩空间，<em class="nd"> X </em> 1可以是<em class="nd"> L </em>，是亮度信息，<em class="nd"> X </em> 2可以是<em class="nd"> ab </em>，是色彩信息。</li><li id="a553" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated"><strong class="jn hj">右</strong>:对于RGB-D图像，<em class="nd"> X </em> 1可以是<em class="nd"> RGB </em>值，<em class="nd"> X </em> 2可以是<em class="nd"> D </em>，是深度信息。</li><li id="350c" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated"><em class="nd"> l </em> 2损失可用于训练回归损失:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nl"><img src="../Images/aacba7514be0a6853905cc76ec12568f.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*W75sCyTFrvGOWdF_j9G5lg.png"/></div></figure><ul class=""><li id="6a0e" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">发现对于自动着色的图形任务，<strong class="jn hj">交叉熵损失比<em class="nd">1</em>2损失</strong>更有效；</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nm"><img src="../Images/61a36c5900d67a48f5a095788f5b10b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*Wq9p_R3ADSLutfDi-7OgNw.png"/></div></figure><h2 id="5a44" class="mp lo hi bd jk mq mr ms ls mt mu mv lw ju mw mx ly jy my mz ma kc na nb mc nc bi translated">1.2.作为聚合跨通道编码器的裂脑自动编码器</h2><ul class=""><li id="49dd" class="kq kr hi jn b jo me jr mf ju mg jy mh kc mi kg kv kw kx ky bi translated">多个交叉通道编码器，<em class="nd"> F </em> 1，<em class="nd"> F </em> 2，针对相反的预测问题，损失函数分别为<em class="nd"> L </em> 1，<em class="nd"> L </em> 2:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nn"><img src="../Images/23e853899557e340c6b3188d9f3a0263.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*ROkvkL8q6eqYWX8Cf6qnZA.png"/></div></figure><ul class=""><li id="622e" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">上图(a)和(b)分别显示了图像和RGB-D域中的裂脑自动编码器示例。</li></ul><blockquote class="nf ng nh"><p id="464b" class="jl jm nd jn b jo jp ij jq jr js im jt ni jv jw jx nj jz ka kb nk kd ke kf kg hb bi translated"><strong class="jn hj">通过逐层连接表示，<em class="hi"> Fl </em> = { <em class="hi"> Fl </em> 1，<em class="hi"> Fl </em> 2}，获得在全输入张量<em class="hi"> X </em>上预训练的表示<em class="hi"> F </em>。</strong></p><p id="54e3" class="jl jm nd jn b jo jp ij jq jr js im jt ni jv jw jx nj jz ka kb nk kd ke kf kg hb bi translated">如果F是期望的固定大小的CNN，例如<a class="ae ix" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160?source=post_page---------------------------"> AlexNet </a>，我们可以通过沿着信道维度将网络<em class="hi"> F </em>的每一层分成两半来设计子网<em class="hi"> F </em> 1、<em class="hi"> F </em> 2。</p></blockquote><ul class=""><li id="53df" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">网络被修改为<strong class="jn hj">完全卷积</strong>并被训练用于<strong class="jn hj">像素预测任务</strong>。</li></ul><h2 id="3aa0" class="mp lo hi bd jk mq mr ms ls mt mu mv lw ju mw mx ly jy my mz ma kc na nb mc nc bi translated">1.3.替代聚集技术</h2><ul class=""><li id="d089" class="kq kr hi jn b jo me jr mf ju mg jy mh kc mi kg kv kw kx ky bi translated">一个替代方案，作为基线:相同的表示<em class="nd"> F </em>可以被训练来同时执行两个映射:</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es no"><img src="../Images/3c528bed1025196cd922c1249adae53b.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*CAQ71aoi4VOOfOnINIxTiA.png"/></div></figure><ul class=""><li id="77ee" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">或者甚至考虑全输入张量<em class="nd"> X </em>。</li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es np"><img src="../Images/6bf7a054f239673c89d2e9ea9ad12d57.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*9k_DZdIQ7An0vtPTUlZuvw.png"/></div></figure><blockquote class="nf ng nh"><p id="2eae" class="jl jm nd jn b jo jp ij jq jr js im jt ni jv jw jx nj jz ka kb nk kd ke kf kg hb bi translated">然而，我们发现，提出的<strong class="jn hj">裂脑汽车(1.2节)优于上述两个备选方案(1.3节)。</strong></p></blockquote></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h1 id="e5a9" class="ln lo hi bd jk lp lq lr ls lt lu lv lw io lx ip ly ir lz is ma iu mb iv mc md bi translated"><strong class="ak"> 2。实验结果</strong></h1><h2 id="b53b" class="mp lo hi bd jk mq mr ms ls mt mu mv lw ju mw mx ly jy my mz ma kc na nb mc nc bi translated">2.1.ImageNet</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nq"><img src="../Images/b36d51f8b60114536b3a01e2b2f19cac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*djaV-I2071EPKLUGmli2QQ.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx"><strong class="bd jk">Task Generalization on ImageNet Classification</strong></figcaption></figure><ul class=""><li id="0f2f" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">所提出的裂脑自动编码器架构<strong class="jn hj">学习来自ImageNet的大规模图像数据的无监督表示</strong>。</li><li id="4bc7" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated"><strong class="jn hj"> Lab颜色空间</strong>用于训练裂脑自动编码器。</li><li id="ff34" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated"><strong class="jn hj">所有权重被冻结</strong>并且<strong class="jn hj">特征图在空间上被调整到9000维。</strong></li><li id="199d" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">所有方法都使用<a class="ae ix" rel="noopener" href="/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160?source=post_page---------------------------"><strong class="jn hj">Alex net</strong></a><strong class="jn hj"/>变体。</li><li id="87d1" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">除了ImageNet-labels之外，没有标签的1.3M ImageNet数据集<strong class="jn hj">用于训练。</strong></li><li id="babb" class="kq kr hi jn b jo kz jr la ju lb jy lc kc ld kg kv kw kx ky bi translated">简而言之，尝试了不同的自动编码器变体。</li></ul><blockquote class="nf ng nh"><p id="544d" class="jl jm nd jn b jo jp ij jq jr js im jt ni jv jw jx nj jz ka kb nk kd ke kf kg hb bi translated"><strong class="jn hj">裂脑自动(<em class="hi"> cl </em>、<em class="hi"> cl </em>)、<em class="hi"> cl </em>表示使用分类损失，胜过所有变体和所有自我监督学习方法</strong>，如拼图游戏【30】、<a class="ae ix" href="https://sh-tsang.medium.com/review-unsupervised-visual-representation-learning-by-context-prediction-self-supervised-51a1d7ce6aff" rel="noopener">上下文预测</a>、<a class="ae ix" rel="noopener" href="/nerd-for-tech/review-bigan-adversarial-feature-learning-gan-535eb76be2ca">阿里【8】/甘比</a>、<a class="ae ix" href="https://sh-tsang.medium.com/review-context-encoders-feature-learning-by-inpainting-bd181e48997" rel="noopener">上下文编码器</a>【34】和着色【47】。</p></blockquote><h2 id="c886" class="mp lo hi bd jk mq mr ms ls mt mu mv lw ju mw mx ly jy my mz ma kc na nb mc nc bi translated">2.2.地方</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nr"><img src="../Images/157cbb72212d3e90988875583e8d0e3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*FfRhWSLVN157sof7DtE4lA.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx"><strong class="bd jk">Dataset &amp; Task Generalization on Places Classification</strong></figcaption></figure><ul class=""><li id="8200" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated"><strong class="jn hj">与<strong class="jn hj">预训练任务(ImageNet)不同的任务(地点)</strong>。</strong></li></ul><blockquote class="nf ng nh"><p id="8aec" class="jl jm nd jn b jo jp ij jq jr js im jt ni jv jw jx nj jz ka kb nk kd ke kf kg hb bi translated">对于地点分类也得到类似的结果，它<strong class="jn hj">优于诸如拼图[30]，</strong> <a class="ae ix" href="https://sh-tsang.medium.com/review-unsupervised-visual-representation-learning-by-context-prediction-self-supervised-51a1d7ce6aff" rel="noopener"> <strong class="jn hj">上下文预测</strong></a><strong class="jn hj">【7】，</strong><a class="ae ix" href="https://sh-tsang.medium.com/review-look-listen-and-learn-self-supervised-learning-ff89a7dee980" rel="noopener"><strong class="jn hj">L-Net</strong></a><strong class="jn hj">【45】，</strong> <a class="ae ix" href="https://sh-tsang.medium.com/review-context-encoders-feature-learning-by-inpainting-bd181e48997" rel="noopener"> <strong class="jn hj">上下文编码器</strong></a><strong class="jn hj">【34】和彩色化[47]。</strong></p></blockquote><h2 id="bcbc" class="mp lo hi bd jk mq mr ms ls mt mu mv lw ju mw mx ly jy my mz ma kc na nb mc nc bi translated">2.3.帕斯卡VOC</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ns"><img src="../Images/5031eeec460218a3935b6cbe8dc086ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*iJ2PPtSxVbd4UoY2MRS08Q.png"/></div><figcaption class="jg jh et er es ji jj bd b be z dx"><strong class="bd jk">Task and Dataset Generalization on PASCAL VOC</strong></figcaption></figure><ul class=""><li id="6e5e" class="kq kr hi jn b jo jp jr js ju ks jy kt kc ku kg kv kw kx ky bi translated">为了进一步测试泛化、分类、检测和分割性能，在PASCAL VOC上进行评估。</li></ul><blockquote class="nf ng nh"><p id="fe3f" class="jl jm nd jn b jo jp ij jq jr js im jt ni jv jw jx nj jz ka kb nk kd ke kf kg hb bi translated">提出的方法，裂脑自动(<em class="hi"> cl </em>，<em class="hi"> cl </em>)，在几乎所有已建立的自我监督基准上都达到了<strong class="jn hj">最先进的性能。</strong></p></blockquote></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><p id="05ef" class="pw-post-body-paragraph jl jm hi jn b jo jp ij jq jr js im jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi translated">论文中还有其他结果。如果感兴趣，请随意阅读该文件。希望在未来我能写一个关于拼图游戏的故事。</p></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h2 id="a694" class="mp lo hi bd jk mq mr ms ls mt mu mv lw ju mw mx ly jy my mz ma kc na nb mc nc bi translated">参考</h2><p id="9f55" class="pw-post-body-paragraph jl jm hi jn b jo me ij jq jr mf im jt ju nt jw jx jy nu ka kb kc nv ke kf kg hb bi translated">【2017 CVPR】【裂脑自动】<br/> <a class="ae ix" href="https://arxiv.org/abs/1611.09842" rel="noopener ugc nofollow" target="_blank">裂脑自动编码器:跨通道预测的无监督学习</a></p><h2 id="2d7b" class="mp lo hi bd jk mq mr ms ls mt mu mv lw ju mw mx ly jy my mz ma kc na nb mc nc bi translated">自我监督学习</h2><p id="b711" class="pw-post-body-paragraph jl jm hi jn b jo me ij jq jr mf im jt ju nt jw jx jy nu ka kb kc nv ke kf kg hb bi translated"><strong class="jn hj">2008–2010</strong><a class="ae ix" href="https://sh-tsang.medium.com/review-stacked-denoising-autoencoders-self-supervised-learning-c8ff81cef34c" rel="noopener">堆叠去噪自动编码器</a><strong class="jn hj">2014</strong><a class="ae ix" href="https://sh-tsang.medium.com/review-exemplar-cnn-discriminative-unsupervised-feature-learning-with-convolutional-neural-fa68abe937cc" rel="noopener">Exemplar-CNN</a><strong class="jn hj">2015</strong><a class="ae ix" href="https://sh-tsang.medium.com/review-unsupervised-visual-representation-learning-by-context-prediction-self-supervised-51a1d7ce6aff" rel="noopener">上下文预测</a><strong class="jn hj">2016</strong><a class="ae ix" href="https://sh-tsang.medium.com/review-context-encoders-feature-learning-by-inpainting-bd181e48997" rel="noopener">上下文编码器</a><strong class="jn hj">2017</strong><a class="ae ix" href="https://sh-tsang.medium.com/review-look-listen-and-learn-self-supervised-learning-ff89a7dee980" rel="noopener">L-Net</a>【t24</p><h2 id="8307" class="mp lo hi bd jk mq mr ms ls mt mu mv lw ju mw mx ly jy my mz ma kc na nb mc nc bi translated"><a class="ae ix" href="https://sh-tsang.medium.com/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e" rel="noopener">我以前的其他论文阅读材料</a></h2></div></div>    
</body>
</html>
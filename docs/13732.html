<html>
<head>
<title>Performing Named Entity Recognition on Audio Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对音频数据执行命名实体识别</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/performing-named-entity-recognition-on-audio-data-6e321843d270?source=collection_archive---------6-----------------------#2022-07-26">https://medium.com/geekculture/performing-named-entity-recognition-on-audio-data-6e321843d270?source=collection_archive---------6-----------------------#2022-07-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="6a3b" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">从语音中提取命名实体</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/5188db9152fba533a9d4df7eaddf9f28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*C_y5eGa1e3Sf7QmL"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Photo by <a class="ae jn" href="https://unsplash.com/@bee_balogun?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Bee Balogun</a> on <a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="f898" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">命名实体识别(或简称NER)被定义为从给定的信息中识别现实世界实体的任务。NER是机器学习中的一个流行任务，它是使用各种自然语言处理(NLP)技术来解决的。具体到文本数据，目标是训练一个模型，该模型可以理解给定的一段文本，并识别和提取指代现实世界实体的单词。这些真实世界的实体也称为命名实体。</p><p id="6e0e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">下图显示了NER系统的高级概览。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kk"><img src="../Images/07fea2b34b2fb244d74ce03325ab326e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*JVOImEj3fGA24r4g6SgRyQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">An overview of the NER model (Image by author)</figcaption></figure><p id="87a0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">NLP研究团体已经提出了许多专门针对文本数据的NER方法。这个领域中两个最广泛实验的数据集是<a class="ae jn" href="https://paperswithcode.com/sota/named-entity-recognition-ner-on-conll-2003" rel="noopener ugc nofollow" target="_blank"> CoNLL 2003 </a>和<a class="ae jn" href="https://paperswithcode.com/sota/named-entity-recognition-ner-on-ontonotes-v5" rel="noopener ugc nofollow" target="_blank"> OntoNotes </a>数据集。然而，由于基于语音的交互工具的广泛采用，最近，我们看到研究人员和组织也在语音领域探索和构建NER系统。</p><p id="c44f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因此，这篇文章将展示如何使用<strong class="jq hj"> AssemblyAI API </strong>和<strong class="jq hj"> Python </strong>在语音数据上构建命名实体识别系统。端到端系统在严格的语言理解、摘要和关键词提取方面具有广泛的适用性，这使得它成为一个需要解决的重要且有价值的问题，尤其是在语音领域。我将用一个全面的分析来结束这篇文章，让你解释获得的结果，并从数据中得出适当的见解。</p><p id="abcf" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">你可以在这里找到这篇文章的代码。此外，文章的亮点如下:</p><p id="0a9b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><a class="ae jn" href="#3f04" rel="noopener ugc nofollow"> <strong class="jq hj">对音频数据进行实体检测</strong></a><strong class="jq hj"><br/></strong><a class="ae jn" href="#7ea6" rel="noopener ugc nofollow"><strong class="jq hj">实体检测结果</strong></a><strong class="jq hj"><br/></strong><a class="ae jn" href="#802b" rel="noopener ugc nofollow"><strong class="jq hj">实体检测见解</strong> </a></p><p id="3a13" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们开始吧🚀！</p></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h1 id="3f04" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated">音频数据的实体检测</h1><p id="da1d" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">在这一节中，我将演示如何使用AssemblyAI API从预先录制的音频文件中识别和提取命名实体。此外，提取的实体将被分类到它们各自的实体类别中，例如:<code class="du lp lq lr ls b">person</code>、<code class="du lp lq lr ls b">location</code>、<code class="du lp lq lr ls b">organisation</code>、<code class="du lp lq lr ls b">date</code>、<code class="du lp lq lr ls b">event</code>、<code class="du lp lq lr ls b">occupation</code>等。，与下图中“伦敦”被归类为<code class="du lp lq lr ls b">location</code>的方式类似:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kk"><img src="../Images/e944b4c5f45f1d503a6cf97d5dcf4cb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*vm4DZXim0lJU49udCJxxHw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">An overview of the NER model through an API (Image by author)</figcaption></figure><h2 id="50cc" class="lt kt hi bd ku lu lv lw ky lx ly lz lc jx ma mb le kb mc md lg kf me mf li mg bi translated">步骤1:安装要求</h2><p id="77c6" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">要从本地机器调用AssemblyAI API并构建实体检测模块和实体分类器，您需要Python中的<code class="du lp lq lr ls b"><a class="ae jn" href="https://pypi.org/project/requests/" rel="noopener ugc nofollow" target="_blank">requests</a></code>包，它可以按如下方式安装:</p><pre class="iy iz ja jb fd mh ls mi mj aw mk bi"><span id="8669" class="lt kt hi ls b fi ml mm l mn mo">pip install requests</span></pre><h2 id="7435" class="lt kt hi bd ku lu lv lw ky lx ly lz lc jx ma mb le kb mc md lg kf me mf li mg bi translated">步骤2:生成您的API令牌</h2><p id="3f62" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">下一步是让您的API键访问AssemblyAI的语音到文本模型。您可以通过在<a class="ae jn" href="https://app.assemblyai.com/signup" rel="noopener ugc nofollow" target="_blank"> AssemblyAI </a>网站上免费创建一个帐户来完成此操作。</p><h2 id="1491" class="lt kt hi bd ku lu lv lw ky lx ly lz lc jx ma mb le kb mc md lg kf me mf li mg bi translated">步骤3:上传音频文件</h2><p id="db36" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">您想要转录和提取命名实体的音频文件应该可以通过URL访问。因此，在调用语音到文本模型之前，您需要将音频文件上传到文件托管服务。选项包括AWS S3桶，音频托管服务，如SoundCloud或AssemblyAI的自托管服务等。对于本教程，我上传了音频文件到SoundCloud。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mp mq l"/></div></figure><h2 id="9d31" class="lt kt hi bd ku lu lv lw ky lx ly lz lc jx ma mb le kb mc md lg kf me mf li mg bi translated">步骤4:执行实体检测和分类</h2><p id="d6f7" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">在这一步，我们已经满足了从音频文件中检测实体的所有必要的先决条件。现在，我们可以调用API来提取命名实体。这是一个进一步的两步过程，在下面的小节中演示。</p><h2 id="2dd4" class="lt kt hi bd ku lu lv lw ky lx ly lz lc jx ma mb le kb mc md lg kf me mf li mg bi translated">步骤4.1:提交文件进行转录</h2><p id="812f" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">第一步是通过调用HTTP Post请求来触发语音到文本模型。POST请求将音频文件作为<code class="du lp lq lr ls b">audio_url</code>，并使用<code class="du lp lq lr ls b">entity_detection</code>标志指示模型执行命名实体识别。由于音频文件包含多个扬声器，我将<code class="du lp lq lr ls b">speaker_labels</code>标志设置为<code class="du lp lq lr ls b">True</code>。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mr mq l"/></div></figure><p id="2f9e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">收到的JSON响应表明post请求的<code class="du lp lq lr ls b">status</code>是<code class="du lp lq lr ls b">queued</code>，意味着文件在等待转录的队列中。</p><p id="c8f0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">而且<code class="du lp lq lr ls b">entity_detection</code>标志也是JSON响应中的<code class="du lp lq lr ls b">True</code>。然而，<code class="du lp lq lr ls b">entities</code>键对应的值为<strong class="jq hj">无，</strong>为当前状态<code class="du lp lq lr ls b">queued</code>。</p><h2 id="8d0b" class="lt kt hi bd ku lu lv lw ky lx ly lz lc jx ma mb le kb mc md lg kf me mf li mg bi translated">步骤4.2:获取转录结果</h2><p id="694a" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">为了检查POST请求的状态并查看转录结果，我们需要使用上面收到的JSON响应中的<code class="du lp lq lr ls b">id</code>键发出GET请求。我们通过从POST请求接收的<code class="du lp lq lr ls b">response_id</code>来检查转录状态。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ms mq l"/></div></figure><blockquote class="mt mu mv"><p id="8b0c" class="jo jp mw jq b jr js ij jt ju jv im jw mx jy jz ka my kc kd ke mz kg kh ki kj hb bi translated"><strong class="jq hj">注</strong>:只有<code class="du lp lq lr ls b"><em class="hi">status</em></code>键变为<code class="du lp lq lr ls b"><em class="hi">completed</em></code>键，转录结果才可用。人工智能模型进行转录和执行下游NLP任务(如本例中的NER)所花费的时间很大程度上取决于音频文件的大小和持续时间。因此，您必须定期重复发出GET请求来检查转录状态:</p></blockquote><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="na mq l"/></div></figure></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h1 id="7ea6" class="ks kt hi bd ku kv kw kx ky kz la lb lc io ld ip le ir lf is lg iu lh iv li lj bi translated">实体检测结果</h1><p id="2e38" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">一旦<code class="du lp lq lr ls b">status</code>变为<code class="du lp lq lr ls b">completed</code>，您将收到类似于下面提到的响应。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nb mq l"/></div></figure><ol class=""><li id="d19b" class="nc nd hi jq b jr js ju jv jx ne kb nf kf ng kj nh ni nj nk bi translated">我们将JSON响应中的<code class="du lp lq lr ls b">status</code>视为<code class="du lp lq lr ls b">completed</code>。这表示音频转录成功。</li><li id="7f2f" class="nc nd hi jq b jr nl ju nm jx nn kb no kf np kj nh ni nj nk bi translated"><code class="du lp lq lr ls b">text</code>键将输入音频文件的转录保存为字符串，不区分扬声器级别。总共有12个句子。</li><li id="e737" class="nc nd hi jq b jr nl ju nm jx nn kb no kf np kj nh ni nj nk bi translated">由于音频文件由多个扬声器组成，我们将<code class="du lp lq lr ls b">words</code>键中的所有<code class="du lp lq lr ls b">speaker</code>键视为<strong class="jq hj">非空</strong>。<code class="du lp lq lr ls b">speaker</code>键为“A”或“b”</li><li id="2ece" class="nc nd hi jq b jr nl ju nm jx nn kb no kf np kj nh ni nj nk bi translated"><code class="du lp lq lr ls b">confidence</code>分数突出了模型在转录单个单词和整个转录文本时的信心。范围从0到1——“0”为最低,“1”为最高。</li><li id="9f1e" class="nc nd hi jq b jr nl ju nm jx nn kb no kf np kj nh ni nj nk bi translated">使用JSON响应的<code class="du lp lq lr ls b">entities</code>键，可以访问音频中12个单独句子中检测到的实体的结果。</li><li id="4a42" class="nc nd hi jq b jr nl ju nm jx nn kb no kf np kj nh ni nj nk bi translated">音频文件中标识的实体数量是17。</li><li id="1afe" class="nc nd hi jq b jr nl ju nm jx nn kb no kf np kj nh ni nj nk bi translated">对应于每个实体，我们得到一个<code class="du lp lq lr ls b">entity_type</code>,它描述了被检测实体的类别。</li></ol><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nq mq l"/></div></figure><h1 id="802b" class="ks kt hi bd ku kv nr kx ky kz ns lb lc io nt ip le ir nu is lg iu nv iv li lj bi translated">实体检测洞察</h1><p id="4f7e" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">由于JSONs通常难以阅读和解释，我们可以通过将上面的实体检测结果转换为数据帧来使数据在视觉上更具吸引力。这也将有助于有效地进行进一步的分析。我们将存储句子的<code class="du lp lq lr ls b">text</code>、句子的<code class="du lp lq lr ls b">duration</code>、句子的<code class="du lp lq lr ls b">speaker</code>和句子的<code class="du lp lq lr ls b">entity_count</code>。这在下面实现:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nw mq l"/></div></figure><p id="5049" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">用上面的代码片段生成的DataFrame如下图所示。这里，我们有音频文件中说出的12个句子，以及相应的说话者标签(“A”和“B”)，它们的持续时间(以秒为单位)，以及一个描述句子中实体数量的字段。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nx"><img src="../Images/b6f1ec1e661373c072629f5fdeedf5f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bajNB4s0wE4TYCRMTKyZ6g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Sentences in the audio file (Image by author)</figcaption></figure><p id="7394" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">接下来，我们生成另一个数据帧，它接受音频文件中标识的实体。这在下面实现，在代码块后生成数据帧:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ny mq l"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nz"><img src="../Images/7f9b614d0d42bf39f352a425f7ab6fad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*Jnleq3voL796Q025SSlZ3A.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Entities in the audio file (Image by author)</figcaption></figure><h2 id="30b5" class="lt kt hi bd ku lu lv lw ky lx ly lz lc jx ma mb le kb mc md lg kf me mf li mg bi translated">#1演讲者-句子分布</h2><p id="60ee" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">首先，让我们计算音频文件中所有单个说话者说出的句子数量。这可以使用如下所示的<code class="du lp lq lr ls b">value_counts()</code>方法来完成:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="oa mq l"/></div></figure><p id="05ee" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">要查看两个扬声器的百分比分布，我们可以将<code class="du lp lq lr ls b">normalize = True</code>传递给<code class="du lp lq lr ls b">value_counts()</code>方法，如下所示:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="oa mq l"/></div></figure><h2 id="97b5" class="lt kt hi bd ku lu lv lw ky lx ly lz lc jx ma mb le kb mc md lg kf me mf li mg bi translated">#2发言者持续时间分布</h2><p id="446b" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">接下来，让我们找出所有单个发言者的总发言时间。如下所示:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ob mq l"/></div></figure><p id="8981" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们使用<code class="du lp lq lr ls b">groupby()</code>方法，将总时长计算为单个句子的<code class="du lp lq lr ls b">duration</code>的<code class="du lp lq lr ls b">sum</code>。就持续时间而言，说话者“B”是占优势的说话者。</p><h2 id="285e" class="lt kt hi bd ku lu lv lw ky lx ly lz lc jx ma mb le kb mc md lg kf me mf li mg bi translated">#3扬声器实体-计数分布</h2><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ob mq l"/></div></figure><p id="48d1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在音频文件中总共提到了17个实体，其中9个由说话者“A”说出，其余的由说话者“B”说出。</p><h2 id="3481" class="lt kt hi bd ku lu lv lw ky lx ly lz lc jx ma mb le kb mc md lg kf me mf li mg bi translated">#4实体类型分布</h2><p id="1b1d" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">接下来，让我们分析音频文件中各个实体类型的分布。这通过使用<code class="du lp lq lr ls b">value_counts()</code>方法实现如下:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="oc mq l"/></div></figure><h2 id="b4c9" class="lt kt hi bd ku lu lv lw ky lx ly lz lc jx ma mb le kb mc md lg kf me mf li mg bi translated">#5扬声器实体类型分布</h2><p id="6849" class="pw-post-body-paragraph jo jp hi jq b jr lk ij jt ju ll im jw jx lm jz ka kb ln kd ke kf lo kh ki kj hb bi translated">最后，让我们评估一下每个说话者所说的实体类型的数量。这里，为了更好的可视化，我们将使用<code class="du lp lq lr ls b">crosstab()</code>，而不是使用<code class="du lp lq lr ls b">groupby()</code>方法。下面演示了这一点:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="od mq l"/></div></figure></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><p id="075a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">总之，在这篇文章中，我们使用AssemblyAI API在预先录制的音频文件上构建了一个命名实体识别模块。最后，我们对检测到的实体进行了广泛的分析。从API获得的结果突出显示了在输入音频文件的12个单独句子中识别的17个实体。</p><p id="5deb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">你可以在这里找到这篇文章的代码。</p><p id="d1e8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">下次见。感谢阅读。</p></div></div>    
</body>
</html>
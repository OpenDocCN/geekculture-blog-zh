<html>
<head>
<title>Machine Learning Fundamentals: Cost Function and Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习基础:成本函数和梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/machine-learning-fundamentals-cost-function-and-gradient-descent-904234019ef9?source=collection_archive---------6-----------------------#2022-08-04">https://medium.com/geekculture/machine-learning-fundamentals-cost-function-and-gradient-descent-904234019ef9?source=collection_archive---------6-----------------------#2022-08-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="7fb3" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">每个数据科学家或ML工程师都应该知道的ML概念</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/45124836dc1084ec3ee95b59588121b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*61cR-EbE5zAwULDDUna7KA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Machine Learning Fundamentals (Pic by Author)</figcaption></figure><h2 id="fcb6" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">简介</strong></h2><p id="4037" class="pw-post-body-paragraph kl km hi kn b ko kp ij kq kr ks im kt jy ku kv kw kc kx ky kz kg la lb lc ld hb bi translated">成本函数和梯度下降是学习机器学习算法如何工作时你应该了解的最重要的概念之一。在这篇博客中，我们将看看这些概念背后的直觉。</p><p id="d8df" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">我们将选择最简单的机器学习算法，即线性回归算法来理解这些概念。</p><p id="51cc" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">我们将依次讨论以下主题:</p><ol class=""><li id="6a8e" class="lj lk hi kn b ko le kr lf jy ll kc lm kg ln ld lo lp lq lr bi translated"><em class="ls">了解线性回归模型，</em></li><li id="1dbc" class="lj lk hi kn b ko lt kr lu jy lv kc lw kg lx ld lo lp lq lr bi translated"><em class="ls">成本函数，和</em></li><li id="2141" class="lj lk hi kn b ko lt kr lu jy lv kc lw kg lx ld lo lp lq lr bi translated"><em class="ls">梯度下降。</em></li></ol><h2 id="35d9" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">线性回归</h2><p id="8f34" class="pw-post-body-paragraph kl km hi kn b ko kp ij kq kr ks im kt jy ku kv kw kc kx ky kz kg la lb lc ld hb bi translated">机器学习模型试图捕捉输入特征和输出特征之间的关系。想象一下，我们有数千人的身高和体重数据。我们想利用这些数据创建一个机器学习模型，以一个人的身高作为输入，预测这个人的体重。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ly"><img src="../Images/5f71afbb2dd76a2a54dcb5a62df7ee96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*X0TFHQL4ddaZjhaG1CSFxw.png"/></div></figure><p id="297a" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">如果我们观察人的体重(磅)和身高(英寸)之间的关系，我们可以看到这种关系看起来是线性的，即随着身高的增加，体重也增加。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lz"><img src="../Images/3f64791e1437d637a0dd2fb40904b823.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*GjGGuRwMBVt0yuaZ_5fo9g.png"/></div></figure><p id="3ca7" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">输入要素(身高)和输出要素(体重)之间的这种关系可以通过线性回归模型来捕捉，该模型会尝试对此数据拟合直线。</p><p id="d381" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">下面是一个简单线性回归模型的直线方程:</p><blockquote class="ma"><p id="7c2d" class="mb mc hi bd md me mf mg mh mi mj ld dx translated">Y = mx + c</p></blockquote><p id="7c1a" class="pw-post-body-paragraph kl km hi kn b ko mk ij kq kr ml im kt jy mm kv kw kc mn ky kz kg mo lb lc ld hb bi translated">y是输出要素(权重)，m是线的斜率，x是输入要素(高度)，c是截距(如下图所示，高度为0时权重等于c)。</p><blockquote class="ma"><p id="088c" class="mb mc hi bd md me mf mg mh mi mj ld dx translated">Y = m(0) + c = c</p></blockquote><p id="7132" class="pw-post-body-paragraph kl km hi kn b ko mk ij kq kr ml im kt jy mm kv kw kc mn ky kz kg mo lb lc ld hb bi translated">对于斜率“m”和常数“c”的不同值，我们将得到不同的线，如下图所示。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mp"><img src="../Images/bdafa9d7dc8208b093d3245425c28f46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*ZEmLOkhZeZJ7yw9ccGopuw.png"/></div></div></figure><h1 id="f985" class="mq jo hi bd jp mr ms mt jt mu mv mw jx io mx ip kb ir my is kf iu mz iv kj na bi translated">线性回归如何找到最佳拟合线？</h1><p id="8031" class="pw-post-body-paragraph kl km hi kn b ko kp ij kq kr ks im kt jy ku kv kw kc kx ky kz kg la lb lc ld hb bi translated">现在，问题是线性回归模型如何找到最适合我们数据的直线？</p><p id="c3bf" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">答案是代价函数和梯度下降！</p><p id="0a3e" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">首先，我们来看看成本函数。</p><h2 id="1789" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">价值函数</h2><p id="0e5c" class="pw-post-body-paragraph kl km hi kn b ko kp ij kq kr ks im kt jy ku kv kw kc kx ky kz kg la lb lc ld hb bi translated">成本函数是一个数学函数，它被最小化以获得斜率“m”和常数“c”的最佳值。与线性回归相关的成本函数称为均方误差，可表示如下:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lz"><img src="../Images/2f894e1362e595ad9247841ed20c9138.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*KrchdbifyOTbVDmVahZPQw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Mean square error (Image by Author)</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nb"><img src="../Images/96401cd59ca95dbb8c8e2d27597181d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oHHpyW3JQLmgueVJj11alg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Error: Difference between actual and predicted. (Image by Author)</figcaption></figure><p id="11e7" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">假设实际重量和预测重量如下:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nc"><img src="../Images/0ff72c392887a20e29887dad7c79a6aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*Jj4Ku01hWJPfwt9Tx4NCoA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Actual vs Predicted Weights</figcaption></figure><p id="007a" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">我们可以计算MSE如下:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nd"><img src="../Images/1af75d7f3ce10c8a8fb3619eba675d63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Iws46jM5FdyDNOLvDYTJYw.png"/></div></div></figure><p id="c1d4" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">我们可以稍微调整一下等式，使计算变得更加简单。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nf"><img src="../Images/924c74beec6f207d295289d287963946.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*9Gaj-BAaaUhvlaAIkOovoQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">The updated sum of square error</figcaption></figure><p id="c731" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">现在，MSE的值将基于斜率‘m’和常数‘c’的值的变化而变化。</p><p id="2de4" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated"><strong class="kn hj">现在，我们需要获得‘m’和‘c’的最佳值，以使MSE最小</strong>。直觉上，我们希望预测的重量尽可能接近实际重量。</p><p id="74b8" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">但是我们怎么做呢？“m”和“c”的组合可能有很多种，我们无法一一测试。</p><p id="f956" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">梯度下降就是解决方案！</p><h2 id="6b55" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">梯度下降</h2><p id="8fd0" class="pw-post-body-paragraph kl km hi kn b ko kp ij kq kr ks im kt jy ku kv kw kc kx ky kz kg la lb lc ld hb bi translated">梯度下降是一种最小化函数结果的技术，在线性回归的情况下，函数结果是均方误差。</p><p id="565f" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">让我们了解一下梯度下降在线性回归的情况下是如何工作的。</p><p id="b662" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">线性回归(MSE)的成本函数是一个凸函数，即在斜率“m”和常数“c”的值范围内只有一个最小值，如下图所示(成本函数由J(m，c)表示)。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ng"><img src="../Images/a40bc96132d3a2a539a1caf416bd8060.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_5QtRMt-uV3xnetKLrC2eg.png"/></div></div></figure><p id="b39d" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">数学上，梯度下降通过计算对应于‘m’和‘c’当前值的<strong class="kn hj">偏导数或斜率</strong>来工作，如下所示。在每一步，“m”和“c”的值同时更新。这些值将持续更新，直到我们达到成本函数达到最小值的‘m’和‘c’的值。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nh"><img src="../Images/6b2ea5e051abdcdfbc78551b0a737c21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*X8AnlcSz8aOJc79vs__4qg.png"/></div></figure><blockquote class="ma"><p id="9db4" class="mb mc hi bd md me ni nj nk nl nm ld dx translated"><em class="nn">学习率(或alpha)是‘m’或‘c’的值更新的速率。α的值越大，对‘m’或‘c’的值的更新就越大。</em></p></blockquote><p id="76b3" class="pw-post-body-paragraph kl km hi kn b ko mk ij kq kr ml im kt jy mm kv kw kc mn ky kz kg mo lb lc ld hb bi translated">如果斜率为负，则‘m’的值增加—学习率*‘m’的斜率，如果斜率为正，则‘m’的值减少—学习率*‘m’的斜率。c的值也是如此。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es no"><img src="../Images/7951931e0de22dd4d763fe507f1dd669.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dEJKGlxexLS70pUMUyRh1g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Pic Credits: <a class="ae ne" href="https://www.jeremyjordan.me/" rel="noopener ugc nofollow" target="_blank">jeremyjordan</a></figcaption></figure><p id="be6f" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">现在，回到我们的偏导数项，这是偏导数如何应用于成本函数(如果你觉得这太数学化，你可以跳过)。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es np"><img src="../Images/4bdbe4c97920829d66adbaecf9532ae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QvuJDxZ3bHGfgRUtu-6Hpg.png"/></div></div></figure><h2 id="d5ee" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">摘要</h2><p id="9340" class="pw-post-body-paragraph kl km hi kn b ko kp ij kq kr ks im kt jy ku kv kw kc kx ky kz kg la lb lc ld hb bi translated">每个机器学习模型都有一个成本函数，在线性回归的情况下是均方误差，基于这个函数，模型评估预测值与实际值的接近程度。</p><p id="b57d" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">梯度下降用于达到成本函数的最小值。直观地，梯度下降在每一步找到成本函数的斜率，并沿着山谷向下行进到达最低点(成本函数的最小值)。</p></div><div class="ab cl nq nr gp ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="hb hc hd he hf"><h2 id="739e" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">谢谢你！</strong></h2><p id="f176" class="pw-post-body-paragraph kl km hi kn b ko kp ij kq kr ks im kt jy ku kv kw kc kx ky kz kg la lb lc ld hb bi translated">首先，你应该在收件箱里收到我的帖子。 <a class="ae ne" href="https://anmol3015.medium.com/subscribe" rel="noopener"> <strong class="kn hj"> <em class="ls">在这里做</em> </strong> <em class="ls">！</em> </a> <em class="ls"> <br/>其次，如果你喜欢体验媒介的自己，可以考虑通过</em> <a class="ae ne" href="https://anmol3015.medium.com/membership" rel="noopener"> <strong class="kn hj"> <em class="ls">报名成为会员</em> </strong> </a> <em class="ls">来支持我和其他几千名作家。它每个月只需要5美元，它极大地支持了我们，作家，而且你也有机会通过你的写作赚钱。</em></p></div></div>    
</body>
</html>
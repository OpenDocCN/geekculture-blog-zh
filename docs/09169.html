<html>
<head>
<title>NLP Article #1 : A word is worth a 1000 pictures</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP文章#1:一个单词抵得上1000张图片</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/nlp-article-1-a-word-is-worth-a-1000-pictures-2a4d027e51fb?source=collection_archive---------17-----------------------#2021-11-29">https://medium.com/geekculture/nlp-article-1-a-word-is-worth-a-1000-pictures-2a4d027e51fb?source=collection_archive---------17-----------------------#2021-11-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/95edc5c359515a6562592eb9165044bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*5xrOW-ZdTMF6ntJ6i85izA.jpeg"/></div><figcaption class="im in et er es io ip bd b be z dx">Image created by Author</figcaption></figure><p id="49d2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">自然语言处理，简称NLP，很简单，就是研究和使用机器来智能地使用和创造自然语言。我有意暂时省略“理解”这个词，因为在使用概率模型时，这是一个有点棘手的话题。</p><p id="65e3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是难题如下:就比特率而言，口头交流是糟糕的。一个演讲者可能每分钟说300个单词，这是每秒70字节的微不足道的速度。但是如果换成双向对话，速度会进一步下降到100字/分钟，或者绝对是25字节/秒。然而，其有效性掩盖了这一事实。会议是有效的。面对面的会议更是如此。这可能与非语言暗示有很大关系。即使在今天，一些人仍然使用/s后缀来表示讽刺，因为没有线索的基于文本的交流充满了困难。但它也可能与被接受的、共享的参照和共同的记忆(有时称为长程模型)有很大关系。就像一个事实上穷尽的散列表。其中对单个令牌的引用——比如说“WW2”——意味着不止2个字节。两个人之间的一个更私人的例子，可能指的是“那个东西”，这里讨论的“那个东西”是一个复杂的理解。</p><p id="f0f4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以难题是，它显然不仅仅是比特率。要实现真正的意义需要付出很多——这就是为什么NLP一直是一个难题。</p><p id="667a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">NLP研究世界-非常明确地定义为2013年前和2013年后。这是高效的深度神经网络(贝尔实验室的LaCun已经在计算机视觉领域高效使用了几年，以惊人的准确度识别手写数字)在NLP领域取得了第一次重大的突破性进展，实现了一个名为Word2Vec (Mikolov)的应用。这使得新(重新)发现的深度学习统计和优化方法能够在NLP空间中使用。在许多任务中，它真的，真的起作用了。翻译，句子完成，内容生成是最重要的。情感分析也非常重要。</p><p id="c784" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是，由于这些是统计和概率方法，任何“理解”的说法充其量是虚假的。并受到当前激烈辩论的影响。</p><p id="7a01" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">出于这个原因，我认为即使是最热心的人工神经网络(ANN)实践者也会同意，在人类的意义上没有理解或“意义”。事实上，几乎所有神经网络研究人员都煞费苦心地明确声明，他们的想法和模型不是基于复制人脑。该领域的一些原创研究人员，如美国的巨人罗森布拉特和日本的福岛，肯定会证明他们的原始灵感来自于观察当达到某个阈值时神经元的放电——特别是在他们的视觉皮层中。但是这两个领域很快就分道扬镳了，因为计算机科学家从根本上说不是认知科学家，因此是由基于经验成功的指标驱动的，而不是为生物学理解而努力。</p><p id="473d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以我们在这里。在特定领域取得惊人的成功，但实际了解很少。</p><p id="0be0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在列举我们的确切位置之前——再多一点历史，和NLP的几个里程碑。</p><p id="f4e9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(NLP最著名的早期概念可能来自英国数学家艾伦·图灵，他在20世纪40年代提出了图灵测试。这是一个思想实验，表明计算机将通过人类理解的某个门槛，当计算机(输出文本)可以欺骗不知情的主体，使其认为它正在与另一个人而不是计算机交谈。(还没有成功通过的测试，但可能很快就会通过……)</p><p id="65a5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(2)在20世纪60年代，控制论领域诞生了，由Weitzenbaum领导的麻省理工学院的一个NLP小组提出了一个名为Eliza的程序，它使用大量手工编码的规则来尝试和交谈——第一个聊天机器人。它使用了一个名为Doctor的相当简单的脚本，该脚本寻找要回流的关键词，然后参与一个开放式的“罗杰式”治疗会话。在没有关键词的情况下，它回复为开放的回答，比如“你对它感觉如何？或者‘你怎么看<em class="jo">？“当它遇到错误时，它表现得好像它令人困惑的反应是一个笑话。尽管它很简单，但可以愚弄很多人。</em></p><p id="f64b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(3)在70年代、80年代甚至90年代的大部分时间里，NLP被称为计算语言学，并且通常在语言学系和计算机科学系中都能找到。继续取得小的进展，但这些进展继续朝着普遍理解的方向发展。他们专注于语法结构，因此他们对手工编码规则和洞察力的依赖阻碍了进度。</p><p id="f97c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">概率方法继续取得小进展，使用“单词袋”方法，其中单词频率和短语(n-gram)频率产生了一些好的结果。递归神经网络架构引入了时间序列组件。洞察力表明一些邻近的单词比其他的更重要。但是n维中不可能大的词向量空间，其中n是字典中的词的数量(比如英语中的100，000)，只考虑相邻的词，并且当相邻词的窗口扩大时，该词向量空间呈指数增长。</p><p id="f315" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(4)然后在2013年9月，谷歌的Miklos等人发表了Word2Vec。</p><p id="2f89" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有了足够的未标记数据，它可以在一个单词和相邻的n个单词(n是预定义的超参数)之间创建可管理的概率分布，然后可以分配哪个单词将是下一个单词的概率。这立即产生了翻译和句子完成方面的最新成果。一旦你有了句子完成和下一个词的生成，它只需要引入生成性对抗网络(GANS)的额外步骤，就可以开始产生散文和副本。</p><p id="402a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(5)然后在2017年，《变形金刚》带来了额外的严格规范，即哪些邻近的词增加了最大的价值，以及如何将注意力集中在相关的地方。随着计算能力的增长，具有越来越大的语料库和越来越多的特征(参数)的变压器模型被训练——花费巨大。其中一个商业模式是，像GPT 2这样的语言模式，可以被许可、转移训练和优化，以达到特定的更狭窄的目的。例如，建立一个关于特定医疗问题的聊天机器人。</p><p id="63b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">事实证明，这些方法非常有效。有一个巨大的警告。</p><p id="f655" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于他们是在巨大的语料库上接受训练的，这些语料库甚至是从网络最黑暗的角落收集来的，所以他们不能轻易地区分事实和虚构。因此，当他们接触到糟糕的数据时——比如大量未经证实的阴谋论，他们很容易做出错误的假设，并得到大量错误断言的支持。</p><p id="3dc6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正因为如此，第一个公开发行的GPT-2版本被证明有能力很快变成种族主义者。所以未来的版本有一些自我监督机制和明确的警告。</p><p id="a238" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是对自然语言处理的历史和现状的概述。下一篇文章NLP #2将着眼于目前市场上存在的特定参与者和产品。</p></div></div>    
</body>
</html>
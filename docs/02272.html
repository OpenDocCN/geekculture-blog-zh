<html>
<head>
<title>A Tutorial on Performing Sentiment Analysis in Python 3 Using the Natural Language Toolkit (NLTK)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用自然语言工具包(NLTK)在Python 3中执行情感分析的教程</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/a-tutorial-on-performing-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk-40e5b35ab440?source=collection_archive---------3-----------------------#2021-05-09">https://medium.com/geekculture/a-tutorial-on-performing-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk-40e5b35ab440?source=collection_archive---------3-----------------------#2021-05-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="3bcb" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">本教程将演示如何使用Python3的NLTK库检测twitter数据中的积极或消极情绪。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/41a6b47f1f6d3de8c14ec682b0dd54c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/0*LhRii2L7kA4GpE6Z.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx">Twitter Sentiment Analysis</figcaption></figure><p id="ce96" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">情感分析简介:</strong></p><p id="1d55" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">情感分析</strong>是一种<strong class="jl hj">自然语言处理(NLP) </strong>技术，它分析一篇文章以确定它携带的情感基调——可能是积极的、消极的或中性的。它本质上是一种工具，可以从非结构化数据中获得意义，并产生一些见解。这样，它在社交媒体监控中变得非常有用，因为它允许我们获得特定主题背后更广泛的公众意见的概览。同样，它让品牌了解什么让客户高兴或沮丧，以便他们可以定制产品和服务来满足客户的需求。</p><p id="fc2c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">如何在Python 3中进行情感分析:</strong></p><p id="3bac" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">本教程将演示如何对推文进行情感分析，以确定它们是正面情感还是负面情感。我们将利用<strong class="jl hj"> Python的NLTK(自然语言工具包)库</strong>，这是一个在文本数据分析中非常常用的库。</p><blockquote class="kf kg kh"><p id="7010" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">请注意，以下代码是在Jupyter笔记本上写的。</p></blockquote><p id="5268" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">首先，首先在终端安装必要的软件包。</p><pre class="iy iz ja jb fd km kn ko kp aw kq bi"><span id="5187" class="kr ks hi kn b fi kt ku l kv kw">% pip3 install emoji<br/>% pip3 install nltk==3.3 <br/>% pip3 install pandas <br/>% pip3 install seaborn<br/>% pip3 install sklearn</span></pre><p id="3c36" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">整个过程可以分为四个部分:</p><ol class=""><li id="d6f7" class="kx ky hi jl b jm jn jp jq js kz jw la ka lb ke lc ld le lf bi translated">准备用于分析的数据</li><li id="4024" class="kx ky hi jl b jm lg jp lh js li jw lj ka lk ke lc ld le lf bi translated">清理数据</li><li id="fde4" class="kx ky hi jl b jm lg jp lh js li jw lj ka lk ke lc ld le lf bi translated">数据的标准化</li><li id="aa27" class="kx ky hi jl b jm lg jp lh js li jw lj ka lk ke lc ld le lf bi translated">构建和评估模型</li></ol><h1 id="86b4" class="ll ks hi bd lm ln lo lp lq lr ls lt lu io lv ip lw ir lx is ly iu lz iv ma mb bi translated"><strong class="ak">准备数据- </strong></h1><p id="92c8" class="pw-post-body-paragraph jj jk hi jl b jm mc ij jo jp md im jr js me ju jv jw mf jy jz ka mg kc kd ke hb bi translated">打开一个新的笔记本，作为第一步，导入nltk库。</p><pre class="iy iz ja jb fd km kn ko kp aw kq bi"><span id="aa6d" class="kr ks hi kn b fi kt ku l kv kw">import nltk</span></pre><p id="4dd8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">从nltk下载“twitter_samples”包，我们将在这里构建样本tweets数据集。</p><pre class="iy iz ja jb fd km kn ko kp aw kq bi"><span id="27fa" class="kr ks hi kn b fi kt ku l kv kw">nltk.download(‘twitter_samples’)</span><span id="b105" class="kr ks hi kn b fi mh ku l kv kw"># Datasets to train and test the model<br/>from nltk.corpus import twitter_samples</span><span id="498d" class="kr ks hi kn b fi mh ku l kv kw">positive_tweets = twitter_samples.strings('positive_tweets.json')<br/>negative_tweets = twitter_samples.strings('negative_tweets.json')</span></pre><p id="2956" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们现在将这些正面和负面的推文合并成一个熊猫数据帧，以使数据预处理更容易。</p><pre class="iy iz ja jb fd km kn ko kp aw kq bi"><span id="1717" class="kr ks hi kn b fi kt ku l kv kw">import pandas as pd</span><span id="b5d2" class="kr ks hi kn b fi mh ku l kv kw"># Create a dataframe from positive tweets<br/>df = pd.DataFrame(positive_tweets, columns=[‘Tweet’])</span><span id="d3f4" class="kr ks hi kn b fi mh ku l kv kw"># Add a column to dataframe for positive sentiment value 1<br/>df[‘Sentiment’] = 1</span><span id="94e4" class="kr ks hi kn b fi mh ku l kv kw"># Create a temporary dataframe for negative tweets<br/>temp_df = pd.DataFrame(negative_tweets, columns=[‘Tweet’])</span><span id="206c" class="kr ks hi kn b fi mh ku l kv kw"># Add a column to temporary dataframe for negative sentiment value 0<br/>temp_df[‘Sentiment’] = 0</span><span id="30c5" class="kr ks hi kn b fi mh ku l kv kw"># Combe positive and negative tweets in one single dataframe<br/>df = df.append(temp_df, ignore_index=True)<br/>df = df.sample(frac = 1) <br/>df.reset_index(drop=True, inplace=True)</span></pre><p id="6bc8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们得到的结果是一个有10，000行的数据帧(5，000行是正面推文，5，000行是负面推文)。正面和负面的推文将被随机化。</p><p id="647a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">dataframe将有两列——Tweet和perspective。这里，情绪将是一个二进制值——0表示负面情绪，1表示正面情绪。</p><pre class="iy iz ja jb fd km kn ko kp aw kq bi"><span id="614b" class="kr ks hi kn b fi kt ku l kv kw"># Displaying shape of dataset<br/>print(‘Dataset size:’,df.shape)<br/>df.groupby(“Sentiment”).count()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mi"><img src="../Images/2d634d9d2cf74bf4d4b79fbc72124de5.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*aK120Mxc2GPMQ9kg7c20mA.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx">Shape of dataset</figcaption></figure><h1 id="b8a0" class="ll ks hi bd lm ln lo lp lq lr ls lt lu io lv ip lw ir lx is ly iu lz iv ma mb bi translated">清理数据-</h1><p id="b5c7" class="pw-post-body-paragraph jj jk hi jl b jm mc ij jo jp md im jr js me ju jv jw mf jy jz ka mg kc kd ke hb bi translated">现在我们已经准备好了数据，我们可以开始清理这些数据，为训练模型做好准备。我们说数据不干净是因为它可能使用俚语或缩写，它可能有表情符号和微笑，它可能有许多对文本没有太大意义的“停用词”。我们将详细阐述每种情况，并演示如何从数据中消除它们。</p><p id="b8e3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">让我们从把所有的单词转换成小写的<strong class="jl hj">开始。</strong></p><p id="34b0" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">你可能会问为什么我们需要将数据转换成小写。假设我们有下面的文本-</p><p id="4892" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这是个度假的好地方。我推荐这个地方。”</p><p id="9ccd" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如果我们要对上面的文本进行“矢量化”，而不先转换成小写字母，“T0”这个“T1”和“T2”这个“T3”会被认为是两个不同的单词。此外，由于停用词列表中的所有单词都是小写的，如果原始文本中的单词不是小写的，那么您可能会在文本中看到混合大小写或大写的停用词。然而，有些人可能会选择保留大写的文本，以便不丢失所传达的情感。</p><p id="8e98" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">使用下面的代码将dataframe中的所有tweets转换为小写。</p><pre class="iy iz ja jb fd km kn ko kp aw kq bi"><span id="9696" class="kr ks hi kn b fi kt ku l kv kw"># Converting all tweets to lowercase<br/>def convert_to_lowercase(tweet):<br/> return tweet.lower()</span><span id="bb78" class="kr ks hi kn b fi mh ku l kv kw">df[‘Tweet’] = df[‘Tweet’].apply(lambda x: convert_to_lowercase(x))</span></pre><p id="ad7a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">下一步我们将做的是<strong class="jl hj">从推文中删除所有的</strong><strong class="jl hj">URL</strong>，因为它们没有增加任何意义，也无助于发现情绪。为此，编写一个函数，用URL的正则表达式匹配文本。</p><pre class="iy iz ja jb fd km kn ko kp aw kq bi"><span id="b71f" class="kr ks hi kn b fi kt ku l kv kw"># Removing any urls from tweets</span><span id="a900" class="kr ks hi kn b fi mh ku l kv kw">import re</span><span id="e3af" class="kr ks hi kn b fi mh ku l kv kw">def remove_urls(tweet):<br/> tweet = re.sub(‘http[s]?://(?:[a-zA-Z]|[0–9]|[$-_@.&amp;+#]|[!*\(\),]|’\<br/> ‘(?:%[0–9a-fA-F][0–9a-fA-F]))+’,’’, tweet)<br/> <br/> return tweet</span><span id="0e30" class="kr ks hi kn b fi mh ku l kv kw">df[‘tweet_no_urls’] = df[‘Tweet’].apply(lambda x: remove_urls(x))</span></pre><p id="e8c8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">俚语和缩略语在社交媒体上非常常见。您可以选择将这些缩写转换成它们的完整形式，以从中提取更多的含义。为此，您需要一个包含常用缩写及其完整形式(用制表符分隔)的CSV文件。将下面代码中的&lt;CSV文件路径&gt;替换为您的CSV文件路径。</p><pre class="iy iz ja jb fd km kn ko kp aw kq bi"><span id="7c46" class="kr ks hi kn b fi kt ku l kv kw"># Removing all abbreviations</span><span id="0dd0" class="kr ks hi kn b fi mh ku l kv kw">import csv</span><span id="ce75" class="kr ks hi kn b fi mh ku l kv kw">def remove_abbreviations(tweet):<br/> tokens = tweet.split(“ “)<br/> j = 0<br/> for token in tokens:<br/> file_name = “&lt;path to CSV file&gt;”<br/> with open(file_name, “r”) as fh:<br/> reader = csv.reader(fh)<br/> for row in reader:<br/> if row[0] == token:<br/> tokens[j] = row[1]<br/> fh.close()<br/> j = j + 1<br/> <br/> return “ “.join(tokens)</span><span id="8c44" class="kr ks hi kn b fi mh ku l kv kw">df[‘corrected_tweet’] = df[‘tweet_no_urls’].apply(lambda x: remove_abbreviations(x))</span></pre><p id="185e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">接下来，我们将处理<strong class="jl hj">微笑和表情符号。这些在表达情感方面很重要，因此将它们从文本中完全删除是不明智的。最好是用他们所传达的真实情感来代替它们。</strong></p><p id="a550" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">为了处理微笑，我们将加载一个常见微笑的Python字典。对于表情符号，我们将使用Python包“表情符号”及其方法“demojize”。在这样做的时候，我们将把所有出现的微笑和表情符号替换为它们的实际含义。</p><pre class="iy iz ja jb fd km kn ko kp aw kq bi"><span id="9160" class="kr ks hi kn b fi kt ku l kv kw">import emoji</span><span id="29d0" class="kr ks hi kn b fi mh ku l kv kw"># Loading dictionary of smilies</span><span id="858e" class="kr ks hi kn b fi mh ku l kv kw">def load_dict_smileys():<br/> <br/> return {<br/> “:-)”: “smiley”,<br/> “:)”: “smiley”,<br/> “:D”: “smiley”,<br/> “xD”: “smiley”,<br/> “:’)”: “smiley”,<br/> “:’D”: “smiley”,<br/> “:3”: “smiley”,<br/> “:]”: “smiley”,<br/> “:^)”: “smiley”,<br/> “:-]”: “smiley”,<br/> “:-3”: “smiley”,<br/> “:-&gt;”: “smiley”,<br/> “:))”: “smiley”,<br/> “8-)”: “cool”,<br/> “8)”: “cool”,<br/> “8-D”: “cool”,<br/> “B)”: “cool”,<br/> “BD”: “cool”,<br/> “:-}”: “smiley”,<br/> “:&gt;”: “smiley”,<br/> “:}”: “smiley”,<br/> “:o)”: “smiley”,<br/> “:c)”: “smiley”,<br/> “=)”: “smiley”,<br/> “=D”: “smiley”,<br/> “=]”: “smiley”,<br/> “:-D”: “smiley”,<br/> “XD”: “smiley”,<br/> “:o”: “surprised”,<br/> “:O”: “surprised”,<br/> “=o”: “surprised”,<br/> “=O”: “surprised”,<br/> “:0”: “surprised”,<br/> “:(“: “sad”,<br/> “:C”: “sad”,<br/> “:c”: “sad”,<br/> “=(“: “sad”,<br/> “=c”: “sad”,<br/> “=C”: “sad”,<br/> “:-(“: “sad”,<br/> “:-c”: “sad”,<br/> “:-C”: “sad”,<br/> “:-&lt;”: “sad”,<br/> “:&lt;”: “sad”,<br/> “:[“: “sad”,<br/> “:{“: “sad”,<br/> “:’(“: “crying”,<br/> “:P”: “playful”,<br/> “:p”: “playful”,<br/> “xP”: “playful”,<br/> “XP”: “playful”,<br/> “=P”: “playful”,<br/> “=p”: “playful”,<br/> “xp”: “playful”,<br/> “xb”: “playful”,<br/> “❤”: “love”,<br/> “&lt;/3”: “sad”,<br/> “:/”: “worried”,<br/> “:-/”: “worried”<br/> }</span><span id="3b05" class="kr ks hi kn b fi mh ku l kv kw"># Removing smilies and emojis<br/>def remove_emoticons(tweet):<br/> smilies = load_dict_smileys()<br/> split_tweet = tweet.split(“ “)<br/> for key,val in smilies.items():<br/> if key in tweet:<br/> new_tweet = tweet.replace(key, val)<br/> tweet = new_tweet<br/> tweet = emoji.demojize(tweet)<br/> tweet = ‘ ‘.join(tweet.split())</span><span id="5bd3" class="kr ks hi kn b fi mh ku l kv kw">return tweet</span><span id="f2ac" class="kr ks hi kn b fi mh ku l kv kw">df[‘tweet_no_emoji’] = df[‘corrected_tweet’].apply(lambda x: remove_emoticons(x))</span></pre><p id="2f10" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">接下来，我们需要<strong class="jl hj">去除twitter句柄、标点符号、数字和特殊字符形式的任何额外干扰</strong>。它们没有增加探测情绪的价值。</p><pre class="iy iz ja jb fd km kn ko kp aw kq bi"><span id="6ccd" class="kr ks hi kn b fi kt ku l kv kw">import string</span><span id="cb6d" class="kr ks hi kn b fi mh ku l kv kw"># Removing twitter handles, punctuation, extra spaces, numbers and special characters<br/>def remove_noise(tweet):<br/> tweet = re.sub(“(@[A-Za-z0–9_]+)”,””, tweet)<br/> tweet = “”.join([char if char not in string.punctuation else “ “ for char in tweet])<br/> tweet = re.sub(‘ +’, ‘ ‘, tweet) <br/> tweet = re.sub(“[0–9]+”, “”, tweet)<br/> tweet = re.sub(“[^A-Za-z0–9_. ]+”,””,tweet)<br/> <br/> return tweet</span><span id="b6bc" class="kr ks hi kn b fi mh ku l kv kw">df[‘cleaned_tweet’] = df[‘tweet_no_emoji’].apply(lambda x: remove_noise(x))</span></pre><p id="f146" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">作为清理文本的最后一步，我们需要<strong class="jl hj">从文本中移除所有停用词</strong>。停用词是任何自然语言中最常见的词。出于分析文本数据和构建NLP模型的目的，这些停用词不会给文档的含义增加太多价值。然而，在我们的用例中，负面停用词在检测负面情绪时非常重要。因此，我们可以在将停用词应用到文本之前编辑它。</p><pre class="iy iz ja jb fd km kn ko kp aw kq bi"><span id="4185" class="kr ks hi kn b fi kt ku l kv kw">nltk.download('punkt')<br/>nltk.download(‘stopwords’)</span><span id="a7b0" class="kr ks hi kn b fi mh ku l kv kw">from nltk.corpus import stopwords</span><span id="060c" class="kr ks hi kn b fi mh ku l kv kw"># Loading stop words and removing negative stop words from the list<br/>stop_words = stopwords.words(‘english’)</span><span id="5739" class="kr ks hi kn b fi mh ku l kv kw">words_to_keep = [‘don’, “don’t”, ‘ain’, ‘aren’, “aren’t”, ‘couldn’, “couldn’t”, ‘didn’, “didn’t”, ‘doesn’, “doesn’t”, ‘hadn’, “hadn’t”, ‘hasn’, “hasn’t”, ‘haven’, “haven’t”, ‘isn’, “isn’t”, ‘ma’, ‘mightn’, “mightn’t”, ‘mustn’, “mustn’t”, ‘needn’, “needn’t”, ‘shan’, “shan’t”, ‘no’, ‘nor’, ‘not’, ‘shouldn’, “shouldn’t”, ‘wasn’, “wasn’t”, ‘weren’, “weren’t”, ‘won’, “won’t”, ‘wouldn’, “wouldn’t”]<br/>my_stop_words = stop_words</span><span id="6b33" class="kr ks hi kn b fi mh ku l kv kw">for word in words_to_keep:<br/> my_stop_words.remove(word)</span></pre><p id="3c93" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在删除停用词之前，需要对文本进行<strong class="jl hj">标记化</strong>或分割成称为标记的更小部分。<strong class="jl hj">标记</strong>是文本中作为一个单元的一系列字符。</p><pre class="iy iz ja jb fd km kn ko kp aw kq bi"><span id="30cd" class="kr ks hi kn b fi kt ku l kv kw">from nltk.tokenize import word_tokenize</span><span id="b4cc" class="kr ks hi kn b fi mh ku l kv kw"># Removing stop words from the tweet<br/>def remove_stop_words(tweet):<br/> tokens = word_tokenize(tweet)<br/> tweet_with_no_stop_words = [token for token in tokens if not token in my_stop_words]<br/> reformed_tweet = ‘ ‘.join(tweet_with_no_stop_words)<br/> <br/> return reformed_tweet</span><span id="fb6e" class="kr ks hi kn b fi mh ku l kv kw">df[‘tweet_no_stop’] = df[‘cleaned_tweet’].apply(lambda x: remove_stop_words(x))</span></pre><p id="b8f7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">至此，我们完成了数据清理阶段！</p><h1 id="f76b" class="ll ks hi bd lm ln lo lp lq lr ls lt lu io lv ip lw ir lx is ly iu lz iv ma mb bi translated">数据的标准化-</h1><p id="67e6" class="pw-post-body-paragraph jj jk hi jl b jm mc ij jo jp md im jr js me ju jv jw mf jy jz ka mg kc kd ke hb bi translated">下一节将涵盖文本的<strong class="jl hj">规范化</strong>。NLP中的规范化是将单词转换成其<strong class="jl hj">规范形式</strong>的过程。它有助于将意思相同但形式不同的单词组合在一起。如果没有规范化，“clean”、“cleans”和“cleaning”将被视为不同的单词，即使您可能希望它们被视为同一个单词。</p><p id="84f1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">两种流行的规范化技术是<em class="ki">词干</em>和<em class="ki">词汇化。</em>词干化和词尾化都会生成词根形式的词形变化。区别在于词干可能不是一个真实的单词，而lemma是一个真实的语言单词。</p><p id="7d08" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">例如，考虑“哭”这个词。词干化会将单词简化为“cri ”,这在英语中并不是一个真正的单词，而词汇化会将单词简化为“cry”。</p><p id="4821" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在这个项目中，我们将利用<strong class="jl hj">术语化</strong>。</p><p id="53f4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，单词形式可能是相同的，但在上下文或语义上是不同的。为了使词汇化更好并依赖于上下文，我们需要找出<strong class="jl hj"> POS(词性)标签</strong>并将其传递给词汇化器。我们将首先找出每个标记的POS标签，然后使用lemmatizer根据标签对标记进行lemmatize。</p><pre class="iy iz ja jb fd km kn ko kp aw kq bi"><span id="cd4b" class="kr ks hi kn b fi kt ku l kv kw">nltk.download(‘wordnet’)<br/>nltk.download(‘averaged_perceptron_tagger’)</span><span id="7ed9" class="kr ks hi kn b fi mh ku l kv kw"># Lemmatize with POS Tag<br/>from nltk.corpus import wordnet<br/>from nltk.stem import WordNetLemmatizer</span><span id="0f86" class="kr ks hi kn b fi mh ku l kv kw">def get_wordnet_pos(word):<br/> “””Map POS tag to first character lemmatize() accepts”””<br/> tag = nltk.pos_tag([word])[0][1][0].upper()<br/> tag_dict = {“J”: wordnet.ADJ,<br/> “N”: wordnet.NOUN,<br/> “V”: wordnet.VERB,<br/> “R”: wordnet.ADV}</span><span id="5704" class="kr ks hi kn b fi mh ku l kv kw">return tag_dict.get(tag, wordnet.NOUN)</span><span id="eab3" class="kr ks hi kn b fi mh ku l kv kw"># Lemmatization<br/>lemmatizer = WordNetLemmatizer()</span><span id="14e1" class="kr ks hi kn b fi mh ku l kv kw">def lemmatize_sentence(tweet):<br/> token_words = word_tokenize(tweet)<br/> lemmatized_tweet = []<br/> for word in token_words:<br/> lemmatized_tweet.append(lemmatizer.lemmatize(word, get_wordnet_pos(word)))<br/> lemmatized_tweet.append(“ “)<br/> <br/> return “”.join(lemmatized_tweet)</span><span id="4990" class="kr ks hi kn b fi mh ku l kv kw">df[‘lemmatized_text’] = df[‘tweet_no_stop’].apply(lambda x: lemmatize_sentence(x))</span></pre><h1 id="8650" class="ll ks hi bd lm ln lo lp lq lr ls lt lu io lv ip lw ir lx is ly iu lz iv ma mb bi translated">建立模型-</h1><p id="10ec" class="pw-post-body-paragraph jj jk hi jl b jm mc ij jo jp md im jr js me ju jv jw mf jy jz ka mg kc kd ke hb bi translated">当我们在数据科学中遇到任何模型(即线性回归、逻辑回归、决策树或神经网络)时，它们只能接受<strong class="jl hj">数字输入</strong>。那么，我们如何将现有的文本转换成计算机能够理解的数字数据呢？<strong class="jl hj">计数矢量器</strong>是Python中的scikit-learn库提供的一个很棒的工具。它用于根据每个单词在整个文本中出现的频率(计数)将给定文本转换为向量。</p><p id="852e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们传递给CountVectorizer的一些参数如下:</p><p id="97c3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> min_df: </strong>在构建词汇表时，忽略文档频率严格低于给定阈值的术语。</p><p id="3bdb" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> max_features: </strong>如果不是没有，则构建一个词汇表，该词汇表只考虑按术语频率在语料库中排序的顶级max_features。</p><pre class="iy iz ja jb fd km kn ko kp aw kq bi"><span id="9ac4" class="kr ks hi kn b fi kt ku l kv kw">from sklearn.feature_extraction.text import CountVectorizer</span><span id="bb99" class="kr ks hi kn b fi mh ku l kv kw">count_vectorizer = CountVectorizer(max_features=1000, min_df=8)<br/>feature_vector = count_vectorizer.fit(df[‘lemmatized_text’])</span><span id="0311" class="kr ks hi kn b fi mh ku l kv kw"># To get a list of all unique words<br/>features = feature_vector.get_feature_names()</span><span id="fc66" class="kr ks hi kn b fi mh ku l kv kw"># To get a sparse matrix of the words in the text<br/>df_features = count_vectorizer.transform(df[“lemmatized_text”]) </span></pre><p id="8186" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如果你真的想看到每个单词的计数，我们可以用所有单词的计数创建另一个数据帧。</p><pre class="iy iz ja jb fd km kn ko kp aw kq bi"><span id="9d38" class="kr ks hi kn b fi kt ku l kv kw">df_sparse = pd.DataFrame(df_features.todense(), columns=features)</span><span id="67f2" class="kr ks hi kn b fi mh ku l kv kw"># Find number of times each word is seen<br/>feature_counts = np.sum(df_sparse.values, axis=0)<br/>feature_counts_df = pd.DataFrame(dict(features=features,<br/> counts=feature_counts))<br/>feature_counts_df.sort_values(‘counts’, ascending=False)[0:15]</span></pre><p id="1d27" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在终于到了建立模型的时候了！我们将使用高斯朴素贝叶斯，这是一种流行的文本分类算法。尽管它相当简单，但它的表现往往和复杂得多的解决方案一样好。</p><p id="a2bc" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">该模型应用贝叶斯定理，并天真地假设不同特征之间没有关系。根据贝叶斯定理:</p><blockquote class="kf kg kh"><p id="8dfe" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">后验=可能性*命题/证据</p><p id="3cf7" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">或者</p><p id="1ab0" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">P(A|B) = P(B|A) * P(A)/P(B)</p></blockquote><p id="f569" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在拟合之前，将数据集分成<strong class="jl hj">训练和测试子集</strong>。</p><pre class="iy iz ja jb fd km kn ko kp aw kq bi"><span id="9433" class="kr ks hi kn b fi kt ku l kv kw">from sklearn.model_selection import train_test_split<br/>from sklearn.naive_bayes import GaussianNB</span><span id="f186" class="kr ks hi kn b fi mh ku l kv kw">X_train, X_test, y_train, y_test = train_test_split(df_features.toarray(),<br/> df[“Sentiment”],<br/> test_size=0.3,<br/> random_state=42)</span><span id="e9c6" class="kr ks hi kn b fi mh ku l kv kw">nb_clf = GaussianNB()</span><span id="065a" class="kr ks hi kn b fi mh ku l kv kw">nb_clf.fit(X_train, y_train)</span><span id="ea43" class="kr ks hi kn b fi mh ku l kv kw">y_pred = nb_clf.predict(X_test)</span></pre><p id="f022" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们使用来自<strong class="jl hj"> sklearn </strong>库的<strong class="jl hj">度量</strong>模块来评估预测。</p><pre class="iy iz ja jb fd km kn ko kp aw kq bi"><span id="e0f6" class="kr ks hi kn b fi kt ku l kv kw">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>from sklearn.metrics import accuracy_score, classification_report, confusion_matrix</span><span id="f8a2" class="kr ks hi kn b fi mh ku l kv kw">print(classification_report(y_test, y_pred))</span><span id="3601" class="kr ks hi kn b fi mh ku l kv kw">cm = confusion_matrix(y_test, y_pred)</span><span id="8ff1" class="kr ks hi kn b fi mh ku l kv kw">sns.heatmap(cm, annot=True, fmt=’.2f’)<br/>plt.xlabel(“Predicted Class”)<br/>plt.ylabel(“Test Set”)<br/>plt.show()</span><span id="9d16" class="kr ks hi kn b fi mh ku l kv kw">print("Accuracy for the test set: ", accuracy_score(y_test, y_pred))</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="er es mj"><img src="../Images/c9f6ba357422275102fb25fc16993b34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZjOq2ik-xk_oPr6amp8JfQ.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx">Metrics for evaluating the model</figcaption></figure><p id="be9e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">结论:</strong></p><p id="b819" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">本教程向您介绍了使用Python 3中的nltk库的基本情感分析模型。首先，您从nltk的样本tweets构建了一个数据集。然后，您对tweets进行预处理，然后对单词进行归一化。最后，您建立了一个模型，将推文与特定的情绪联系起来。</p><p id="c0f3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">监督学习模型的好坏取决于它的训练数据。为了进一步加强模型，您可以收集更大更好的数据集。你也可以考虑增加更多的类别，比如兴奋和愤怒。在本教程中，通过构建一个基本的模型，您仅仅触及了皮毛。</p></div></div>    
</body>
</html>
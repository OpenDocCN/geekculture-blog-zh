<html>
<head>
<title>Decision Trees | Classification Intuition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树|分类直觉</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/decision-trees-classification-intuition-a593c4eab160?source=collection_archive---------44-----------------------#2021-06-16">https://medium.com/geekculture/decision-trees-classification-intuition-a593c4eab160?source=collection_archive---------44-----------------------#2021-06-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/98e8000a9e1614a6da643cc48aa9afd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8eHu0CN5JUDjygmi7WXSZA@2x.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Decision Tree Algorithm</figcaption></figure><p id="a919" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">今天让我们了解更多关于监督学习算法的知识。首先，我们有线性回归，它是关于预测一个连续的数字变量，逻辑回归可以帮助我们在两个或更多的分类变量之间进行分类。然后，我们今天将学习决策树，它具有执行两种类型任务的能力。</p><p id="a3b7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">决策树</strong>是一个树状模型，用于制定决策及其可能的结果。它是一种监督学习算法，用于回归和分类任务，并且易于实现。这个模型/算法帮助你做决定。</p><p id="7f8e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">决策树通常从单个节点开始，分支成可能的结果。这些结果中的每一个都会导致额外的节点，这些节点又分支成其他的可能性。它给我们一个树状的形状。决策树是一个类似流程图的<strong class="iw hj">结构</strong>，其中:</p><ul class=""><li id="0ebc" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">内部节点(非叶节点)表示对属性的测试。</li><li id="6e35" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">分支代表测试的结果。</li><li id="2b5b" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">叶节点保存一个类标签。</li></ul><h1 id="dc39" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">术语</h1><ul class=""><li id="afc9" class="js jt hi iw b ix le jb lf jf lg jj lh jn li jr jx jy jz ka bi translated"><strong class="iw hj">根节点:</strong>该节点是树中最高的节点，没有父节点。</li><li id="48a3" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hj">拆分:</strong>将一个节点分成两个或更多子节点的过程称为拆分。</li><li id="be45" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hj">父节点和子节点:</strong>被分成子节点的节点被称为子节点的父节点，而子节点是父节点的子节点。</li><li id="e1ee" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hj">分支/子树:</strong>整个树的一个子部分称为分支或子树。</li><li id="cfa6" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hj">决策节点:</strong>将一个子节点拆分成更多的子节点称为决策节点。</li><li id="db40" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hj">终端节点:</strong>不分裂的节点称为叶/终端/末端节点。</li></ul><h1 id="604a" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">直觉</h1><p id="d6a4" class="pw-post-body-paragraph iu iv hi iw b ix le iz ja jb lf jd je jf lj jh ji jj lk jl jm jn ll jp jq jr hb bi translated">决策树在构造树时遵循不同类型的算法。在这篇博客中，我们将讨论ID3算法。我们的目标是快速到达终端节点。该算法断言，我们应该在第一步中选择合适的属性来分割决策树。现在，为了选择第一个合适的属性来分割决策树，熵就出现了。<strong class="iw hj">熵(H) </strong>帮助我们测量分裂的纯度。为了快速到达叶节点，我们必须选择合适的参数。分裂继续下去，直到我们得到一个纯粹的子集(要么纯粹是，要么纯粹不是)。更深意味着更多的时间消耗。熵的值介于0和1之间。熵越高，就越难从这些信息中得出任何结论。熵为零的分支是叶节点，熵大于零的分支需要进一步分裂。</p><div class="lm ln lo lp fd ab cb"><figure class="lq ij lr ls lt lu lv paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/cedb29dfc9ea9d4ecf19a68ee4e0adfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*uvkExa1_Ytbq7Ged4K89SQ@2x.jpeg"/></div></figure><figure class="lq ij lr ls lt lu lv paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/b82b64bcad4e992f74685c9495e44ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*tXqaor7p9WpYK4Gtu9wnVA@2x.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx lw di lx ly">Entropy</figcaption></figure></div><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/719fa2ffba0df90471cedbcdff3af2af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rtQzmufVPkb8h201g4UF9w@2x.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Information Gain</figcaption></figure><p id="6f88" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们如何确定熵在减少或很低？下面来说说<strong class="iw hj">信息增益(IG) </strong>的话题。它是一种统计属性，用于衡量给定属性根据目标分类将训练示例分开的程度。它计算所有熵的平均值，并且是熵的减少。当分裂决策树时，它将采用具有较高信息增益的特定分裂。</p><p id="17e3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">所以算法如下:</p><ol class=""><li id="e5d5" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr lz jy jz ka bi translated">它以原始集合S作为根节点开始，即以完整的训练数据集作为其根节点。</li><li id="e0d5" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr lz jy jz ka bi translated">该算法的每次迭代遍历集合S的非常未使用的属性，并计算<strong class="iw hj">该属性的熵(H) </strong>和<strong class="iw hj">信息增益(IG) </strong>。</li><li id="55f2" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr lz jy jz ka bi translated">然后它选择具有最小熵和最大信息增益的属性。</li><li id="a04d" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr lz jy jz ka bi translated">然后，选定的属性对集合进行分割，以产生数据的子集。</li><li id="3bbc" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr lz jy jz ka bi translated">该算法继续在每个子集上重复，直到它到达叶节点。</li></ol><h1 id="1bb6" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">假设</h1><ul class=""><li id="5da6" class="js jt hi iw b ix le jb lf jf lg jj lh jn li jr jx jy jz ka bi translated">一开始，整个训练集被认为是<strong class="iw hj">根。</strong></li><li id="59ad" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">特征值最好是分类的。如果这些值是连续的，则在构建模型之前会将其离散化。</li><li id="0f42" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">记录是基于属性值递归分布的。</li><li id="407f" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">使用统计方法完成将属性放置为树的根或内部节点的顺序。</li></ul><h1 id="15ba" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">优势</h1><ul class=""><li id="5996" class="js jt hi iw b ix le jb lf jf lg jj lh jn li jr jx jy jz ka bi translated">易于理解和实施。</li><li id="5f96" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">需要更少的数据清理。</li><li id="559d" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">它可以处理回归和分类问题。</li></ul><h1 id="4823" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">不足之处</h1><ul class=""><li id="e8a5" class="js jt hi iw b ix le jb lf jf lg jj lh jn li jr jx jy jz ka bi translated">过度拟合:我们正在训练我们的模型，以获得良好的训练数据准确性，但每当新的测试数据出现时，它都不会表现得很好。</li></ul><p id="e023" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这篇博客谈到了决策树中熵和信息增益背后的直觉。在接下来的博客中，我们将了解比熵更好的基尼系数、决策树剪枝、回归方法以及脏数据对决策树的影响。</p></div></div>    
</body>
</html>
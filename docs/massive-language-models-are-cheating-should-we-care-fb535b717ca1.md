# 海量语言模型作弊。我们应该关心吗？

> 原文：<https://medium.com/geekculture/massive-language-models-are-cheating-should-we-care-fb535b717ca1?source=collection_archive---------7----------------------->

![](img/609b323c69c4917cd7ad124ca17ea99a.png)

Photo by [Possessed Photography](https://unsplash.com/@possessedphotography?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

人工智能的世界最近似乎分成了两个阵营:顽固分子和开沟者。

## 顽固分子

充满希望的阵营包括 OpenAI 的首席科学家 Ilya Sutskever 和特斯拉的 AI 高级总监 Andre Karpathy。他们认为当今最好的语言模型的一些能力预示着更强大的涌现特性。两位研究者甚至认为[轻描淡写地暗示](https://futurism.com/the-byte/openai-already-sentient)这些模型可能有一些意识的暗示。对于这一点，也许最反乌托邦和最引人注目的观点是，从有意识语言模型的角度来看卡帕西的[短篇小说](https://karpathy.github.io/2021/03/27/forward-pass/)(相关:[人工智能时刻](https://towardsdatascience.com/the-ai-moment-12761189b87d))。

## 挖沟机

但是还有另一个更严峻的观点也有一些吸引力。开沟者不仅说模特们没有意识(他们真的是为了他的利益而去追求苏斯克弗)，有些人还说他们永远不可能成为。人们持有这种观点有很多原因。以我的经验来看，这些想法很棘手，因为它们中的大部分一开始都有道理，但它们的结论最终是见仁见智的问题。

以最著名的中国房间实验为例。它的基本意思是，如果一个说英语的人拿着一本英汉词典走进一个房间，她可以让外面给她写信的人相信她说中文。论点是，说英语的人可以模拟语言理解，而实际上并没有。由此可见，ML 模型可以学习规则来模拟意识，而实际上并没有意识。但说到底，即使房间里的人不懂中文，房间(包含人和字典)是否可以说懂中文，这是个人看法的问题。

为了看到这一点，想象你认识的每个人的大脑都居住着只会说一种外星语言的小外星人。他们在和你说话之前把他们的想法翻译成英语。你会怀疑你认识的每个人都懂英语吗？即使在发现外星人之后？在我看来，这是个人观点。

## 模特在作弊

当我读到亚历克斯·塔姆金的这个帖子时，我第一次有了抛弃卡帕西和他的朋友成为悲观主义者的冲动，亚历克斯·塔姆金是斯坦福大学的一名研究员，他曾经指导过我一段时间。在这篇文章中，他简要解释了为什么 GPT 风格模型只是很好地记忆了训练数据，而不是学习可概括的模式。从表面上看，这似乎打破了意识的杂音。如果一个模型只是在正确的时间记住正确的事情，它怎么可能是意识呢？

但是，当然，我们不都只是在正确的时间记住了正确的事情吗？想想你说你以前听过的短语到底有多少。Tamkin 引用的一篇论文暗示，一个模型的训练数据中算术表达式的频率可以预测该模型使用这些表达式的准确度。另一个神经系统也是如此，那就是我的小表妹伊莎贝拉，她在练习乘法表并收到反馈后，能够持续地做得更好。电影制作人吉姆·贾姆什说得好:

> “没有什么是原创的。从任何能激发灵感或激发你想象力的地方偷东西。吞噬旧电影、新电影、音乐、书籍、绘画、照片、诗歌、梦想、随意的对话、建筑、桥梁、街道标志、树木、云彩、水体、光和阴影。只选择那些直接和你的灵魂对话的东西来偷。如果你这样做，你的作品(和盗窃)将是真实的。真实性是无价的；原创是不存在的。”

就像中国房间实验一样，你是否会称一个高级记忆重现者为有意识的似乎是个人观点的问题。这种对相关文本的筛选和选择真的是思想的起源吗？在这种情况下，也许我们脑海中闪现的想法只不过是翻阅我们自己的经历和经历。
<html>
<head>
<title>Deep Learning Part 2: Vanilla vs Stochastic Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习第2部分:香草vs随机梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/deep-learning-part-2-vanilla-vs-stochastic-gradient-descent-6bcecc26fd51?source=collection_archive---------8-----------------------#2021-08-16">https://medium.com/geekculture/deep-learning-part-2-vanilla-vs-stochastic-gradient-descent-6bcecc26fd51?source=collection_archive---------8-----------------------#2021-08-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/a50d8d6083a4f16c906b0c66b50212a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qc9mr_xULZMNYBhVdnFcyQ.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="fdcb" class="pw-subtitle-paragraph iq hs ht bd b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh dx translated">普通梯度下降是如何工作的，为什么它的效率低到需要随机梯度下降的地步</h2></div><p id="4f7c" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">欢迎来到我关于深度学习的介绍性系列的第2部分，我们的目标是让你熟悉基本的DL概念。请参考底部的D <em class="ke"> eep学习系列</em>部分，了解之前的所有文章。在这篇文章中，我们讨论了香草和随机梯度下降(SGD)。</p><p id="4219" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">梯度下降(GD)是在训练机器学习系统和神经网络时使用最广泛的学习算法之一。这是计算机智能世界中的一个基本概念，因此，也是传授给新的人工智能学生的第一个概念。</p><p id="86b4" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">梯度下降并不是什么新概念。事实上，它早在1807年就由法国数学家奥古斯丁·路易·柯西首次提出。因此，描述和分析算法的工作量是没有限制的。因此，再写一篇关于GD如何工作的文章没有多大价值，因为这是重复许多其他作者一遍又一遍描述的内容。</p><p id="63c3" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">然而，一个不经常详细讨论的想法是基本(普通)梯度下降算法的低效率，以及随机梯度下降如何帮助解决它们。这就是我们在本文中的目标。</p><p id="fcc2" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">如果你还不完全熟悉优化算法，我们将从简单描述开始。然后，我们将描述普通梯度下降和随机梯度下降之间的差异，最后，通过在神经网络上使用它们并比较不同的性能指标，对两者进行比较分析。</p><p id="0ca3" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">让我们开始吧。</p><h1 id="7c64" class="kf kg ht bd kh ki kj kk kl km kn ko kp iz kq ja kr jc ks jd kt jf ku jg kv kw bi translated">梯度下降</h1><h2 id="1311" class="kx kg ht bd kh ky kz la kl lb lc ld kp jr le lf kr jv lg lh kt jz li lj kv lk bi translated">算法</h2><p id="e71a" class="pw-post-body-paragraph ji jj ht jk b jl ll iu jn jo lm ix jq jr ln jt ju jv lo jx jy jz lp kb kc kd hb bi translated">在理解vanilla和随机梯度下降的区别之前，让我们先看看vanilla GD是如何工作的，并从那里看到我们对随机GD做了什么改变。</p><p id="73d9" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">如前所述，梯度下降是一种<em class="ke">学习</em>算法。但是，它学的是什么？梯度下降也被称为<em class="ke">优化</em>算法。但是，它在优化什么呢？先回答第一个问题。</p><p id="3f0e" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">机器学习领域背后的整个想法是，我们的算法可以“从经验中”自主学习。以下面的神经网络为例。该网络将28x28灰度图像的784个像素作为输入，并输出写在图像上的1到9之间的数字:</p><figure class="lr ls lt lu fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lq"><img src="../Images/f6f5cd56fab128952a968b51c9be2fce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Of-VAamW_gp35l4B6K-QDQ.jpeg"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Figure 1:</strong> Example Neural Network for Digit Recognition</figcaption></figure><p id="c94b" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">输入层和隐藏层之间有<code class="du lz ma mb mc b">3 * 784 = 2352</code>个权重，隐藏层和输出层之间有<code class="du lz ma mb mc b">3 * 15 = 45</code>个权重，总共2397个权重。更不用说偏见了。我们要手动选择这些权重和偏差吗？然后，如果它们没有按照我们想要的方式执行，就手动更新它们？当然不是。相反，我们将使用梯度下降来<em class="ke">学习</em>这些权重。但是怎么做呢？“如何”是我们提出的第二个问题的答案。</p><p id="8907" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我们的目标是优化神经网络产生的误差。在我们的例子中，优化指的是最小化误差。</p><p id="1f35" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">梯度下降会通过例子学习。我们将输入我们的网络图像，这些图像中的数字是已知的，我们将通过比较输出和实际结果来看看我们的神经网络是否能够正确地识别它。然后，我们分析这个误差有多大，并尝试相应地调整我们的权重，以尽量减少产生的误差。我们用来计算我们的学习算法的误差的函数被称为<strong class="jk hu">成本函数</strong>。</p><p id="6aa5" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">有许多不同的成本函数可用。在我们的例子中，我们将使用均方误差(MSE):</p><figure class="lr ls lt lu fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es md"><img src="../Images/adef24f9b42580e891b37611c14b3155.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8HeKZ9cpH8q-meG7MKs1lg.jpeg"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Equation 1: </strong>Mean Squared Error</figcaption></figure><p id="6c15" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">其中<code class="du lz ma mb mc b">J</code>代表成本函数，<code class="du lz ma mb mc b">w</code>和<code class="du lz ma mb mc b">b</code>是我们的模型的权重和偏差的向量，<code class="du lz ma mb mc b">n</code>是我们拥有的训练样本的数量，<code class="du lz ma mb mc b">y_i</code>是我们输入的实际结果，<code class="du lz ma mb mc b">z</code>是在<a class="ae me" href="https://ali-h-khanafer.medium.com/deep-learning-meaning-motivation-and-nn-basic-structure-44b57b481e4c?source=your_stories_page-------------------------------------" rel="noopener">第一部分</a>的<strong class="jk hu">等式1 </strong>中呈现的加权和，即我们的模型预测的结果。</p><p id="7161" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">请注意，该错误是基于所有训练示例的平均结果。记住这一点，因为当讨论随机梯度下降和普通梯度下降之间的差异时，它还会回来。</p><p id="74ca" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">目标是最小化<code class="du lz ma mb mc b">J</code>。也就是说，找到产生最小误差的<code class="du lz ma mb mc b">w</code>和<code class="du lz ma mb mc b">b</code>:</p><figure class="lr ls lt lu fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mf"><img src="../Images/ca54491d34c413c61cc31d988db56122.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IRUOUpj0HBaeGkuoJjBIhQ.jpeg"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Figure 2:</strong> Minimizing Cost Function</figcaption></figure><p id="a77a" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">理解如何最小化<code class="du lz ma mb mc b">J</code>的最好方法是看一下向量<code class="du lz ma mb mc b">w</code>的大小为1(<code class="du lz ma mb mc b">[w_1]</code>)的情况，我们没有偏差。我们可以将成本<code class="du lz ma mb mc b">J(w)</code>绘制成<code class="du lz ma mb mc b">w_1</code>的函数:</p><figure class="lr ls lt lu fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mg"><img src="../Images/88bba2684f92636e8de1ad920fbb79b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AO-t5J_xP2KtKJPg9BO3zw.jpeg"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Figure 3: </strong>J as a Function of w_1</figcaption></figure><p id="8574" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">从视觉上，很容易看出什么<code class="du lz ma mb mc b">w_1</code>会最小化<code class="du lz ma mb mc b">J</code>。但是我们如何用代数方法来做呢？一些微积分学生可能会考虑求解<code class="du lz ma mb mc b">dJ/dw_1 = 0</code>，但是对于更复杂的函数来说，这并不总是可行的[2]，例如:</p><figure class="lr ls lt lu fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mh"><img src="../Images/1390678320263c9b5c624198eb98f27e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9BhQEoQkbD6Iydq2zyg7IQ.jpeg"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Figure 4: </strong>Example of More Complicated Cost Function</figcaption></figure><p id="55e3" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">在上面的数字识别问题中，事情变得特别复杂，我们有超过2000个权重和一个超过2000维的函数。</p><p id="576a" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">更好的方法是从一个随机的<code class="du lz ma mb mc b">w_1</code>值开始，然后确定步进的方向，以获得一个较低的结果。我们可以通过找到<code class="du lz ma mb mc b">w_1</code>处<code class="du lz ma mb mc b">J(w)</code>的斜率来做到这一点。考虑图3 中的一个随机点:</p><figure class="lr ls lt lu fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mi"><img src="../Images/de55718f5da5b764fb02141fb343a586.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YQ_cO1PovoP6__BuRg96GA.jpeg"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Figure 5: </strong>Random Point On J vs w_1 Graph</figcaption></figure><p id="2b20" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我们可以计算该点的斜率:</p><figure class="lr ls lt lu fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mj"><img src="../Images/589455ecd3c154e6f2179b84ffc57171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*33G50CqxnppTLMXU6ugeGQ.jpeg"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Figure 6: </strong>Slope of<strong class="bd kh"> </strong>Random Point On J vs w_1 Graph</figcaption></figure><p id="f72d" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">因为它是负的，我们知道我们需要向右移动<code class="du lz ma mb mc b">w_1</code>，即增加<code class="du lz ma mb mc b">w_1</code>。如果是正数，我们就向左移动。我们可以继续这样做，直到我们达到接近零的斜率，这表明我们已经达到了全局或局部最小值:</p><figure class="lr ls lt lu fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mk"><img src="../Images/28020a621576793a3f91fdad70d7f287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bDEh1PnnVLPSszbJTujcEQ.jpeg"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Figure 7: </strong>Different Slopes Before Reaching Minimum</figcaption></figure><p id="95b1" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我们可以将这个想法扩展到多变量成本函数<code class="du lz ma mb mc b">J</code>，就像我们在<strong class="jk hu">等式1 </strong>中一样，其中<code class="du lz ma mb mc b">J</code>是<code class="du lz ma mb mc b">w</code>和<code class="du lz ma mb mc b">b</code>的函数。正如我们在单变量情况下计算斜率一样，在多变量情况下，我们将计算<strong class="jk hu">梯度</strong> (∇J)，一个给我们最陡<strong class="jk hu">上升</strong>方向的向量。直觉上，为了找到最陡下降的方向，我们取梯度-∇J.的负值</p><p id="d96a" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">如果您理解了我刚才描述的所有内容，那么算法的其余部分就不言自明了:</p><ol class=""><li id="acc8" class="ml mm ht jk b jl jm jo jp jr mn jv mo jz mp kd mq mr ms mt bi translated">随机初始化权重和偏差</li><li id="9245" class="ml mm ht jk b jl mu jo mv jr mw jv mx jz my kd mq mr ms mt bi translated">计算这些权重和偏差造成的成本</li><li id="a4bf" class="ml mm ht jk b jl mu jo mv jr mw jv mx jz my kd mq mr ms mt bi translated">基于更新规则更新所有权重<code class="du lz ma mb mc b">w_k</code>和偏差<code class="du lz ma mb mc b">b_l</code>，其中<code class="du lz ma mb mc b">k</code>是权重的数量，<code class="du lz ma mb mc b">l</code>是偏差的数量:</li></ol><figure class="lr ls lt lu fd hk er es paragraph-image"><div class="er es mz"><img src="../Images/b0c41976e3db016a80833fcaf38d45d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*8bJ5CfIuj55GbSY5ERGpAA.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Figure 7: </strong>Update The Weights and Biases Rule [3]</figcaption></figure><p id="76a3" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">4.从步骤2重新开始，直到算法收敛到局部或全局最小值</p><p id="b08b" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">η被称为<strong class="jk hu">学习率</strong>。我不会在这篇文章中解释它，将它留给你自己的研究。</p><h2 id="4be3" class="kx kg ht bd kh ky kz la kl lb lc ld kp jr le lf kr jv lg lh kt jz li lj kv lk bi translated">问题:效率低下</h2><p id="e16b" class="pw-post-body-paragraph ji jj ht jk b jl ll iu jn jo lm ix jq jr ln jt ju jv lo jx jy jz lp kb kc kd hb bi translated">那么问题出在哪里呢？为什么这样效率低？为什么我们在随机梯度下降中需要一个变量？为了回答这些问题，让我们暂时离开神经网络，考虑这样一种情况:我们希望最小化线性回归模型的均方误差。由于我们在处理线性回归时不需要链式法则来计算偏导数，这个假设会让我们更容易继续GD的解释。计算神经网络的偏导数需要一种称为<strong class="jk hu">反向传播</strong>的算法，这是下周文章的主题。<strong class="jk hu">方程1 </strong>的偏导数为:</p><figure class="lr ls lt lu fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es na"><img src="../Images/fd7c2e86c6506ccf561273fc05fe38df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dlQu_-XOxXl-Aeyq6pGWbw.jpeg"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Equation 2: </strong>Gradient of the Mean Squared Error Function</figcaption></figure><p id="c2a1" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">注意，在上面的等式中，我们将<code class="du lz ma mb mc b">z</code>展开为它的完整形式。偏差的梯度将是相同的，除了我们用<code class="du lz ma mb mc b">b_l</code>替换<strong class="jk hu">等式2 </strong>中的所有<code class="du lz ma mb mc b">w_k</code>。</p><p id="442a" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">你看出问题了吗？假设我们有20，000个权重和偏差，以及<code class="du lz ma mb mc b">n = 10,000</code>个训练示例。对于我们想要对某些<code class="du lz ma mb mc b">w_k</code>进行的每一次更新，即上述GD算法中的步骤3，我们必须对所有<code class="du lz ma mb mc b">n</code>训练示例进行总结。每次运行第3步时，我们都会看到<code class="du lz ma mb mc b">20000*10000=200000000 operations</code>。在算法收敛之前，乘以我们运行这个更新步骤的次数…这是一个很大的操作。这就是基本(普通)GD算法效率低下的原因。尽管神经网络的偏导数会有所不同，但这种对所有训练示例求和的思想仍然成立。</p><h2 id="4173" class="kx kg ht bd kh ky kz la kl lb lc ld kp jr le lf kr jv lg lh kt jz li lj kv lk bi translated">解决方案:随机梯度下降</h2><p id="fa73" class="pw-post-body-paragraph ji jj ht jk b jl ll iu jn jo lm ix jq jr ln jt ju jv lo jx jy jz lp kb kc kd hb bi translated">我们可以把随机梯度下降和普通梯度下降的区别分别看作瀑布和敏捷设计的区别。在瀑布设计过程中，我们试图从第一次迭代开始就把一切都做好。这是一个线性过程，包括需求收集、设计、实现、验证和维护。在进入下一步之前，必须完成每一步，我们永远不能回到上一步。最终结果？您到达过程的末尾，并注意到您的项目充满了bug，您的客户的需求已经完全改变了，并且您在过去六个月中投入的所有工作都不是很好。另一方面，敏捷方法涉及较小的迭代，不断地部署产品，以便获得客户反馈，进行适当的更改，重新部署和重复流程，所有这些都在一两周内完成。</p><p id="b282" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">在随机梯度下降中，我们从整个数据集中选择<code class="du lz ma mb mc b">m</code>个随机训练样本，并将其用于我们的成本函数，而不是将误差计算为所有<strong class="jk hu">个训练样本的平均值。我们称这个子集为<strong class="jk hu">小批量</strong>。一旦我们的网络已经在我们的小批量中的所有数据点上被训练，我们选择一个新的随机点子集，并用它来训练我们的模型。我们继续这个过程，直到我们用尽所有的训练点，在这一点上，我们已经完成了一个<strong class="jk hu">纪元</strong>。然后，我们从新的<strong class="jk hu">时期</strong>开始，并继续直到收敛。</strong></p><p id="6d4c" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">更正式地说，<strong class="jk hu">等式2 </strong>变成:</p><figure class="lr ls lt lu fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nb"><img src="../Images/7ef5ed1b82ef3d3e7270c8c60b61ef78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0dpTUjYWXuSHKAgdNalacg.jpeg"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Equation 3: </strong>Stochastic Gradient Descent Cost Function</figcaption></figure><p id="c6c3" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">其中<code class="du lz ma mb mc b">m</code>是小批量的大小。如果<code class="du lz ma mb mc b">m</code>足够大，这行得通是因为[3]:</p><figure class="lr ls lt lu fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nc"><img src="../Images/63bcf5b60b254b99b4e95c482284530d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w7box4LhZXFYVWMEgn2PPA.jpeg"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Equation 4: </strong>Mini-Batches Compared to Full Batch</figcaption></figure><h1 id="6364" class="kf kg ht bd kh ki kj kk kl km kn ko kp iz kq ja kr jc ks jd kt jf ku jg kv kw bi translated">比较</h1><p id="99da" class="pw-post-body-paragraph ji jj ht jk b jl ll iu jn jo lm ix jq jr ln jt ju jv lo jx jy jz lp kb kc kd hb bi translated">那么新加坡元和基本元相比如何呢？在本节中，我们将通过运行分析和查看一些指标来尝试回答这个问题。更具体地说，我们将看看它们在收敛时间、CPU利用率和不同数据集大小的准确性方面的比较。</p><p id="8227" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我们将使用迈克尔·尼尔森在他的<a class="ae me" href="http://neuralnetworksanddeeplearning.com/" rel="noopener ugc nofollow" target="_blank">在线介绍性深度学习书籍</a>的第一章中创建的数字识别神经网络。该网络将具有28×28 = 784个输入，一个具有30个神经元的隐藏层，以及一个具有10个输出神经元的输出层。可以在<code class="du lz ma mb mc b">src/network.py</code>下的处找到代码<a class="ae me" href="https://github.com/mnielsen/neural-networks-and-deep-learning" rel="noopener ugc nofollow" target="_blank">。我们将使用的数据集是</a><a class="ae me" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">MNIST手写数字数据库</a>，它包含60，000个手写数字。</p><p id="aef0" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">正如我们将看到的，当与神经网络一起使用时，基本梯度下降可能需要很长时间才能收敛。有时超过24小时。因此，我们将通过运行SGD 200个时期和小批量的训练集来模拟梯度下降。然后，我们将使用收到的指标来估计梯度下降的实际结果，如果我们要运行它来完成。对于香草梯度下降的完整模拟，我们将不得不运行SGD最少的<code class="du lz ma mb mc b">(784*30) + (30*10) = 23820</code>时期，但这将需要很长时间才能结束。按照我们现在的方式做事会给我们带来不太准确的结果，但会节省我们很多时间，并让我们很好地了解SGD和vanilla GD之间的差异。</p><p id="d367" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">至于纪元、小批量和学习率，我们将分别保持它们为常数30、10和3。</p><h2 id="11ae" class="kx kg ht bd kh ky kz la kl lb lc ld kp jr le lf kr jv lg lh kt jz li lj kv lk bi translated">随机梯度下降</h2><p id="f8d0" class="pw-post-body-paragraph ji jj ht jk b jl ll iu jn jo lm ix jq jr ln jt ju jv lo jx jy jz lp kb kc kd hb bi translated">我们先来看看SGD表现如何。</p><figure class="lr ls lt lu fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nd"><img src="../Images/2f0a1930f50846fef68867588d25182e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rCVDKAVaDbYZG7B--1hr7Q.png"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Table 1: </strong>Stochastic Gradient Descent Results</figcaption></figure><figure class="lr ls lt lu fd hk er es paragraph-image"><div class="er es ne"><img src="../Images/b190198586cd4e553bd673d06251a384.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*uftwFbnffLo0jTL6QAMOGg.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Figure 9: </strong>Training Set Size vs Time to Completion (SGD)</figcaption></figure><p id="dae1" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">直观上，时间随着训练集规模的增大而线性增加。数据集大小为50，000时，大约需要两分钟半(148秒)才能完成。请记住，这是与常数超参数(学习率，小批量，等等。)我们选择了。我们可以尝试调整这些值，也许可以得到一个稍微快一点的算法。这条线的斜率大约为0.0031，每增加10，000个训练点，我们的时间大约增加30秒。</p><p id="fa0f" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">现在让我们看看它的CPU利用率:</p><figure class="lr ls lt lu fd hk er es paragraph-image"><div class="er es ne"><img src="../Images/64c104026a7e01dae01ca2aab9219585.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*DRNZ0jKn_EgEUhBwor8sxg.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Figure 10: </strong>Training Set Size vs CPU Utilization (SGD)</figcaption></figure><p id="a42d" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我们正在使用的Mac的CPU利用率通常在2%到10%之间。我们从<strong class="jk hu">图10 </strong>中看到，CPU利用率基本保持在50%左右，数据集大小之间只有两到三个百分点的变化。这些结果符合我们的预期吗？不完全是。正如我们将在后面看到的，vanilla GD将有类似的CPU利用率结果。但是SGD的全部意义在于它更有效，所以这些结果与我们的假设相反。发生这种情况有多种原因:</p><ol class=""><li id="0717" class="ml mm ht jk b jl jm jo jp jr mn jv mo jz mp kd mq mr ms mt bi translated">我用的是比较老的电脑(MacBook Pro 2016)</li><li id="9066" class="ml mm ht jk b jl mu jo mv jr mw jv mx jz my kd mq mr ms mt bi translated">我没有运行香草动力局完成</li><li id="eadf" class="ml mm ht jk b jl mu jo mv jr mw jv mx jz my kd mq mr ms mt bi translated">计算CPU利用率时的人为错误</li></ol><p id="b051" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">因此，我们已经确认了该算法是快速的，并且它的CPU利用率不是太高。但是，最重要的是，它有多准确？</p><figure class="lr ls lt lu fd hk er es paragraph-image"><div class="er es ne"><img src="../Images/06f4d9819b6b0ef51c6af7eceeb789f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*WURYENSNzFwRIqEWxK1jug.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Figure 11: </strong>Training Set Size vs CPU Utilization (SGD)</figcaption></figure><p id="3863" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">厉害！通过一个只有10，000个数据点的训练集，我们能够在30多秒内获得92%的准确率。另一方面，如果我们希望更准确，我们可以将训练集大小增加到50，000，并在两分钟半内获得95%的准确率。所有这些都没有超过50%的CPU利用率。</p><h2 id="81e9" class="kx kg ht bd kh ky kz la kl lb lc ld kp jr le lf kr jv lg lh kt jz li lj kv lk bi translated">香草梯度下降</h2><p id="9cc1" class="pw-post-body-paragraph ji jj ht jk b jl ll iu jn jo lm ix jq jr ln jt ju jv lo jx jy jz lp kb kc kd hb bi translated">现在让我们看看香草梯度下降如何在相同的精确训练数据和神经网络上执行。</p><figure class="lr ls lt lu fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nf"><img src="../Images/8c35313bd7370beea32d17ea3ab937a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A6CKLEAujNqrJe6N3XAfgw.png"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Table 2: </strong>Vanilla Gradient Descent Results</figcaption></figure><p id="9b92" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">为了估计总时间，我们计算了完成一个历元所需的时间，然后乘以如果我们要运行完成所需的最小历元数(23820个历元)。</p><figure class="lr ls lt lu fd hk er es paragraph-image"><div class="er es ne"><img src="../Images/bbfee26241936cef8f66bf0f57475b5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*E4NzqzIPYQ4kcxCgIyV8Qg.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Figure 12: </strong>Training Size vs Estimated Time to Completion (GD)</figcaption></figure><p id="c585" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">现在，你明白为什么我们需要SGD来训练神经网络了吗？对于只有10，000人的训练集，我们需要大约3个小时来训练我们的模型。将这个数字与现实世界的场景进行比较，在现实世界中，通常有数百万个数据点来训练我们的神经网络。请注意，斜率约为1.19，相比之下，使用SGD时的斜率为0.0031。每增加10，000个数据点，完成时间的变化几乎相差200%。</p><p id="7818" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">这个算法计算量大吗？让我们看看:</p><figure class="lr ls lt lu fd hk er es paragraph-image"><div class="er es ne"><img src="../Images/de83290ec74418ed386d275481e4aef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*2gCuRTXNfvlkIprue1Bp8g.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Figure 13: </strong>Training Set Size vs CPU Utilization (GD)</figcaption></figure><p id="4c9a" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">虽然利用率的波动比SGD稍大，但结果非常相似。同样，如上所述，这可能是由许多不同的原因造成的。</p><p id="f724" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">最后，让我们看看GD如何训练我们的模型:</p><figure class="lr ls lt lu fd hk er es paragraph-image"><div class="er es ng"><img src="../Images/6145a80c7e115c271a27248e583f684b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*m90EdIcXOXUBTZt8gXGTsA.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx"><strong class="bd kh">Figure 14: </strong>Training Set Size vs CPU Utilization (GD)</figcaption></figure><p id="8098" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">这些结果完全有意义吗？<strong class="jk hu">不。</strong>我们应该得到足够接近SGD接收到的精度。但是我们预料到了这一点，因为我们没有运行完算法。重要的是要认识到梯度下降最终会收敛到局部最小值，产生与我们在SGD中看到的一样好的结果。</p><p id="de66" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">然而，要认识到的一个有趣的特征是，随着训练集大小的增加，准确率会降低。为什么会这样？一个合理的答案是我们的学习率太高/太低。这也可能是我们通过SGD模拟梯度下降的方式。有关系吗？不完全是。我们所要做的就是了解SGD的表现有多好。在跑了211秒的基本梯度下降后，我们用10，000的训练集能够获得的最高精度是61%。对于相同数量的数据，更少的时间，我们能够用SGD获得92%的准确率。</p><h1 id="82ac" class="kf kg ht bd kh ki kj kk kl km kn ko kp iz kq ja kr jc ks jd kt jf ku jg kv kw bi translated">结论</h1><p id="e7e8" class="pw-post-body-paragraph ji jj ht jk b jl ll iu jn jo lm ix jq jr ln jt ju jv lo jx jy jz lp kb kc kd hb bi translated">在这篇文章中，我们的目的是回答一些围绕香草和随机梯度下降的重要问题。</p><p id="58ee" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">通过首先描述梯度下降的基本功能，我们能够指出一个非常大的问题:为了计算我们的成本函数在某个点<code class="du lz ma mb mc b">w</code>的梯度，需要计算所有训练点的总和。这给我们留下了一个计算量极大的算法。有多重？本文的第二部分探讨了这种低效率，旨在确定随机梯度下降的性能有多好。</p><p id="8755" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">虽然我们采取了一些捷径来完成基本梯度下降，但我们能够得出结论，随机梯度下降不仅更快，而且在更短的时间内提供了更好的结果。</p><p id="c834" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">但是基本梯度下降<strong class="jk hu">永远不会使用</strong>吗？不完全是。还有其他机器学习算法不像神经网络那样需要那么多求和，比如线性回归。在这种情况下，避免随机梯度下降可能更好，因为代码更复杂，并且有更多的参数要处理。</p><h1 id="37cb" class="kf kg ht bd kh ki kj kk kl km kn ko kp iz kq ja kr jc ks jd kt jf ku jg kv kw bi translated">深度学习系列</h1><ul class=""><li id="c828" class="ml mm ht jk b jl ll jo lm jr nh jv ni jz nj kd nk mr ms mt bi translated"><strong class="jk hu">第一部分:</strong> <a class="ae me" href="https://ali-h-khanafer.medium.com/deep-learning-meaning-motivation-and-nn-basic-structure-44b57b481e4c?source=your_stories_page-------------------------------------" rel="noopener">深度学习:意义、动机、NN基本结构</a></li></ul><h1 id="f341" class="kf kg ht bd kh ki kj kk kl km kn ko kp iz kq ja kr jc ks jd kt jf ku jg kv kw bi translated">参考</h1><p id="edb1" class="pw-post-body-paragraph ji jj ht jk b jl ll iu jn jo lm ix jq jr ln jt ju jv lo jx jy jz lp kb kc kd hb bi translated">[1]维基百科，<a class="ae me" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a> (2020)，维基百科关于梯度下降的部分</p><p id="9563" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">[2]格兰特·桑德森，<a class="ae me" href="https://www.youtube.com/watch?v=IHZwWFHWa-w&amp;t=454s&amp;ab_channel=3Blue1Brown" rel="noopener ugc nofollow" target="_blank">梯度下降，神经网络如何学习|第二章，深度学习</a> (2017)，3Blue1Brown Youtube频道</p><p id="ff79" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">[3]迈克尔·a·尼尔森，<a class="ae me" href="http://neuralnetworksanddeeplearning.com/" rel="noopener ugc nofollow" target="_blank">神经网络与深度学习</a> (2015)，决心出版社</p></div></div>    
</body>
</html>
<html>
<head>
<title>Sentence Correction using Recurrent Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用递归神经网络的句子校正</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/sentence-correction-using-recurrent-neural-networks-3779b4ed0acc?source=collection_archive---------25-----------------------#2021-09-01">https://medium.com/geekculture/sentence-correction-using-recurrent-neural-networks-3779b4ed0acc?source=collection_archive---------25-----------------------#2021-09-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es et"><img src="../Images/0e21d6ae4577c0740d899e8ffe558241.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pHROhZhqFu2xcCgt.jpg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">image source:<a class="ae it" href="https://www.shutterstock.com/image-vector/creative-colorful-sentence-text-design-written-1828761893" rel="noopener ugc nofollow" target="_blank">https://www.shutterstock.com/image-vector/creative-colorful-sentence-text-design-written-1828761893</a></figcaption></figure><h1 id="98c9" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">目录<strong class="ak"> : </strong></h1><ul class=""><li id="ce6e" class="js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">介绍</li><li id="9dac" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">商业问题</li><li id="58f2" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">映射到深度学习问题</li><li id="13cf" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">数据解释</li><li id="dbc1" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">自然语言处理的数据扩充</li><li id="e7cc" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">探索性数据分析</li><li id="fa50" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">预处理</li><li id="c96f" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">标记化和填充</li><li id="e8c3" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">深度学习模型</li><li id="e262" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">波束搜索解码器和推理装置</li><li id="c16c" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">最终管道</li></ul></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><h1 id="23ad" class="iu iv hi bd iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr bi translated">简介:</h1><p id="4e1a" class="pw-post-body-paragraph lb lc hi ju b jv jw ld le jx jy lf lg jz lh li lj kb lk ll lm kd ln lo lp kf hb bi translated">本文提出了一种有效的预处理方法来改变文本数据，使其更接近标准英语，这将提高最新的自然语言处理模型的性能。我们经常会遇到不符合标准英语的规则、措辞和语义的文本数据。文本数据的预处理在提高自然语言处理模型的性能方面起着重要的作用。</p><p id="3732" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated"><strong class="ju hj"> <em class="lv">研究论文:</em></strong><a class="ae it" href="https://cs224d.stanford.edu/reports/Lewis.pdf" rel="noopener ugc nofollow" target="_blank">https://cs224d.stanford.edu/reports/Lewis.pdf</a></p></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><h1 id="639a" class="iu iv hi bd iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr bi translated">业务问题:</h1><p id="65a4" class="pw-post-body-paragraph lb lc hi ju b jv jw ld le jx jy lf lg jz lh li lj kb lk ll lm kd ln lo lp kf hb bi translated">社交媒体是最强大的平台之一，人们通过在虚拟网络中创建和交换想法来相互交流。因此，ML/DL模型使用这些文本/数据来确定情绪、行为和许多其他类似的NLP相关任务。</p><p id="51c2" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">标准英语用于训练这些模型来完成NLP相关的任务。但事实上，大多数时候人们在他们的文本中使用简短的形式/缩写，这可能对训练这些模型或执行NLP任务没有帮助。将文本从非标准英语更改为标准英语将有助于提高许多基于NLP的模型的性能。这里，非标准英语或使用缩写的英语是目标数据的子集，因此我们尝试将这些文本转换为标准英语，同时保留文本的语义。</p></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><h1 id="9776" class="iu iv hi bd iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr bi translated">深度学习问题:</h1><p id="bf5d" class="pw-post-body-paragraph lb lc hi ju b jv jw ld le jx jy lf lg jz lh li lj kb lk ll lm kd ln lo lp kf hb bi translated">递归神经网络已被证明更善于捕捉英语的高级动态和长期依赖性。因此，我们使用rnn作为隐藏单元，将非标准英语SMS文本转换为标准英语SMS文本，同时保留文本的语义，唯一目的是提高NLP模型的性能。</p><p id="eb31" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated"><strong class="ju hj">损失函数- </strong></p><p id="9a16" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">类别交叉熵在这里被用作损失函数。类别交叉熵是用于多类别分类任务的损失函数。在这些任务中，一个示例只能属于许多可能类别中的一个，模型必须决定属于哪一个。<br/>形式上，它是为了量化两个概率分布之间的差异而设计的。</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lw"><img src="../Images/dab0a026eab8a5029800e028251b9a49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jUrJ3cnhHiBoqaFyN4WdpA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">image source: <a class="ae it" href="https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy" rel="noopener ugc nofollow" target="_blank">https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy</a></figcaption></figure><figure class="lx ly lz ma fd ii er es paragraph-image"><div class="er es mb"><img src="../Images/a94448d8e188fd85395a4f58d55b90fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*hMyADGFj7BQPtasHReJSSw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">image source: <a class="ae it" href="https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy" rel="noopener ugc nofollow" target="_blank">https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy</a></figcaption></figure></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><h1 id="e619" class="iu iv hi bd iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr bi translated">数据解释:</h1><p id="1b1a" class="pw-post-body-paragraph lb lc hi ju b jv jw ld le jx jy lf lg jz lh li lj kb lk ll lm kd ln lo lp kf hb bi translated">这个深度学习问题要用的数据集是公开的。</p><div class="mc md ez fb me mf"><a href="https://www.comp.nus.edu.sg/~nlp/corpora.html" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hj fi z dy mk ea eb ml ed ef hh bi translated">数据</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">这个数据集包含了华尔街日报(WSJ)中介词短语的介词词义…</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">www.comp.nus.edu.sg</p></div></div></div></a></div><p id="af73" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">该数据集由大约2000条短信组成，其中每条短信都有非标准英语版本、标准英语版本和中文版本。对于我们的问题，我们将只使用非标准英语版本和标准英语版本。</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mo"><img src="../Images/9f6c632d52c04dd972a116ef06b29c42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jmOCTspujV8hoi6e.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Source Dataset</figcaption></figure></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><h1 id="82fd" class="iu iv hi bd iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr bi translated">自然语言处理的数据扩充；</h1><p id="2965" class="pw-post-body-paragraph lb lc hi ju b jv jw ld le jx jy lf lg jz lh li lj kb lk ll lm kd ln lo lp kf hb bi translated">数据越多，性能越好。然而，注释大量数据是一种奢侈。因此，适当的数据扩充有助于提高模型性能。使用<strong class="ju hj"> nlpaug </strong>库，进行了两种类型的扩增。</p><ul class=""><li id="3a44" class="js jt hi ju b jv lq jx lr jz mp kb mq kd mr kf kg kh ki kj bi translated"><strong class="ju hj">同义词</strong> —这个增强器基于文本输入应用语义。它利用语义来代替单词。</li><li id="c72b" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated"><strong class="ju hj">单词嵌入</strong> —该增强器基于单词嵌入将操作应用于文本输入。它利用单词嵌入来寻找前n个相似的单词进行扩充。</li></ul><figure class="lx ly lz ma fd ii"><div class="bz dy l di"><div class="ms mt l"/></div></figure></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><h1 id="479c" class="iu iv hi bd iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr bi translated">探索性数据分析:</h1><p id="bcba" class="pw-post-body-paragraph lb lc hi ju b jv jw ld le jx jy lf lg jz lh li lj kb lk ll lm kd ln lo lp kf hb bi translated">数据可视化是信息和数据的图形化表示。我们的眼睛被颜色和图案所吸引。当我们看到一张图表或一个曲线图时，我们可以很快理解趋势或识别异常值。</p><h2 id="ff55" class="mu iv hi bd iw mv mw mx ja my mz na je jz nb nc ji kb nd ne jm kd nf ng jq nh bi translated">短信文本</h2><p id="b7b8" class="pw-post-body-paragraph lb lc hi ju b jv jw ld le jx jy lf lg jz lh li lj kb lk ll lm kd ln lo lp kf hb bi translated"><strong class="ju hj">句子长度- </strong></p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ni"><img src="../Images/1461d9bf8b45d6b69441527230cab447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JwJM_BTrmonLTLD31UBVtw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Sentence Length in SMS texts</figcaption></figure><p id="1c36" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">观察-</p><ul class=""><li id="547c" class="js jt hi ju b jv lq jx lr jz mp kb mq kd mr kf kg kh ki kj bi translated">上图显示了短信中句子长度的密度分布。</li><li id="cfa0" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">我们可以看到大多数短信的长度在30-70左右。</li><li id="021b" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">很少有句子的长度在150到170之间。</li><li id="cfcc" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">从以上计算的百分位值，我们可以看到，从99.1到99.8，该值持续增加，但是从99.8到99.9，该值突然增加。这可以被认为是我们数据集的异常值，因此我们只能保留句子长度= &lt; 161.</li></ul><p id="ee0e" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated"><strong class="ju hj">字数- </strong>的SMS文本</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nj"><img src="../Images/558553d81790b8a0351e2daa76a7c0ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*90V8bwe9B1cEX7yMJtv01w.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Number of Words in SMS Texts</figcaption></figure><p id="ea7c" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">观察-</p><ul class=""><li id="02a8" class="js jt hi ju b jv lq jx lr jz mp kb mq kd mr kf kg kh ki kj bi translated">上图显示了短信文本中字数的密度分布。</li><li id="f020" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">大多数句子有5到15个单词。</li><li id="8d13" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">很少有句子的字数超过30个。</li><li id="6fb7" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">从计算的百分位值中，我们可以看到第99百分位值是34，第100百分位值是49。</li><li id="73df" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">我们进一步研究了从99.1到100的计算百分位值。我们可以看到，99.9的值是39，这意味着短信文本中99.9 %的句子有39个或少于39个单词。</li><li id="d527" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">因此，我们可以忽略字数超过39的短信文本/句子。</li></ul><p id="ddf7" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated"><strong class="ju hj">最常出现的25个停用词- </strong></p><figure class="lx ly lz ma fd ii er es paragraph-image"><div class="er es nk"><img src="../Images/dfa1e2351cebfe51309804623a1cedf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*v7e2PHlg5QBh1ajqY9JqFg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Top 25 frequently occurring stop words</figcaption></figure><p id="469e" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">观察-</p><ul class=""><li id="9c00" class="js jt hi ju b jv lq jx lr jz mp kb mq kd mr kf kg kh ki kj bi translated">在所有停用词中,“to”是最常用的停用词。</li><li id="b3dd" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">在最常使用的前25个中，“if”是最少使用的。</li></ul><h2 id="bab4" class="mu iv hi bd iw mv mw mx ja my mz na je jz nb nc ji kb nd ne jm kd nf ng jq nh bi translated">标准英语文本</h2><p id="043c" class="pw-post-body-paragraph lb lc hi ju b jv jw ld le jx jy lf lg jz lh li lj kb lk ll lm kd ln lo lp kf hb bi translated"><strong class="ju hj">句子长度- </strong></p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ni"><img src="../Images/633a9011dcf8abb821276a243daa6b1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yCVsTMhPydkTo0P_sObFeQ.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Sentence Length in Standard English Text</figcaption></figure><p id="147c" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">观察-</p><ul class=""><li id="34ea" class="js jt hi ju b jv lq jx lr jz mp kb mq kd mr kf kg kh ki kj bi translated">上图显示了短信中句子长度的密度分布。</li><li id="f5b2" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">我们可以看到，大多数标准英语文本的长度约为30-70。</li><li id="3e38" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">很少有句子的长度在150到170之间。</li><li id="2717" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">从计算的百分位值中，我们可以看到第99.7百分位值是200。</li><li id="e92d" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">我们只有大约6个句子，句子长度在200到281之间。</li><li id="128e" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">因此，我们可以将最大句子长度保持为200，并在开始预处理之前删除剩余的句子。</li></ul><p id="d1ce" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated"><strong class="ju hj">字数- </strong></p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nj"><img src="../Images/bec2695e8cef5a06b59a0cc054e498f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PvEj-ek-uU4rJQPN5PV_0A.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Number of words in standard English text</figcaption></figure><p id="3dca" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">观察-</p><ul class=""><li id="bec1" class="js jt hi ju b jv lq jx lr jz mp kb mq kd mr kf kg kh ki kj bi translated">上图显示了标准英语文本中单词数量的密度分布。</li><li id="e12b" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">大多数句子有5到15个单词。</li><li id="af3c" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">很少有句子的字数超过30个。</li><li id="3128" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">基于计算的百分位数值，第99.9百分位数值是48，这意味着大多数标准英语句子有48个或少于48个单词。</li><li id="b025" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">因此，在预处理时，如果标准英语句子超过48个单词，我们可以过滤掉数据。</li></ul><p id="b598" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated"><strong class="ju hj">最常出现的25个停用词- </strong></p><figure class="lx ly lz ma fd ii er es paragraph-image"><div class="er es nl"><img src="../Images/5f35918cd3bc0bd90d7f6639d3b0f869.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*yPv4-ZOvz5SaVzgY4tuP_w.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Frequency of stop words</figcaption></figure><ul class=""><li id="7d91" class="js jt hi ju b jv lq jx lr jz mp kb mq kd mr kf kg kh ki kj bi translated">在所有的停用词中，“你”是使用频率最高的停用词。</li><li id="ef6d" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">在最常使用的前25个中，“it”是最少使用的。</li></ul><h1 id="96a3" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">预处理:</h1><p id="8818" class="pw-post-body-paragraph lb lc hi ju b jv jw ld le jx jy lf lg jz lh li lj kb lk ll lm kd ln lo lp kf hb bi translated">在任何机器学习或深度学习任务中，清理或预处理数据与建模一样重要。当涉及到像文本这样的非结构化数据时，这个过程就更加重要了。</p><p id="8582" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">一些常见的文本预处理/清理步骤是:</p><ul class=""><li id="1b3e" class="js jt hi ju b jv lq jx lr jz mp kb mq kd mr kf kg kh ki kj bi translated">下部外壳</li><li id="8cfc" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">删除标点符号</li><li id="c6a4" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">停用词的删除</li><li id="c9ff" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">去除常用词</li><li id="b2d4" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">移除HTML标签</li><li id="7774" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">聊天词汇转换</li><li id="3479" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">拼写纠正等等。</li></ul><figure class="lx ly lz ma fd ii"><div class="bz dy l di"><div class="ms mt l"/></div></figure></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><h1 id="8ece" class="iu iv hi bd iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr bi translated">标记化和填充:</h1><p id="fbfa" class="pw-post-body-paragraph lb lc hi ju b jv jw ld le jx jy lf lg jz lh li lj kb lk ll lm kd ln lo lp kf hb bi translated">记号化是一种将文本转换成称为记号的更小单元的方法。这些标记可以是字符或单词的形式。为了使文本具有模型可理解的格式，添加了<em class="lv"> &lt;开始&gt; </em>和<em class="lv"> &lt;结束&gt; </em>标记。编码器输入是SMS文本。解码器输入将是在句子前带有开始标签(<em class="lv"> &lt; start &gt; </em>)的英语文本，并且解码器输出是带有结束标签(<em class="lv"> &lt; end &gt; </em>)的同一句子的一个时移版本。</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nm"><img src="../Images/cfa658c60488508f5f116ae0ce02eada.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JRTv-ZgG-qUcCHOlwRaoaw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Sample input</figcaption></figure><p id="702f" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">Keras提供了<a class="ae it" href="https://keras.io/api/preprocessing/text/#tokenizer" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> Tokenizer类</em> </a>用于准备深度学习的文本文档。必须构造记号赋予器，然后使其适合原始文本文档或整数编码的文本文档。</p><figure class="lx ly lz ma fd ii"><div class="bz dy l di"><div class="ms mt l"/></div></figure><p id="0133" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">Keras深度学习库中的<a class="ae it" href="https://keras.io/preprocessing/sequence/" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> pad_sequences()函数</em> </a>可以用来填充变长序列。</p><figure class="lx ly lz ma fd ii"><div class="bz dy l di"><div class="ms mt l"/></div></figure></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><h1 id="304f" class="iu iv hi bd iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr bi translated">深度学习模型:</h1><h2 id="f83c" class="mu iv hi bd iw mv mw mx ja my mz na je jz nb nc ji kb nd ne jm kd nf ng jq nh bi translated"><strong class="ak">编码器-解码器架构- </strong></h2><p id="72fb" class="pw-post-body-paragraph lb lc hi ju b jv jw ld le jx jy lf lg jz lh li lj kb lk ll lm kd ln lo lp kf hb bi translated">编码器-解码器架构有两个模块，即编码器和解码器。这些模块中的每一个都包含几个递归神经网络(RNN)栈。</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nn"><img src="../Images/41e66f534fb4802ceac025c50125becd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R8p3Xj4GHpfJy_p3MPqAVg.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">image source: <a class="ae it" href="https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346" rel="noopener" target="_blank">https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346</a></figcaption></figure><p id="879e" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated"><strong class="ju hj">编码器- </strong></p><ul class=""><li id="e5f2" class="js jt hi ju b jv lq jx lr jz mp kb mq kd mr kf kg kh ki kj bi translated">一个编码器将有几个堆栈的递归神经网络(RNN)，如LSTM，GRU等。</li><li id="bbb6" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">编码器接收输入序列，并将信息封装为内部状态向量。</li><li id="1859" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">编码器的输出被忽略，只有内部状态被用作解码器的输入。</li></ul><p id="1006" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated"><strong class="ju hj">解码器- </strong></p><ul class=""><li id="8a72" class="js jt hi ju b jv lq jx lr jz mp kb mq kd mr kf kg kh ki kj bi translated">一个解码器也将有几个堆栈的递归神经网络(RNN)，如LSTM，GRU等。</li><li id="9102" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">与编码器部分不同，解码器的工作在训练和测试阶段是不同的。</li><li id="6bea" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">在训练阶段，我们用<em class="lv"> &lt;开始&gt; </em>令牌输入，以便解码器开始生成下一个字。在最后一个字之后，我们让解码器学习预测<em class="lv"> &lt;结束&gt; </em>令牌。我们使用教师强制技术，该技术使用输入作为基础事实，而不是来自前一时间步的输出</li><li id="d0ba" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf kg kh ki kj bi translated">在测试阶段，初始状态被设置为编码器的最终状态。在第一时间步，我们提供<em class="lv"> &lt;开始&gt; </em>令牌作为输入，以便解码器知道何时开始预测。每个时间步长的输入将是前一个时间步长的输出。然后，我们停止预测解码器何时预测<em class="lv"> &lt;结束&gt; </em>或者我们是否达到最大目标长度。</li></ul><h2 id="187e" class="mu iv hi bd iw mv mw mx ja my mz na je jz nb nc ji kb nd ne jm kd nf ng jq nh bi translated">1.具有一个热编码输入的序列到序列模型:</h2><p id="3b72" class="pw-post-body-paragraph lb lc hi ju b jv jw ld le jx jy lf lg jz lh li lj kb lk ll lm kd ln lo lp kf hb bi translated">在这种情况下，句子被转换成<strong class="ju hj">基于字符的记号</strong>，并且这些字符中的每一个都是一个独热编码向量。</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es no"><img src="../Images/54eeccdfda293e7e3c1864761a69068e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P3AW5DpcofMLfkDPOzJQ5A.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Encoder-Decoder with LSTM as hidden units</figcaption></figure><p id="bdd0" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">使用<strong class="ju hj"> 100个单位的LSTM </strong>，没有数据扩充，分类交叉熵为损失，Adam优化器的学习率为<strong class="ju hj"> 0.0001 </strong>，我们得到的损失为<strong class="ju hj"> 0.5906 </strong>。</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es np"><img src="../Images/87bc2f9af2065adddc67813bd47027b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QFIoVQZcAbXPxVkUiBY_Pg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Encoder-Decoder with GRU as hidden units</figcaption></figure><p id="fc59" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">使用<strong class="ju hj"> 100单位的GRU </strong>，没有数据扩充，分类交叉熵为损失，Adam optimizer的学习率为<strong class="ju hj"> 0.0001 </strong>，我们得到的损失为0.5729。</p><h2 id="4da2" class="mu iv hi bd iw mv mw mx ja my mz na je jz nb nc ji kb nd ne jm kd nf ng jq nh bi translated">2.具有单词级标记化的序列到序列模型:</h2><p id="8e98" class="pw-post-body-paragraph lb lc hi ju b jv jw ld le jx jy lf lg jz lh li lj kb lk ll lm kd ln lo lp kf hb bi translated">在这种情况下，句子被转换成基于单词的标记。使用<strong class="ju hj"> 100单位的LSTM </strong>，没有数据扩充，分类交叉熵为损失，对于任何学习率，模型都没有给我们好的结果。</p><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nq"><img src="../Images/88116fca0c7f202857b85ef4fa92dc52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oWh1tUKxyH7844IeT0HpUg.png"/></div></div></figure><h2 id="e753" class="mu iv hi bd iw mv mw mx ja my mz na je jz nb nc ji kb nd ne jm kd nf ng jq nh bi translated">3.具有单词级标记化和Bahadanau注意力的序列到序列模型:</h2><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nr"><img src="../Images/edee641bdc61b888ab0de286274ee315.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NhBF4O1NzzohcRdRl87KbQ.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">image source: <a class="ae it" href="https://blog.floydhub.com/attention-mechanism/amp/" rel="noopener ugc nofollow" target="_blank">https://blog.floydhub.com/attention-mechanism/amp/</a></figcaption></figure><p id="c173" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">这使得编码器-解码器模型处理长句具有挑战性。因此出现了一种叫做“注意”的技术，它极大地提高了机器翻译系统的质量。注意力允许模型根据需要关注输入序列的相关部分。</p><p id="1e11" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">在这种情况下，句子被标记为单词级别，并且这些标记化的句子被用于使用<strong class="ju hj">快速文本模型</strong>创建<strong class="ju hj">嵌入矩阵</strong>，其将被用作编码器嵌入层中的权重。</p><p id="f7bd" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">用300个单位的LSTM，<strong class="ju hj">用数据扩充</strong>和分类交叉熵作为损失，我们得到98%的训练准确度，交叉熵损失为0.0208，以及0.77  的<strong class="ju hj"> <em class="lv">平均BLEU分数。</em></strong></p><p id="a496" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">从上面的结果可以看出，Bahadanau注意力模型给了我们一个非常好的分数。这个分数很大程度上受预处理之前执行的数据扩充技术的影响。因此，我们将考虑使用<strong class="ju hj"> Bahadanau注意力模型</strong>来预测句子。</p></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><h1 id="4768" class="iu iv hi bd iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr bi translated">模型预测:</h1><h2 id="c324" class="mu iv hi bd iw mv mw mx ja my mz na je jz nb nc ji kb nd ne jm kd nf ng jq nh bi translated">波束搜索解码器和推理装置；</h2><p id="a434" class="pw-post-body-paragraph lb lc hi ju b jv jw ld le jx jy lf lg jz lh li lj kb lk ll lm kd ln lo lp kf hb bi translated"><strong class="ju hj">波束搜索解码器- </strong></p><blockquote class="ns nt nu"><p id="978c" class="lb lc lv ju b jv lq ld le jx lr lf lg nv ls li lj nw lt ll lm nx lu lo lp kf hb bi translated"><em class="hi">在NMT，新句子由简单的波束搜索解码器翻译，该解码器找到近似最大化训练的NMT模型的条件概率的翻译。波束搜索策略从左到右逐字生成翻译，同时在每个时间步长保持固定数量(波束)的活动候选。通过增加波束大小，可以以显著降低解码器速度为代价来提高翻译性能。</em></p></blockquote><p id="454f" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">— <a class="ae it" href="https://arxiv.org/abs/1702.01806" rel="noopener ugc nofollow" target="_blank">神经机器翻译的波束搜索策略</a>，2017。</p><figure class="lx ly lz ma fd ii"><div class="bz dy l di"><div class="ms mt l"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Beam Search Decoder</figcaption></figure><p id="f811" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated"><strong class="ju hj">推理设置:</strong></p><p id="320f" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">建立推理模型来判定句子。这里给出多个句子作为输入，推理模型能够预测输出。</p><figure class="lx ly lz ma fd ii"><div class="bz dy l di"><div class="ms mt l"/></div><figcaption class="ip iq et er es ir is bd b be z dx">Inference Setup</figcaption></figure><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ny"><img src="../Images/ce33ee72c1d0ea40a5d7dde53e129856.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bZnxrUbebFGilcSUJTRXQg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Attention plot</figcaption></figure><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nz"><img src="../Images/75ae0653e9f8e01101649f2c32bb1dfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y_cexWFzWCl9m-M1UVPiMg.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Attention plot</figcaption></figure><h2 id="eb8f" class="mu iv hi bd iw mv mw mx ja my mz na je jz nb nc ji kb nd ne jm kd nf ng jq nh bi translated">预测文本的BLEU分数分布图-</h2><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es oa"><img src="../Images/717e3094d6b810d2cfe596429ccebf5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3fiXqBQvFfXLtdVcielGbw.png"/></div></div></figure><p id="4b1c" class="pw-post-body-paragraph lb lc hi ju b jv lq ld le jx lr lf lg jz ls li lj kb lt ll lm kd lu lo lp kf hb bi translated">从上面的情节中，我们可以看到大多数句子的BLEU得分在0.7到0.9之间，最高分为0.97，最低分为0.59。</p></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><h1 id="77fb" class="iu iv hi bd iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr bi translated">最终管道:</h1><figure class="lx ly lz ma fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ob"><img src="../Images/b6fc81ee60037988595808e4fd7f1cdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*F1YKk65DvkaQGtJ8_4xudQ.gif"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx">Final Pipeline</figcaption></figure></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><h1 id="c1dc" class="iu iv hi bd iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr bi translated">未来作品:</h1><ol class=""><li id="68a8" class="js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf oc kh ki kj bi translated">由于使用&lt; 2000 data points, we could try to use other techniques of data augmentation.</li><li id="a88d" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf oc kh ki kj bi translated">Using a different RNN such as the Bi-directional RNN might also improve the model performance.</li></ol></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><h1 id="2071" class="iu iv hi bd iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr bi translated">References :</h1><ol class=""><li id="9275" class="js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf oc kh ki kj bi translated"><a class="ae it" href="https://keras.io/examples/generative/lstm_character_level_text_generation/" rel="noopener ugc nofollow" target="_blank">https://keras . io/examples/generative/lstm _ character _ level _ text _ generation/</a>时源数据集的大小非常小</li><li id="6c17" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf oc kh ki kj bi translated"><a class="ae it" href="https://www.tensorflow.org/text/tutorials/nmt_with_attention#the_attention_head" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/text/tutorials/NMT _ with _ attention # the _ attention _ head</a></li><li id="3ab0" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf oc kh ki kj bi translated"><a class="ae it" href="https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/beam-search-decoder-natural-language-processing/</a></li><li id="8e2d" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf oc kh ki kj bi translated"><a class="ae it" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank">https://jalammar . github . io/visualizing-neural-machine-translation-mechanics-of-seq 2 seq-models-with-attention/</a></li><li id="4fc0" class="js jt hi ju b jv kk jx kl jz km kb kn kd ko kf oc kh ki kj bi translated"><a class="ae it" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">应用人工智能课程</a></li></ol><h2 id="d88d" class="mu iv hi bd iw mv mw mx ja my mz na je jz nb nc ji kb nd ne jm kd nf ng jq nh bi translated">Github资源库:</h2><p id="eb62" class="pw-post-body-paragraph lb lc hi ju b jv jw ld le jx jy lf lg jz lh li lj kb lk ll lm kd ln lo lp kf hb bi translated">你可以在这个GitHub <a class="ae it" href="https://github.com/srinidhikarjol/Sentence-Correction-using-Recurrent-Neural-Networks" rel="noopener ugc nofollow" target="_blank">链接</a>上找到完整的代码。</p><h2 id="a29d" class="mu iv hi bd iw mv mw mx ja my mz na je jz nb nc ji kb nd ne jm kd nf ng jq nh bi translated">Linkedin个人资料:</h2><p id="e1ea" class="pw-post-body-paragraph lb lc hi ju b jv jw ld le jx jy lf lg jz lh li lj kb lk ll lm kd ln lo lp kf hb bi translated">也可以在<a class="ae it" href="https://www.linkedin.com/in/srinidhi-karjol-aba072103/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和我联系。</p></div></div>    
</body>
</html>
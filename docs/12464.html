<html>
<head>
<title>Boruta Feature Selection Explained in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python解释Boruta特征选择</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/boruta-feature-selection-explained-in-python-7ae8bf4aa1e7?source=collection_archive---------3-----------------------#2022-05-14">https://medium.com/geekculture/boruta-feature-selection-explained-in-python-7ae8bf4aa1e7?source=collection_archive---------3-----------------------#2022-05-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="a81e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">从头开始实现和解释</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/0c6a0911fba953ae4b45a15a5861c6bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Te999PiAT-bUHkSRFr7FlQ.png"/></div></div></figure><p id="6041" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">本文旨在解释，非常流行的，Boruta特征选择算法。Boruta自动执行要素选择过程，因为它会自动确定任何阈值并返回数据集中最有意义的要素。Boruta基于“所有相关”原则工作，因为它为您提供了与您的机器学习问题相关的所有功能。</p><h2 id="6782" class="kf kg hi bd kh ki kj kk kl km kn ko kp js kq kr ks jw kt ku kv ka kw kx ky kz bi translated">需要功能选择？</h2><p id="58cf" class="pw-post-body-paragraph jj jk hi jl b jm la ij jo jp lb im jr js lc ju jv jw ld jy jz ka le kc kd ke hb bi translated">数据集可能包含与您的问题完全无关的要素。这些特征增加了数据集的大小，增加了人工智能模型的复杂性，并且对输出没有影响，或者使结果恶化。在进入训练阶段之前，识别这些特征并去除它们是很重要的。</p><p id="55e5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">您可以在下面的<a class="ae lf" href="https://writersbyte.com/featured-post/feature-selection-algorithms-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">文章</a>中找到更多关于特性选择的细节。</p><div class="lg lh ez fb li lj"><a href="https://writersbyte.com/featured-post/feature-selection-algorithms-for-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="lk ab dw"><div class="ll ab lm cl cj ln"><h2 class="bd hj fi z dy lo ea eb lp ed ef hh bi translated">机器学习的特征选择算法</h2><div class="lq l"><h3 class="bd b fi z dy lo ea eb lp ed ef dx translated">对于机器学习模型，特征选择是一个可选但重要的预处理步骤。这是一个常见的…</h3></div><div class="lr l"><p class="bd b fp z dy lo ea eb lp ed ef dx translated">writersbyte.com</p></div></div><div class="ls l"><div class="lt l lu lv lw ls lx jh lj"/></div></div></a></div><h2 id="9043" class="kf kg hi bd kh ki kj kk kl km kn ko kp js kq kr ks jw kt ku kv ka kw kx ky kz bi translated">博鲁塔算法</h2><p id="0649" class="pw-post-body-paragraph jj jk hi jl b jm la ij jo jp lb im jr js lc ju jv jw ld jy jz ka le kc kd ke hb bi translated">该算法最初是作为r的一个包引入的，它包括以下步骤:</p><p id="ca35" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">通过随机洗牌创建原始特征的副本(阴影特征)。</p><ol class=""><li id="a06b" class="ly lz hi jl b jm jn jp jq js ma jw mb ka mc ke md me mf mg bi translated">将这些阴影要素连接到原始数据集。</li></ol><p id="99d2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">2.使用随机森林分类器训练这个新数据集。</p><p id="1449" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">3.检查评分最高的阴影特征的特征重要性。</p><p id="6a70" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">4.所有比<strong class="jl hj"> <em class="mh">最重要的阴影特征</em> </strong>更重要的原始特征都是我们想要保留的。</p><p id="9545" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">5.对一些迭代重复3和4(20是一个合理的数字),并跟踪在每次迭代中显得重要的特性。</p><p id="7328" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">6.使用二项式分布来最终确定哪些功能具有足够的可信度，可以保留在最终列表中。</p><p id="cd9b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在继续之前，如果你觉得这篇文章有帮助，请考虑在Ko-Fi上支持我。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><a href="https://ko-fi.com/moosaali9906"><div class="er es mi"><img src="../Images/a9b75b9a2d9b6bf73b23151d0a95f04f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RAVsw7Mcvsh8aJL3LuG6bg.png"/></div></a></figure><h2 id="26c4" class="kf kg hi bd kh ki kj kk kl km kn ko kp js kq kr ks jw kt ku kv ka kw kx ky kz bi translated">履行</h2><p id="4779" class="pw-post-body-paragraph jj jk hi jl b jm la ij jo jp lb im jr js lc ju jv jw ld jy jz ka le kc kd ke hb bi translated">您可以在资源库中找到完整的代码:<a class="ae lf" href="https://github.com/Moosa-Ali/Boruta-Feature-Selection-Implementation/blob/main/Boruta-feature-selection-implementation.ipynb" rel="noopener ugc nofollow" target="_blank"> Boruta特性选择</a></p><p id="446a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在任何算法之前，我们显然需要某种数据来执行特征选择。为此，我们将使用我们在上一篇关于<a class="ae lf" href="https://writersbyte.com/featured-post/feature-selection-algorithms-for-machine-learning/?swcfpc=1" rel="noopener ugc nofollow" target="_blank">特性选择</a>的文章中使用的相同数据集。</p><p id="09f1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">该数据集可在<a class="ae lf" href="https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset?rvi=1" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>的以下链接中找到。</p><h2 id="26d4" class="kf kg hi bd kh ki kj kk kl km kn ko kp js kq kr ks jw kt ku kv ka kw kx ky kz bi translated">加载和处理数据</h2><pre class="iy iz ja jb fd mj mk ml mm aw mn bi"><span id="8838" class="kf kg hi mk b fi mo mp l mq mr"># important libraries</span><span id="f78c" class="kf kg hi mk b fi ms mp l mq mr">import pandas as pd<br/>import numpy as np<br/>from tqdm.notebook import tqdm<br/>import scipy as sp<br/>from sklearn.ensemble import RandomForestClassifier<br/>import matplotlib.pyplot as plt<br/>from sklearn.utils import shuffle</span></pre><p id="0b4c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们将使用上述所有的库。</p><pre class="iy iz ja jb fd mj mk ml mm aw mn bi"><span id="2a43" class="kf kg hi mk b fi mo mp l mq mr">data =  pd.read_csv("healthcare-dataset-stroke-data.csv")<br/>data.head()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mt"><img src="../Images/47f59961bea4ddd649ee024f7673f6fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mzln5PGJ-Gu5IJz7YPvyqw.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx">Heart Stroke Dataset</figcaption></figure><p id="6534" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">很多没用的数据，没有我们没见过的。</p><p id="3405" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">是时候大扫除了。有关数据清理和处理的高级课程，请参考下面的<a class="ae lf" href="https://writersbyte.com/featured-post/applied-data-science-with-python-and-pandas/?swcfpc=1" rel="noopener ugc nofollow" target="_blank">帖子</a>。</p><div class="lg lh ez fb li lj"><a href="https://writersbyte.com/featured-post/applied-data-science-with-python-and-pandas/?swcfpc=1" rel="noopener  ugc nofollow" target="_blank"><div class="lk ab dw"><div class="ll ab lm cl cj ln"><h2 class="bd hj fi z dy lo ea eb lp ed ef hh bi translated">Python和熊猫的应用数据科学</h2><div class="lq l"><h3 class="bd b fi z dy lo ea eb lp ed ef dx translated">数据科学是一项非常重要的技能，已经成为21世纪的必备技能。随着数据的增加…</h3></div><div class="lr l"><p class="bd b fp z dy lo ea eb lp ed ef dx translated">writersbyte.com</p></div></div><div class="ls l"><div class="my l lu lv lw ls lx jh lj"/></div></div></a></div><pre class="iy iz ja jb fd mj mk ml mm aw mn bi"><span id="3860" class="kf kg hi mk b fi mo mp l mq mr"># converting to numeric</span><span id="764e" class="kf kg hi mk b fi ms mp l mq mr">data["gender"] = pd.factorize(data["gender"])[0]<br/>data["ever_married"] = pd.factorize(data["ever_married"])[0]<br/>data["work_type"] = pd.factorize(data["work_type"])[0]<br/>data["Residence_type"] = pd.factorize(data["Residence_type"])[0]<br/>data["smoking_status"] = pd.factorize(data["smoking_status"])[0]</span><span id="97ff" class="kf kg hi mk b fi ms mp l mq mr"># additional cleaning</span><span id="e72f" class="kf kg hi mk b fi ms mp l mq mr">data.dropna(inplace =True)<br/>data.drop("id", axis =1, inplace = True)<br/>data.reset_index(inplace=True, drop=True)<br/>data.head()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mz"><img src="../Images/655cc91856f8bc59fd30e52d4a866859.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-g8rppRf2uqS_J2XvrO_8g.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx">Cleaned data for feature selection</figcaption></figure><p id="2063" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">一切都焕然一新。</p><pre class="iy iz ja jb fd mj mk ml mm aw mn bi"><span id="df3e" class="kf kg hi mk b fi mo mp l mq mr"># seperate input and output variables</span><span id="73ea" class="kf kg hi mk b fi ms mp l mq mr">X = data.drop("stroke", axis = 1)<br/>y = data["stroke"]</span></pre><p id="91d8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">分离输入和输出。</p><ol class=""><li id="dabf" class="ly lz hi jl b jm jn jp jq js ma jw mb ka mc ke md me mf mg bi translated">创建阴影特征</li></ol><p id="0bca" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">为此，我们只需调整原始要素，并将它们连接到原始数据集。</p><pre class="iy iz ja jb fd mj mk ml mm aw mn bi"><span id="d6c0" class="kf kg hi mk b fi mo mp l mq mr">for col in X.columns:<br/>    X[f"shadow_{col}"] = X[col].sample(frac=1).reset_index(drop=True)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es na"><img src="../Images/3b77b68952a7f059376d3904542a70d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dvgZ9TCVywOpB3qPvqsyTw.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx">Shadow features concatenated</figcaption></figure><p id="e47e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">2.计算重要性</p><pre class="iy iz ja jb fd mj mk ml mm aw mn bi"><span id="faa6" class="kf kg hi mk b fi mo mp l mq mr">def get_important_features(X, y):</span><span id="55a0" class="kf kg hi mk b fi ms mp l mq mr"># Initiliaze Random Forest CLassifier<br/>rf = RandomForestClassifier(max_depth=20)</span><span id="ef72" class="kf kg hi mk b fi ms mp l mq mr"># Fit Random Forest on provided data<br/>rf.fit(X,y)</span><span id="a34f" class="kf kg hi mk b fi ms mp l mq mr"># Create dictionary of feature importances<br/>importances = {feature_name: f_importance for feature_name, f_importance in zip(X.columns, rf.feature_importances_)}</span><span id="5543" class="kf kg hi mk b fi ms mp l mq mr"># Isolate importances of Shadow features<br/>only_shadow_feat_importance = {key:value for key,value in importances.items() if "shadow" in key}</span><span id="c846" class="kf kg hi mk b fi ms mp l mq mr"># get importance level of most important shadow feature<br/>highest_shadow_feature = list(dict(sorted(only_shadow_feat_importance.items(), key=lambda item: item[1], reverse=True)).values())[0]</span><span id="eb48" class="kf kg hi mk b fi ms mp l mq mr"># get original feature which fulfill boruta selection criteria<br/>selected_features = [key for key, value in importances.items() if value &gt; highest_shadow_feature]</span><span id="d20c" class="kf kg hi mk b fi ms mp l mq mr">return selected_features</span></pre><p id="9fb2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这个函数在我们的心脏病数据集上训练一个<strong class="jl hj">随机森林分类器</strong>。分类器返回它在变量“feature_importances_”中分配给每个特征的重要性。</p><p id="05fa" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">然后，我们为每个特征及其重要性创建一个字典，并挑选出最重要的阴影特征。</p><p id="2fa3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">最后，它返回包含所有原始特征的字典，这些特征的重要性分数大于选出的阴影特征。</p><p id="dd15" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">既然一次试验是不够的，我们需要进行多次试验以确保我们得到满意的结果。</p><h2 id="84c2" class="kf kg hi bd kh ki kj kk kl km kn ko kp js kq kr ks jw kt ku kv ka kw kx ky kz bi translated">多次试验</h2><pre class="iy iz ja jb fd mj mk ml mm aw mn bi"><span id="ff6c" class="kf kg hi mk b fi mo mp l mq mr">TRIALS = 50</span><span id="3d59" class="kf kg hi mk b fi ms mp l mq mr">feature_hits = {i:0 for i in data.columns}</span><span id="a726" class="kf kg hi mk b fi ms mp l mq mr">for _ in tqdm(range(TRIALS)):</span><span id="a1d0" class="kf kg hi mk b fi ms mp l mq mr">    imp_features = get_important_features(X, y)</span><span id="fe03" class="kf kg hi mk b fi ms mp l mq mr">        for key, _ in feature_hits.items():</span><span id="6632" class="kf kg hi mk b fi ms mp l mq mr">            if key in imp_features: feature_hits[key] += 1</span><span id="4d4b" class="kf kg hi mk b fi ms mp l mq mr">print(feature_hits)</span></pre><p id="2a76" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们50次运行的结果如下</p><pre class="iy iz ja jb fd mj mk ml mm aw mn bi"><span id="be26" class="kf kg hi mk b fi mo mp l mq mr">{'gender': 0,  'age': 50,  'hypertension': 0,  'heart_disease': 0,  'ever_married': 0,  'work_type': 0,  'Residence_type': 0,  'avg_glucose_level': 50,  'bmi': 1,  'smoking_status': 0,  'stroke': 0}</span></pre><p id="2d37" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">年龄和平均葡萄糖水平同等重要，50倍，身体质量指数在1次试验中表现重要。现在，为了证明身体质量指数在一次试验中的重要性，我们将使用二项分布。</p><h2 id="729e" class="kf kg hi bd kh ki kj kk kl km kn ko kp js kq kr ks jw kt ku kv ka kw kx ky kz bi translated">二项分布</h2><p id="23fa" class="pw-post-body-paragraph jj jk hi jl b jm la ij jo jp lb im jr js lc ju jv jw ld jy jz ka le kc kd ke hb bi translated">下面一行代码根据二项式分布返回概率。</p><pre class="iy iz ja jb fd mj mk ml mm aw mn bi"><span id="72d3" class="kf kg hi mk b fi mo mp l mq mr"># Calculate the probability mass function<br/>pmf = [sp.stats.binom.pmf(x, TRIALS, .5) for x in range(TRIALS + 1)]</span></pre><p id="6ef3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">概率为0.5的二项式分布有一个钟形曲线，总概率的5%在尾部。</p><p id="f9ae" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">首先，我们需要一个函数，给出形成尾部的迭代次数。</p><pre class="iy iz ja jb fd mj mk ml mm aw mn bi"><span id="1e22" class="kf kg hi mk b fi mo mp l mq mr"># trails_in_green_zone</span><span id="c129" class="kf kg hi mk b fi ms mp l mq mr">def get_tail_items(pmf):<br/>   total = 0<br/>       for i, x in enumerate(pmf):<br/>           total += x<br/>           if total &gt;= 0.05:<br/>               break</span><span id="015c" class="kf kg hi mk b fi ms mp l mq mr">return i</span></pre><p id="f876" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">规则很简单。如果迭代的数量落在右边，我们称之为绿色区域(必须保留的特性)。如果它位于钟形之间，我们称之为蓝区(可以玩的功能)，如果它们位于右尾，我们称之为红区(应该放弃的功能)。</p><p id="41b9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">让我们想象一下我们创建的分布。</p><pre class="iy iz ja jb fd mj mk ml mm aw mn bi"><span id="e451" class="kf kg hi mk b fi mo mp l mq mr"># plot the binomial distribution</span><span id="2d57" class="kf kg hi mk b fi ms mp l mq mr">plt.plot([i for i in range(TRIALS + 1)], pmf,"-o")<br/>plt.title(f"Binomial distribution for {TRIALS} trials")<br/>plt.xlabel("No. of trials")<br/>plt.ylabel("Probability")<br/>plt.grid(True)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nb"><img src="../Images/aba4d24192f42229de022826d6b24e65.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*Tx99YpE2hWg3ZXeDi4phjg.png"/></div><figcaption class="mu mv et er es mw mx bd b be z dx">Binomial distribution for 50 trials</figcaption></figure><h2 id="607f" class="kf kg hi bd kh ki kj kk kl km kn ko kp js kq kr ks jw kt ku kv ka kw kx ky kz bi translated">最终选择</h2><p id="f6b9" class="pw-post-body-paragraph jj jk hi jl b jm la ij jo jp lb im jr js lc ju jv jw ld jy jz ka le kc kd ke hb bi translated">现在我们只需要编写我们上面讨论的规则，决定哪些特征属于绿色、蓝色和红色区域。</p><pre class="iy iz ja jb fd mj mk ml mm aw mn bi"><span id="0d35" class="kf kg hi mk b fi mo mp l mq mr"># select features from n number of trials</span><span id="8be0" class="kf kg hi mk b fi ms mp l mq mr">def choose_features(feature_hits, TRIALS, thresh):</span><span id="1111" class="kf kg hi mk b fi ms mp l mq mr">    #define boundries<br/>    green_zone_thresh = TRIALS - thresh<br/>    blue_zone_upper = green_zone_thresh<br/>    blue_zone_lower = thresh</span><span id="0068" class="kf kg hi mk b fi ms mp l mq mr">    green_zone = [key for key, value in feature_hits.items() if    value &gt;= green_zone_thresh]</span><span id="584d" class="kf kg hi mk b fi ms mp l mq mr">    blue_zone = [key for key, value in feature_hits.items() if (value &gt;= blue_zone_lower and value &lt; blue_zone_upper)]</span><span id="9b54" class="kf kg hi mk b fi ms mp l mq mr">    return green_zone, blue_zone</span></pre><p id="552e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在按以下顺序运行上述函数</p><pre class="iy iz ja jb fd mj mk ml mm aw mn bi"><span id="4b31" class="kf kg hi mk b fi mo mp l mq mr">thresh = get_tail_items(pmf)<br/>green, blue = choose_features(feature_hits, TRIALS, thresh)</span><span id="5a6c" class="kf kg hi mk b fi ms mp l mq mr">green,blue</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nc"><img src="../Images/28d5cc68110502cc2dbb0d82e3410234.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*-Fv1_ePlpMoH0VX_kSNY0Q.png"/></div><figcaption class="mu mv et er es mw mx bd b be z dx">Important Features according to our Boruta Algorithm</figcaption></figure><p id="ab04" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">正如我们所看到的，这些正是我们在另一篇文章的<a class="ae lf" href="https://writersbyte.com/featured-post/feature-selection-algorithms-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">中用Boruta运行Python实现时得到的特性。</a></p><p id="88a4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如果你喜欢这篇文章，请访问我的其他博客:</p><p id="5e50" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">www.writersbyte.com<a class="ae lf" href="http://www.writersbyte.com" rel="noopener ugc nofollow" target="_blank"/></p><p id="9358" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">还有，别忘了给我买杯科菲。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><a href="http://ko-fi.com/moosaali9906"><div class="er es mi"><img src="../Images/a9b75b9a2d9b6bf73b23151d0a95f04f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RAVsw7Mcvsh8aJL3LuG6bg.png"/></div></a></figure></div></div>    
</body>
</html>
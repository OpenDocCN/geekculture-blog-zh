<html>
<head>
<title>Linear Regression- First Model for ML</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归-ML的第一模型</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/linear-regression-first-model-for-ml-cce82b2468e?source=collection_archive---------20-----------------------#2021-04-05">https://medium.com/geekculture/linear-regression-first-model-for-ml-cce82b2468e?source=collection_archive---------20-----------------------#2021-04-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8b74" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归是进入机器学习算法时要研究的基本模型。它是一种ML模型，用于预测数据集的数值(离散/连续)或问题，如预测房价、股票价格、票价、空气污染等。它不能用于分类值预测。它是监督最大似然算法中的基本算法之一(其中标签是已知的)。</p><blockquote class="jd je jf"><p id="eb89" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">回归一词最早是由弗朗西斯·高尔顿创造的，用来描述高个子祖先的后代的身高向正常平均值回归的生物现象。这就是高尔顿如何把“回归”这个词带进现实的，后来尤德内·尤尔和卡尔·皮尔逊把高尔顿的工作扩展到更一般的统计范围和使用。</p></blockquote><p id="8ff4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归模型由一个因变量或预测变量或输出变量以及一个或多个自变量或预测变量或输入变量组成。因变量和自变量之间的关系必须是线性的。因此，线性回归方程与直线方程y=mx+c + e相同，其中m =变量x的斜率，c=y截距，x =独立变量，e =误差或噪声，y =因变量。</p><p id="d969" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个等式也可以用不同的变量写成不同的形式，但是变量的含义是相同的。例如:y=B0 + B1X +e。</p><p id="6fd5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归的挑战是找出数据误差或噪声最小的最佳拟合线，即min(SSE)=σ(I = 1ton)(y-yi)。通过改变不同的参数值(Bp ),可以确定预测值，给出最小SSE的值产生最佳拟合线。这里，xp系数的线性方程可以取无穷多个值，因此存在像梯度下降这样的优化技术，可以用来获得最佳拟合线。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jk"><img src="../Images/6b8cf013dbacdd885e7742bedbd27748.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iZ3E6f8pNkMKJ4HR.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx">Gradient descent</figcaption></figure><p id="ee3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用于最小化线性回归模型的函数是MSE。MSE用于代替SSE，以节省空间并避免大型数据集的内存爆炸。在平滑变化的函数中，最大值或最小值总是在函数变平的地方(鞍点除外)。当斜率为零时，它变平，当斜率为零时，f1(x)=0-&gt; x的f导数= 0。要知道该点是最大值还是最小值，F11(x)&lt;0 -&gt;f x的二重导数=&gt; x0是最大值点，否则，如果F11(x)&gt; 0-&gt; f x的二重导数=&gt; x0是最小值点。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es ka"><img src="../Images/6affb33c60963c360f0c99ed25cbbfc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/0*5QAfqYiQbmozW5CF.jpg"/></div><figcaption class="jw jx et er es jy jz bd b be z dx">Multivariate Linear regression with gradient descent</figcaption></figure><p id="1701" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">学习率是重要的超参数，需要仔细设置，以便找到全局最小值。当它太低时，函数达到全局最小值需要很多时间，而如果它太高，那么函数可能越过全局最小值。它必须在太低和太高之间，因此必须选择一个最佳的学习速率。下图显示了各种学习率的影响:</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es kb"><img src="../Images/1ec8a43af678ae7ed6895a5212a598b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YtctUnSiX5RC7GFf.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx">Learning rate effects</figcaption></figure><p id="c39d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归的假设:</p><ol class=""><li id="1210" class="kc kd hi ih b ii ij im in iq ke iu kf iy kg jc kh ki kj kk bi translated"><strong class="ih hj">线性</strong>:X和Y的关系是线性的。</li><li id="2268" class="kc kd hi ih b ii kl im km iq kn iu ko iy kp jc kh ki kj kk bi translated"><strong class="ih hj">同方差</strong>:残差的方差对于x的任意值都是相同的</li><li id="3dd1" class="kc kd hi ih b ii kl im km iq kn iu ko iy kp jc kh ki kj kk bi translated"><strong class="ih hj">独立性</strong>:观测值相互独立。误差项之间没有相关性。没有这种现象就是自相关。</li><li id="b762" class="kc kd hi ih b ii kl im km iq kn iu ko iy kp jc kh ki kj kk bi translated"><strong class="ih hj">无多重共线性</strong>:回归模型中不应有多重共线性。当两个独立变量之间高度相关时，就会出现多重共线性。</li><li id="2aa3" class="kc kd hi ih b ii kl im km iq kn iu ko iy kp jc kh ki kj kk bi translated"><strong class="ih hj">正态性</strong>:对于X的任意固定值，Y是正态分布的。误差项必须呈正态分布。</li></ol><ul class=""><li id="983a" class="kc kd hi ih b ii ij im in iq ke iu kf iy kg jc kq ki kj kk bi translated">异方差正好与同方差相反。当误差项的大小在独立变量的值之间不同时，异方差就出现了。(可以通过在残差和拟合值之间绘制散点图得知)。异方差可以通过特征的对数变换、异常值处理和多项式拟合来减少。</li><li id="1fe2" class="kc kd hi ih b ii kl im km iq kn iu ko iy kp jc kq ki kj kk bi translated">VIF(方差膨胀因子)用于检测多重共线性。VIF=1/(1-R2)，当VIF=1 = &gt;不相关时，VIF = 1–5 = &gt;中度相关，VIF&gt;5 = &gt;高度相关。当多重共线性存在时，必须去除高度相关变量，线性组合独立变量，例如将它们相加，使用降维技术PCA或SVD，...</li></ul><p id="0d8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归的优势:</p><ol class=""><li id="b1ae" class="kc kd hi ih b ii ij im in iq ke iu kf iy kg jc kh ki kj kk bi translated">这易于实现，并且更容易解释输出系数。</li><li id="92f5" class="kc kd hi ih b ii kl im km iq kn iu ko iy kp jc kh ki kj kk bi translated">当您知道目标变量和预测变量之间存在线性关系时，这是最好的选择，因为与其他模型相比，它没有那么复杂。</li><li id="34a9" class="kc kd hi ih b ii kl im km iq kn iu ko iy kp jc kh ki kj kk bi translated">不管数据大小如何，它都能很好地工作。</li></ol><p id="6c32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归的局限性:</p><ol class=""><li id="aa79" class="kc kd hi ih b ii ij im in iq ke iu kf iy kg jc kh ki kj kk bi translated">离群值对这个模型有巨大的影响</li><li id="767a" class="kc kd hi ih b ii kl im km iq kn iu ko iy kp jc kh ki kj kk bi translated">该模型假设X和y之间存在线性关系，而现实世界中的问题并非如此。</li><li id="f12d" class="kc kd hi ih b ii kl im km iq kn iu ko iy kp jc kh ki kj kk bi translated">这种模型容易不合适</li></ol><p id="c2b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归的评估指标:</p><ul class=""><li id="92f0" class="kc kd hi ih b ii ij im in iq ke iu kf iy kg jc kq ki kj kk bi translated">均方误差(mean square error)</li><li id="bedb" class="kc kd hi ih b ii kl im km iq kn iu ko iy kp jc kq ki kj kk bi translated">均方根误差</li><li id="bf6e" class="kc kd hi ih b ii kl im km iq kn iu ko iy kp jc kq ki kj kk bi translated">平均绝对误差</li><li id="8e28" class="kc kd hi ih b ii kl im km iq kn iu ko iy kp jc kq ki kj kk bi translated">multidimensional assessment of philosophy of education 教育哲学的多维评价</li><li id="08f6" class="kc kd hi ih b ii kl im km iq kn iu ko iy kp jc kq ki kj kk bi translated">R2 (R平方)</li><li id="7c27" class="kc kd hi ih b ii kl im km iq kn iu ko iy kp jc kq ki kj kk bi translated">调整后的R2(调整后的R2广场)</li></ul></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><p id="8d11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是线性回归的工作方式，假设非常重要，并成为模型理解和实现的基础。这通常是已知的学习ML算法的第一个模型。</p><blockquote class="ky"><p id="51fd" class="kz la hi bd lb lc ld le lf lg lh jc dx translated">如果你不能做大事，就用伟大的方式做小事</p><p id="22cb" class="kz la hi bd lb lc ld le lf lg lh jc dx translated">为了成功，你对成功的渴望必须大于对失败的恐惧。</p><p id="c320" class="kz la hi bd lb lc ld le lf lg lh jc dx translated">成功是准备和机遇的结合</p></blockquote><blockquote class="jd je jf"><p id="d316" class="if ig jg ih b ii li ik il im lj io ip jh lk is it ji ll iw ix jj lm ja jb jc hb bi translated">如果你想飞，放弃一切让你沮丧的事情，总是让事情发生。</p></blockquote><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es ln"><img src="../Images/7dacc620545508139c14649f80b11058.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8sha3nslTScD4IPU"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx">Photo by <a class="ae lo" href="https://unsplash.com/@sharonmccutcheon?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Sharon McCutcheon</a> on <a class="ae lo" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><p id="e6a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你喜欢这个故事，而且它对学习线性回归或任何东西有所帮助，那就通过喜欢它来表示支持，并与有用的人分享。如果你有任何观点或任何疑问或想说什么，评论框随时欢迎你的建议和善意。保持积极，保持安全，继续尽最大努力，给世界带来光明。</p></div></div>    
</body>
</html>
<html>
<head>
<title>OPTIMAL or SAFEST? The brief reason why Q-learning and SARSA choose different path in cliff walking problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最优还是最安全？Q-learning和SARSA在悬崖行走问题中选择不同路径的简要原因</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/optimal-or-safest-q-learning-vs-sarsa-in-cliff-walking-problem-f1e7d77dcc5?source=collection_archive---------5-----------------------#2021-09-03">https://medium.com/geekculture/optimal-or-safest-q-learning-vs-sarsa-in-cliff-walking-problem-f1e7d77dcc5?source=collection_archive---------5-----------------------#2021-09-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="5cc1" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">假设你需要从A点到b点，你会选择最优但最危险的路径吗？还是宁愿选择最安全但最耗时的路径？</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/6081c0722557130ca70249d16962b39b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H5d1gSVtkx3vMC070ULAPg.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Photo by <a class="ae jn" href="https://unsplash.com/@josephtpearson?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Joseph Pearson</a> on <a class="ae jn" href="https://unsplash.com/s/photos/seven-sister?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="ca84" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">探索与利用</h1><p id="123b" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">在强化学习的背景下，开发是指主体选择最佳行动，而探索是指主体随机行动以探索是否有其他更好的方式来达到目标。</p><h1 id="4370" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">什么是政策？</h1><p id="2f19" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">强化学习中的策略指的是决定采取什么行动的方法。强化学习中最常见的策略是ε贪婪策略，其中ε指的是探索的概率。因此，ε-贪婪策略将使代理能够在百分之<em class="lc">ε</em>的时间内执行探索，并在百分之<em class="lc">1ε</em>的时间内执行开发。</p><h1 id="6484" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">为什么SARSA和Q-learning的思考方式不同？</h1><p id="967e" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">首先让我们谈谈主要的相似之处。SARSA和Q-learning都采取一些行动，获得即时奖励，并观察给定环境中的新状态，以便学习行动-值函数或Q-表中的Q值。Q-table的维度是动作的数量乘以状态的数量，其中Q值是给定状态时动作的好坏程度。</p><p id="dd63" class="pw-post-body-paragraph kg kh hi ki b kj ld ij kl km le im ko kp lf kr ks kt lg kv kw kx lh kz la lb hb bi translated">使SARSA和Q-learning做出不同决定的唯一区别是，SARSA使用策略上的方法，而Q-learning使用策略外的方法。给定策略是ε贪婪的，非策略是当代理不从策略学习动作值函数时，而策略上是当代理从策略学习动作值函数时。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es li"><img src="../Images/0b3f987c9f2fa36ed3bc747578bc64da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*whZI8F6tiyQBCrUzBXTb9A.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">image from <a class="ae jn" href="https://www.google.com/search?q=q+learning+pseudcode&amp;tbm=isch&amp;ved=2ahUKEwigso_m9uLyAhWTnUsFHZD9AfoQ2-cCegQIABAA&amp;oq=q+learning+pseudcode&amp;gs_lcp=CgNpbWcQAzoICAAQgAQQsQM6BAgAEAM6BQgAEIAEOgYIABAFEB46BggAEAgQHjoECAAQGFDangdY-dcHYObYB2gBcAB4AIABkAGIAc4NkgEEMTguM5gBAKABAaoBC2d3cy13aXotaW1nsAEAwAEB&amp;sclient=img&amp;ei=gSUyYeBtk7uu2g-Q-4fQDw&amp;bih=837&amp;biw=1707#imgrc=FoBJBlwFaLBqMM" rel="noopener ugc nofollow" target="_blank">Google</a></figcaption></figure><p id="882c" class="pw-post-body-paragraph kg kh hi ki b kj ld ij kl km le im ko kp lf kr ks kt lg kv kw kx lh kz la lb hb bi translated">用于Q学习的当前状态和当前动作的Q值的更新等式是基于用max函数表示的下一个状态的最佳动作，其根本没有考虑ε-贪婪策略。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lj"><img src="../Images/22b1f46e97ab45f953c363a0e65a4ae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rLDHP5LVAnMMaFsOk71I9A.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">image from <a class="ae jn" href="https://www.google.com/search?q=sarsa++pseudcode&amp;tbm=isch&amp;ved=2ahUKEwii5uCi9-LyAhXgn0sFHRZ0BWQQ2-cCegQIABAA&amp;oq=sarsa++pseudcode&amp;gs_lcp=CgNpbWcQA1Cq6QFY-u8BYKbyAWgAcAB4AIABAIgBAJIBAJgBAKABAaoBC2d3cy13aXotaW1nwAEB&amp;sclient=img&amp;ei=ACYyYaLmCuC_rtoPluiVoAY&amp;bih=837&amp;biw=1707#imgrc=1kGHFqEPlyFmeM" rel="noopener ugc nofollow" target="_blank">Google</a></figcaption></figure><p id="33fc" class="pw-post-body-paragraph kg kh hi ki b kj ld ij kl km le im ko kp lf kr ks kt lg kv kw kx lh kz la lb hb bi translated">而SARSA的更新规则不是基于下一个状态的最佳动作，而是基于由ε-贪婪策略决定的动作。这就是SARSA被称为on-policy的原因，它使两种方法的行为不同。</p><h1 id="4422" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">悬崖行走问题</h1><p id="43c0" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">在悬崖问题中，代理需要从左边的白点行进到右边的白点，红点是悬崖。达到目标的代理人将获得10英镑的奖励，如果掉下悬崖，将受到-100英镑的惩罚。代理在网格中移动的时间越长，代理得到的惩罚就越多，每个网格的惩罚为-1。</p><p id="5752" class="pw-post-body-paragraph kg kh hi ki b kj ld ij kl km le im ko kp lf kr ks kt lg kv kw kx lh kz la lb hb bi translated">Q学习代理由绿线表示，而SARSA代理由蓝线表示。完整的python代码可以在文章末尾找到。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lk"><img src="../Images/2f3ce4d8408d8760dfdd36e9fef2ab69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*at7DDC3VSxrAttPcOGZd-g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">My screen shot</figcaption></figure><p id="25e6" class="pw-post-body-paragraph kg kh hi ki b kj ld ij kl km le im ko kp lf kr ks kt lg kv kw kx lh kz la lb hb bi translated">毫无疑问，Q-learning为什么选择最优路径，因为该方法只学习最优动作。</p><p id="a8d3" class="pw-post-body-paragraph kg kh hi ki b kj ld ij kl km le im ko kp lf kr ks kt lg kv kw kx lh kz la lb hb bi translated">但SARSA选择最安全路径的原因是因为驱动SARSA的行动价值函数学习的策略是ε贪婪的，其中,<em class="lc">ε%的时间代理采取随机行走。这意味着，学习是由epsilon策略驱动的，即大多数时间随机行走，为了通过接受长时间旅行的小惩罚来避免智能体掉下悬崖的大惩罚而靠近悬崖行走根本不安全。</em></p><p id="1596" class="pw-post-body-paragraph kg kh hi ki b kj ld ij kl km le im ko kp lf kr ks kt lg kv kw kx lh kz la lb hb bi translated">这意味着ε值减少得越多，SARSA选择的路径就越接近悬崖。</p><p id="4686" class="pw-post-body-paragraph kg kh hi ki b kj ld ij kl km le im ko kp lf kr ks kt lg kv kw kx lh kz la lb hb bi translated">如果我们将ε指定为0.8，这意味着80%的时间代理将执行随机动作。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ll"><img src="../Images/1ad16c34d4c15acac0f0e0f39392b13a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*Lh_xuE09CzACw1qYnZVd8Q.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">SARSA took safest path while Q-learning took optimal path (My screen shot)</figcaption></figure><p id="2f69" class="pw-post-body-paragraph kg kh hi ki b kj ld ij kl km le im ko kp lf kr ks kt lg kv kw kx lh kz la lb hb bi translated">这就是为什么从政策中学习的SARSA试图远离悬崖，以尽可能地防止巨大的负回报，因为它的政策将在80%的时间里采取随机移动。即使代理人在网格中行进的时间越长，代理人将获得越多的负回报(-1)，但让“ε贪婪策略驱动”的代理人呆在悬崖附近仍然太危险，因为它有很多机会掉下来。</p><p id="0933" class="pw-post-body-paragraph kg kh hi ki b kj ld ij kl km le im ko kp lf kr ks kt lg kv kw kx lh kz la lb hb bi translated">然后，如果我们将探索度降低到0.2，这意味着只有20%的时间代理会执行随机动作。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lm"><img src="../Images/d346a4493e8a45565f0f8f85679c321b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*ce_0dUlccuVSvwOfy90yIA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">SARSA’s path move closer to optimal path (My screen shot)</figcaption></figure><p id="9ed0" class="pw-post-body-paragraph kg kh hi ki b kj ld ij kl km le im ko kp lf kr ks kt lg kv kw kx lh kz la lb hb bi translated">因此，SARSA选择的路径更接近悬崖，更接近最优路径，因为与以前相比，现在的策略不采取那么多随机移动。</p><p id="1c57" class="pw-post-body-paragraph kg kh hi ki b kj ld ij kl km le im ko kp lf kr ks kt lg kv kw kx lh kz la lb hb bi translated">然而，如果我们把ε设为0，你可能会猜到…</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ln"><img src="../Images/3ae3c617b5ecc11297a4198a48674e5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*M59aLUC7U7CN6iwyriceWQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Optimal path by SARSA (My screen shot)</figcaption></figure><p id="7221" class="pw-post-body-paragraph kg kh hi ki b kj ld ij kl km le im ko kp lf kr ks kt lg kv kw kx lh kz la lb hb bi translated">SARSA采取了与Q-learning相同的路径，因为SARSA现在学习的策略是最优策略，而不再是贪婪的。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lo lp l"/></div></figure></div></div>    
</body>
</html>
<html>
<head>
<title>Criterion used in Constructing Decision Tree</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构造决策树的标准</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/criterion-used-in-constructing-decision-tree-c89b7339600f?source=collection_archive---------2-----------------------#2022-03-08">https://medium.com/geekculture/criterion-used-in-constructing-decision-tree-c89b7339600f?source=collection_archive---------2-----------------------#2022-03-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="697a" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">熵、基尼不纯与信息增益</h2><div class=""/><figure class="ev ex ip iq ir is er es paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="er es io"><img src="../Images/95f97096c9a79553722f39131b2ab64b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UvlOyDsdIbPvTACqcLTffg.jpeg"/></div></div><figcaption class="iz ja et er es jb jc bd b be z dx">Photo by <a class="ae jd" href="https://unsplash.com/@garri?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Vladislav Babienko</a> on <a class="ae jd" href="https://unsplash.com/s/photos/decision-tree?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="effa" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">决策树是通用机器学习算法，能够执行回归和分类任务，并具有处理复杂和非线性数据集的能力。他们的决定很容易理解。它们是通过基于数据集的一个特征和一组if-then-else决策规则在节点处进行分割而形成的。但是它使用什么标准在特定节点上进行分割呢？我们如何量化分裂的质量？为了回答所有这些问题，我们将研究3个重要标准，以及它们如何用于构建决策树。这些是</p><ol class=""><li id="730f" class="kc kd hi jg b jh ji jl jm jp ke jt kf jx kg kb kh ki kj kk bi translated">熵</li><li id="cec5" class="kc kd hi jg b jh kl jl km jp kn jt ko jx kp kb kh ki kj kk bi translated">基尼杂质</li><li id="007c" class="kc kd hi jg b jh kl jl km jp kn jt ko jx kp kb kh ki kj kk bi translated">信息增益</li></ol></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><p id="50fa" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi kx translated"><span class="l ky kz la bm lb lc ld le lf di"> 1。</span> <strong class="jg hs"> <em class="lg">熵:</em> </strong>熵代表随机的顺序。在决策树中，它通过测量分裂的纯度来帮助模型选择分裂的特征。如果，</p><ol class=""><li id="1cce" class="kc kd hi jg b jh ji jl jm jp ke jt kf jx kg kb kh ki kj kk bi translated">熵= 0意味着它是纯分裂的，即所有实例都只属于一个类。</li></ol><p id="ed53" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">2.熵=1意味着完全不纯的分裂，即在节点处两类的相等实例(50%–50%)导致极端无序。</p><p id="deec" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">熵(H <em class="lg"> i </em>由数学公式给出:</p><figure class="li lj lk ll fd is er es paragraph-image"><div class="er es lh"><img src="../Images/bf6836bdb3352eba90a05869f8454c1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*5SOtcqGijB0Pv-UaezywwQ.png"/></div><figcaption class="iz ja et er es jb jc bd b be z dx">Source: Oreilly ‘s Hands-On machine learning with Scikit-learn, Keras &amp; Tensor flow</figcaption></figure><p id="adc6" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">其中，<em class="lg"> p(i，k ) </em>是特定节点上正负类<em class="lg"> i </em>的概率。</p><p id="7405" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">n =特定节点处不同类值的数量。</p><p id="6a8c" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">熵H的范围在0-1之间变化。</p><figure class="li lj lk ll fd is er es paragraph-image"><div class="er es lm"><img src="../Images/27df7443ed216b6363f7e6556fabb46b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*hO5XMYe-O5vY9urWaFZHAQ.jpeg"/></div><figcaption class="iz ja et er es jb jc bd b be z dx">Source : Google</figcaption></figure><p id="c813" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">让我们通过在下面的例子中的一个节点进行快速计算来理解它:使用决策树分类器根据花瓣长度和宽度将鸢尾花分类为3类，决策树看起来像，</p><figure class="li lj lk ll fd is er es paragraph-image"><div class="er es ln"><img src="../Images/b03b72e8d74f2793fd29851644cd6489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*g58MBmjXRycyPRSOoHixnw.png"/></div><figcaption class="iz ja et er es jb jc bd b be z dx">Source: Oreilly ‘s Hands-On machine learning with Scikit-learn, Keras &amp; Tensor flow</figcaption></figure><p id="6fef" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">对于深度为2的左节点，熵(H <em class="lg"> i </em>)将为</p><figure class="li lj lk ll fd is er es paragraph-image"><div class="er es lo"><img src="../Images/f84448ac16ffb8384a512127a2d798de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*pA0DUv7CTucvym7eNHqmDw.png"/></div><figcaption class="iz ja et er es jb jc bd b be z dx">Credits : Oreilly ‘s Hands-On machine learning with Scikit-learn, Keras &amp; Tensor flow</figcaption></figure></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><p id="9084" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi kx translated"><span class="l ky kz la bm lb lc ld le lf di"> 2。</span> <strong class="jg hs"> <em class="lg">基尼杂质:</em> </strong>它还计算决策树节点处的分裂纯度。在第<em class="lg"> i </em>个节点处的基尼属性(G <em class="lg"> i </em>)的数学方程由下式给出:</p><figure class="li lj lk ll fd is er es paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="er es lp"><img src="../Images/86ff86e4becd8c6c41766ced454e15ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OAjkAC1gbgIEj6ro-wIE1g.png"/></div></div></figure><p id="8836" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">计算深度为2的左侧节点的GI</p><figure class="li lj lk ll fd is er es paragraph-image"><div class="er es lq"><img src="../Images/59f3a6418fd2151ca12533b51bed80ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*b-IDNscgvoy-fKGFPxih9A.png"/></div><figcaption class="iz ja et er es jb jc bd b be z dx">Source: Author</figcaption></figure><p id="a8e3" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">与熵不同，基尼系数在0-0.5之间变化。当Gini属性为0时，节点是纯的，即所有实例都属于同一类。</p><figure class="li lj lk ll fd is er es paragraph-image"><div class="er es lr"><img src="../Images/dd16073dec1f48118e33922553e3b73b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*hAwnSRhn19ECna6cSNTdnw.png"/></div><figcaption class="iz ja et er es jb jc bd b be z dx">Source: Google</figcaption></figure><h1 id="f724" class="ls lt hi bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated"><strong class="ak">那么应该用熵还是基尼杂质呢？</strong></h1><p id="7e8d" class="pw-post-body-paragraph je jf hi jg b jh mq jj jk jl mr jn jo jp ms jr js jt mt jv jw jx mu jz ka kb hb bi translated">大多数情况下，它们不会产生太大的差异，它们会导致类似的树，但使用基尼系数的优势在于，与熵相比，基尼系数的计算更有效，因为熵涉及对数计算，需要更多的时间。</p></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><p id="9884" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">由于熵和Gini杂质是针对节点处的特定分裂计算的，但是为了构建完全生长的树，算法创建多个树，然后使用在每个节点处产生最大信息增益的特征和阈值来选择树。让我们理解什么是信息增益？</p><p id="a6d5" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi kx translated"><span class="l ky kz la bm lb lc ld le lf di"> 3。</span>信息增益:表示在一个节点分裂时，熵被去除了多少。信息增益越高，移除的熵越多，因此在训练决策树期间，选择具有最大信息增益的最佳分裂。</p><p id="bf28" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">信息增益的数学公式为:</p><figure class="li lj lk ll fd is er es paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="er es mv"><img src="../Images/ab4620993d77d965486bc66c9496099c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C-ri7k1pWDoRFXzLQ-_XxQ.png"/></div></div></figure><p id="a149" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">其中，<strong class="jg hs"> <em class="lg"> IG </em> </strong>为信息增益，熵(<strong class="jg hs"> <em class="lg"> T </em> </strong>)为分裂前节点熵(父节点)，熵(<strong class="jg hs"> <em class="lg"> Tv </em> </strong>)为分裂后熵(子节点)。<em class="lg"> </em> <strong class="jg hs"> <em class="lg"> T </em> </strong>是拆分前的实例总数，<strong class="jg hs"> <em class="lg"> Tv </em> </strong>是拆分后的实例数。</p><p id="70a1" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">对于上述示例，计算右侧分割的信息增益:</p><figure class="li lj lk ll fd is er es paragraph-image"><div class="er es mw"><img src="../Images/0983cf663825660f35f72e2495a24d2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*NRnNuOPBOBvdDYNcxTJ4Fw.png"/></div><figcaption class="iz ja et er es jb jc bd b be z dx">Source: Oreilly ‘s Hands-On machine learning with Scikit-learn, Keras &amp; Tensor flow</figcaption></figure><p id="7bff" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">父节点的熵:</p><figure class="li lj lk ll fd is er es paragraph-image"><div class="er es mx"><img src="../Images/2c4cfac724fb45cab33228311e70d6d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*4R7qdKUlBXdmfIsLSSUXeQ.png"/></div></figure><p id="f76a" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">子节点的熵:</p><div class="li lj lk ll fd ab cb"><figure class="my is mz na nb nc nd paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/1cc48a3a4b1527bf5aeaed250f2155c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*F9ngJdErttZBV5Sa39mA9Q.png"/></div></figure><figure class="my is ne na nb nc nd paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/81966b54b58241259e8e9f16029e9b01.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*cDclY6geDsOx9dOlKDG2Ew.png"/></div><figcaption class="iz ja et er es jb jc bd b be z dx nf di ng nh">Credits: Author</figcaption></figure></div><p id="0ecd" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">所以，对于这种情况，<strong class="jg hs">熵(T) =1，T= 100 </strong>，<strong class="jg hs"> <em class="lg">熵(Tv) = 0.445对于Tv=54 </em> </strong>，<strong class="jg hs"> <em class="lg">熵(Tv) = 0.1511对于Tv=46。</em> </strong> <em class="lg">所以信息增益会，</em></p><figure class="li lj lk ll fd is er es paragraph-image"><div class="er es ni"><img src="../Images/543e72eaab92dd88ceded7f35e6a8949.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*8UWzOh3hON3qI_7ktwlLuQ.png"/></div><figcaption class="iz ja et er es jb jc bd b be z dx">Credits: Author</figcaption></figure><p id="a4f7" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">所以这是关于建造完全成长的树的标准。谢谢你阅读它。快乐的数据驱动学习！！</p><p id="181d" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">参考资料:</p><ol class=""><li id="5fba" class="kc kd hi jg b jh ji jl jm jp ke jt kf jx kg kb kh ki kj kk bi translated"><a class="ae jd" href="https://scikit-learn.org/stable/modules/tree.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/tree.html</a></li><li id="d333" class="kc kd hi jg b jh kl jl km jp kn jt ko jx kp kb kh ki kj kk bi translated">O'Reilly Media，Inc .使用Scikit-Learn、Keras和TensorFlow进行机器学习。</li></ol></div></div>    
</body>
</html>
<html>
<head>
<title>A Simple Guide To Reinforcement Learning With The Super Mario Bros. Environment</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超级马里奥兄弟环境下强化学习的简单指南</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/a-simple-guide-to-reinforcement-learning-with-the-super-mario-bros-environment-495a13974a54?source=collection_archive---------13-----------------------#2021-07-06">https://medium.com/geekculture/a-simple-guide-to-reinforcement-learning-with-the-super-mario-bros-environment-495a13974a54?source=collection_archive---------13-----------------------#2021-07-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/eaa453efc30486371d6b15707361d83f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sf0Jqjwt9LaHQNC52W54ug.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx">Photo by <a class="ae hv" href="http://twitter.com/cpwrm" rel="noopener ugc nofollow" target="_blank">@cpwrm</a> on Unsplash</figcaption></figure><div class=""/><h1 id="596b" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">理论</h1><p id="b10f" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">假设我们想要设计一个能够完成超级马里奥兄弟游戏第一关的算法。我们将如何做到这一点？</p><p id="1106" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">这个游戏的目标很简单——尽可能快地拿到关卡最右端的旗子。为此，我们需要观察<strong class="jv hz"/><strong class="jv hz">游戏的当前状态<em class="kw"> s </em> </strong>，通过按下控制按钮移动马里奥(<strong class="jv hz">姑且称之为动作<em class="kw"> a </em> </strong>)，并检查在此之后我们离目标还有多远(<strong class="jv hz">在强化学习动作中的表现由奖励<em class="kw"> r </em>)，</strong>观察一些<strong class="jv hz">新状态<em class="kw">s’</em>(在我们的新框架中结果，我们将得到一系列的<strong class="jv hz">动作、状态和奖励</strong>称为<strong class="jv hz">轨迹:</strong></strong></p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es kx"><img src="../Images/8cdbe48281dc8807b7a9190ce3b45bf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*r_9m7QmDtRg9YVekiEJQKw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx">A trajectory of <strong class="bd ix">(s,a,r)</strong> tuples</figcaption></figure><p id="b857" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">用更正式的术语来说，这种环境可以描述为一个<a class="ae hv" href="https://en.wikipedia.org/wiki/Markov_decision_process" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hz">马尔可夫决策过程</strong> </a> <strong class="jv hz"> </strong>如果我们通过一次考虑多个帧来结合角色的速度来行动。这意味着状态之间的转换只取决于最新的状态和动作对，而没有先前的历史。</p><p id="50a3" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">让我们称之为估计器(可以是神经网络),它将<strong class="jv hz">作为状态</strong>,并告诉我们采取哪个动作<strong class="jv hz">作为策略</strong>,因此我们的目标是最大化轨迹上的预期回报:</p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es lc"><img src="../Images/a93c86e07ea65b0dbc540c955ddeb912.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*qepnBEyAs2qevUxqVQ9ylw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx">Policy performance <strong class="bd ix">J</strong></figcaption></figure><h1 id="f54a" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">政策梯度</h1><p id="b976" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">为了最大化预期回报，我们希望通过梯度上升来优化策略参数，就像任何其他神经网络一样:</p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es ld"><img src="../Images/6a2ba3314bc9f2f3052ca4f2bc469896.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*0RuihqLEmWNGTGQ6-su-bg.png"/></div></figure><p id="31aa" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">政策绩效<strong class="jv hz"> <em class="kw"> J </em> </strong>的梯度称为<strong class="jv hz">政策梯度。</strong>现在我们需要以一种计算上可行的形式来表达策略梯度。首先，让我们推导出<strong class="jv hz">政策梯度</strong>的解析形式:</p><figure class="ky kz la lb fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es le"><img src="../Images/16b503e3e02b6fbe0111ad558a7de6ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FhRlIBOajFq0CusZTYHoIA.png"/></div></div></figure><p id="bda9" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">正如你在最后一个公式中看到的，这里我们取一个轨迹概率的对数，可以这样表示:</p><figure class="ky kz la lb fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lf"><img src="../Images/aa8d6d343bcc8f04da23298d03d1714f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tPb18MXTt6jqwFXpwl4FAg.png"/></div></div></figure><p id="df1f" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">因此，<strong class="jv hz">政策梯度</strong>的推导采用如下形式:</p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es lg"><img src="../Images/7285c37ea379d8427b05cf0de0d636a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*443xJBAkkOIsmZP7BSeGuQ.png"/></div></figure><p id="05ac" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">但是，实际上，我们应该只增加考虑其后果的行动的可能性，并在<strong class="jv hz">【0，1】</strong>之间引入贴现因子<strong class="jv hz"> γ </strong>，作为对<strong class="jv hz">未来报酬</strong>的不确定性的惩罚</p><figure class="ky kz la lb fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lh"><img src="../Images/df0056946dabb8dcdd961cc107e0f290.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VbvDkOwM-PvBUmPThYo4QQ.png"/></div></div></figure><p id="ab4b" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们可以通过收集一组轨迹来估计这种期望，在这些轨迹中，代理根据当前策略产生动作。这一系列优化被称为<strong class="jv hz">基于策略的方法。</strong></p><h2 id="b2d3" class="li iw hy bd ix lj lk ll jb lm ln lo jf ke lp lq jj ki lr ls jn km lt lu jr lv bi translated">基线</h2><p id="29a9" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">让我们考虑随机变量的参数化概率分布，所以:</p><figure class="ky kz la lb fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lw"><img src="../Images/f7c016a9b064c8845ba04ea8c2c59d65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6A8Gpi8OdphUF7P3QtrW5Q.png"/></div></div></figure><p id="f2e3" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">这意味着我们可以从策略梯度表达式中增加或减去任何<strong class="jv hz">基线</strong>函数，该函数仅取决于状态，而不会改变结果预期:</p><figure class="ky kz la lb fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lx"><img src="../Images/8c7badbc467bc83ac91f9beacc4e3f7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rIFtVJQG_-gXXaH1Yv2u_A.png"/></div></div></figure><p id="a432" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在实践中，基线最常见的选择是由神经网络(有时称为<strong class="jv hz">评论家</strong>)近似的<strong class="jv hz">策略上的价值函数</strong> <strong class="jv hz"> <em class="kw"> V(s) </em> </strong>，它与策略同时更新。一般来说，<strong class="jv hz">基线</strong>有助于减少方差，增加训练过程的稳定性。</p><h1 id="ad76" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">具有广义优势估计的近似策略优化</h1><p id="4eec" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">由于梯度估计的高方差，常规策略梯度方法的采样效率很差。这是由影响未来回报的行为的信用分配的困难造成的，特别是在长轨迹的环境中。这种差异有时可以通过将<strong class="jv hz">基线用作上述</strong>来减少。但是还有另外一种方法来估计一个轨迹的回报——<strong class="jv hz">重要性抽样。</strong>在这种方法中，可以用不同的策略来计算预期报酬，并在以后通过<strong class="jv hz">重要性比率<em class="kw"> r </em> : </strong>来细化</p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es ly"><img src="../Images/1069eafaacca0e96b1492a4dc138df4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*LDhaRjga1moQTafNQ4POTw.png"/></div></figure><p id="1695" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">因此，目标函数采用以下形式，其中<strong class="jv hz"><em class="kw"/></strong>可以是<strong class="jv hz">贴现回报，如在政策梯度法</strong>或<strong class="jv hz">中的优势函数</strong> <strong class="jv hz">，如在</strong><a class="ae hv" href="https://arxiv.org/abs/1506.02438" rel="noopener ugc nofollow" target="_blank"><strong class="jv hz">【GAE</strong></a><strong class="jv hz">(下面解释)</strong>:</p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es lz"><img src="../Images/cfb27b7db97fd2a497324453ac3c00c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*bpLQ_hI0TVzaUJ-UeqUOXQ.png"/></div></figure><p id="3bd6" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">如果两个政策相同，那么这个公式直接转化为<strong class="jv hz">政策梯度</strong>:</p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es ma"><img src="../Images/8a3e454e3383003d62366c7e8c2507b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*-JHokT9cqWOoU_5i_k25cQ.png"/></div></figure><p id="5989" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">实际上，<strong class="jv hz">重要率</strong>只取决于政策，不取决于环境:</p><figure class="ky kz la lb fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mb"><img src="../Images/5caf002ae45565756ac7eb78d91d5fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yvQ-AJl1UuKX7ZQaHkMkMA.png"/></div></div></figure><p id="3a9f" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">策略梯度的<strong class="jv hz">低样本效率的另一个原因是，它需要使用<strong class="jv hz">新策略</strong>来收集样本的<strong class="jv hz">新轨迹</strong>以计算<strong class="jv hz">下一个梯度更新</strong>。使用旧策略收集的旧样本不可重复使用。因此，为了克服这个问题，我们可以引入一个<strong class="jv hz">代理目标函数</strong>，只要<strong class="jv hz">新旧策略相似</strong>，它的优化就能保证策略的改进。这个误差可以由这两个策略之间的KL差异来限定。增加的约束有助于我们不要采取会损害训练进度的过于乐观的行动。</strong></p><p id="ed4c" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">考虑到这些事实，<a class="ae hv" href="https://arxiv.org/pdf/1502.05477.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="jv hz">TRPO</strong></a><strong class="jv hz"/>论文介绍了一种新颖的<strong class="jv hz"> </strong>方法，旨在最大化目标函数，同时使新旧策略之间的<strong class="jv hz">KL-距离</strong>足够小(所谓的<strong class="jv hz">信任区域约束</strong>):</p><figure class="ky kz la lb fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mc"><img src="../Images/8eca3666198b3728c209f6afe135e81e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*ygMKryvsRaoi1pcDtH5T6Q.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><a class="ae hv" href="https://arxiv.org/pdf/1707.06347.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="bd ix">TRPO</strong></a><strong class="bd ix"> objective function</strong></figcaption></figure><p id="2127" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hz"> TRPO </strong>的作者提出了一种相当复杂的方法来近似解决这个问题，但是OpenAI通过在他们的<a class="ae hv" href="https://arxiv.org/pdf/1707.06347.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hz">近似策略优化</strong> </a>论文中引入<strong class="jv hz">裁剪代理目标</strong>函数，创建了一种限制策略更新大小的新方法:</p><figure class="ky kz la lb fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es md"><img src="../Images/1d3c3fc404c82974bc89d7e3436e5ec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7aU9q03p0Mqg96FpZ8voLQ.png"/></div></div></figure><p id="42bd" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">最小化内的第一项同上，但在第二项中，<strong class="jv hz">比值被限幅在(1</strong>–<strong class="jv hz">ε，1+ε) </strong>之间。</p><p id="b294" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">当在具有策略<strong class="jv hz">(演员)</strong>和值<strong class="jv hz">(评论家)</strong>函数的共享参数的神经网络上应用PPO时，除了<strong class="jv hz">裁剪代理</strong>之外，目标函数还与值估计的<strong class="jv hz">误差(第二项)</strong>和<a class="ae hv" href="https://arxiv.org/pdf/1602.01783.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hz">熵加成</strong> </a> <strong class="jv hz">(第三项)</strong>相结合，以鼓励充分探索(与<strong class="jv hz"> c(1) </strong></p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es me"><img src="../Images/14b486b0d4e761db3095eff12c2f133c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*R0wAvY1CuDglWznpC5KBUg.png"/></div></figure><p id="da08" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">价值估计的误差只是实际收益和评论家估计的收益之间的MSE损失。</p><p id="b5ab" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">此外，由于截取的替代目标函数，<strong class="jv hz">PPO算法允许我们在相同样本上运行多个时期的梯度上升</strong>，而不会损害我们的策略。<strong class="jv hz">这种方法极大地提高了采样效率。</strong></p><h2 id="a4c4" class="li iw hy bd ix lj lk ll jb lm ln lo jf ke lp lq jj ki lr ls jn km lt lu jr lv bi translated">优势函数</h2><p id="a4ab" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">关于PPO算法，我们需要弄清楚的最后一件事是<strong class="jv hz">如何计算优势值</strong>。与策略梯度方法不同，PPO算法意味着使用两个估计器— <strong class="jv hz"> actor和critical</strong>。第二个——批评家<strong class="jv hz">——返回给定状态的预期收益的估计值。因此，自然地，我们希望使用这个估计来计算advantage — <strong class="jv hz">一个告诉我们模型所选择的行为是否会导致比我们预期的更好或更差的回报的值</strong>:</strong></p><figure class="ky kz la lb fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mf"><img src="../Images/1c5ed54e485accafed9a736c7a343d98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Csp2fasnMXNM7S5_Q9sEgg.png"/></div></div></figure><p id="689e" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们采用优势的指数加权平均值来平衡偏差和方差。这就是所谓的<a class="ae hv" href="https://arxiv.org/pdf/1506.02438.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hz">广义优势估计</strong> </a> <strong class="jv hz"> : </strong></p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es mg"><img src="../Images/caf0f0f3517d910db65cb598fb6de6e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*SCTXrp14dfgqzCZyIIcuSg.png"/></div></figure><h1 id="3197" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">双深度Q网络</h1><p id="d4fe" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">解决强化学习问题的另一种方法是近似<strong class="jv hz">一个最优行动值函数</strong>，如果我们从状态<strong class="jv hz"> <em class="kw"> s </em> </strong>开始并采取一些行动<strong class="jv hz"> <em class="kw"> a </em> </strong>，它会给我们最大的期望回报:</p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es mh"><img src="../Images/5d2ea99fa91aaa35f112e9c08c08e05c.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*vSGBd25409peadgyc6hftw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd ix">Optimal Action-Value function</strong></figcaption></figure><p id="079b" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">这个函数服从<strong class="jv hz">贝尔曼方程，</strong>它告诉我们，如果状态<strong class="jv hz"><em class="kw"/></strong>的最优值<strong class="jv hz"><em class="kw"><em class="kw">Q</em></em></strong>s已知所有可能的动作<strong class="jv hz"><em class="kw"/></strong>，那么最优策略是选择动作<strong class="jv hz"><em class="kw"/></strong>最大化报酬加上你下一步到达的状态值:</p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es mi"><img src="../Images/241e8e28e2df5ef10f6c845d2d2788d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*fE6tvit-HIAtiN8BaYQcog.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd ix">The Bellman equation (the next state s’ is sampled from the environment’s transition rules P)</strong></figcaption></figure><p id="0987" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">用于估计所有状态-动作对的最优<strong class="jv hz"> <em class="kw"> Q </em> </strong>值的最简单(也是最老的)算法是<a class="ae hv" href="https://link.springer.com/content/pdf/10.1007/BF00992698.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="jv hz">Q-学习</strong> </a>。它假设我们对于所有这样的对有一个大的查找表，并且在一个情节内，估计发生如下:</p><ul class=""><li id="15dc" class="mj mk hy jv b jw kr ka ks ke ml ki mm km mn kq mo mp mq mr bi translated">在时间步<strong class="jv hz"><em class="kw"/></strong>观察当前状态<strong class="jv hz"> <em class="kw"> s </em> </strong></li><li id="f9cc" class="mj mk hy jv b jw ms ka mt ke mu ki mv km mw kq mo mp mq mr bi translated">选择并执行一个动作<strong class="jv hz"> <em class="kw">一个当前状态具有最高<strong class="jv hz"><em class="kw"/></strong>值的</em> </strong>(或者有时选择随机动作— <strong class="jv hz">它被称为ϵ-greedy方法</strong>)。</li><li id="06f5" class="mj mk hy jv b jw ms ka mt ke mu ki mv km mw kq mo mp mq mr bi translated">在时间步<strong class="jv hz"> <em class="kw"> t+1观察下一个状态<strong class="jv hz"><em class="kw">s’</em>T3】和奖励<strong class="jv hz">T5】r’</strong></strong></em></strong>。T11】</li><li id="8b8a" class="mj mk hy jv b jw ms ka mt ke mu ki mv km mw kq mo mp mq mr bi translated">根据以下公式更新<strong class="jv hz"> Q值。这里我们估计<em class="kw"> Q' </em> </strong>出最佳<strong class="jv hz"> Q值</strong>为下一个状态，但是哪个动作<strong class="jv hz"> <em class="kw"> a' </em> </strong>导致这个最大值<strong class="jv hz"> Q </strong>并不十分重要:</li></ul><figure class="ky kz la lb fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lx"><img src="../Images/ebcaea9936ea7d3a06c3793f0f26c49f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FVS90CBYJuJAANj-dinc0g.png"/></div></div></figure><p id="5ad5" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">记忆所有状态-动作对的所有<strong class="jv hz"> <em class="kw"> Q </em> </strong>值是不切实际的。一个更好的方法是使用一个函数来逼近<strong class="jv hz"> <em class="kw"> Q </em> </strong>值。这就是神经网络发挥作用的地方。</p><p id="9745" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">一种叫做<a class="ae hv" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="jv hz">Deep Q-Network</strong></a>的算法，通过引入两种创新方法——<strong class="jv hz">经验重放</strong>和<strong class="jv hz">定期更新目标网络</strong>，大大提高了训练稳定性，降低了资源需求。</p><p id="7405" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hz">经验重放</strong>机制使用固定大小的单个重放存储器，其中存储了<strong class="jv hz"><em class="kw"/></strong>N个<strong class="jv hz"> <em class="kw"> (s，a，r，s’)</em></strong>元组。这些样本是在训练期间从重放存储器中随机抽取的。<strong class="jv hz">它显著提高了采样效率，降低了观察序列之间的相关性。</strong></p><p id="5c17" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hz">周期性更新的目标网络</strong>意味着保持<strong class="jv hz"> </strong>一个单独的<strong class="jv hz"> </strong>克隆的神经网络实例，其权重被冻结，并且仅在每个<strong class="jv hz"> <em class="kw"> C </em> </strong>步骤与主导网络同步。该网络用于在训练期间估计目标<strong class="jv hz"> <em class="kw"> Q </em> </strong>值。<strong class="jv hz">这种机制减少了短期波动的影响，从而稳定了训练过程。</strong></p><p id="b0f5" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hz">深度Q网络</strong>的损失函数看起来像这样(其中<strong class="jv hz"><em class="kw">【D】</em></strong>是来自重放存储器的均匀随机采样):</p><figure class="ky kz la lb fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mx"><img src="../Images/d4e095756244b5bbf5dc5731cf5b3fe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6RjVrGQdBrWl1H7CnCWZkw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd ix">Deep Q-Network loss function</strong></figcaption></figure><p id="9768" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">不幸的是，<strong class="jv hz"> Q-Learning </strong>(以及基于它的<strong class="jv hz"> DQN </strong>)算法有一个重大缺陷——<strong class="jv hz">它倾向于明显高估动作值</strong>。这是因为<strong class="jv hz"> Q-Learning </strong>算法使用相同的样本集来寻找最佳行动(具有最高的预期回报)并估计行动值。因此，如果该动作的值被高估，并且被选为最佳动作，那么<strong class="jv hz"> <em class="kw"> Q </em> </strong>的值也被高估。如果对所有<strong class="jv hz"> <em class="kw"> Q </em> </strong>值的高估不一致(这很大程度上取决于环境的转变规则和行动空间的大小)，我们将花费更多的时间探索这种非最优状态，学习过程将会很慢。你可以在这里 找到更正式的解释<a class="ae hv" href="https://papers.nips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hz">。</strong></a></p><p id="5283" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">这就是为什么在2010年，<strong class="jv hz"> Hado van Hasselt </strong>推出了一种估算<strong class="jv hz"><em class="kw"/></strong>Q值的新方法，称为<a class="ae hv" href="https://papers.nips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hz">双Q-learning </strong> </a>。<strong class="jv hz"> </strong>该方法<strong class="jv hz"> </strong>使用两个估值器<strong class="jv hz"> <em class="kw"> A </em> </strong>和<strong class="jv hz"> <em class="kw"> B，</em> </strong>交替更新。如果我们想要更新估计量<strong class="jv hz"> <em class="kw"> A </em> </strong>，那么使用估计量<strong class="jv hz"> <em class="kw"> B </em> </strong>来评估下一步的<strong class="jv hz"> <em class="kw"> Q </em> </strong>值。这种方法解决了高估问题，因为其中一个估计器可能会看到高估动作<strong class="jv hz"> <em class="kw"> a1 </em> </strong>的样本，而另一个估计器会看到高估动作<strong class="jv hz"> <em class="kw"> a2的样本。</em> </strong></p><p id="9da0" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">因此，在2015年，同一个团队发表了一篇论文，其中有一个更新版本的<strong class="jv hz"> DQN </strong>算法，称为<a class="ae hv" href="https://arxiv.org/abs/1509.06461" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hz">双深度Q-网络</strong> </a> <strong class="jv hz"> </strong>，它具有以下损失函数:</p><figure class="ky kz la lb fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es my"><img src="../Images/ca97c55fab7499ba835b8f5c42a0ac6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nUTamakZCMoufxdyociB1g.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd ix">Double Deep Q-Network loss function</strong></figcaption></figure><p id="9883" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">正如你在这里看到的，<strong class="jv hz"> argmax </strong>中动作的选择是由<strong class="jv hz">在线网络</strong>决定的，所以这个价值估计遵循了根据当前值的贪婪策略，但是我们使用<strong class="jv hz">目标网络</strong>来评估这个策略的价值。<strong class="jv hz">我们可以通过定期将目标网络</strong>的权重与<strong class="jv hz">在线网络</strong>同步或者<strong class="jv hz">切换这两个网络的角色来更新目标网络</strong>。这个损失函数将在本文后面的练习部分使用。</p><h1 id="d55e" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">实践</h1><p id="29b4" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在本文的练习部分，我们将使用来自超级马里奥兄弟<a class="ae hv" href="https://github.com/Kautenja/gym-super-mario-bros" rel="noopener ugc nofollow" target="_blank"><strong class="jv hz"/></a><strong class="jv hz">的第一关。</strong>游戏作为一种环境。默认情况下，单次观察是一张<strong class="jv hz">240×256</strong>像素的RGB图像，所以我们需要编写几个<a class="ae hv" href="https://github.com/openai/gym/tree/master/gym/wrappers" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hz">包装器</strong> </a> <strong class="jv hz"> </strong>将其转换为分辨率为<strong class="jv hz">84×84</strong>像素的灰度图像。同样，并不是所有的观察值都有用，所以我们将只使用<strong class="jv hz">每四个观察值</strong>和<a class="ae hv" href="https://github.com/openai/gym/blob/master/gym/wrappers/frame_stack.py" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hz">将它们堆叠在一起</strong> </a>:</p><figure class="ky kz la lb fd hk"><div class="bz dy l di"><div class="mz na l"/></div></figure><p id="16fd" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在我们可以创建我们的环境并且<strong class="jv hz">将随机种子设置为固定值以获得可重复的结果</strong>。同样，为了简单起见，<strong class="jv hz">动作空间被限制为两个动作</strong>——向右移动以及向右移动和跳跃的组合:</p><figure class="ky kz la lb fd hk"><div class="bz dy l di"><div class="mz na l"/></div></figure><p id="54a7" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在下面的所有实现中，我试图使用相同的模型架构、优化器、学习率和常见的超参数(如gamma)来直接比较算法的效率。</p><h2 id="d149" class="li iw hy bd ix lj lk ll jb lm ln lo jf ke lp lq jj ki lr ls jn km lt lu jr lv bi translated">政策梯度</h2><p id="b7ef" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">策略梯度算法的<strong class="jv hz">实现直接遵循上一节描述的算法，并使用<a class="ae hv" href="https://pytorch.org/docs/stable/distributions.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hz"> PyTorch发行版</strong> </a>包:</strong></p><figure class="ky kz la lb fd hk"><div class="bz dy l di"><div class="mz na l"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="ak">Policy Gradient implementation</strong></figcaption></figure><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es nb"><img src="../Images/977089d5833a3c26d22457722997db0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*-2_XNw1t6o9JUoDOwEiQqA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd ix">The plot of average reward per 10 episodes</strong></figcaption></figure><h2 id="c178" class="li iw hy bd ix lj lk ll jb lm ln lo jf ke lp lq jj ki lr ls jn km lt lu jr lv bi translated">双深度Q网络</h2><p id="2f38" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><strong class="jv hz"> DDQN </strong>的实现与上述理论仅在一个细节上有所不同——使用<a class="ae hv" href="https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html" rel="noopener ugc nofollow" target="_blank"><strong class="jv hz">smoothl loss</strong></a><strong class="jv hz"/>代替<a class="ae hv" href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hz"> MSE </strong> </a> <strong class="jv hz"> </strong>以获得更好的结果:</p><figure class="ky kz la lb fd hk"><div class="bz dy l di"><div class="mz na l"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="ak">Double Deep Q-Network implementation</strong></figcaption></figure><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es nb"><img src="../Images/eea5c1323feaf9844c0e0b844f9da6c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*kKWNbvv4Rr5a3q3E9iZv7g.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd ix">The plot of average reward per 10 episodes</strong></figcaption></figure><h2 id="dc54" class="li iw hy bd ix lj lk ll jb lm ln lo jf ke lp lq jj ki lr ls jn km lt lu jr lv bi translated">具有广义优势估计的近似策略优化</h2><p id="f5f6" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><strong class="jv hz"> PPO+GAE </strong>算法通过引入随机小批量训练稳定性，尽可能遵循理论解释来实现；</p><figure class="ky kz la lb fd hk"><div class="bz dy l di"><div class="mz na l"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="ak">PPO+GAE implementation</strong></figcaption></figure><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es nb"><img src="../Images/4e2bd1b69fc3933326fbad05fdcbf75a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*-9QBaXLOnjE8BzuHHFdBXw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd ix">The plot of average reward per 10 episodes</strong></figcaption></figure><h2 id="d169" class="li iw hy bd ix lj lk ll jb lm ln lo jf ke lp lq jj ki lr ls jn km lt lu jr lv bi translated">结果</h2><p id="62de" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">从上面的<strong class="jv hz"> </strong>图中可以看出，所述算法的相对样本效率符合其理论估计值。</p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es nc"><img src="../Images/13d4531d28ec53a9399889a8df44c14f.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/1*DmWrxmWbomfNC6IJge2Zog.gif"/></div></figure><p id="17ad" class="pw-post-body-paragraph jt ju hy jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hz">这个项目在我的GitHub上也有</strong><a class="ae hv" href="https://github.com/dredwardhyde/reinforcement-learning" rel="noopener ugc nofollow" target="_blank"><strong class="jv hz"/></a><strong class="jv hz">。</strong></p></div></div>    
</body>
</html>
<html>
<head>
<title>Getting started with web scraping for data analysis.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">开始使用web抓取进行数据分析。</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/getting-started-with-web-scraping-for-data-analysis-fe55cc0b8d61?source=collection_archive---------10-----------------------#2021-06-25">https://medium.com/geekculture/getting-started-with-web-scraping-for-data-analysis-fe55cc0b8d61?source=collection_archive---------10-----------------------#2021-06-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/8fdc8719e7847939babfe44dd900af3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qgQti9FY0Dp5oCKt8e8LLQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Image from undraw</figcaption></figure><blockquote class="iu iv iw"><p id="8354" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hb bi translated">“不做数据集就成不了数据科学家”~ kaggle</p></blockquote><p id="efd1" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">如果您正在为数据分析或机器学习收集数据，那么您正在创建自己的数据集！恭喜🎉。如果你热爱这个数据科学和分析的世界，你会遇到需要创建自己的数据集的时刻，而网络抓取是你获取文本数据的<strong class="ja hj"><em class="iz"/></strong>。所有的网站都会不断更新，因此网站结构可能会发生变化，导致代码可能无法工作，但策略的有效性仍将保持不变。</p><p id="f267" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated"><strong class="ja hj"> <em class="iz">我给你总结一下</em> </strong></p><p id="30d0" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">是的，我不会用从网上复制粘贴的方式来烦你们，但是我会简单介绍一下网络抓取，以及我们将在本文中讨论的内容。网页抓取是一种可以从实际网页中提取并保存数据的方法🤭。已经有很多资料来源，那么这篇文章有什么不同呢？你会问。你不仅会学到数据提取的基础知识，还会学到一些抽象概念，这些抽象概念会让你爱上数据提取的美妙之处，此外，我们还会讲述如何通过自动化和网络爬行来完成不完整的数据。我们的项目是提取地区<em class="iz">孟买</em>和<em class="iz">浦那</em>的大学分班数据，这是研究论文链接，如果你要实现它，我会推荐你浏览一下。</p><p id="9266" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated"><a class="ae jz" href="https://www.irjet.net/archives/V8/i6/IRJET-V8I6216.pdf" rel="noopener ugc nofollow" target="_blank">https://www.irjet.net/archives/V8/i6/IRJET-V8I6216.pdf</a></p><p id="c31d" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated"><strong class="ja hj"> <em class="iz">宝贝步骤</em> </strong></p><p id="b953" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">还有比蟒蛇🥺更漂亮的东西吗？帮助我们的是请求、美丽的海豚和熊猫</p><p id="4be5" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">要求:将以代码的形式为我们获取网页的内容</p><p id="c069" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">beautifulSoup:将通过从html元素中遍历和获取文本数据来帮助我们提取数据，它可以是带有一些文本的<p/></p><p id="7355" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">熊猫:它将为我们提供数据框架，它将为我们保存数据</p><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ka"><img src="../Images/7cb142972ffd325b0dfe0a5fa1c65225.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q5B9KwiFN7-DY3BV9zeLPQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Web scraping</figcaption></figure><blockquote class="iu iv iw"><p id="5289" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hb bi translated">请注意，上面代码片段中的代码将无法工作，因为网站正在更新，您可以考虑下一个示例中的代码，但提取策略仍然可以使用</p></blockquote><p id="97e4" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">好了，我们直接进入棘手的部分😬但是我们有这个。既然你已经看到了代码片段，你们中的许多人应该已经认识到了这个结构，但是我仍然会详细解释它。初始化导入后，我们在<em class="iz">请求的帮助下获取网站的内容。get </em>然后我们用<em class="iz"> BeautifulSoup </em>解析内容。之后你应该查看你的源代码，如果你使用笔记本，只需运行带有<em class="iz"> src </em>的代码单元，因为我们的源代码就存储在其中。</p><p id="36b9" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">还记得抽象这个词吗？🤔在这里，如果你能看到我们正在提取的网页，它不包含文本，但图像。但是我们已经有了知道一切的消息来源😌虽然Verbos很大！亲提示✨只是做一个快速搜索的cmd + F和搜索你想为我提取的是一个公司的名称，我可以看到很多名字，我只是搜索其中一个，并找到了值的结构在“alt”描述符的img标签在html中。现在，我们希望提取所有公司的名称，我们使用<em class="iz"> find_all </em>，因为<em class="iz"> &lt; img &gt; </em>标签中的信息，我们已经将其指定为img，并为其分配了类名，因为所有公司都有相同的类名。然后我们将它附加到dataframe中并导出csv文件。因此，即使有许多方法可以将数据以不同的结构呈现，您仍然可以提取它。</p><div class="kf kg ez fb kh ki"><a href="https://github.com/siddheshshivdikar/college-placement-scraping" rel="noopener  ugc nofollow" target="_blank"><div class="kj ab dw"><div class="kk ab kl cl cj km"><h2 class="bd hj fi z dy kn ea eb ko ed ef hh bi translated">siddheshvdikar/大学-安置-刮刮</h2><div class="kp l"><h3 class="bd b fi z dy kn ea eb ko ed ef dx translated">孟买和浦那大学50 +所大学数据的搜集与分析👨🏻‍🏫 …</h3></div><div class="kq l"><p class="bd b fp z dy kn ea eb ko ed ef dx translated">github.com</p></div></div><div class="kr l"><div class="ks l kt ku kv kr kw io ki"/></div></div></a></div><p id="c34c" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">与上面的例子相似，你会发现我的github repo有很多变化，它包含了从表格、div甚至动态出现的html片段中提取信息的变化。</p><p id="853a" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated"><strong class="ja hj">欺骗网站:不是机器人🤖</strong></p><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kx"><img src="../Images/4ac69fbded0d55865bf5ff2bede2ed9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ER-HVpfW1Xp-xVcxETIOVw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Forbidden to access url</figcaption></figure><pre class="kb kc kd ke fd ky kz la lb aw lc bi"><span id="5d47" class="ld le hi kz b fi lf lg l lh li">headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}</span><span id="3681" class="ld le hi kz b fi lj lg l lh li">url = 'enter_your_url'</span><span id="9c2a" class="ld le hi kz b fi lj lg l lh li">r= requests.get(base_url+str(page)+'.htm',headers=headers)</span></pre><p id="8bba" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">粘贴网站的网址，那容易吗？是的，但是有一个问题，一个网站可能会认为这个程序是一个机器人，并会否认发送页面源。这是一个非常简单的步骤来欺骗web源，它可能会拒绝你获取页面源，你猜对了，他们不想让你抓取数据。头考虑元信息，你的电脑可以欺骗这将允许你刮你的数据。为了进一步增强这一点，您甚至可以创建一个虚拟设备，并为其分配一个虚拟mac地址。在我们的场景中，我们仅仅通过简单的头球就晋级了。</p><p id="ec32" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated"><strong class="ja hj">另一个:表格示例💁</strong></p><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lk"><img src="../Images/00c3233c476ff226d380075619bd25fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CiMIm24dEJ6sEFr2am_bmw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">html table extraction</figcaption></figure><p id="d194" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">与上述提取方法类似，我们添加了标题以避免禁止访问，这里用于提取的策略称为<em class="iz">组件挖掘。</em>向下挖掘嵌套的html元素，直到找到可以循环提取的数据。进一步简化步骤html元素只是深入考虑它是一个地雷！</p><pre class="kb kc kd ke fd ky kz la lb aw lc bi"><span id="6379" class="ld le hi kz b fi lf lg l lh li">Layer1 | Layer2 | Layer3 | <strong class="kz hj">Amethyst</strong></span></pre><blockquote class="iu iv iw"><p id="b367" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hb bi translated">类似地</p></blockquote><pre class="kb kc kd ke fd ky kz la lb aw lc bi"><span id="11b9" class="ld le hi kz b fi lf lg l lh li">Src | Table | Tr | Td |<strong class="kz hj"> Data</strong></span></pre><p id="6064" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">你们都可以自己尝试这段代码，它已经过更新，可以工作了，下面的代码片段也可以工作了，🧑🏻‍💻。</p><p id="cd27" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated"><strong class="ja hj">自动化:将死♛ </strong></p><p id="629d" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">这是自然语言处理和网页抓取的另一部分。正如你所看到的，我们已经从网上搜集了学院的名字，但是现在我们需要公司当年和本地区的工资。</p><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ll"><img src="../Images/768d7a0e4561854c40b89b1de32d901d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kowUUHFrsFER9s94gqZpyQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Automated Natural Language Processing with web scraping</figcaption></figure><p id="623e" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">下面是自动化代码片段的一个简单想法，现在您可以使用Google api来生成自定义查询。我们需要我们的查询有点像这样“公司的薪金<strong class="ja hj"> <em class="iz">公司名称</em></strong><strong class="ja hj"><em class="iz">地区名称</em> </strong>年份<strong class="ja hj"> <em class="iz">年份编号</em> </strong>”，因为我们已经提取了查询中粗体列名称的所有值。我们可以从csv加载数据帧，对于中的每一行，我们可以将值放在搜索查询中，此外，您可以在查询中包括新的角色。我们需要缺失的金额值，此外，我们知道INR以₹开始，所以只需找到货币符号，然后提取前面的数字条目，我们前面只使用了7位数字，因为我们已经研究了查询结果，就像一个迷人的✨.</p><p id="0ec9" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated"><strong class="ja hj">爬行是一种方式:网络爬行🕸 </strong></p><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/43720fa5570c80aba4c50a13d3bcc094.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YWeH_GZa1MfJSJPGwnsDjQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Pagination with similar structure.</figcaption></figure><p id="f333" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">网页抓取简化！有些网页包含分页，分页就像你在Amazon.com上看到的，你在那里搜索一个产品，会有不同的结果，也有不同的页面。如果你注意到这些网页的结构是一样的，对吧。此外，如果你注意到链接保持不变，但只有一个数字的变化！</p><pre class="kb kc kd ke fd ky kz la lb aw lc bi"><span id="ebe3" class="ld le hi kz b fi lf lg l lh li">https://www.amazon.in/s?k=macbook&amp;page=<strong class="kz hj"><em class="iz">2</em></strong>&amp;qid=1624594558&amp;ref=sr_pg_<strong class="kz hj"><em class="iz">2</em></strong></span><span id="094d" class="ld le hi kz b fi lj lg l lh li">https://www.amazon.in/s?k=macbook&amp;page=<strong class="kz hj"><em class="iz">3</em></strong>&amp;qid=1624594582&amp;ref=sr_pg_<strong class="kz hj"><em class="iz">3</em></strong></span></pre><p id="6ae0" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">如果你参考上面的链接，你可以看到页码是唯一更新的东西。为什么这对我们来说是一件大事，我们可以一页一页地提取页面？不，我们使用这种抓取技术来抓取相似的信息。</p><p id="6079" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated"><strong class="ja hj">让我们爬起来:拼起来😶‍🌫️ </strong></p><p id="32a7" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">我们拥有实现网络爬虫的一切关键因素。让我们抓取glassdoor作为参考，并提取工资和公司，以匹配我们以前获得的仅包含图像的大学数据集的缺失值。</p><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lk"><img src="../Images/ab4c9cc27b317bc698c05c5a2415cf41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9xWC-9MlkIPNu321OSJtOA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">web crawling</figcaption></figure><p id="858e" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">为了从一定数量的页面中抓取所有数据，我们需要知道页面的结束限制，这可以通过查看分页计数很容易地知道。我们将使用我们已经学到的知识，随着url中页码的变化，我们将使用字符串操作技术来生成我们的url，正如您可以看到的，我们的循环将运行92次，每次迭代都会在URL中插入编号，如下所示。</p><pre class="kb kc kd ke fd ky kz la lb aw lc bi"><span id="5b50" class="ld le hi kz b fi lf lg l lh li">r= requests.get(base_url+<strong class="kz hj">str(page)</strong>+'.htm',headers=headers)</span></pre><p id="c9e8" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">获得url后，我们将收集必要的数据，这类似于我们已经讨论过的过程。我还是会把它摘录下来，这样你就能理解我在这里提到的那块。</p><pre class="kb kc kd ke fd ky kz la lb aw lc bi"><span id="e86e" class="ld le hi kz b fi lf lg l lh li">c=r.content<br/>    soup=BeautifulSoup(c,"html.parser")<br/>    div=soup.find_all("div",{"class":"flex__topSelf SalaryRowStyle__employerAndJobInfo flex__flex6of12"})<br/>    for item in div:<br/>        d={}<br/>        d["Company Name"]=(item.find("div",{"class":"JobInfoStyle__employerName"}).text)<br/>        d["Salary"]=(item.find("div",{"class":"JobInfoStyle__meanBasePay formFactorHelpers__showHH"}).text)<br/>        l.append(d)<br/>df=pandas.DataFrame(l)</span></pre><p id="407e" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">接下来，您可以简单地将其附加到您的数据帧中，并根据您的喜好导出为csv或xlxs格式。这使您可以自由地进行列匹配，以将那些不完整的值有效地填充到80%的准确度。</p><p id="cc22" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated"><strong class="ja hj">总结:结论🥳 </strong></p><p id="e797" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated">爱上数据科学和分析领域是如此容易，它让你研究各种有趣的主题，挖掘已经挖掘出来但尚未发现的成果。我和我的团队成员以及项目向导也成功地生成了数据集。我将链接下面的数据集，如果你喜欢这个数据集，请考虑投票支持这个数据集并分享这篇文章。我要感谢并提及我们的项目指南<a class="ln lo ge" href="https://medium.com/u/4b07afb4ea42?source=post_page-----fe55cc0b8d61--------------------------------" rel="noopener" target="_blank"> Sharvari Govilkar </a>为我们提供指导，激励我们选择一个未经研究的、区域性的、最重要的是对安置团队有所贡献和帮助的主题。我要感谢我们的团队成员<a class="ln lo ge" href="https://medium.com/u/ee1e9288d2e5?source=post_page-----fe55cc0b8d61--------------------------------" rel="noopener" target="_blank"> Akhilesh Shinde </a>、<a class="ln lo ge" href="https://medium.com/u/5480407f3523?source=post_page-----fe55cc0b8d61--------------------------------" rel="noopener" target="_blank"> Aniket Shinde </a>和<a class="ln lo ge" href="https://medium.com/u/2ca651efa263?source=post_page-----fe55cc0b8d61--------------------------------" rel="noopener" target="_blank"> Vivek Singh </a>对工作的贡献。这的确是一次难忘的经历。我将很快写一篇关于发布数据集的文章，直到那时，快乐编码每个人🦦.</p><h2 id="201f" class="ld le hi bd lp lq lr ls lt lu lv lw lx jw ly lz ma jx mb mc md jy me mf mg mh bi translated">你准备好下一部分了吗？动态刮擦来了🙃</h2><p id="c98e" class="pw-post-body-paragraph ix iy hi ja b jb mi jd je jf mj jh ji jw mk jl jm jx ml jp jq jy mm jt ju jv hb bi translated"><a class="ae jz" href="https://siddheshshivdikar.medium.com/dynamic-web-scraping-using-selenium-scrape-protected-websites-401de3497939" rel="noopener">https://siddheshshivdikar . medium . com/dynamic-web-scrape-using-selenium-scrape-protected-websites-401 de 3497939</a></p><p id="4194" class="pw-post-body-paragraph ix iy hi ja b jb jc jd je jf jg jh ji jw jk jl jm jx jo jp jq jy js jt ju jv hb bi translated"><a class="ae jz" href="https://www.kaggle.com/siddheshshivdikar/college-placement" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/siddheshshivdikar/college-placement</a></p></div></div>    
</body>
</html>
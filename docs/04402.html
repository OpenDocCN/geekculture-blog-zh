<html>
<head>
<title>Logistic Regression: Implementation from Scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归:用Python从头实现</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/logistic-regression-implementation-from-scratch-in-python-f9d6cd4a0747?source=collection_archive---------5-----------------------#2021-06-26">https://medium.com/geekculture/logistic-regression-implementation-from-scratch-in-python-f9d6cd4a0747?source=collection_archive---------5-----------------------#2021-06-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="1ba3" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">包括二元分类和多类分类。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/52b8b2fc3ca12f7d9ecd93ee3effc6c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jBBmKt07SrO4q8MGYLgBYg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Drawn by author in excalidraw.com</figcaption></figure><p id="d584" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">你想知道机器学习模型是如何做出“这个人到底有没有心脏病”这样的分类的吗？？逻辑回归是最常用的算法之一。逻辑回归在某些方面类似于传统的线性回归。它们通常是向学习机器学习模型的初学者介绍的前两个模型。然而，逻辑回归是一个非常强大的分类模型，能够处理大型数据集和大量的特征。</p><p id="25d6" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我能想到的逻辑回归的一个显著应用是，在自然语言处理领域，特别是在情感分析方面。普遍存在的是，在决定是否真的有必要继续使用更复杂的语言模型，或者至少使用与复杂模型的比较之前，人们经常试图将逻辑回归模型应用于情感分析以测试性能。事实上，我已经使用逻辑回归模型构建了一个用于IMDB电影评论情感分析的<a class="ae kj" href="https://imdb-review-sentiment-1.herokuapp.com/" rel="noopener ugc nofollow" target="_blank"> Streamlit应用</a>，该应用开箱即用，已经实现了50k条评论中90%的准确率。</p><p id="482b" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">今天，我将分享我如何通过在Python中从头实现逻辑回归算法来巩固我对逻辑回归的理解。还将介绍二元分类和多类分类的实现。</p><p id="8b85" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">训练大多数机器学习模型的一些关键概念已经在<a class="ae kj" rel="noopener" href="/geekculture/understanding-linear-regression-by-implementing-from-scratch-761e2a48fce4">我的上一篇文章</a>从零开始实现线性回归中涉及到了，其中大部分可以应用于逻辑回归，因此，在本文中不再详细解释。</p><div class="kk kl ez fb km kn"><a rel="noopener follow" target="_blank" href="/geekculture/understanding-linear-regression-by-implementing-from-scratch-761e2a48fce4"><div class="ko ab dw"><div class="kp ab kq cl cj kr"><h2 class="bd hj fi z dy ks ea eb kt ed ef hh bi translated">通过从头开始实施来理解线性回归</h2><div class="ku l"><h3 class="bd b fi z dy ks ea eb kt ed ef dx translated">有梯度下降和tf。梯度胶带</h3></div><div class="kv l"><p class="bd b fp z dy ks ea eb kt ed ef dx translated">medium.com</p></div></div><div class="kw l"><div class="kx l ky kz la kw lb jh kn"/></div></div></a></div><h1 id="8768" class="lc ld hi bd le lf lg lh li lj lk ll lm io ln ip lo ir lp is lq iu lr iv ls lt bi translated">逻辑回归简介</h1><p id="08c3" class="pw-post-body-paragraph jn jo hi jp b jq lu ij js jt lv im jv jw lw jy jz ka lx kc kd ke ly kg kh ki hb bi translated">逻辑回归类似于线性回归，它们都是<strong class="jp hj">监督的机器学习模型</strong>，但是逻辑回归是为<strong class="jp hj">分类</strong>任务而不是回归任务设计的。换句话说，逻辑回归用于预测离散变量(也称为分类变量)，而不是连续变量(数值变量)。</p><p id="8799" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">一般情况下，逻辑回归用于<strong class="jp hj">二元分类</strong>，这里只有两类需要分类，如健康或不健康。但是，它可以很容易地扩展到多类分类，因为逻辑回归的本质是用二项式概率分布建模的。在本文中，<strong class="jp hj">将涵盖</strong>二元分类和多类分类实现，但是为了进一步理解多类分类的工作原理，你可以参考<a class="ae kj" href="https://machinelearningmastery.com/multinomial-logistic-regression-with-python/" rel="noopener ugc nofollow" target="_blank">在Jason Brownlee的Machine Learning Mastery上发表的这篇惊人的文章</a>。</p><p id="5763" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">就像线性回归一样，逻辑回归也是对数据拟合一条线来进行预测。但是，逻辑回归拟合一个“S”形的<strong class="jp hj">S形函数</strong>(或“<strong class="jp hj">逻辑函数</strong>”，因此得名)，而不是一条直线，如下图中的<strong class="jp hj">所示。这个函数用一个称为<strong class="jp hj">二项式</strong> <strong class="jp hj">分布</strong>的概率分布来建模。</strong></p><p id="eba0" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">因此，在<strong class="jp hj"> y轴</strong>上，逻辑回归的y轴使用获得“是”或“否”的概率，例如在下方的<strong class="jp hj">图中，“是健康的”或“不健康的”，而不是使用直接来自特征本身的连续值(如线性回归)。然后，通过与指定的阈值(通常为<strong class="jp hj"> 0.5 </strong>)进行比较，使用这些概率来预测该人是“健康”还是“不健康”，以决定输出类别。在本例中，任何高于0.5的数据点都将被归类为“健康”，任何低于0.5的数据点都将被归类为“不健康”。</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lz"><img src="../Images/eab1041dba4bb4621750f81ca49b7d37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FZVxEulyZhXGSA1KNOnC7w.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Drawn by author in excalidraw.com. Inspired by Josh Starmer’s <a class="ae kj" href="https://youtu.be/yIYKR4sgzI8" rel="noopener ugc nofollow" target="_blank">video</a>.</figcaption></figure><p id="f5c9" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">注意</strong>:在这篇文章中，我将使用一些美丽的图表，这些图表的灵感来自于<a class="ae kj" href="https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw" rel="noopener ugc nofollow" target="_blank"> Josh Starmer </a>在他的<strong class="jp hj"> YouTube StatQuest视频系列</strong>中制作的关于逻辑回归的图表(下面显示了该系列的第一个视频)，因为他的图表真的很棒，我真的很喜欢看他用这些视觉效果教学。我推荐你去看他的视频，真正了解逻辑回归是如何工作的，他清晰的解释和精彩的视觉效果(更不用说吸引人的歌曲和“BAMS”)，通过他的视频学习真的很享受，也很有教益。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ma mb l"/></div></figure><blockquote class="mc md me"><p id="5ced" class="jn jo mf jp b jq jr ij js jt ju im jv mg jx jy jz mh kb kc kd mi kf kg kh ki hb bi translated">在本文中，我将通过在Python中从头实现逻辑回归，在直觉和数学之间架起一座桥梁。直觉主要来自逻辑回归的StatQuest视频，而数学主要来自吴恩达在Coursera上的<a class="ae kj" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习课程</a>(第3周)。</p></blockquote></div><div class="ab cl mj mk gp ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="hb hc hd he hf"><h2 id="6ff0" class="mq ld hi bd le mr ms mt li mu mv mw lm jw mx my lo ka mz na lq ke nb nc ls nd bi translated">那么我们如何获得每个数据点的概率呢？</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ne"><img src="../Images/847358a91f20d9da99e5d612924a7033.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w5F5ALv2fAAfNsGlZBn_ZA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image drawn by author in excalidraw.com. Inspired by Josh Starmer’s <a class="ae kj" href="https://youtu.be/BfKanl1aSG0?list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF" rel="noopener ugc nofollow" target="_blank">video</a> about Logistic Regression.</figcaption></figure><p id="b1ee" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">图的右侧显示了如何将一条线拟合到数据点上(就像线性回归中一样)，以获得每个数据点的<em class="mf"> log(odds) </em>、<em class="mf"> </em> aka <strong class="jp hj"> logits </strong>。<em class="mf"> log(odds) </em>通过将它们投影到拟合线上并在<strong class="jp hj"> y轴</strong>上取值获得。因此，用于逻辑回归的<em class="mf"> log(odds) </em>的等式可以写成:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nf"><img src="../Images/e074982fb5cc2f8dd1a16fac268173a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*WB4xvPCLpX49NkY5vzzwRw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Equation to obtain the best-fit line for log(odds).</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ng"><img src="../Images/f829e63dbd2b727f40a5f1b4c1a9e96f.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*agQjr8hF96MHRlEfknTTpg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Vectorized version.</figcaption></figure><p id="7582" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">等式的右边就像我上一篇文章中的<a class="ae kj" rel="noopener" href="/geekculture/understanding-linear-regression-by-implementing-from-scratch-761e2a48fce4">所示，拟合一条直线进行线性回归，其中<code class="du nh ni nj nk b">W</code>是由每个特征的斜率组成的矩阵，具有<code class="du nh ni nj nk b">(number_of_features, 1)</code>的<strong class="jp hj">形状</strong>；而<code class="du nh ni nj nk b">X</code>是由每个样本的特征组成的矩阵，具有<code class="du nh ni nj nk b">(number_of_samples, number_of_features)</code>的<strong class="jp hj">形状</strong>。然后将<em class="mf">对数(赔率)</em>转换为图左侧的概率方程<code class="du nh ni nj nk b">p</code>，其中图中的<strong class="jp hj"> y轴</strong>代表概率。</a></p><p id="52b7" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><em class="mf">赔率</em>类似于概率，它们是通过计算“是”类别相对于“否”类别的出现次数的比率而获得的。例如，假设有5个<strong class="jp hj">肥胖</strong>的人和10个<strong class="jp hj">不肥胖</strong>的人，那么<strong class="jp hj">肥胖</strong>的<em class="mf">几率</em>为5:10或5/10，而<strong class="jp hj">肥胖</strong>的几率为5/(5+10)。<em class="mf"> log(odds) </em>仅仅是几率的自然对数。为了进一步理解关于赔率和<em class="mf"> log(odds) </em>的概念，你可以参考Josh Starmer的这个<a class="ae kj" href="https://youtu.be/ARfXDSkQf1Y?list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF" rel="noopener ugc nofollow" target="_blank">惊人视频</a>。</p><p id="bfd9" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在上图中，我们可以看到概率的公式<code class="du nh ni nj nk b">p = np.exp(np.log(odds)) / (1 + np.exp(np.log(odds))</code>。这个公式是由<em class="mf">log(odds)</em>=<em class="mf">log(p/(1-p))</em>的方程推导出来的。要看推导过程，可以参考上图链接的视频(时间为<strong class="jp hj"> 4:00 </strong>)。通过将分子和分母都除以指数项<code class="du nh ni nj nk b">np.exp(np.log(odds)</code>，该公式还可以进一步简化为<code class="du nh ni nj nk b">p = np.exp(1 / (1 + np.exp( - np.log(odds))</code>。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nl"><img src="../Images/60b2aa71e220a1578a91830e88fccb7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*sN2SuZbs-X5Hwj79KSyN3A.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Sigmoid function of logits in logistic regression.</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nm"><img src="../Images/4796278bf6b37ca4303df6bd2f71e1b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:232/format:webp/1*ekcvGx1C4nPvW71dYRBtrw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Sigmoid function.</figcaption></figure><p id="1c12" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这是最常见的公式形式，称为<strong class="jp hj"> sigmoid函数</strong>或<strong class="jp hj">逻辑函数</strong>。在逻辑回归的情况下，我们具体使用<em class="mf"> log(odds) </em> 的<strong class="jp hj"> sigmoid函数来获得概率。</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nn"><img src="../Images/86d4e6b0d77c34df59b194171d7fb89e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cbU9i63iMD9jpZHO.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Sigmoid function. Image from <a class="ae kj" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank">Wikipedia</a>.</figcaption></figure><p id="acbd" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">同时，<em class="mf">对数(赔率)</em>是通过将一条线拟合到数据点而获得的，如本节上文所述。稍后，这些概念将应用于构建实现。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es no"><img src="../Images/c4542a07b33931d7e16a5c81d28bbba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*zblJQURTM-acnTUakxXo8Q.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">The final equation to compute the probability.</figcaption></figure><h1 id="e419" class="lc ld hi bd le lf lg lh li lj lk ll lm io ln ip lo ir lp is lq iu lr iv ls lt bi translated">为二元分类训练逻辑回归模型的关键概念概述</h1><h2 id="5249" class="mq ld hi bd le mr ms mt li mu mv mw lm jw mx my lo ka mz na lq ke nb nc ls nd bi translated">进行预测的假设方程</h2><p id="7fd1" class="pw-post-body-paragraph jn jo hi jp b jq lu ij js jt lv im jv jw lw jy jz ka lx kc kd ke ly kg kh ki hb bi translated">概率从下面的等式中获得。然后，如上所述，通过与阈值进行比较来预测类别。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es np"><img src="../Images/eda338fb287e3ee9e7c50fb069fe8c59.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*vWZIPKlJ9qRDL7EosII2xg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">The sigmoid function of logits (equivalent to the equation from the fitted line) in logistic regression</figcaption></figure><h2 id="c599" class="mq ld hi bd le mr ms mt li mu mv mw lm jw mx my lo ka mz na lq ke nb nc ls nd bi translated">因素</h2><p id="8f8a" class="pw-post-body-paragraph jn jo hi jp b jq lu ij js jt lv im jv jw lw jy jz ka lx kc kd ke ly kg kh ki hb bi translated">待训练参数与线性回归相同。</p><p id="b7cc" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><code class="du nh ni nj nk b">W</code>:坡度系数，形状为<code class="du nh ni nj nk b">(number_of_features, 1)</code>。</p><p id="38e3" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><code class="du nh ni nj nk b">b</code>:y轴截距。</p><h2 id="6c28" class="mq ld hi bd le mr ms mt li mu mv mw lm jw mx my lo ka mz na lq ke nb nc ls nd bi translated">成本函数(或损失函数)</h2><p id="6a13" class="pw-post-body-paragraph jn jo hi jp b jq lu ij js jt lv im jv jw lw jy jz ka lx kc kd ke ly kg kh ki hb bi translated">二元交叉熵损失(又名对数损失):</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nq"><img src="../Images/3ca1105a3f1cb091e1d385ee13b2aaf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*6Dsu0EjojE-HKoM6g3W8eg.png"/></div></figure><p id="1998" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">其中:<br/> <strong class="jp hj"> m </strong> =样本数。<br/> <strong class="jp hj"> y </strong> =真值，通常在二进制分类的情况下只由0和1组成。<br/> <strong class="jp hj"> h </strong> =假设方程，在这种情况下，方程获得概率(如上图)。</p><p id="da1e" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这种最小化交叉熵损失的方法也被称为<strong class="jp hj">最大似然估计(MLE) </strong>。</p><h2 id="418d" class="mq ld hi bd le mr ms mt li mu mv mw lm jw mx my lo ka mz na lq ke nb nc ls nd bi translated">派生物</h2><p id="0f6e" class="pw-post-body-paragraph jn jo hi jp b jq lu ij js jt lv im jv jw lw jy jz ka lx kc kd ke ly kg kh ki hb bi translated">成本函数相对于<strong class="jp hj"> b </strong>的导数(图中<strong class="jp hj"><em class="mf"/></strong>):</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nr"><img src="../Images/8b699b68d30a8c7cb7a7967e2c6d3433.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*7EGKAxhCKt9EeSNbBvkjGw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Derivation for the y-intercept in cross-entropy, from Wikipedia. (<a class="ae kj" href="https://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="ce41" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">成本函数相对于<strong class="jp hj"> W </strong>的导数(图中<strong class="jp hj"><em class="mf"/></strong>):</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ns"><img src="../Images/4f331819fe0028ab40c6910e091c49f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*C0uJblrhwmrmlqEF0i8PdA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Derivation for the slope coefficient in cross-entropy, from Wikipedia. (<a class="ae kj" href="https://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="56b9" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在成本函数中有一个<code class="du nh ni nj nk b">(1 / m)</code>的乘法，但是它们不包括在维基百科的推导中。因此，代码的“翻译”应该是:</p><pre class="iy iz ja jb fd nt nk nu nv aw nw bi"><span id="0d54" class="mq ld hi nk b fi nx ny l nz oa"># Let y_proba = the probability computed from the hypothesis equation<br/>db = (1 / m) * np.sum(y_proba - y)<br/>dW = (1 / m) * (X.T @ (y_proba - y))</span></pre><h1 id="c545" class="lc ld hi bd le lf lg lh li lj lk ll lm io ln ip lo ir lp is lq iu lr iv ls lt bi translated">二元分类的实现</h1><p id="5edd" class="pw-post-body-paragraph jn jo hi jp b jq lu ij js jt lv im jv jw lw jy jz ka lx kc kd ke ly kg kh ki hb bi translated">让我们先准备好数据集。我们使用<a class="ae kj" href="https://www.kaggle.com/ronitf/heart-disease-uci" rel="noopener ugc nofollow" target="_blank">心脏病UCI数据集</a>来实现对一个人是否患有心脏病的二元分类。</p><pre class="iy iz ja jb fd nt nk nu nv aw nw bi"><span id="e28a" class="mq ld hi nk b fi nx ny l nz oa">df = pd.read_csv('heart.csv')<br/>df.head()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ob"><img src="../Images/cc237ecd64cb0a6840eb1d2950db5695.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*D9aDhpH7E6Pm1_aNUeXXrA.png"/></div></figure><p id="78fa" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">将数据集分为训练集和测试集:</p><pre class="iy iz ja jb fd nt nk nu nv aw nw bi"><span id="6537" class="mq ld hi nk b fi nx ny l nz oa">X = df.drop(columns='target')<br/>y = df['target']<br/>X.shape, y.shape<br/># ((303, 13), (303,))</span><span id="761e" class="mq ld hi nk b fi oc ny l nz oa">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)<br/>y_train.shape, y_test.shape<br/># ((242,), (61,))</span><span id="5ebb" class="mq ld hi nk b fi oc ny l nz oa"># IMPORTANT STEP (as explained in my previous article)<br/>y_train = y_train.values.reshape(-1, 1)<br/>y_test = y_test.values.reshape(-1, 1)<br/>y_train.shape, y_test.shape<br/>((242, 1), (61, 1))</span></pre><p id="87fd" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">缩放数据集，以确保在稍后与Scikit-learn实施进行比较时获得更准确和一致的结果。</p><pre class="iy iz ja jb fd nt nk nu nv aw nw bi"><span id="7ce2" class="mq ld hi nk b fi nx ny l nz oa">scaler = StandardScaler()<br/>X_train = scaler.fit_transform(X_train)<br/>X_test = scaler.transform(X_test)</span></pre><p id="38bc" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">那么培训过程可以分为:</p><ol class=""><li id="28a3" class="od oe hi jp b jq jr jt ju jw of ka og ke oh ki oi oj ok ol bi translated">随机初始化参数</li><li id="9de5" class="od oe hi jp b jq om jt on jw oo ka op ke oq ki oi oj ok ol bi translated">使用当前状态的模型进行预测以获得概率</li><li id="12da" class="od oe hi jp b jq om jt on jw oo ka op ke oq ki oi oj ok ol bi translated">计算成本函数的导数</li><li id="0357" class="od oe hi jp b jq om jt on jw oo ka op ke oq ki oi oj ok ol bi translated">使用梯度下降来更新参数</li><li id="0b7e" class="od oe hi jp b jq om jt on jw oo ka op ke oq ki oi oj ok ol bi translated">根据给定的迭代次数，循环重复步骤2-4</li></ol><p id="bb4a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">下面的<code class="du nh ni nj nk b">class</code>中的<code class="du nh ni nj nk b">fit</code>方法包含了整个训练过程的代码。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="or mb l"/></div></figure><p id="a9b3" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">第10行</strong>:<code class="du nh ni nj nk b">init_params</code>方法实现参数的随机初始化。</p><p id="1ebd" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">第23行</strong>:使用<code class="du nh ni nj nk b">get_logits</code>方法得到拟合直线的方程，相当于逻辑回归情况下的<em class="mf"> log(odds) </em>或logits。</p><p id="5eff" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">第30行</strong>:<strong class="jp hj"/><code class="du nh ni nj nk b">predict_proba</code>方法是用于获得二元分类概率的sigmoid函数。</p><p id="d159" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">第38行</strong>:<code class="du nh ni nj nk b">fit</code>方法实现整个训练过程。</p><p id="5eee" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">Line 76–84</strong>:<code class="du nh ni nj nk b">predict</code>方法用于通过将概率与指定阈值进行比较来获得预测的类别，而<code class="du nh ni nj nk b">predict_score</code>方法用于计算预测的准确性。</p><p id="dcdb" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我决定继承<code class="du nh ni nj nk b">BaseEstimator</code>和<code class="du nh ni nj nk b">ClassifierMixin</code>类，以便能够使用这个类通过<code class="du nh ni nj nk b">sklearn</code>的<code class="du nh ni nj nk b">cross_val_score</code>进一步计算交叉验证。但是如果认为没有必要，你可以省略它们。</p><p id="5bc1" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">现在，我们来看看训练结果。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="or mb l"/></div></figure><pre class="iy iz ja jb fd nt nk nu nv aw nw bi"><span id="0a65" class="mq ld hi nk b fi nx ny l nz oa"># OUTPUT:<br/>My implementation:<br/>0.8524590163934426<br/>[[25  4]<br/> [ 5 27]]</span><span id="bd1d" class="mq ld hi nk b fi oc ny l nz oa">Scikit-learn implementation:<br/>0.8524590163934426<br/>[[25  4]<br/> [ 5 27]]</span><span id="3aa3" class="mq ld hi nk b fi oc ny l nz oa">Cross-validation score:<br/>0.8179421768707483<br/>0.8179421768707483</span></pre><p id="26d0" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">太好了！测试精度，混淆矩阵，甚至交叉验证分数都和<code class="du nh ni nj nk b">sklearn</code>实现一样。但是，有一个问题，每次我拟合模型都得到相同的结果，虽然我已经将NumPy的随机种子设置为42，但我不确定为什么每次结果都不同。因此，您可能需要重复几次模型创建和训练，以获得与<code class="du nh ni nj nk b">sklearn</code>实现相同的结果。现在让我们展示一些漂亮的视觉效果。</p><h1 id="8dac" class="lc ld hi bd le lf lg lh li lj lk ll lm io ln ip lo ir lp is lq iu lr iv ls lt bi translated">形象化</h1><pre class="iy iz ja jb fd nt nk nu nv aw nw bi"><span id="596d" class="mq ld hi nk b fi nx ny l nz oa"># plot confusion matrix for binary classification<br/>from sklearn.metrics import plot_confusion_matrix<br/>plot_confusion_matrix(my_log_reg, X_test, y_test, cmap='Blues', display_labels=['Healthy', 'Heart Disease'])<br/>plt.grid(None);</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es os"><img src="../Images/ec8be655994696507bfbb83e45ef00cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*Rt-rB4H-uqFZXki0i9yocw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by author.</figcaption></figure><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="or mb l"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ot"><img src="../Images/0dd94c2445ad95611b3a6d270b75dd58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*xXyTChQGw1YQ7nkRn3fx3w.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Image by author.</figcaption></figure><p id="dbaa" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这种可视化显示了由训练模型计算的预测概率，然后数据点的不同颜色显示了“不健康”(患有心脏病)或“健康”的实际标签。请注意，该曲线看起来类似于s形曲线。</p><p id="cb25" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">好的，我们对二元分类的实现结果很满意。接下来，我也想谈谈多类分类的实现，因为它实际上是当前类的简单扩展。</p><h1 id="87a2" class="lc ld hi bd le lf lg lh li lj lk ll lm io ln ip lo ir lp is lq iu lr iv ls lt bi translated">多项式逻辑回归的关键概念</h1><blockquote class="mc md me"><p id="aa0e" class="jn jo mf jp b jq jr ij js jt ju im jv mg jx jy jz mh kb kc kd mi kf kg kh ki hb bi translated">定义多类概率的概率分布称为多项式概率分布。适于学习和预测多项概率分布的逻辑回归模型被称为<strong class="jp hj">多项逻辑回归</strong>。<br/> —杰森·布朗利<a class="ae kj" href="https://machinelearningmastery.com/multinomial-logistic-regression-with-python/" rel="noopener ugc nofollow" target="_blank">机器学习掌握</a></p></blockquote><h2 id="0a97" class="mq ld hi bd le mr ms mt li mu mv mw lm jw mx my lo ka mz na lq ke nb nc ls nd bi translated">假设方程</h2><p id="7162" class="pw-post-body-paragraph jn jo hi jp b jq lu ij js jt lv im jv jw lw jy jz ka lx kc kd ke ly kg kh ki hb bi translated"><strong class="jp hj"> softmax函数</strong>用于计算概率:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ou"><img src="../Images/0e7721d91fdb3f1da70de2600f701730.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*xBLSfr75eoe2T7ATB8a7_A.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Softmax function. (<a class="ae kj" href="https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="3432" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在逻辑回归的情况下，上面等式中的<strong class="jp hj"> <em class="mf"> z </em> </strong>被从拟合直线的等式中计算出的逻辑矩阵代替，这类似于二进制分类版本中的逻辑，但是用<code class="du nh ni nj nk b">(number_of_samples, number_of_classes)</code>的<strong class="jp hj">形状</strong>代替<code class="du nh ni nj nk b">(number_of_samples, 1)</code>。本质上，这里的softmax函数将<strong class="jp hj">输出每个类别</strong>的概率，这与二进制分类中的sigmoid函数不同，后者只计算获得“是”类别的概率。下面是softmax使用代码的基本介绍。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="or mb l"/></div></figure><pre class="iy iz ja jb fd nt nk nu nv aw nw bi"><span id="c202" class="mq ld hi nk b fi nx ny l nz oa"># OUTPUT:<br/>[[1002 1080 1100]<br/> [   5    7    8]<br/> [   2    4    6]<br/> [   3    4    5]]<br/>mx = array([[1100],<br/>       [   8],<br/>       [   6],<br/>       [   5]])<br/>x - mx = array([[-98, -20,   0],<br/>       [ -3,  -1,   0],<br/>       [ -4,  -2,   0],<br/>       [ -2,  -1,   0]])<br/>denominator = array([[1.        ],<br/>       [1.41766651],<br/>       [1.15365092],<br/>       [1.50321472]])<br/>result = array([[2.74878500e-43, 2.06115362e-09, 9.99999998e-01],<br/>       [3.51190270e-02, 2.59496460e-01, 7.05384513e-01],<br/>       [1.58762400e-02, 1.17310428e-01, 8.66813332e-01],<br/>       [9.00305732e-02, 2.44728471e-01, 6.65240956e-01]])<br/>Sum of probabilities = [[1.]<br/> [1.]<br/> [1.]<br/> [1.]]</span></pre><p id="5088" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">如输出中所示，每一行(每一个数据记录)都被转换成概率，并且每一行的总和为1，这证明softmax函数将它们转换成每个类的概率，在本例中，有3个类(3列)。顺便提一下，从<code class="du nh ni nj nk b">scipy.special.softmax</code>开始<code class="du nh ni nj nk b">scipy</code>已经有了一个现有的实现。</p><h2 id="ced0" class="mq ld hi bd le mr ms mt li mu mv mw lm jw mx my lo ka mz na lq ke nb nc ls nd bi translated">因素</h2><p id="109e" class="pw-post-body-paragraph jn jo hi jp b jq lu ij js jt lv im jv jw lw jy jz ka lx kc kd ke ly kg kh ki hb bi translated">待训练的参数仍然与二进制分类的情况相同，但是具有不同形状的<strong class="jp hj"/>以适应<strong class="jp hj">更高数量的类别</strong>。</p><p id="6c14" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><code class="du nh ni nj nk b">W</code>:每个特征和每个类别的斜率系数，形状<strong class="jp hj">为<code class="du nh ni nj nk b">(number_of_features, number_of_classes).</code>的</strong></p><p id="e8b5" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><code class="du nh ni nj nk b">b</code>:每一类的y轴截距，形状为<code class="du nh ni nj nk b">(1, number_of_classes)</code>的<strong class="jp hj">形状</strong></p><h2 id="98da" class="mq ld hi bd le mr ms mt li mu mv mw lm jw mx my lo ka mz na lq ke nb nc ls nd bi translated">价值函数</h2><p id="0d6f" class="pw-post-body-paragraph jn jo hi jp b jq lu ij js jt lv im jv jw lw jy jz ka lx kc kd ke ly kg kh ki hb bi translated">代价函数是<strong class="jp hj">分类交叉熵损失</strong>，它是二元交叉熵损失的推广。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ov"><img src="../Images/8523a415b54ac87a568eae82adbdf765.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*NAD3PzZR-RZi6bQRFXaOGg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Cross-entropy for multi-class classification. (<a class="ae kj" href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="9aee" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这里的<code class="du nh ni nj nk b">y</code>代表“目标”(真实的类标签)，而<code class="du nh ni nj nk b">h</code>代表输出(通过softmax计算的概率；<strong class="jp hj"> <em class="mf">不是</em> </strong>的预测类标签)。</p><h2 id="a42b" class="mq ld hi bd le mr ms mt li mu mv mw lm jw mx my lo ka mz na lq ke nb nc ls nd bi translated">派生物</h2><p id="4f2e" class="pw-post-body-paragraph jn jo hi jp b jq lu ij js jt lv im jv jw lw jy jz ka lx kc kd ke ly kg kh ki hb bi translated">这里是这两个参数的偏导数，因为它们相当复杂，所以这里就不展示推导的繁琐细节了。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ow"><img src="../Images/29ad055a38e09d1bca19fba61beb8828.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*v0E3GqqMcSvbdzN7V9Avog.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Derivatives obtained from the first article reference below.</figcaption></figure><p id="1e97" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">代码的“翻译”将是:</p><pre class="iy iz ja jb fd nt nk nu nv aw nw bi"><span id="3861" class="mq ld hi nk b fi nx ny l nz oa">dW = (1 / m) * (X.T @ (y_proba - y))<br/>db = (1 / m) * np.sum((y_proba - y), axis=0, keepdims=True)</span></pre><p id="70fd" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">要进一步了解softmax如何工作，如何定义成本函数，以及它们如何与多项式逻辑回归相关，您可以参考下面的文章。</p><div class="kk kl ez fb km kn"><a href="https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html" rel="noopener  ugc nofollow" target="_blank"><div class="ko ab dw"><div class="kp ab kq cl cj kr"><h2 class="bd hj fi z dy ks ea eb kt ed ef hh bi translated">什么是Softmax回归，它与Logistic回归有什么关系？- KDnuggets</h2><div class="ku l"><h3 class="bd b fi z dy ks ea eb kt ed ef dx translated">Softmax回归(同义词:多项式逻辑，最大熵分类器，或只是多类逻辑…</h3></div><div class="kv l"><p class="bd b fp z dy ks ea eb kt ed ef dx translated">www.kdnuggets.com</p></div></div><div class="kw l"><div class="ox l ky kz la kw lb jh kn"/></div></div></a></div><p id="f1fc" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">关于softmax的详细解释，你也可以参考下面的文章。</p><div class="kk kl ez fb km kn"><a href="https://towardsdatascience.com/softmax-activation-function-how-it-actually-works-d292d335bd78" rel="noopener follow" target="_blank"><div class="ko ab dw"><div class="kp ab kq cl cj kr"><h2 class="bd hj fi z dy ks ea eb kt ed ef hh bi translated">Softmax激活功能——实际工作原理</h2><div class="ku l"><h3 class="bd b fi z dy ks ea eb kt ed ef dx translated">Softmax是放置在深度学习网络末端的函数，用于将logits转换为分类概率。</h3></div><div class="kv l"><p class="bd b fp z dy ks ea eb kt ed ef dx translated">towardsdatascience.com</p></div></div><div class="kw l"><div class="oy l ky kz la kw lb jh kn"/></div></div></a></div><h1 id="0825" class="lc ld hi bd le lf lg lh li lj lk ll lm io ln ip lo ir lp is lq iu lr iv ls lt bi translated">多类分类的实现</h1><p id="8067" class="pw-post-body-paragraph jn jo hi jp b jq lu ij js jt lv im jv jw lw jy jz ka lx kc kd ke ly kg kh ki hb bi translated">二进制分类版本只有一些变化。首先，我们使用来自<code class="du nh ni nj nk b">sklearn</code>的iris数据集，因为有3个目标类。然而，首先，目标标签必须是一位热编码的，以确保softmax和导数的正确计算:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="or mb l"/></div></figure><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="or mb l"/></div></figure><p id="087c" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">然后对我们的<code class="du nh ni nj nk b">MyLogisticReg</code>类的第一个更改是对系数和截距的参数初始化— <code class="du nh ni nj nk b">init_params</code>方法的更改，其中形状与上面解释的不同。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="or mb l"/></div></figure><p id="2711" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">然后用softmax函数代替sigmoid函数进行多类分类。上面显示的<code class="du nh ni nj nk b">predict_proba</code>方法可以适应二元和多类分类。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="or mb l"/></div></figure><p id="99f1" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在<code class="du nh ni nj nk b">fit</code>方法中，对于<code class="du nh ni nj nk b">b</code>、<code class="du nh ni nj nk b">db</code>的导数，梯度的计算也略有变化，以计算右轴的和。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="or mb l"/></div></figure><p id="30b1" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">接下来，<code class="du nh ni nj nk b">predict</code>方法将通过计算所有类别概率中最高概率的索引来获得预测的类别标签。<code class="du nh ni nj nk b">predict_score</code>方法将从独热编码的<code class="du nh ni nj nk b">y</code>值和预测的准确度分数中计算出真实类别。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="or mb l"/></div></figure><pre class="iy iz ja jb fd nt nk nu nv aw nw bi"><span id="2407" class="mq ld hi nk b fi nx ny l nz oa"># OUTPUT:<br/>Result in accuracy:<br/>My implementation	: 0.98<br/>Sklearn implementation	: 0.98</span></pre><p id="a3d9" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">训练结果与Scikit相同-再次学习实现。因此，我们的实现应该非常接近Scikit-learn的<code class="du nh ni nj nk b">LogisticRegression</code>类的<code class="du nh ni nj nk b">multinomial</code>实现。</p><p id="343b" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">整堂课可以在这里看到:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="or mb l"/></div></figure><p id="00d8" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">对于本文中使用的完整代码，您可以参考GitHub 中的<a class="ae kj" href="https://github.com/ansonnn07/ML-implmentations/blob/main/logistic_regression/log_reg_from_scratch.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本。</a></p><h1 id="fb6c" class="lc ld hi bd le lf lg lh li lj lk ll lm io ln ip lo ir lp is lq iu lr iv ls lt bi translated">最后的想法</h1><p id="d3fe" class="pw-post-body-paragraph jn jo hi jp b jq lu ij js jt lv im jv jw lw jy jz ka lx kc kd ke ly kg kh ki hb bi translated">在本文中，我们学习了逻辑回归的基本概念，以及二元和多类分类的关键概念及其各自的实现。我相信这将有助于巩固我们对逻辑回归的理解。</p><p id="9973" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这些概念，尤其是softmax和分类交叉熵损失，在神经网络领域中非常常见，因为存在许多多类分类问题。因此，通过进一步理解这些的基本概念，我相信你会对将它们应用于神经网络以及这里没有提到的其他一些应用更有信心。</p></div></div>    
</body>
</html>
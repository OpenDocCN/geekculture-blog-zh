<html>
<head>
<title>Logistic Regression in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch中的逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/logistic-regression-in-pytorch-32efcf783fc1?source=collection_archive---------17-----------------------#2021-09-18">https://medium.com/geekculture/logistic-regression-in-pytorch-32efcf783fc1?source=collection_archive---------17-----------------------#2021-09-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/70ea0fd8ea3590df9e14e8db0fb00880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q7Ov4EWyNzuXGNTIDSbBag.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">PHOTOGRAPH: MARTIN HARVEY/GETTY IMAGES, Check out CobraML link at the bottom</figcaption></figure><h1 id="3d0e" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">我们离开的地方</h1><p id="e253" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在上一篇文章中，我们讨论了线性回归。我们讨论了该算法是如何实现的，以及它在真实数据集上的表现如何。在本文中，我们将经历相同的过程，但逻辑回归。</p><p id="b7c8" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">详细地说，我们将回顾</p><ul class=""><li id="2176" class="kv kw hi ju b jv kq jz kr kd kx kh ky kl kz kp la lb lc ld bi translated">逻辑回归背后的理论</li><li id="7d40" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">实现二元分类器</li><li id="1b73" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">实现多类分类器</li><li id="9900" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">每个算法在真实数据集上的性能</li></ul><h1 id="c847" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">什么是逻辑回归？</h1><p id="54fb" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">逻辑回归是一种能够预测二元结果的分类算法。我们将深入了解它是如何工作的，但首先让我们建立一些关于它的基本概念。</p><p id="6c53" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">逻辑回归是<strong class="ju hj">而不是</strong>一种回归算法。</p><ul class=""><li id="ca24" class="kv kw hi ju b jv kq jz kr kd kx kh ky kl kz kp la lb lc ld bi translated">这可能看起来很奇怪，因为它的名字中有回归，但事实上，逻辑回归是一种分类算法。这意味着它不预测一个连续的值，而是预测一个特定的类。</li></ul><p id="f383" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">与线性回归一样，逻辑回归也采用梯度下降算法来调整权重(θ)。</p><h1 id="a7bf" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">逻辑回归是如何工作的？</h1><p id="3457" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">像线性回归一样，我们首先需要一组权重，我们称之为θ。这些是我们在训练模型时改变的权重。那么我们需要多少砝码呢？这取决于你有多少特征(X)。让我给你举个例子，如果你试图预测一个孩子是否会准时到校，你可能想要收集的数据点是:就寝时间，他们离学校有多远，以及他们醒来的时间。这些点中的每一个都算作一个单独的特征。在这种情况下，您的数据集将由每个孩子的三个要素组成。因为你有三个特征，我们有四个权重，一个对应于每个特征，一个是偏差。如果特征的数量是“<strong class="ju hj"> n </strong>，你将拥有“<strong class="ju hj">n</strong>”+1个权重。现在我们有了自己的特性和参数，我们该如何利用它们呢？</p><h2 id="b58e" class="lj iv hi bd iw lk ll lm ja ln lo lp je kd lq lr ji kh ls lt jm kl lu lv jq lw bi translated">假设</h2><p id="f5d3" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">首先让我们想象一下重量和特征是什么样子的。回到我们的“上学迟到”思想实验，我们可以用一个数组来表示这个特征，比如[-7200，1000，28800]。第一个值是孩子睡觉的时间转换为从午夜开始的秒数，第二个值是孩子住在离学校多远的地方，以米为单位，第三个值是孩子醒来的时间，也转换为从午夜开始的秒数。你可能已经注意到这不仅仅是一个数组，也是一个向量和矩阵。让我们看看权重可能是什么样子的。当第一次声明权重时，初始值并不太重要，所以通常的做法是用随机值或全零来设置权重。因此，我们的权重可能看起来像[0.123，0.823，0.241，0.395]。数组中的第一个值对应于偏差，第二个值对应于特征数组的第一个值，第三个值对应于第二个值，依此类推。</p><p id="c5c0" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">我们当前的目标是找到一种方法，使用这两个数组来生成一个奇异值，这就是我们的预测。最简单的方法是得到两个数组之间的点积。但是，这里有一个主要问题，数组的大小不一样！为了解决这个问题，我们可以采用两种方法。第一种方法是弹出偏差并将其添加到点积中，或者简单地在特征向量前面滑动一个1，使其成为[1，-7200，1000，28800]，现在当您执行点积时，它只是添加了偏差。当在更大的范围内思考时，如果您拥有的数据比一组要素多得多，那么最好使用矩阵乘法。为此，我们转置权重矩阵，并将其乘以特征矩阵。因此，我们得到我们的基本假设方程θ转置x。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/426ea17886b2df1d1381b99588f56d58.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*umydICfaQoR0pbhAKTQ6LA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">The Hypothesis</figcaption></figure><h2 id="bf84" class="lj iv hi bd iw lk ll lm ja ln lo lp je kd lq lr ji kh ls lt jm kl lu lv jq lw bi translated">Sigmoid函数</h2><p id="249f" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">如果我们停在假设函数，我们会有线性回归，但我们不想这样。我们想要解决分类问题，这就是sigmoid / logistic函数的用武之地(因此得名逻辑回归)。sigmoid方程为:</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/35e5b6ed780818dca5a8079408b1874e.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*LRRTlPikmvUahwumG5Mrgg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">The Sigmoid equation</figcaption></figure><p id="a73e" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">Sigmoid图看起来像:</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/79029d07d8ec9f1ecf88a4b829120d13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HXCBO-Wx5XhuY_OwMl0Phw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Sigmoid Graph</figcaption></figure><p id="c359" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">从图中我们可以看到，当“<strong class="ju hj"> z </strong>趋近于正无穷大时，图渐近于1，当“<strong class="ju hj"> z </strong>趋近于负无穷大时，图渐近于0。有了这些知识，我们可以利用sigmoid函数将我们的逻辑回归模型变成二元分类器，即返回预测值0或1的分类器。为此，我们将所有大于或等于0.5的预测四舍五入为1，小于0的预测四舍五入为0。假设我们的假设等于sigmoid函数中的“<strong class="ju hj"> z </strong>，我们现在已经准备好了整个模型。我们现在缺少的只是一个成本函数。</p><h2 id="4ff7" class="lj iv hi bd iw lk ll lm ja ln lo lp je kd lq lr ji kh ls lt jm kl lu lv jq lw bi translated">成本函数</h2><p id="622e" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">成本函数帮助我们的梯度下降算法找到到达<em class="me">最小值</em>的最佳路径。我们需要一个代价函数，在两种不同的情况下惩罚我们的模型。第一种情况是目标为1，第二种情况是目标为0。预测离目标越远，惩罚应该越大。实现这一点的算法是二进制交叉熵，其公式为:</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/409eb09de429d18a17b65a79302f2f69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*Xoo6JLxPpd-RY7K1q3AUSw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Binary Cross Entropy Loss</figcaption></figure><p id="3a53" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">现在请记住，这个特定的模型是一个二元分类器，这意味着两个可预测的类(即"<strong class="ju hj"> y </strong>"值)可以是0或1。让我们看看y = 0的情况。如果y = 0，成本函数变成</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/0e1aaacf03d3d22204ea5dc375575488.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*5z2H9VKY9njMSYjGqx2k9g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Binary Cross Entropy when y = 0</figcaption></figure><p id="fdaf" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">这张图最终看起来像</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/603344bcc7adf314b2bb17abfbfc2093.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nQjKdX6vqRrvFfNSQ47q_g.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">BCE y = 0 graph</figcaption></figure><p id="2cbd" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">从该图中我们可以看出，“<strong class="ju hj"> L </strong>值离0越远，惩罚越高。现在让我们看看y = 1的情况。成本函数现在变成了</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/2800241a9a5a3da4ac325f81f04ac551.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*G-ilBIqgineS06diUnTLNw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Binary Cross Entropy when y = 1</figcaption></figure><p id="3182" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">这张图看起来是这样的</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/fd6a29e5bfafa567bad69f48321ee5e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zA87oFKQXnkY3uonrtw9iw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">BCE y = 1 graph</figcaption></figure><p id="dbc8" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated"><strong class="ju hj"> L </strong>的值离1越远，惩罚越大。</p><p id="a403" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">随着成本函数的建立，我们现在可以用它来推导梯度下降方程。幸运的是，由于PyTorch等库的强大功能，这已经为我们处理好了。如果你想更深入地了解这些推导，我们建议你查看文章末尾的一些资料。</p><h1 id="f96f" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">多类预测</h1><p id="919d" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">你可能注意到了，因为sigmoid层，我们只能得到两个预测，0或者1。如果我们想要预测两个以上的类呢？有许多方法可以解决这样的问题，但最常用的一种方法是实现One Vs. Rest方法。OVR方法将多类问题转化为二进制问题。这种情况发生的方式是我们为每一个我们想要能够预测的类训练一个模型。每个模型都在数据集上进行训练，其中一个类为1，其他所有类为0。之后，你会有一套模型，每个模型都能够预测某一类。然后，您将从所有模型中选择最高的预测作为您的总体预测。虽然这是一个好方法，但它很昂贵，因为你需要为“<strong class="ju hj"> n </strong>班训练“<strong class="ju hj"> n </strong>”个模特。你会在我的代码中看到，我实现了一个非常相似的方法，但又不完全一样。</p><h1 id="559e" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">二元逻辑回归实现</h1><p id="8f62" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">有了理论支持，我们现在可以看看实际代码的实现。按照PyTorch结构，我们应该从模型开始。</p><h2 id="d96b" class="lj iv hi bd iw lk ll lm ja ln lo lp je kd lq lr ji kh ls lt jm kl lu lv jq lw bi translated">模型</h2><figure class="ly lz ma mb fd ij"><div class="bz dy l di"><div class="mj mk l"/></div></figure><p id="9021" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">熟悉PyTorch的人可能会感到困惑，为什么这个模型看起来像一个单层感知器(SLP)？这是因为这种特定类型的SLP实际上与逻辑回归模型完全相同。如果你还记得前面的解释，逻辑回归模型只是一个围绕假设函数的sigmoid函数。我们在这里基本上做完全相同的事情，如果你看正向方法，我们围绕线性层包裹sigmoid函数。线性层与我们的假设相同，但它为我们处理所有的权重和偏差。这一点可以通过文档页面来证明。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/3adf7b68ffa6c131a910e80cd66f523f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1jy2svLnXHCTdkRNNXQTbA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">The bias is b and A represents the weights, x is the feature vector</figcaption></figure><p id="b6e1" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">通过使用线性层，我们所要做的就是传递特征，它将为我们处理所有的计算。如预测的那样导致一个神经元的激活。这里是证明sigmoid函数也和理论一样的文档。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/c3ac95cf92eb3a2c945440a218dd3399.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1HJRXKj8tpFF-GF7MCxW7Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">The Sigmoid equation used in PyTorch’s Sigmoid activation function.</figcaption></figure><h2 id="f03e" class="lj iv hi bd iw lk ll lm ja ln lo lp je kd lq lr ji kh ls lt jm kl lu lv jq lw bi translated">训练模型</h2><p id="6b4f" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">训练模型是非常具体的优化器，所以我将分享两种训练方法。第一种训练方法最适合像Adam和SGD这样的优化者</p><figure class="ly lz ma mb fd ij"><div class="bz dy l di"><div class="mj mk l"/></div></figure><p id="f35e" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">如您所见，这遵循了一个非常常见的PyTorch培训结构。</p><ul class=""><li id="e40c" class="kv kw hi ju b jv kq jz kr kd kx kh ky kl kz kp la lb lc ld bi translated">我们遍历数据加载器中的每个批次</li><li id="c71f" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">在每次迭代中，我们将梯度归零</li><li id="3e4f" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">将特征张量传递到模型中以获得输出(预测)</li><li id="f783" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">得到输出和目标之间的损失</li><li id="397a" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">反向传播损失</li><li id="10bf" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">优化器使用阶跃函数对参数进行更改</li><li id="f664" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">该批次的损失随后保存在损失张量中</li><li id="b2ed" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">最后返回历元的平均损失</li></ul><figure class="ly lz ma mb fd ij"><div class="bz dy l di"><div class="mj mk l"/></div></figure><p id="0f6c" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">这种训练方法主要用于每批需要几次调整优化器。像:LBFGS(有限记忆Broyden–Fletcher–gold farb–Shanno<strong class="ju hj">)</strong>和共轭梯度下降等算法就属于这一类。这两种训练方法的主要区别是嵌套闭包方法。嵌套闭包法允许对权重和学习速率等参数进行微调，理想情况下可以为用户节省大量时间，并提供更好的结果。</p><h2 id="5c8f" class="lj iv hi bd iw lk ll lm ja ln lo lp je kd lq lr ji kh ls lt jm kl lu lv jq lw bi translated">损失函数</h2><p id="3442" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">为了训练模型，我们不仅需要优化器，还需要成本函数/损失函数。我们使用二元交叉熵，如果你看一下文献，你会发现它与我们之前在理论中陈述的相匹配。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mn"><img src="../Images/b0b0bffd5c0c53803ac54cf5e77615f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ki0VcKK68pGsPYuDMzBADA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Pytorch’s implementation of Binary Cross Entropy Loss</figcaption></figure><h2 id="331b" class="lj iv hi bd iw lk ll lm ja ln lo lp je kd lq lr ji kh ls lt jm kl lu lv jq lw bi translated">评估模型</h2><figure class="ly lz ma mb fd ij"><div class="bz dy l di"><div class="mj mk l"/></div></figure><p id="0493" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">在训练模型之后，我们需要评估它在看不见的数据上的表现如何，这就是评估方法的用武之地。此方法对预测进行舍入(将所有预测变为0或1)，然后检查有多少预测等于实际目标。最终返回正确率。</p><h1 id="a8fe" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">在数据集上测试二元逻辑回归</h1><figure class="ly lz ma mb fd ij"><div class="bz dy l di"><div class="mj mk l"/></div></figure><p id="7cdd" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">准备好所有的方法后，我们现在可以回顾一下模型在真实数据集上的表现。从SciKit Learn加载乳腺癌数据集后，我们可以看到一个时期后我们获得的准确度为</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mo"><img src="../Images/ef5cb46f656c51a462c02bee7fc47b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u05zpLSZJ3i408hn-XXGtA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Performance on the Breast Cancer Dataset (99% after 1 Epoch)</figcaption></figure><p id="f4d8" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">学习了二元逻辑回归之后，让我们继续学习多类逻辑回归的实现。</p><h1 id="8cd0" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">多类逻辑回归实现</h1><h2 id="4c70" class="lj iv hi bd iw lk ll lm ja ln lo lp je kd lq lr ji kh ls lt jm kl lu lv jq lw bi translated">模型</h2><figure class="ly lz ma mb fd ij"><div class="bz dy l di"><div class="mj mk l"/></div></figure><p id="a210" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">你可以看到我们仍然有一个线性层，但现在有点不同。不是所有的特征都与一个神经元相关联，而是现在与“<strong class="ju hj"> n </strong>个神经元相关联。视觉表现是</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/6296d6d85ea2afff8830f41efb4d370d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*TgCLiD5ChfZxeuHlourEOQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Binary Logistic Regression</figcaption></figure><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mq"><img src="../Images/6ac5f4aa346372d2f581691ab4eed61a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z61ERo7z0eFocp1ZtjdUOA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Multi Class Logistic Regression</figcaption></figure><p id="e16e" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">现在，你可能认为我们这样做没有遵循一对一静止原理，但是我想说在大多数情况下我们是这样做的。如果仔细观察第二个图，可以看到每个类节点(右边的圆圈)都有自己的一组权重(线条)。因此，你可以把它看作是三个独立的模型，它们被训练来识别三个不同的类。主要区别是:它们都是并行训练的，它们存储在一个模型中，并且不需要改变数据来训练每个特定的模型。</p><p id="afa6" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">你可能也注意到乙状结肠层完全消失了。它不存在的原因是因为我们的损失函数交叉熵损失将为我们应用Log SoftMax损失(这比多类问题的sigmoid函数更好)，因此再次添加它是不必要的。</p><h2 id="a667" class="lj iv hi bd iw lk ll lm ja ln lo lp je kd lq lr ji kh ls lt jm kl lu lv jq lw bi translated">评估模型</h2><p id="31b8" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">该函数的训练方法与二元逻辑回归相同，因此我们唯一需要做的是如何评估该模型。</p><figure class="ly lz ma mb fd ij"><div class="bz dy l di"><div class="mj mk l"/></div></figure><p id="3345" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">在这种评估方法中，我们将Log SoftMax层应用于输出。原因是，我们不再通过损失函数来传递预测。因此，我们需要手动添加层。我们得到最活跃的神经元的指数(指示神经元预测的类别),并通过将预测的类别与实际类别进行比较来返回总体准确度。</p><h1 id="e60a" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">在数据集上测试多类逻辑回归</h1><p id="9c26" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">那么这个模型在真实世界的数据集上表现如何呢？我们决定通过在最著名的数据集之一MNIST数据集上进行测试来找出答案。手写数字的数据集。我们的目标是从手写样本中准确预测数字。</p><figure class="ly lz ma mb fd ij"><div class="bz dy l di"><div class="mj mk l"/></div></figure><p id="79e4" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">在导入展平每个张量的数据集并在其上训练我们的模型后，一个历元后的精度为</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mr"><img src="../Images/712919d2ae837786cc3a6fe6e92ed7f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R1I3YuHSI7r1Y1jqBYbnEQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Accuracy on the MNIST dataset (92% accuracy after 1 epoch)</figcaption></figure><p id="e40e" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">这是非常好的，特别是对于一个简单的逻辑回归模型，我敢肯定，随着更好的超参数调整，你可能会做得更好。</p><h1 id="ab69" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">结论</h1><p id="bed4" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在这篇文章中，我们回顾了</p><ul class=""><li id="b272" class="kv kw hi ju b jv kq jz kr kd kx kh ky kl kz kp la lb lc ld bi translated">逻辑回归背后的理论</li><li id="75a6" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">多类逻辑回归</li><li id="6e02" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">二元逻辑回归。</li><li id="d79d" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">实施准则</li><li id="0978" class="kv kw hi ju b jv le jz lf kd lg kh lh kl li kp la lb lc ld bi translated">在真实数据集上测试代码</li></ul><p id="aa90" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">如果你喜欢这篇文章，考虑与他人分享这篇文章或者为它鼓掌。如果你对开源ML社区感兴趣，试着注册来帮助CobraML。我们正在编写一个ML库，它利用了PyTorch框架。如果你想联系我，请在LinkedIn上联系我和/或对文章发表评论。我会尽快回复你。请关注即将发表的下一篇文章。</p><h1 id="6367" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">资源</h1><ul class=""><li id="7d07" class="kv kw hi ju b jv jw jz ka kd ms kh mt kl mu kp la lb lc ld bi translated">CobraML GitHub链接(所有源代码都在这里)</li></ul><div class="mv mw ez fb mx my"><a href="https://github.com/govindansriram/CobraML" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab dw"><div class="na ab nb cl cj nc"><h2 class="bd hj fi z dy nd ea eb ne ed ef hh bi translated">GitHub - govindansriram/CobraML:构建在PyTorch框架之上的Python库，旨在…</h2><div class="nf l"><h3 class="bd b fi z dy nd ea eb ne ed ef dx translated">Python库建立在PyTorch框架之上，旨在让用户访问大量的ML模型…</h3></div><div class="ng l"><p class="bd b fp z dy nd ea eb ne ed ef dx translated">github.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm io my"/></div></div></a></div><ul class=""><li id="5980" class="kv kw hi ju b jv kq jz kr kd kx kh ky kl kz kp la lb lc ld bi translated">CobraML投稿表格</li></ul><figure class="ly lz ma mb fd ij"><div class="bz dy l di"><div class="nn mk l"/></div><figcaption class="iq ir et er es is it bd b be z dx">Sign up to help contribute to CobraML!</figcaption></figure><ul class=""><li id="f4b1" class="kv kw hi ju b jv kq jz kr kd kx kh ky kl kz kp la lb lc ld bi translated">解释逻辑回归和梯度下降以及其他ML主题的重要资源</li></ul><figure class="ly lz ma mb fd ij"><div class="bz dy l di"><div class="no mk l"/></div></figure><figure class="ly lz ma mb fd ij"><div class="bz dy l di"><div class="no mk l"/></div></figure><ul class=""><li id="ac8a" class="kv kw hi ju b jv kq jz kr kd kx kh ky kl kz kp la lb lc ld bi translated">我们LinkedIn的</li></ul><div class="mv mw ez fb mx my"><a href="https://www.linkedin.com/in/sriram-govindan/" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab dw"><div class="na ab nb cl cj nc"><h2 class="bd hj fi z dy nd ea eb ne ed ef hh bi translated">Sriram Govindan -机器学习工程师- Samotal，Inc. | LinkedIn</h2><div class="nf l"><h3 class="bd b fi z dy nd ea eb ne ed ef dx translated">查看Sriram Govindan在世界上最大的职业社区LinkedIn上的个人资料。Sriram有一个工作列在…</h3></div><div class="ng l"><p class="bd b fp z dy nd ea eb ne ed ef dx translated">www.linkedin.com</p></div></div><div class="nh l"><div class="np l nj nk nl nh nm io my"/></div></div></a></div><div class="mv mw ez fb mx my"><a href="https://www.linkedin.com/in/saatvik-anumalasetty-8214b8167/" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab dw"><div class="na ab nb cl cj nc"><h2 class="bd hj fi z dy nd ea eb ne ed ef hh bi translated">美国印第安纳州西拉斐特普渡大学</h2><div class="nf l"><h3 class="bd b fi z dy nd ea eb ne ed ef dx translated">我是普渡大学的本科生，主修数据科学，辅修心理学、经济学和…</h3></div><div class="ng l"><p class="bd b fp z dy nd ea eb ne ed ef dx translated">www.linkedin.com</p></div></div><div class="nh l"><div class="nq l nj nk nl nh nm io my"/></div></div></a></div><ul class=""><li id="d609" class="kv kw hi ju b jv kq jz kr kd kx kh ky kl kz kp la lb lc ld bi translated">前一篇文章</li></ul><div class="mv mw ez fb mx my"><a rel="noopener follow" target="_blank" href="/geekculture/linear-regression-in-pytorch-262a4b7ed610"><div class="mz ab dw"><div class="na ab nb cl cj nc"><h2 class="bd hj fi z dy nd ea eb ne ed ef hh bi translated">PyTorch中的线性回归</h2><div class="nf l"><h3 class="bd b fi z dy nd ea eb ne ed ef dx translated">放弃</h3></div><div class="ng l"><p class="bd b fp z dy nd ea eb ne ed ef dx translated">medium.com</p></div></div><div class="nh l"><div class="nr l nj nk nl nh nm io my"/></div></div></a></div></div></div>    
</body>
</html>
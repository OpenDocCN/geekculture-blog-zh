<html>
<head>
<title>NLP: Text Pre-processing and Feature Engineering. Python.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本预处理和特征工程。Python。</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/nlp-text-pre-processing-and-feature-engineering-python-69338fa0372e?source=collection_archive---------0-----------------------#2021-01-31">https://medium.com/geekculture/nlp-text-pre-processing-and-feature-engineering-python-69338fa0372e?source=collection_archive---------0-----------------------#2021-01-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="8f5e" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">逐步指南</h2><div class=""/><div class=""><h2 id="5a4e" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">使用Python: Numpy、Pandas、Regex、Spacy和Tensorflow预处理文本数据、创建新要素(包括二进制分类的目标变量)。</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/279414e733b22cf3214f18c4a261f98a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QX3QuezXJ63kgt4XMUPBzQ.jpeg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Photo by <a class="ae jw" href="https://www.pexels.com/@pixabay?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pixabay</a> from <a class="ae jw" href="https://www.pexels.com/photo/text-on-shelf-256417/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><h1 id="a91f" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">介绍。</h1><p id="d6d9" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated"><a class="ae jw" href="https://en.wikipedia.org/wiki/Data_pre-processing" rel="noopener ugc nofollow" target="_blank">数据预处理</a>是数据科学家工作的基础部分。除了<a class="ae jw" href="https://en.wikipedia.org/wiki/Data_collection" rel="noopener ugc nofollow" target="_blank">数据采集</a>之外，这也是主要阶段之一。这取决于我们未来模型的质量和准确性。我们清理/准备数据越好:</p><ul class=""><li id="10f9" class="ll lm hi kr b ks ln kv lo ky lp lc lq lg lr lk ls lt lu lv bi translated">我们越不需要在下一阶段调整模型，</li><li id="fd02" class="ll lm hi kr b ks lw kv lx ky ly lc lz lg ma lk ls lt lu lv bi translated">我们可以应用的模型越简单，</li><li id="0634" class="ll lm hi kr b ks lw kv lx ky ly lc lz lg ma lk ls lt lu lv bi translated">我们看到的洞察力/模式越多，</li><li id="baef" class="ll lm hi kr b ks lw kv lx ky ly lc lz lg ma lk ls lt lu lv bi translated">我们的模型就越精确。</li></ul><p id="6a57" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated"><strong class="kr hs"> <em class="me">那么在我们目前的情况下什么是预处理呢？</em> </strong></p><p id="af0f" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">简而言之:它是文本转换的过程。你必须让文本对你的商业目标的分析和预测有用。</p><p id="4f43" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">本文将涵盖(在代码示例中)几种重要的文本数据处理方法。此外，您将看到使用特征工程技术为二元分类任务进行的数据准备。这些章节将讲述:</p><p id="a1b8" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">-数据描述、业务目标探索；</p><p id="aabf" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">-数据操作(我们将从刚刚收集的非结构化数据中创建一个基础数据集)；</p><p id="b8e6" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">-使用自然语言处理方法的特征工程(基本上基于使用<em class="me">空间</em>库的<em class="me">词性标注</em>，基于语法+语义上下文，基于数据分析/洞察力/直觉)；</p><p id="43f9" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated"><em class="me"> -【文本转序列】</em>用<em class="me"> Tensorflow </em>转换——创建预测模型前的最后一步。</p><h1 id="93de" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated"><strong class="ak"> <em class="mf">数据描述。</em>T19】</strong></h1><p id="a928" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">本教程的数据集来自我之前的数据收集任务:我从一个食谱网站上收集的。<strong class="kr hs">业务目标</strong>是确定每个段落的标签是<em class="me">“配料”</em>还是<em class="me">“配方”。</em></p><p id="bee5" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">首先，我们应该导入<em class="me"> Pandas </em>库并将显示选项设置为<em class="me"> `max_colwithd` </em>以获得更好的文本可读性:</p><pre class="jh ji jj jk fd mg mh mi mj aw mk bi"><span id="2e20" class="ml jy hi mh b fi mm mn l mo mp">import pandas as pd<br/>pd.set_option('display.max_colwidth', None)</span></pre><p id="0f7f" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">当您打开一个带有未清理文本数据的<a class="ae jw" href="https://github.com/Galina-Blokh/ai_assignment_aidock/blob/refator/data/new_recipe.pkl" rel="noopener ugc nofollow" target="_blank">文件</a>(使用<em class="me">熊猫</em>)时，您会看到一个带有两列的集合:<em class="me">“配方”</em>和<em class="me">“说明”。</em>来自<em class="me">数据框</em>中<em class="me"> 1009 </em>行的每一行都是来自<em class="me"> 1009 </em>带配方的唯一页面的数据。每个单元格中的每一项都是段落列表:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mq"><img src="../Images/338cf6ae89715e12cd1f8194ab889d74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*753EdeeWiQFl3kH5CZbDcQ.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Pic.1 Read text data from pickle file in Pandas DataFrame</figcaption></figure><p id="f9b4" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">每个单元格中的说明列中的<em class="me">“段落”</em>我们用<em class="me">“\ n \ n”来定义。</em>配方栏中的<em class="me">【段落】</em>保存每个单元格本身。</p><h1 id="b91a" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">数据操作。创建一个基础数据集。</h1><p id="5f74" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">起初，似乎没有多少数据。但是让我们制作一个表格，在每个单元格中有一个实际的段落。<em class="me"> Lambda函数</em>和<em class="me">列表理解</em>适用于配方列转换。对于说明列将足够嵌入<em class="me"> Series.str() </em>函数:</p><pre class="jh ji jj jk fd mg mh mi mj aw mk bi"><span id="9cc3" class="ml jy hi mh b fi mm mn l mo mp"><em class="me">#ingredients<br/></em>recipe_col = df["Recipe"].apply( \<br/>    lambda series: ' '.join([words for words in series])).to_numpy()</span><span id="c3c5" class="ml jy hi mh b fi mr mn l mo mp"><em class="me">#instructions<br/></em>instr_col = df["INSTRUCTIONS"].str.split('\n\n').to_numpy()</span></pre><p id="6e60" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">在下一步中，我们添加一个带有标签的列:1 —成分，0 —说明。注意:通过这种方式，<strong class="kr hs">我们创建了目标变量</strong>。从现在开始，我们将使用<em class="me"> NumPy </em>进行数据操作。<em class="me"> Numpy </em>的速度明显高于<em class="me"> pandas </em>(和往常一样，它在大数据集下更明显):</p><pre class="jh ji jj jk fd mg mh mi mj aw mk bi"><span id="cdd1" class="ml jy hi mh b fi mm mn l mo mp">import numpy as np</span><span id="8a4c" class="ml jy hi mh b fi mr mn l mo mp"><em class="me">#ingredients<br/></em>recipe=recipe_col.reshape(-1, 1)<br/>recipe=np.hstack((recipe, np.ones(len(recipe), int).reshape(-1, 1)))</span><span id="9852" class="ml jy hi mh b fi mr mn l mo mp"><em class="me">#instructions<br/></em>instr=np.concatenate(instr_col).reshape(-1, 1)<br/>instr=np.hstack((instr, np.zeros(len(instr), int).reshape(-1, 1)))</span></pre><p id="463e" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">下一步是将这两个<em class="me">n数组</em>连接成一个:</p><pre class="jh ji jj jk fd mg mh mi mj aw mk bi"><span id="c575" class="ml jy hi mh b fi mm mn l mo mp"><em class="me"># forming a full data array with labels<br/></em>data = np.concatenate((instr, recipe), axis=0)</span></pre><blockquote class="ms mt mu"><p id="37e2" class="kp kq me kr b ks ln is ku kv lo iv kx mv mb la lb mw mc le lf mx md li lj lk hb bi translated">从我自己的经验中得到的启示:有时，将你的数据标记并输入到神经网络中比应用无监督技术(Keras/Tensorflow)更方便。</p></blockquote><p id="7bca" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">在我们创建了一个属性数据集后，我们可以减少空的和重复的行。然后，再次回到p <em class="me">和as数据帧</em>格式进行进一步的特征工程:</p><pre class="jh ji jj jk fd mg mh mi mj aw mk bi"><span id="cbed" class="ml jy hi mh b fi mm mn l mo mp"><em class="me"># remove duplicates<br/></em>unique = np.unique(data.astype(str), axis=0)<br/><br/><em class="me"># #remove empty string rows<br/></em>unique=np.delete(unique.astype(str),np.where(unique == <strong class="mh hs">''</strong>),axis=0)<br/>data = pd.DataFrame(unique, columns=[<strong class="mh hs">'paragraph'</strong>, <strong class="mh hs">'label'</strong>])</span></pre><p id="f157" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">如下图(图2)可以看到，现在一个新集合的形状是5144行！同样，它有两列:字符串<em class="me">“段落”</em>和整数<em class="me">“标签”</em>。删除重复项和空行后，形状变小了。目前，我们得到了4871 行用于分类——比上传的pickle文件多了四倍。太好了，我们终于有数据可用了！</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es my"><img src="../Images/bbe5be15024695a7b88e90336471a088.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nL-cnyJG3g7eAW5FdGrSSg.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Pic.2 Sample from created data set</figcaption></figure><p id="c75d" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">每个单元格都包含一个字符串段落表示。现在我们终于可以开始特征工程部分了。</p><h1 id="16db" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">自然语言处理方法的特征工程。</h1><p id="a3cc" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">对于接下来的文本版本，我们需要将<strong class="kr hs"> <em class="me">拆分</em> </strong> <strong class="kr hs"> <em class="me">集合</em> </strong>进行训练和测试(以防止过度拟合)。在我们的例子中，我们使用了一个<em class="me"> Sklearn </em>库。在分割之后，对于训练和测试数据，所有的操作将是相同的。</p><p id="884c" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">如果你检查标签的比例，你会看到不平衡:大约80%的0和20%的1。测试集的分裂分数也取0.2(我们不能取更多，因为我们没有太多的数据)。</p><pre class="jh ji jj jk fd mg mh mi mj aw mk bi"><span id="6c48" class="ml jy hi mh b fi mm mn l mo mp"><em class="me">#splin on train and test data sets</em><br/>from sklearn.model_selection import train_test_split</span><span id="5055" class="ml jy hi mh b fi mr mn l mo mp">X = data['paragraph'].copy()<br/>y = data['label'].astype(int).copy()</span><span id="3311" class="ml jy hi mh b fi mr mn l mo mp">X_train, X_test, y_train, y_test = train_test_split(X,y,<br/>                                                    stratify=y,<br/>                                                    test_size=0.2,<br/>                                                    random_state=42)</span></pre><p id="aaa8" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">最好在目标变量上做一个<strong class="kr hs"><em class="me"/></strong>分层分割。<br/>考虑到数据量小，标签不均衡，<br/>我们希望保持两个集合中的分布比例相同。一个简单的检查(见图3)表明分割进行得很顺利，正如预期的那样。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mz"><img src="../Images/9f18d993b1d59a66102c1486261bb261.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zfJTgILjN5f6Ob1RiR4TWA.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Pic.3 Target distribution plot in three data sets (data set, train set and test set).</figcaption></figure><p id="c17a" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">让我们涉及一些<strong class="kr hs"> <em class="me">语言学见解</em> </strong>和<strong class="kr hs"> <em class="me">分析</em> </strong>我们的数据。当你看<a class="ae jw" href="https://cdn-images-1.medium.com/max/1200/1*753EdeeWiQFl3kH5CZbDcQ.png" rel="noopener">图1 </a>时，你可能会注意到:</p><ul class=""><li id="c303" class="ll lm hi kr b ks ln kv lo ky lp lc lq lg lr lk ls lt lu lv bi translated">带<em class="me">【配料】</em>的段落包含很多数字；</li><li id="2a60" class="ll lm hi kr b ks lw kv lx ky ly lc lz lg ma lk ls lt lu lv bi translated"><em class="me">【配料】</em>可以绝对不含<em class="me">动词</em>，与<em class="me">【说明】</em>相反；</li><li id="36a8" class="ll lm hi kr b ks lw kv lx ky ly lc lz lg ma lk ls lt lu lv bi translated">一般来说，<em class="me">【成分】</em>只保存数字，<em class="me">名词</em><em class="me">形容词</em>。</li><li id="df58" class="ll lm hi kr b ks lw kv lx ky ly lc lz lg ma lk ls lt lu lv bi translated"><em class="me">【配料】</em>段落不是完整的句子(可能不含<em class="me">圆点</em>)；</li></ul><p id="9a78" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">因此，我们这里使用的第一种方法是<strong class="kr hs"> <em class="me">词性标注</em> </strong> <em class="me">。</em> <strong class="kr hs">用</strong>专用<strong class="kr hs">标签</strong>代替数字、和<strong class="kr hs">冒号</strong> <strong class="kr hs">。<strong class="kr hs">将</strong>其他<strong class="kr hs">标点</strong> <strong class="kr hs">换一个空格</strong>。为此，合理使用<em class="me"> regex </em>和<em class="me"> lambda函数</em>。在此之前，为了更方便使用，将设置确定为熊猫<em class="me">数据帧</em>:</strong></p><pre class="jh ji jj jk fd mg mh mi mj aw mk bi"><span id="1ca6" class="ml jy hi mh b fi mm mn l mo mp">import re</span><span id="00b1" class="ml jy hi mh b fi mr mn l mo mp">DIGIT_RX = "($\d+([\.|,]\d+)?[\w]?[\s|-]){1,8}|[^A-Za-z\,()\.'\-:\!\? ]{1,8}"<br/>SYMBOL_RX = "[/(/)\-/*/,\!\?]|[^ -~]"<br/>DOT_RX = "\.{1,4}|\:"</span><span id="2d61" class="ml jy hi mh b fi mr mn l mo mp">train = pd.DataFrame(X_train, columns=['paragraph'])</span><span id="d9f0" class="ml jy hi mh b fi mr mn l mo mp">train ['replaced_num']= train ['paragraph'].apply(lambda <br/>series: re.sub(DIGIT_RX, " zNUM ", series))</span><span id="5bb5" class="ml jy hi mh b fi mr mn l mo mp">train ['replaced_symb'] = train ['replaced_num'].apply(lambda series: re.sub(SYMBOL_RX, " ", series))</span><span id="9899" class="ml jy hi mh b fi mr mn l mo mp">train ['replaced_dot'] = train ['replaced_symb'].apply(lambda series: re.sub(DOT_RX, " zDOT ", series))</span></pre><p id="4c4a" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">结果如下图(图4)。我们需要跟踪这四列的变化。你可以只留下<em class="me">"段落"</em>作为原文，以及<em class="me">"替换点"</em>(因为我们需要它来应用下一个<em class="me"> NLP </em>方法):</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es my"><img src="../Images/88c13ddc421ac3b44ff51b86b6fd0af9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UXzlRpmRP8STeMgy3koJsA.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Pic.4 Text transformation example</figcaption></figure><p id="8769" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">在图4中，几个数字用绿色下划线标出，以显示我们将标签<em class="me"> zNUM </em>更改为什么。红色—删除了额外的标点符号。蓝色——标记<em class="me"> zDOT </em>而不是dots。此刻大写或小写标签并不重要。以后，所有文本都将是小写的。在这一点上，我们只想看看我们做了什么，并确保转型进展顺利。最后可以进入<em class="me">词条化</em>步骤。</p><p id="48ab" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated"><strong class="kr hs"> <em class="me">词汇化、特征工程和停用词删除同</em></strong><a class="ae jw" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank"><strong class="kr hs"><em class="me">SpaCy</em></strong></a><strong class="kr hs"><em class="me">。这个出色的库对于任何NLP任务都是有用的。<em class="me"> SpaCy Lemmatizer </em>支持简单的词性敏感后缀规则和查找表。在对一个<em class="me">字符串</em>的应用中，它原样返回可用的<em class="me">词条</em>或字符串。</em></strong></p><p id="573d" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">现在看看这个库是如何处理我们的文本的:我们希望<strong class="kr hs">计算代词、动词并删除停用词。</strong>第一步—安装/导入空间，加载英语词汇，定义tokenaizer(我们这里称之为“nlp”)，准备停用词集:</p><pre class="jh ji jj jk fd mg mh mi mj aw mk bi"><span id="32bd" class="ml jy hi mh b fi mm mn l mo mp"># !pip install spacy<br/># !python -m spacy download en_core_web_sm</span><span id="d041" class="ml jy hi mh b fi mr mn l mo mp">import spacy<br/>nlp = spacy.load('en_core_web_sm')</span><span id="59bc" class="ml jy hi mh b fi mr mn l mo mp">from spacy.lang.en.stop_words import STOP_WORDS<br/>stop_words = set([w.lower() for w in list(STOP_WORDS)])<br/>nlp.Defaults.stop_words |= {" f ", " s ", " etc"}</span></pre><p id="9462" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">现在和我们之前在创建基础集合时做的一样:<em class="me">熊猫</em> <em class="me"> + </em> l <em class="me"> ambda函数</em>，但是这次应用一个<em class="me">空间</em> <em class="me">词汇化</em>:</p><pre class="jh ji jj jk fd mg mh mi mj aw mk bi"><span id="2631" class="ml jy hi mh b fi mm mn l mo mp">train['lemmatiz'] = train.replaced_dot.apply(lambda series: \        ' '.join([word.lemma_ for word in nlp(series)]))</span></pre><p id="0328" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">需要一些时间才能得到结果。<em class="me">词汇化</em>并不是一个快速的过程。顺便说一下，我们将单词转换成它们的基本形式，同时我们还得到另一个标签<em class="me"> -PRON- </em>。我们立即开始新的<strong class="kr hs"> <em class="me">特征创建</em> </strong>:</p><ul class=""><li id="8f70" class="ll lm hi kr b ks ln kv lo ky lp lc lq lg lr lk ls lt lu lv bi translated">使用标签<em class="me"> -PRON- </em>用于二进制列<strong class="kr hs">包含_pron </strong>(段落是否包含此标签:1 —是，0 —否)；</li><li id="7739" class="ll lm hi kr b ks lw kv lx ky ly lc lz lg ma lk ls lt lu lv bi translated"><strong class="kr hs">每段动词_计数</strong>。</li></ul><pre class="jh ji jj jk fd mg mh mi mj aw mk bi"><span id="fb3e" class="ml jy hi mh b fi mm mn l mo mp">train['contains_pron'] = train.lemmatiz.apply(lambda series: \<br/>1 if series.__contains__('-PRON-') else 0 )</span><span id="4862" class="ml jy hi mh b fi mr mn l mo mp">train['verb_count'] = train.lemmatiz.apply(lambda series: \<br/>len([token for token in nlp(series) if token.pos_ == 'VERB'])]))</span></pre><p id="c333" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">下一步:<strong class="kr hs">删除</strong> <strong class="kr hs">多余的空格</strong>/仍然可能出现的标点符号，<strong class="kr hs">删除</strong> <strong class="kr hs">停止</strong> <strong class="kr hs">单词:</strong></p><pre class="jh ji jj jk fd mg mh mi mj aw mk bi"><span id="3287" class="ml jy hi mh b fi mm mn l mo mp"><em class="me">#remove punctuation via String.tranlate(table)<br/></em>def remove_punct(series):<br/>   table = str.maketrans('', '', string.punctuation)<br/>   tokens_punct = series.translate(table).lower()<br/>   tokens_spaces = ' '.join(<br/>   [token.strip() for token in tokens_punct.split() if token != ' ']<br/>   )<br/>   return tokens_spaces</span><span id="4d2f" class="ml jy hi mh b fi mr mn l mo mp">train['tokens_punct'] = train.lemmatiz.apply(remove_punct)</span><span id="d61f" class="ml jy hi mh b fi mr mn l mo mp"><br/><em class="me">#delete stopwords<br/></em>train['remove_stop_words'] = train.tokens_punct.apply(lambda series: ' '.join([word for word in series.split() <br/>                                if word not in stop_words]))</span></pre><p id="885f" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">基本的<em class="me">语言统计</em>对于这个集合上的特征工程也是有价值的。我们可能会创建其他代表性特征，例如:</p><ul class=""><li id="448c" class="ll lm hi kr b ks ln kv lo ky lp lc lq lg lr lk ls lt lu lv bi translated"><strong class="kr hs"> count_sent </strong>(每个清洗过的段落有多少句)；</li><li id="5e91" class="ll lm hi kr b ks lw kv lx ky ly lc lz lg ma lk ls lt lu lv bi translated"><strong class="kr hs"> num_count </strong>(每清理一段出现多少个数字)；</li><li id="507a" class="ll lm hi kr b ks lw kv lx ky ly lc lz lg ma lk ls lt lu lv bi translated"><strong class="kr hs"> count_words </strong>(每清理一段包含多少字)。</li></ul><pre class="jh ji jj jk fd mg mh mi mj aw mk bi"><span id="c071" class="ml jy hi mh b fi mm mn l mo mp">train['count_sent'] = train.remove_stop_words.apply(lambda series: <br/>1 if series.count('zdot') == 0 else series.count('zdot'))</span><span id="613d" class="ml jy hi mh b fi mr mn l mo mp">train['num_count'] = train.remove_stop_words.apply(lambda series: series.count('znum'))</span><span id="f810" class="ml jy hi mh b fi mr mn l mo mp">train['count_words'] = train.remove_stop_words.apply(lambda series: len(nlp(series)))</span></pre><p id="5455" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">现在，我认为，我们已经为基本模型运行创建了足够多的额外功能。看下图(图5)。它显示五个数字列(用红色方块标记)和两个文本列:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es na"><img src="../Images/beb93ba3719897fc0f92d4af48cc31ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x-CxUb0MlIcY1tqiOxN_5g.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Pic. 5 The train set: sourse text, preprocessed paragraphs and new features.</figcaption></figure><p id="7628" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">我们留下的“段落”列是整个研究任务结束时模型解释的源文本(不会出现在本文中)。我们暂时将数字特征放在一边，因为它们已经准备好作为模型输入。现在，我们将把焦点放在准备好的文本列上。</p><h1 id="c889" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">用Tensorflow实现“文本到序列”的转换。</h1><p id="808b" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated"><em class="me"> remove_stop_words </em>我们将使用<em class="me"> Tokenizer </em>和<a class="ae jw" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences" rel="noopener ugc nofollow" target="_blank"><strong class="kr hs"><em class="me">text _ to _ sequence</em></strong></a>方法用<strong class="kr hs"> <em class="me"> Tensorflow </em> </strong>进行转换。</p><p id="6aa9" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated"><em class="me"> Tensorflow </em>中的Tokenizer允许对文本语料库进行矢量化，通过将每个文本转换为整数序列(每个整数是字典中某个标记的索引)或向量，其中每个标记的系数可以是二进制的，基于单词计数，基于tf-idf等。在我们的例子中，我们应用了以<a class="ae jw" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> <em class="me">为基数的TF </em> - <em class="me"> IDF </em> </a>系数(术语频率的缩写-逆文档频率，是一种数字统计，旨在反映一个词对集合或语料库中的文档有多重要)。</p><p id="5964" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">当我们将一个<em class="me">记号赋予器</em>点定义为<em class="me">【OOV】</em>词汇时，这种方式会使我们的数据免受损失:</p><pre class="jh ji jj jk fd mg mh mi mj aw mk bi"><span id="1225" class="ml jy hi mh b fi mm mn l mo mp">from tensorflow.keras.preprocessing.text import Tokenizer<br/></span><span id="dbf0" class="ml jy hi mh b fi mr mn l mo mp">tokenizer = Tokenizer(oov_token="&lt;OOV&gt;")<br/>tokens = tokenizer.fit_on_texts(train['remove_stop_words'])</span><span id="c39d" class="ml jy hi mh b fi mr mn l mo mp"><br/><em class="me"># Turn text into sequences (word --&gt; num )<br/></em>text_sequences = tokenizer.texts_to_sequences(train['remove_stop_words'])</span><span id="86b2" class="ml jy hi mh b fi mr mn l mo mp"><em class="me"># Turn text sequences into tf-idf sequences<br/></em>tfidf_train = <br/>tokenizer.sequences_to_matrix(text_sequences , mode='tfidf')</span></pre><p id="4553" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated"><em class="me"> Tensorflow Tokenizer </em>类有一个内嵌函数<em class="me"> sequence_to_matrix() </em>带参数<em class="me"> mode </em> —只需选择<em class="me"> mode="tfidf" </em>并用它包装您的文本序列。现在，您已经为每个文本段落中的每个单词赋予了权重/重要性(当您的集合很小时，<em class="me"> mode="tfidf" </em>可能会充分改善您未来的结果)。完美！让我们检查tfidf矩阵的形状；它的大小应为(集合中的行数，词汇大小):</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nb"><img src="../Images/319de84fac557160333c430259c00f01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ae_qCklRTcjg5uWOBOmtVw.png"/></div></div></figure><p id="a529" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">好了，这就是我们需要对文本做的所有事情，让它准备好作为模型的输入。不要忘记为测试集重复所有的转换和特性工程步骤！！！最后，您应该得到六个数组:三个用于训练，三个用于测试(带有<em class="me"> tfidf </em>、非文本特征和标签的序列)。</p><p id="6a33" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">最后，我们可以说:“我们已经完成了文本预处理和特征工程”。下一步应该是—创建一个分类模型。这可能是下一篇文章的一个大主题。</p><p id="0805" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated">在本教程中，您学习了如何使用<em class="me"> Pandas </em>和<em class="me"> Numpy </em>操作文本数据。熟悉作为模型输入的文本准备流程。看到了如何使用文本的<em class="me"> Spacy </em>和<em class="me"> Tensorflow </em>库的真实数据示例。现在你明白什么是<em class="me">文本预处理</em>了吧。</p><p id="46cc" class="pw-post-body-paragraph kp kq hi kr b ks ln is ku kv lo iv kx ky mb la lb lc mc le lf lg md li lj lk hb bi translated"><em class="me"/><a class="ae jw" href="https://colab.research.google.com/drive/1YOZ60sdOjSbIiB3IiJVUYprQhJDEVxXB?usp=sharing" rel="noopener ugc nofollow" target="_blank"><em class="me">Google collab</em></a><em class="me"/><a class="ae jw" href="https://github.com/Galina-Blokh/ai_assignment_aidock/blob/refator/preprocess.py" rel="noopener ugc nofollow" target="_blank"><em class="me">Github</em></a><em class="me">。</em></p></div></div>    
</body>
</html>
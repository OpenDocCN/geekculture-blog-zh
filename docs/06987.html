<html>
<head>
<title>Use GPT-J 6 Billion Parameters Model with Huggingface</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用GPT-J 60亿参数模型与拥抱脸</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/use-gpt-j-6-billion-parameters-model-with-huggingface-2ea30e151003?source=collection_archive---------24-----------------------#2021-09-03">https://medium.com/geekculture/use-gpt-j-6-billion-parameters-model-with-huggingface-2ea30e151003?source=collection_archive---------24-----------------------#2021-09-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/9fe96971a2c0b2bbf3a4b52610520d05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MV7AkiDUcXOxw_c1.png"/></div></div></figure><p id="0a30" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了寻求复制OpenAI的GPT-3模型，EleutherAI的研究人员已经发布了强大的语言模型。在GPT-近地天体之后，最新的一个是GPT-J，它有60亿个参数，与类似大小的GPT-3模型相比，它的工作性能相当。</p><p id="240f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在零短学习方面，GPT-J的性能被认为是与其他开源语言模型相比最好的。</p><p id="2b41" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于它是在大型编码文本语料库(来自GitHub和StackExchange)上训练的，GPT-J在编写代码方面的性能优于GPT-3。</p><p id="e9c7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">GPT-J 6B 的<a class="ae jo" href="https://6b.eleuther.ai/" rel="noopener ugc nofollow" target="_blank">演示早些时候由艾勒瑟·艾提供。由于它是开源的，我们可以下载模型并在我们自己的服务器上运行。</a></p><p id="6fc5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用https://github.com/kingoflolz/mesh-transformer-jax/的T4，我们可以在我们的服务器上运行GPT-J，我们计划发布相同的docker容器，这样任何人都可以下载容器并在他们的服务器上运行。</p><p id="ea48" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但我们一直在等待GPT-J被包含在Huggingface回购中，以便我们可以通过Huggingface直接使用它。</p><p id="a51d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">等待终于结束了！Huggingface终于在回购中加入了GPT J模型。我们已经试过了，结果令人印象深刻。</p><p id="b423" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从<a class="ae jo" href="https://huggingface.co/EleutherAI/gpt-j-6B" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/EleutherAI/gpt-j-6B</a>可以进入拥抱脸GPT-J模型。</p><p id="4bb9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们的实验中，我们试图在有和没有GPU的情况下运行它。Huggingface使模型的使用变得非常容易。让我们带您了解如何在您自己的服务器上运行它。</p><h1 id="8218" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">GPT-J带CPU(不带GPU)</h1><p id="82a7" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">如果你在没有GPU的情况下运行GPT-J，那么你将需要一个大约50 GB内存的系统。</p><p id="8a65" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦系统安装了所需的RAM、python和virtualenv库，请遵循以下步骤:</p><p id="ff54" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1.创建和激活虚拟环境</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="f32a" class="lb jq hi kx b fi lc ld l le lf">virtualenv env_cpu --python=python3 <br/>source env_cpu/bin/activate</span></pre><p id="0531" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.克隆和设置变压器存储库</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="447d" class="lb jq hi kx b fi lc ld l le lf">git clone https://github.com/huggingface/transformers <br/>cd transformers <br/>pip install .</span></pre><p id="2ad6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.安装火炬</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="e349" class="lb jq hi kx b fi lc ld l le lf">pip3 install torch==1.9.0+cpu torchvision==0.10.0+cpu torchaudio==0.9.0 -f <a class="ae jo" href="https://download.pytorch.org/whl/torch_stable.html" rel="noopener ugc nofollow" target="_blank">https://download.pytorch.org/whl/torch_stable.html</a></span></pre><p id="4f2a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4.现在创建一个python文件并粘贴下面的代码块。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="83c0" class="lb jq hi kx b fi lc ld l le lf">from transformers import AutoTokenizer, AutoModelForCausalLM<br/>import time</span><span id="46c3" class="lb jq hi kx b fi lg ld l le lf">tokenizer = AutoTokenizer.from_pretrained("gpt2")<br/>model = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-j-6B")</span><span id="0bfc" class="lb jq hi kx b fi lg ld l le lf">print("Model Loaded..!")</span><span id="08d3" class="lb jq hi kx b fi lg ld l le lf">start_time = time.time()</span><span id="a49a" class="lb jq hi kx b fi lg ld l le lf">input_text = "Google was founded by"<br/>inputs = tokenizer(input_text, return_tensors="pt")<br/>input_ids = inputs["input_ids"]</span><span id="509d" class="lb jq hi kx b fi lg ld l le lf">output = model.generate(<br/>input_ids,<br/>attention_mask=inputs["attention_mask"],<br/>do_sample=True,<br/>max_length=150,<br/>temperature=0.8,<br/>use_cache=True,<br/>top_p=0.9<br/>)</span><span id="8e8b" class="lb jq hi kx b fi lg ld l le lf">end_time = time.time() - start_time<br/>print("Total Taken =&gt; ",end_time)<br/>print(tokenizer.decode(output[0]))</span></pre><p id="ae44" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">5.就是这样！运行python文件，您将获得输出。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/8b19b9e04c2944346dc12c348bef16b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/0*m4JJmVHYE_GcYJpe.png"/></div></figure><h1 id="122f" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">带GPU的GPT J</h1><p id="8846" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">我们在AWS EC2实例上使用了T4 GPU。使用GPU时，系统所需的RAM大约为38GB。</p><p id="78d3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在使用GPU时，主要的区别是我们必须用CUDA安装torch并在代码中使用它。</p><p id="b714" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">遵循“带CPU的GPT-J”中提供的前两步，然后</p><p id="90af" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.用CUDA安装火炬</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="76c6" class="lb jq hi kx b fi lc ld l le lf">pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f <a class="ae jo" href="https://download.pytorch.org/whl/torch_stable.html" rel="noopener ugc nofollow" target="_blank">https://download.pytorch.org/whl/torch_stable.html</a></span></pre><p id="a637" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意:从这里根据你的平台安装兼容的torch版本<a class="ae jo" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/</a></p><p id="ac0c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4.创建一个python文件并粘贴下面的代码块。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="58ac" class="lb jq hi kx b fi lc ld l le lf">from transformers import AutoTokenizer, GPTJForCausalLM<br/>import time<br/> <br/>tokenizer = AutoTokenizer.from_pretrained("gpt2")<br/>model =  GPTJForCausalLM.from_pretrained("EleutherAI/gpt-j-6B", torch_dtype=torch.float16).to("cuda")<br/> <br/>print("Model Loaded..!")<br/> <br/>start_time = time.time()<br/> <br/>input_text = "Google was founded by"<br/> <br/>inputs = tokenizer(input_text, return_tensors="pt")<br/>input_ids = inputs["input_ids"].to("cuda")<br/>      <br/>output = model.generate(<br/>   input_ids,<br/>   attention_mask=inputs["attention_mask"].to("cuda"),<br/>   do_sample=True,<br/>   max_length=150,<br/>   temperature=0.8,<br/>   use_cache=True,<br/>   top_p=0.9<br/>)<br/> <br/>end_time = time.time() - start_time<br/>print("Total Time =&gt; ",end_time)<br/>print(tokenizer.decode(output[0]))</span></pre><p id="1880" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">5.运行该文件，您将获得由GPT-J生成的输出文本</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es li"><img src="../Images/da26766dae13f0681731996909c2546a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/0*ll2EooqZvMlUFWYO.png"/></div></figure><h1 id="de67" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">多GPU支持</h1><p id="1f06" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">在上面的例子中，我们使用了一个T4 GPU。如果您想要使用多个GPU，那么您可以将其添加到您的系统中，然后在代码中进行以下更改，以便该模型将利用系统上可用的多个GPU。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="9f8d" class="lb jq hi kx b fi lc ld l le lf">from transformers import AutoTokenizer, GPTJForCausalLM<br/> <br/>tokenizer = AutoTokenizer.from_pretrained("gpt2")<br/>model = GPTJForCausalLM.from_pretrained("EleutherAI/gpt-j-6B")<br/>model.parallelize()</span></pre><p id="e589" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如我们所看到的，我们只需要在代码中添加“model.parallelize()”。</p><p id="df83" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">去试试吧，在你自己的服务器上运行GPT-J！如果你面临任何困难，请在评论中告诉我们。</p><p id="0036" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们提供<a class="ae jo" href="https://www.pragnakalp.com/services/natural-language-processing-services/" rel="noopener ugc nofollow" target="_blank">自然语言处理</a>咨询服务。请务必在letstalk@pragnakalp.com<a class="ae jo" href="mailto:letstalk@pragnakalp.com" rel="noopener ugc nofollow" target="_blank">与我们联系任何与GPT-J、GPT-3、BERT或NLP相关的项目。</a></p></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><p id="5e43" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="lq">最初发布于2021年9月3日</em> <a class="ae jo" href="https://www.pragnakalp.com/gpt-j-6b-parameters-model-huggingface/" rel="noopener ugc nofollow" target="_blank"> <em class="lq">使用GPT-J 60亿参数模型与Huggingface </em> </a> <em class="lq">。</em></p></div></div>    
</body>
</html>
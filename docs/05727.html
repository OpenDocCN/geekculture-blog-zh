<html>
<head>
<title>Convolutional Autoencoder based Dimension Estimation from Depth Map of Monocular Images</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于卷积自动编码器的单目图像深度图维数估计</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/convolutional-autoencoder-based-dimension-estimation-from-depth-map-of-monocular-images-9269d966bb86?source=collection_archive---------43-----------------------#2021-07-28">https://medium.com/geekculture/convolutional-autoencoder-based-dimension-estimation-from-depth-map-of-monocular-images-9269d966bb86?source=collection_archive---------43-----------------------#2021-07-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="69f9" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">这个项目是我本科毕业论文(2020)的一部分。我的队友是古鲁普拉萨德·维斯瓦纳坦·拉梅什和T2【拉凯什·瓦伊迪斯瓦兰。我们是在NITT国家理工学院的<a class="ae jh" href="https://www.nitt.edu/home/academics/departments/ece/faculty/gopi/" rel="noopener ugc nofollow" target="_blank">e . s . gopi</a>博士的指导下完成这个项目的。本文描述了这个过程。</p></blockquote><h2 id="c14d" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">介绍</h2><p id="b8b0" class="pw-post-body-paragraph ii ij hi il b im kg io ip iq kh is it jt ki iw ix jx kj ja jb kb kk je jf jg hb bi translated">这个项目是关于使用深度学习方法来估计三维空间中任意两点之间的欧几里德距离，给定单目图像和相应的深度图像。所提出的技术是用户友好的，其中用户需要在单目图像上选择两个任意点。使用自动编码器-人工神经网络架构方法，其中使用自动编码器网络提取特征向量，并用于构建基于人工神经网络(ANN)的回归模型。估计欧几里德距离的平均偏差误差被获得为0.059米。实验结果揭示了所提出的技术的重要性，它可以被结合到各种尺寸测量应用中。下面给出的流程图描述了该方法。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es kl"><img src="../Images/bc4c218fdf223ab476ac744231ab8d00.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*3olWX2sXbmO7-nn7rtAiow.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Fig 1: Flow Chart of the method</figcaption></figure><h2 id="f2ff" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">数据收集</h2><p id="5468" class="pw-post-body-paragraph ii ij hi il b im kg io ip iq kh is it jt ki iw ix jx kj ja jb kb kk je jf jg hb bi translated">任何机器学习项目的前提都是数据。微软Kinect用于准备数据集。门被用作参考对象，一组作为兴趣点的贴纸被放置在门上(图2)。我们收集数据的三扇门上都贴了七张黄色贴纸。因此，每个车门总共有21个不同的目标值(贴纸对)。使用放置在任意距离的Kinect摄像头同时收集门的颜色和深度图像。对于摄像机的一个位置，每个门有6-8个不同的角度。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es kx"><img src="../Images/f71e186795b023fa919d4ad5af396019.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*0NnsO6jcFrlZLhrtGIRZ9w.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Fig 2: Three different doors are used for the data collection process. Each row represents a unique door (Door 1, Door 2, Door 3 in order) and each column represents an arbitrary angle relative to an arbitrary position of the Kinect at which the image of the door was captured. Stickers are placed on it so that different distances can be measured between each sticker pair.</figcaption></figure><p id="e4de" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix jx iz ja jb kb jd je jf jg hb bi translated">为了同时以相同的分辨率捕捉彩色和深度图像，一个<a class="ae jh" href="https://github.com/code-iai/iai_kinect2" rel="noopener ugc nofollow" target="_blank">机器人操作系统(ROS)模块</a>与Kinect摄像头相连接。通过将Kinect摄像头(在1.5-4米范围内效果最佳)放置在合适的位置，以使Kinect软件完全可以想象所获得的颜色和深度图像，为三扇门中的每一扇门收集数据。通过确保不同门的目标值是唯一的，总共获得了63个不同的目标值。总共收集了99幅原始彩色图像和相应的深度图像。每个图像进一步提供21对点用于距离估计，从而给出总共2079个预处理图像。2079张图片中的每一张都标有相应的尺寸。下表给出了。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es ky"><img src="../Images/b75c5d9517033e090e6faf252edf4392.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*nr4TPrxPqV9THwIH7D8uog.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Fig 3: Table mentioning the number of values for each position and angle for each door.</figcaption></figure><h2 id="efee" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">数据预处理</h2><p id="2515" class="pw-post-body-paragraph ii ij hi il b im kg io ip iq kh is it jt ki iw ix jx kj ja jb kb kk je jf jg hb bi translated">开发了卷积自动编码器模型，以便在不丢失信息的情况下从图像中提取特征。原始深度图像通过自动编码器模型，并使用编码部分的特征图。所选点的坐标随后与特征图一起被馈送给人工神经网络。自动编码器型号的压缩系数为4。体系结构如图4所示。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es kz"><img src="../Images/8edd9cc1540b929bd736a7622b40fc43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rde0AZARdmT2KCUGAvl-zw.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx">Fig 4: Auto Encoder-Decoder Architecture</figcaption></figure><p id="a899" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix jx iz ja jb kb jd je jf jg hb bi translated">使用均方误差(MSE)损失函数和Adam优化器。在图5中可以看到作为模型的历元数的函数的训练和验证损失的值。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es le"><img src="../Images/277ad50758c28601e55ddbb57dc9b772.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*mAvJvrJwGgjuc_7cinTKHw.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Fig 5: Loss plot for the auto encoder-decoder</figcaption></figure><p id="b76e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix jx iz ja jb kb jd je jf jg hb bi translated">图6示出了通过模型后的原始深度图像及其相应的重建图像。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es lf"><img src="../Images/d2ccda8d14662cf55b071211f89ff05e.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*cZgqkHOawg6ZJdzyRD55Nw.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Fig 6: Top row shows the input depth images and the bottom row shows the corresponding reconstructed images from the encoder-decoder model.</figcaption></figure><h2 id="e1dc" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">人工神经网络</h2><p id="6100" class="pw-post-body-paragraph ii ij hi il b im kg io ip iq kh is it jt ki iw ix jx kj ja jb kb kk je jf jg hb bi translated">从自动编码器模型(图6)的编码器部分获得的深度图像的特征图(编码)是平坦的。然后，用户选择的点的坐标与扁平化编码连接，作为ANN的输入。输入层连接到具有1000个神经元的层，该层之后是批量标准化层。后续层中的神经元数量减少到100，然后最后，只有一个神经元用于输出。使用MSE损失函数和RMSProp优化器。</p><h2 id="28fe" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">结果</h2><p id="d499" class="pw-post-body-paragraph ii ij hi il b im kg io ip iq kh is it jt ki iw ix jx kj ja jb kb kk je jf jg hb bi translated">这些模型是使用Keras和Tensorflow后端实现的，并在Google Colab TPU上进行训练。模型的性能取决于预测维度与实际维度的接近程度。由于问题陈述是一个回归问题(因为输出变量可以取任何实际值)，因此不能根据正确预测的数量来测量准确性，因为很有可能没有准确预测的值。然而，模型的性能可以通过其预测中的误差量来评估。例如，如果尺寸的真实值是150厘米，则预测148厘米或152厘米的模型比预测165厘米的模型更好。散点图如图7所示。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es lg"><img src="../Images/c81dbc14324a67bf373cc8dcbf751809.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*kGFaMb9lmQpM-oI-Ha4TEA.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Fig 7: Scatter plot between the True value (original measurement) and the Predicted Value.</figcaption></figure><p id="ae5e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix jx iz ja jb kb jd je jf jg hb bi translated">发现该模型的均方误差为0.00339，平均结果误差为(+/-) 0.059米。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es lh"><img src="../Images/fd831871bb4bd8e377d697b2dc140329.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*ltSb5iOhX2SogwPEEnUZDw.jpeg"/></div><figcaption class="kt ku et er es kv kw bd b be z dx">Fig 8: True value: 0.95m, Predicted value 0.912m</figcaption></figure><h2 id="c4ce" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">限制</h2><ol class=""><li id="23c6" class="li lj hi il b im kg iq kh jt lk jx ll kb lm jg ln lo lp lq bi translated">我们估计两点之间距离的方法是基于使用Kinect生成的深度图。它假设图像的深度图的可用性来估计三维空间中的距离。</li><li id="9b8b" class="li lj hi il b im lr iq ls jt lt jx lu kb lv jg ln lo lp lq bi translated">提高机器学习问题准确性的过程涉及大量的尝试。有必要尝试不同的方法来帮助提高性能。在机器学习问题中，可用数据的数量和类型是实现最佳结果的重要限制。因此，通过收集更多的数据，可以提高模型的性能，前提是数据增加了模型可以学习的模式的多样性。选择合适的超参数对提高模型的性能也有一定的作用。</li></ol></div><div class="ab cl lw lx gp ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="hb hc hd he hf"><p id="1bb6" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix jx iz ja jb kb jd je jf jg hb bi translated"><em class="ik">链接到Github知识库:</em><a class="ae jh" href="https://github.com/hariharannatesh/Dimension-Estimation-from-Depth-Map-of-Monocular-Image" rel="noopener ugc nofollow" target="_blank"><em class="ik">https://Github . com/hariharannatesh/Dimension-Estimation-from-Depth-Map-of-single-Image</em></a></p></div></div>    
</body>
</html>
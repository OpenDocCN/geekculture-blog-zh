<html>
<head>
<title>Logistic Regression Without Probabilities Theory</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无概率逻辑回归理论</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/logistic-regression-without-probabilities-theory-1994000bf927?source=collection_archive---------24-----------------------#2021-08-05">https://medium.com/geekculture/logistic-regression-without-probabilities-theory-1994000bf927?source=collection_archive---------24-----------------------#2021-08-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="6afc" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">机器学习变得简单</h2><div class=""/><div class=""><h2 id="6b8f" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">这是一个扭曲的线性回归</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/93690763ee5289e8df67a1aa93ed59c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lHVz-zbOMa2kYmZ8rbauTA.jpeg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">by <strong class="bd jw">Austin Chan</strong></figcaption></figure><h1 id="60f2" class="jx jy hi bd jw jz ka kb kc kd ke kf kg ix kh iy ki ja kj jb kk jd kl je km kn bi translated">介绍</h1><p id="f5d0" class="pw-post-body-paragraph ko kp hi kq b kr ks is kt ku kv iv kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">逻辑回归是一种在许多方面类似于线性回归的统计模型。但是，与线性回归相反，逻辑回归涉及强概率理论概念，如对数似然、贝叶斯定理或条件概率。事实上，逻辑回归解通常是从二项式分布的对数似然性的最大化得到的。</p><p id="1ea3" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">在本文中，我们提出了一种<strong class="kq hs">线性回归方法作为替代公式</strong>，其中您不需要知道什么是二项式分布或对数似然性。但是，如果您知道什么是线性回归，您将学习如何毫不费力地导出逻辑回归，如何使用它，以及如何使用python scikit-learn正确地解决它。</p></div><div class="ab cl lp lq gp lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="hb hc hd he hf"><h1 id="6898" class="jx jy hi bd jw jz lw kb kc kd lx kf kg ix ly iy ki ja lz jb kk jd ma je km kn bi translated">逻辑回归再探</h1><p id="8d1f" class="pw-post-body-paragraph ko kp hi kq b kr ks is kt ku kv iv kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">在线性回归中，我们要预测一个<strong class="kq hs">连续</strong>变量(<strong class="kq hs"> y) </strong>，用模型的参数(<strong class="kq hs"> β </strong>)加权的一组特征(<strong class="kq hs"> X) </strong>)。假设我们已经为两者(<strong class="kq hs"> y，X </strong>)收集了<strong class="kq hs"> n </strong>个数据点。例如，我们可以通过一名球员的位置、进球数和年龄来预测他的转会金额。在这种情况下，<strong class="kq hs"> X </strong>是一个具有<strong class="kq hs"> n </strong>行和3列的矩阵，而<strong class="kq hs"> y </strong>是一个大小为<strong class="kq hs"> n. </strong> <strong class="kq hs">的向量，那么预测值(hat y)就是这三个特征的线性组合</strong>。数学上我们有:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mb"><img src="../Images/f8001a040d099ea1c9649a364c34d5c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:122/0*Nne_GMb3NmpTv06w"/></div><figcaption class="js jt et er es ju jv bd b be z dx">The linear predictor</figcaption></figure><p id="6f2b" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">现在，您可以为设置的参数<strong class="kq hs"> β </strong>取任何您想要的数字，并进行预测。但是，如果您想变得更聪明，您可以定义一个函数来优化，以找到一组最佳参数。该函数将测量线性预测值和真实目标值的接近程度。</p><blockquote class="mc md me"><p id="6d8b" class="ko kp mf kq b kr lk is kt ku ll iv kw mg lm kz la mh ln ld le mi lo lh li lj hb bi translated">为了确保参数集是唯一的，我们需要函数在最小化时是凸的，在最大化时是凹的。</p></blockquote><p id="14d3" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">通常选择<strong class="kq hs">均方误差(MSE) </strong>作为最小化函数。有像平均绝对误差、Huber损失和许多其他的选择，它们中的每一个都有特定的属性。</p><p id="70ed" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">在我们优化均方误差的情况下，我们将需要找到使以下量最小化的一组参数:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mj"><img src="../Images/80e25211cfc75cd66729e76ae28c1ff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/0*JVQaWJpAleVHae1B"/></div><figcaption class="js jt et er es ju jv bd b be z dx">minimizing the mean squared error</figcaption></figure><p id="56c6" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq hs">它是我们误差的2范数平方，</strong>误差是我们预测的值(<strong class="kq hs"> y </strong>)和实际目标值(<strong class="kq hs"> y </strong>)之间的差值。这只是MSE的一个奇特符号。有时候我们可以在约束下优化。例如，如果您希望参数为正，那么您可以在约束条件下最小化MSE，即<strong class="kq hs"> β &gt; 0 </strong>。</p><p id="4c5c" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">所以线性回归的步骤只是:获得一些数据，有一个函数来优化(MSE)，并找到最佳的模型参数。我们不会推导出解决方案，因为网络上有成千上万篇文章在做这件事，而这不是本文的目的。</p><h1 id="4369" class="jx jy hi bd jw jz ka kb kc kd ke kf kg ix kh iy ki ja kj jb kk jd kl je km kn bi translated">逻辑回归再探</h1><p id="7be4" class="pw-post-body-paragraph ko kp hi kq b kr ks is kt ku kv iv kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">本文的目标是说明逻辑回归等价于线性回归。但是有一点小小的改变。</p><p id="04f6" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">与线性回归一样，逻辑回归也是一种线性模型:预测依赖于特征的线性组合。这次我们要预测一个<strong class="kq hs">二元</strong>变量(<strong class="kq hs"> y) </strong>，用模型的参数(<strong class="kq hs"> β </strong>)加权一组特征(<strong class="kq hs"> X) </strong>)。什么是二元变量？这意味着目标只能取两个值:0或1。这是一种数学方式来表达一个事件是否发生。例如，球队得分(y=1)对没有得分(y=0)，或者主队获胜(y=1)对主队失败(y=0)。两者之间没有可能的值。</p><p id="9f3d" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">到目前为止，它就像线性回归一样，但事实上<strong class="kq hs"> y </strong>是二元的，这给问题增加了挑战:现在<strong class="kq hs">预测必须在相同的二元尺度上</strong>。事实上，你不可能有一个负面的预测，或者高于1的预测。但是你可以在0和1之间进行连续预测。我们如何实现这一目标？</p><blockquote class="ml"><p id="ac06" class="mm mn hi bd mo mp mq mr ms mt mu lj dx translated">线性回归和逻辑回归的主要区别在于预测被限制在连续区间[0，1]内。</p></blockquote><p id="cd00" class="pw-post-body-paragraph ko kp hi kq b kr mv is kt ku mw iv kw kx mx kz la lb my ld le lf mz lh li lj hb bi translated">第一种解决方案是将预测限制在相同的区间内:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es na"><img src="../Images/d45fe881f19594a739f34d7d0a500e58.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/0*Ntx8U7iSundI3RO1"/></div><figcaption class="js jt et er es ju jv bd b be z dx">MSE of constrained linear regression</figcaption></figure><p id="3847" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">这个问题是线性的，也是可解的，但是<strong class="kq hs">只适用于样本</strong>。没有什么能保证模型从未见过的新数据会满足约束。</p><p id="39b0" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">一个始终有效的更好的解决方案是转换预测，使其始终处于正确的区间。一个可能的转换是<strong class="kq hs">sigmoid函数(σ): </strong></p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nb"><img src="../Images/d6bb4eb74d8ccb841f1462b22e1cd8c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:204/0*BDeVadjqBfEjXqWo"/></div><figcaption class="js jt et er es ju jv bd b be z dx">the sigmoid function</figcaption></figure><p id="0191" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">如你所见<strong class="kq hs"> t </strong>可以取任意值，<strong class="kq hs"> σ(t) </strong>将始终在[0，1]区间内。问题解决了。要最小化的新MSE函数为:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nc"><img src="../Images/f3a6a8c222d07e05492f0da830f1961b.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/0*PJTsymRaltdrf_ff"/></div><figcaption class="js jt et er es ju jv bd b be z dx">MSE of logistic regression (norm form)</figcaption></figure><p id="3aee" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">上面定义的均方误差被称为<strong class="kq hs"> Brier分数</strong>。让我们用它们的值来代替所有的数学量:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nd"><img src="../Images/dd7d4263d703d31911dbf5921082b044.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/0*sRrnIRfvqHFj-cBJ"/></div><figcaption class="js jt et er es ju jv bd b be z dx">MSE of logistic regression (scalar form)</figcaption></figure><p id="d429" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">不失一般性，我们可以最小化各个平方表达式的对数之和。然后，由于<strong class="kq hs"> y </strong>只能取两个值，最小化简化为:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ne"><img src="../Images/0e8c1f64b039c852bfe88477dce71a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/0*zKdAJL-fSHcB5FhG"/></div><figcaption class="js jt et er es ju jv bd b be z dx">MSE of logistic regression (reduced form)</figcaption></figure><p id="b772" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">这种简化形式正好是逻辑回归模型的标准概率推导的对数似然比的两倍。解决方案是一样的。</p><blockquote class="ml"><p id="6beb" class="mm mn hi bd mo mp mq mr ms mt mu lj dx translated">逻辑回归是一种线性回归，其中预测值通过sigmoid函数进行转换</p></blockquote><p id="0ecd" class="pw-post-body-paragraph ko kp hi kq b kr mv is kt ku mw iv kw kx mx kz la lb my ld le lf mz lh li lj hb bi translated">最小化可以在scikit-learn中使用正确的输入集通过类<code class="du nf ng nh ni b">LogisticRegression</code>来执行:</p><pre class="jh ji jj jk fd nj ni nk nl aw nm bi"><span id="2b99" class="nn jy hi ni b fi no np l nq nr">from sklearn.linear_model import LogisticRegression</span><span id="9e9e" class="nn jy hi ni b fi ns np l nq nr">#fit_intercept can be False if we need to<br/>model = LogisticRegression(penalty=”<em class="mf">none</em>",fit_intercept=True) <br/>model.fit(X, y)</span></pre><p id="7ccf" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">至此，模型求解完毕，可以在<code class="du nf ng nh ni b">model.coef_</code>中找到参数<strong class="kq hs"> β </strong>。根据我们的框架，预测如下:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nt"><img src="../Images/aeac70ab73f232ddbfb6b9fc9ed4c3f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:170/0*nNM_t86OnSUUPYcX"/></div><figcaption class="js jt et er es ju jv bd b be z dx">the logistic regression predictor</figcaption></figure><p id="4c47" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">我们可能会争辩说，我们可以使用任何在0和1之间缩放预测的函数，这并不是完全错误的。不是所有的函数都适合，因为最后一步必须是凸的或凹的才能优化。但是乙状结肠就是这种情况。</p><p id="8565" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">此外，sigmoid是使线性回归框架等同于概率方法的唯一函数。</p><h1 id="07f0" class="jx jy hi bd jw jz ka kb kc kd ke kf kg ix kh iy ki ja kj jb kk jd kl je km kn bi translated">分类符合回归</h1><p id="f7fc" class="pw-post-body-paragraph ko kp hi kq b kr ks is kt ku kv iv kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">到目前为止，我们只谈到了回归，这就是我们所做的。但是你可以在任何一本统计学书籍中查找，<strong class="kq hs">逻辑回归是一个分类模型。</strong>那么我们遗漏了什么？</p><p id="fb84" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">使用scikit-learn在回归框架中进行预测，您通常称之为<code class="du nf ng nh ni b">model.predict(X)</code>。如果你用上面的<code class="du nf ng nh ni b">LogisticRegression</code>来做，你将会得到一个0和1的二进制集合，<strong class="kq hs">类</strong>。其实有点误导。你想称之为<code class="du nf ng nh ni b">model.predict_proba(X)</code>,这正是我们目前在线性回归框架中定义的预测。</p><p id="691e" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">由于这些预测在区间[0，1] <strong class="kq hs">中是连续的，它们可以被解释为概率。</strong>如果预测值接近1，表示0.87，那么更有可能是1而不是0 <strong class="kq hs">。因此，通过一个简单的规则，这些预测可以变成双星预测:</strong></p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nu"><img src="../Images/079eb3efb27af7bbce0e9396e1e6bfd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/0*ekjDrtW-PeQ0WJe_"/></div></figure><p id="1028" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">这正是<code class="du nf ng nh ni b">model.predict(X)</code>为<code class="du nf ng nh ni b">LogisticRegression</code>型号所做的。显然，例如，当预测接近0.5时，您可以更改0.5阈值或使用未知决策。所以使用预测比使用类更有趣。</p></div><div class="ab cl lp lq gp lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="hb hc hd he hf"><h1 id="7943" class="jx jy hi bd jw jz lw kb kc kd lx kf kg ix ly iy ki ja lz jb kk jd ma je km kn bi translated">结论</h1><p id="ca7d" class="pw-post-body-paragraph ko kp hi kq b kr ks is kt ku kv iv kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">在这篇文章中，我们展示了如何逻辑回归可以推导出没有知识的概率理论。仅使用线性回归框架，我们解释了约束线性回归和逻辑回归是等价的。</p><p id="7d20" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">概率往往比经典代数更难理解。通常存在一个替代概率方法的角度，但这不应该阻止你学习概率models⁵和贝叶斯统计。</p><p id="94a7" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">[1] P. J. Huber，<a class="ae mk" href="https://www.jstor.org/stable/2238020" rel="noopener ugc nofollow" target="_blank">位置参数的稳健估计</a>，1994年，数理统计年鉴，35(1)，第73–101页</p><p id="b65d" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">[2]主要用于优化目的，因为它使问题变得光滑、凸和可微</p><p id="2e18" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">【3】代数的细节可以在<a class="ae mk" href="https://www.octosport.io/post/logistic-regression-revisited" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="496e" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">[4] T. Hastie、R. Tibshirani和J H. Friedman，<a class="ae mk" href="https://web.stanford.edu/~hastie/ElemStatLearn/download.html" rel="noopener ugc nofollow" target="_blank"> <em class="mf">《统计学习的要素:数据挖掘、推理和预测</em> </a>，纽约:斯普林格，2001年。</p><p id="e606" class="pw-post-body-paragraph ko kp hi kq b kr lk is kt ku ll iv kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">[5] K. P. Murphy，《机器学习的概率观点》，麻省理工学院出版社，2012年。</p></div></div>    
</body>
</html>
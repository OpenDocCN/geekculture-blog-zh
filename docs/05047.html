<html>
<head>
<title>Variational Autoencoder(VAE)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可变自动编码器(VAE)</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/variational-autoencoder-vae-9b8ce5475f68?source=collection_archive---------3-----------------------#2021-07-08">https://medium.com/geekculture/variational-autoencoder-vae-9b8ce5475f68?source=collection_archive---------3-----------------------#2021-07-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="904e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为一种生成模型，VAE的基本思想很容易理解:通过编码器网络将真实样本转化为理想的数据分布，然后将这种数据分布传递给解码器网络，从而获得生成的样本。如果生成的样本和真实样本足够接近，则训练自动编码器模型。</p><h1 id="093e" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">降维、PCA和自动编码器(AE)</h1><h2 id="92fc" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">主成分分析</h2><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es kp"><img src="../Images/832c6b36eaf1fa710a9a557358a9f1a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9D-CKsfuSlG-MVNQVbgLVg.png"/></div></div></figure><p id="fcd4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如图所示，<em class="lb"> x </em>是一个可以通过变换<em class="lb"> W </em>成为低维矩阵<em class="lb"> c </em>的矩阵。因为这个过程是线性的，所以可以用<em class="lb"> W </em>的转置来还原一个<em class="lb"> x </em>帽子。PCA就是通过SVD(奇异值分解)找到一个<em class="lb"> W </em>，使得矩阵<em class="lb"> x </em>和<em class="lb"> x </em>尽可能的一致。</p><p id="0e67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，AE和PCA的区别在于AE使用神经网络而不是SVD。</p><h2 id="4b6c" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">自动编码器</h2><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lc"><img src="../Images/77c3a84c99fbab9d92021cb0e9c1b1cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nGFy96r63GwSE_EsJDLMDw.png"/></div></div></figure><h1 id="3b2f" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">可变自动编码器(VAE)</h1><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ld"><img src="../Images/09470ec1e431994f4aa49573e8c16a3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r1R0cxCnErWgE0P4Q-hI0Q.jpeg"/></div></div></figure><h2 id="574e" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">编码器</h2><p id="ac8d" class="pw-post-body-paragraph if ig hi ih b ii le ik il im lf io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated">这定义了近似的后验分布<strong class="ih hj"><em class="lb">q</em>(<em class="lb">z | x</em>)</strong>，其将观察作为输入，并输出一组用于指定潜在表示<strong class="ih hj"> <em class="lb"> z </em> </strong>的条件分布的参数。在本例中，简单地将分布建模为对角高斯分布，网络输出因子分解高斯分布的均值和对数方差参数。输出对数方差，而不是直接输出方差，以获得数值稳定性。</p><h2 id="0845" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">解码器</h2><p id="077c" class="pw-post-body-paragraph if ig hi ih b ii le ik il im lf io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated">这定义了观察值<strong class="ih hj"><em class="lb">q</em>(<em class="lb">x | z</em>)</strong>的条件分布，它以一个潜在样本作为输入，输出观察值的条件分布的参数。将潜在分布先验<strong class="ih hj"><em class="lb">P</em>(<em class="lb">z</em>)</strong>建模为单位高斯。</p><h2 id="034e" class="kb je hi bd jf kc kd ke jj kf kg kh jn iq ki kj jr iu kk kl jv iy km kn jz ko bi translated">重新参数化技巧</h2><p id="ef38" class="pw-post-body-paragraph if ig hi ih b ii le ik il im lf io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated">为了在训练期间为解码器生成样本<strong class="ih hj"> <em class="lb"> z </em> </strong>，可以从编码器输出的参数定义的潜在分布中采样，给定一个输入观测值<strong class="ih hj"> <em class="lb"> x </em> </strong>。然而，这种采样操作产生了瓶颈，因为反向传播不能流过随机节点。</p><p id="e6ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要解决这个问题，请使用重新参数化技巧。在我们的示例中，使用解码器参数和另一个参数<strong class="ih hj"> ε </strong>来近似<strong class="ih hj"> <em class="lb"> z </em> </strong>，如下所示:</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lj"><img src="../Images/c86928d0d9a4154609ec4a7a27320c8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t_LDtUnroBccBykfOO_5jA@2x.jpeg"/></div></div></figure><p id="ac0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中<strong class="ih hj"> μ </strong>和<strong class="ih hj"> σ </strong>分别代表高斯分布的均值和标准差。它们可以从解码器输出中获得。<strong class="ih hj"> ε </strong>可以认为是一个随机噪声，用来保持<strong class="ih hj">zT9】的随机性。从标准正态分布生成<strong class="ih hj"> ε </strong>。<br/>潜变量<strong class="ih hj"> <em class="lb"> z </em> </strong>现在由<strong class="ih hj"> μ </strong>、<strong class="ih hj"> σ </strong>和<strong class="ih hj"> ε </strong>的函数生成，这将使模型分别通过<strong class="ih hj"> μ </strong>和<strong class="ih hj"> σ </strong>反向传播编码器中的梯度，同时通过<strong class="ih hj"> ε </strong>保持随机性。</strong></p><h1 id="2f39" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">VAE是如何运作的？</h1><p id="0f5e" class="pw-post-body-paragraph if ig hi ih b ii le ik il im lf io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated">VAE的理论基础是<a class="ae lk" href="https://roger010620.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-em%E6%BC%94%E7%AE%97%E6%B3%95-expectation-maximization-algorithm-%E4%B8%89-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%92%8C%E6%A8%A1%E5%9E%8Bgaussian-mixture-model-gmm-84286c2d64c7" rel="noopener">高斯混合模型(GMM) </a>。不同的是，我们的代码被一个连续的变量<strong class="ih hj"> <em class="lb"> z </em> </strong>代替，<strong class="ih hj"> <em class="lb"> z </em> </strong>遵循标准正态分布<strong class="ih hj"> <em class="lb"> N </em> ( <em class="lb"> 0，1 </em> ) </strong>。<br/>对于每个样本<strong class="ih hj"> <em class="lb"> z </em> </strong>会有两个变量<strong class="ih hj"> <em class="lb"> μ </em> </strong>和<strong class="ih hj"> <em class="lb"> σ </em> </strong>，分别决定<strong class="ih hj"> <em class="lb"> z </em> </strong>对应的高斯分布的均值和标准差，然后积分域内所有高斯分布的累加就成为原始分布<strong class="ih hj"><em class="lb">P</em></strong></p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ll"><img src="../Images/97e4ba9f1cd057a0f329e00942a884af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mBEMgLQbdUnPkvUB7PAB8Q@2x.jpeg"/></div></div></figure><p id="9999" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中<strong class="ih hj"> <em class="lb"> z </em> ~ <em class="lb"> N </em> (0，1) </strong>，<strong class="ih hj"><em class="lb">x</em>|<em class="lb">z</em>~<em class="lb">N</em>(<em class="lb">μ</em>(<em class="lb">z</em>)，<em class="lb">σ</em>(<em class="lb">z</em>)</strong>，自<strong class="ih hj"><em class="lb">P</em>(<em class="lb">z<em class="lb"> 以及<strong class="ih hj"><em class="lb">x</em>|<em class="lb">z</em>~<em class="lb">N</em>(<em class="lb">μ</em>(<em class="lb">z</em>)，<em class="lb">σ</em>(<em class="lb">z</em>)</strong>。 我们真正需要求解的是<strong class="ih hj"> <em class="lb"> μ </em> </strong>和<strong class="ih hj"> <em class="lb"> σ </em> </strong>的表达式，但是<strong class="ih hj"><em class="lb">P</em>(<em class="lb">x</em>)</strong>非常复杂，μ和σ很难计算，需要引入两个神经网络来帮助我们求解。：</em></em></strong></p><p id="f9a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们希望<strong class="ih hj">P(<em class="lb">x</em>)</strong>越大越好，那么</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lm"><img src="../Images/c53329e411b41bb3ee221172d61de7a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R15h7xOTCLAehxxr5TEu1g@2x.jpeg"/></div></div></figure><p id="d9bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中<strong class="ih hj">日志<em class="lb"> P </em> ( <em class="lb"> x </em> ) </strong></p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ln"><img src="../Images/c31b79d5ed64980b4b4a2648f3aa1fff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iHTyHOmRVRJsil17yaVBuw@2x.jpeg"/></div></div></figure><p id="e577" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上式的第二项是大于等于0的值，所以我们找到了<strong class="ih hj">log<em class="lb">P</em>(<em class="lb">x</em>)</strong>的一个下界</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lo"><img src="../Images/7d38ba55472ed601906441e82ac2a984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qXPfMme1o-LQ_qKDi84Zmg@2x.jpeg"/></div></div></figure><p id="9ed6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将这个下限表示为ELBO:</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lp"><img src="../Images/a138f3edb195a482ae3b6e40ee630e93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xbqJS30jIwE3lLQW8Au1OA@2x.jpeg"/></div></div></figure><p id="acaa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以我们可以把原来的形式修改为:</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lq"><img src="../Images/ad1afdf64d89a1b9686d727d76fe854d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IxyfCHuCr-P22qZ0syyiiA@2x.jpeg"/></div></div></figure><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lr"><img src="../Images/4c97c855d8a9a28cea067951895d8bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*adWF4RN7v_PqXPV984hjSw.png"/></div></div></figure><p id="9a8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过调整<strong class="ih hj"><em class="lb">q</em>(<em class="lb">z</em>|<em class="lb">x</em>)</strong>让ELBO越来越高，KL散度越来越小。当我们调整<strong class="ih hj"><em class="lb">q</em>(<em class="lb">z</em>|<em class="lb">x</em>)</strong>使<strong class="ih hj"><em class="lb">q</em>(<em class="lb">z</em>|<em class="lb">x</em>)</strong>和<strong class="ih hj"> P(z|x) </strong>相同时，KL发散消失为0，ELBO和<strong class="ih hj"> log <em class="lb">可以得出结论，我们总是可以调整ELBO等于<strong class="ih hj">log<em class="lb">P</em>(<em class="lb">x</em>)</strong>，并且因为ELBO是<strong class="ih hj">log<em class="lb">P</em>(<em class="lb">x</em>)</strong>的下界，求解最大<strong class="ih hj">log<em class="lb">P</em>(<em class="lb">x</em><br/>调整<strong class="ih hj"><em class="lb">P</em>(<em class="lb">x</em>|<em class="lb">z</em>)</strong>是调整解码器，调整<strong class="ih hj"><em class="lb">q</em>(<em class="lb">z</em>|<em class="lb">x</em>)</strong>是调整编码器。每次解码器前进时，编码器被调整以与其一致，使得解码器在下一个训练时期之后只会更好。</strong></em></strong></p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ls"><img src="../Images/7e3af88dd2d75b4b751a401f74ecf673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hBGR2fy4XqA8QKCNJ0Gebw@2x.jpeg"/></div></div></figure><p id="07db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，最大化ELBO等价于最小化<strong class="ih hj">KL(<em class="lb">q</em>(<em class="lb">z</em>|<em class="lb">x</em>)|<em class="lb">P</em>(<em class="lb">z</em>)</strong>并最大化上图第二项的积分方程。</p><p id="abec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，检查上图的第二项:</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lt"><img src="../Images/84362bb668d99b6cadd665990b3e00d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aen_sr7j6pQVYzEi-ZhauA@2x.jpeg"/></div></div></figure><p id="f8c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上述期望值是指在给定<strong class="ih hj"><em class="lb">q</em>(<em class="lb">z</em>|<em class="lb">x</em>)</strong>(编码器输出)尽可能高的情况下，<strong class="ih hj"><em class="lb">P</em>(<em class="lb">x</em>|<em class="lb">|</em>)</strong>(解码器输出)。这类似于AutoEncoder的损失函数(重建误差):</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div class="er es lu"><img src="../Images/5b6413825650e24fb0b2c398fb0c9f95.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*BXPqrX4PYgN24tK9pdgPjg@2x.jpeg"/></div></figure><p id="66ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们来讨论一下<strong class="ih hj">-KL(<em class="lb">q</em>(<em class="lb">z</em>|<em class="lb">x</em>)| |<em class="lb">P</em>(<em class="lb">z</em>)</strong>:</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lv"><img src="../Images/a070b85970d7ea30446cbdf622eb1ef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AJatEAxtPgVbM07Qr5hwPw@2x.jpeg"/></div></div></figure><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lw"><img src="../Images/3515637c090adbc54823f39e3e3fb12f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rodj_p9Y0R9WVJz9OnyI5w@2x.jpeg"/></div></div></figure><p id="2adf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么，我们可以把<strong class="ih hj">-KL(<em class="lb">q</em>(<em class="lb">z</em>|<em class="lb">x</em>)| |<em class="lb">P</em>(<em class="lb">z</em>)</strong>写成:</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lx"><img src="../Images/21bfdcdb82b47a7d521a210eddc76019.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J3yx4gyUKtnB1yRxZTQEkg@2x.jpeg"/></div></div></figure><h1 id="4db0" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">结论</h1><p id="7655" class="pw-post-body-paragraph if ig hi ih b ii le ik il im lf io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated">EM和VAE都是寻找潜在变量<strong class="ih hj"> z </strong>的机器学习技术/算法。然而，尽管总体目标甚至<strong class="ih hj">目标函数</strong>都是相同的，但由于模型的复杂性，还是存在差异。EM(及其变体)有两个局限性。这些在金玛最初的<a class="ae lk" href="https://arxiv.org/abs/1312.6114" rel="noopener ugc nofollow" target="_blank"> VAE论文中提到过。</a></p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ly"><img src="../Images/89054247eb41880db1c5d277c2b46443.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zzObTQ6SBNAFIC8VIi8XXw.png"/></div></div></figure><p id="5205" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在EM算法中，我们可以计算后验概率，而在VAE解决的问题中，我们的后验是难以处理的，即无法计算。所以我们必须在VAE近似这个后验概率，这就是为什么我们在公式中使用KL散度，这个方法实际上是后验概率的变分近似。</p></div></div>    
</body>
</html>
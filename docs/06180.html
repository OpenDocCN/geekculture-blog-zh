<html>
<head>
<title>Machine Learning Series: Regression-3 (More examples)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习系列:回归-3(更多示例)</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/machine-learning-series-regression-3-more-examples-2e3f827d5e18?source=collection_archive---------19-----------------------#2021-08-09">https://medium.com/geekculture/machine-learning-series-regression-3-more-examples-2e3f827d5e18?source=collection_archive---------19-----------------------#2021-08-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/df806f3def0959cbc589c1448fd48269.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tMR_7hxhFC7mdW6PXVvIJg.jpeg"/></div></div></figure><p id="6995" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">之前，我们学习了使用糖尿病数据集进行回归的基础知识，然后我们学习了如何使用南瓜数据集可视化数据。现在，我们更深入地研究回归算法，以加深我们的知识。</p><p id="d843" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将集中讨论基本的线性回归和多项式回归。</p><h1 id="b226" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">基本线性回归</h1><p id="cf7c" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">正如我们已经讨论过的，线性回归显示了自变量和因变量之间的线性关系。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/fc64f045f67ad81b0344441e39c52dfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*iycu7qKS045TwN3YGhGMng.png"/></div></figure><p id="63bf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在执行线性回归算法后，我们在糖尿病示例中得到的结果线称为回归线。我们需要理解的另一个概念是最小二乘回归线。</p><h2 id="1ac1" class="kw jp hi bd jq kx ky kz ju la lb lc jy jb ld le kc jf lf lg kg jj lh li kk lj bi translated">最小二乘回归线</h2><p id="cab2" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">它是使数据点到回归线的垂直距离尽可能小的线。术语“最小二乘法”意味着回归线周围的所有数据点都被平方，然后相加。最终的总数应该越少越好。</p><p id="9570" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">表示回归线(也称为最佳拟合线)的方程，</p><blockquote class="lk ll lm"><p id="ef29" class="iq ir ln is b it iu iv iw ix iy iz ja lo jc jd je lp jg jh ji lq jk jl jm jn hb bi translated"><strong class="is hj"> Y = a + bX </strong></p><p id="9fd9" class="iq ir ln is b it iu iv iw ix iy iz ja lo jc jd je lp jg jh ji lq jk jl jm jn hb bi translated">其中“Y”是因变量，“X”是自变量，“b”是直线的斜率，“a”是Y截距(X=0时Y的值)</p></blockquote><h2 id="f730" class="kw jp hi bd jq kx ky kz ju la lb lc jy jb ld le kc jf lf lg kg jj lh li kk lj bi translated">相关系数</h2><p id="a157" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">我们需要了解的另一个术语是X和y之间的相关系数。它只是衡量两个变量之间的关系有多强的一个指标。在散点图中，如果数据点更接近回归线，则相关性较高，而高度不均匀分散的数据点相关性较低。</p><div class="lr ls ez fb lt lu"><a href="http://sites.utexas.edu/sos/guided/inferential/numeric/bivariate/cor/" rel="noopener  ugc nofollow" target="_blank"><div class="lv ab dw"><div class="lw ab lx cl cj ly"><h2 class="bd hj fi z dy lz ea eb ma ed ef hh bi translated">皮尔逊相关和线性回归</h2><div class="mb l"><h3 class="bd b fi z dy lz ea eb ma ed ef dx translated">相关性或简单线性回归分析可以确定两个数值变量是否显著线性相关</h3></div><div class="mc l"><p class="bd b fp z dy lz ea eb ma ed ef dx translated">sites.utexas.edu</p></div></div><div class="md l"><div class="me l mf mg mh md mi io lu"/></div></div></a></div><p id="dc37" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，好的线性回归模型将是具有高相关系数的模型。</p><p id="f8d1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将在另一个例子中看到所有这些，我们将再次使用南瓜数据集。我们将预测哪包南瓜的价格最高。</p><figure class="ks kt ku kv fd ij"><div class="bz dy l di"><div class="mj mk l"/></div></figure><h1 id="d456" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">多项式回归</h1><p id="d7c7" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">我们也已经讨论过多项式回归。在这种类型的回归中，自变量“y”和因变量“x”之间的关系被建模为“x”中的n次多项式。这种回归产生一条曲线来更好地拟合非线性数据。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/dd63073cc1e53e57a9fdf66ee6a3d321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D6dBjyosHsWN9Qn6fwop0A.png"/></div></div></figure><p id="8249" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有时关系不能仅仅用直线绘制，因此我们需要多项式回归。</p><p id="9982" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">同样，我们将通过同一个南瓜例子来学习多项式回归。</p><figure class="ks kt ku kv fd ij"><div class="bz dy l di"><div class="mj mk l"/></div></figure></div><div class="ab cl mm mn gp mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hb hc hd he hf"><p id="6d86" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如我们可以看到的，多项式回归比线性回归对我们的数据集给出了更好的准确性。我们还观察到准确性取决于相关性。相关性越高，精确度越高，反之亦然。</p></div><div class="ab cl mm mn gp mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hb hc hd he hf"><p id="02e6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">本系列的下一部分，</p><div class="lr ls ez fb lt lu"><a rel="noopener follow" target="_blank" href="/@1runx3na/machine-learning-series-regression-4-logistic-regression-e52325042d39"><div class="lv ab dw"><div class="lw ab lx cl cj ly"><h2 class="bd hj fi z dy lz ea eb ma ed ef hh bi translated">机器学习系列:回归-4(逻辑回归)</h2><div class="mb l"><h3 class="bd b fi z dy lz ea eb ma ed ef dx translated">之前，我们学习了基本的线性回归和多项式回归。现在，我们将专注于…</h3></div><div class="mc l"><p class="bd b fp z dy lz ea eb ma ed ef dx translated">medium.com</p></div></div><div class="md l"><div class="mt l mf mg mh md mi io lu"/></div></div></a></div></div><div class="ab cl mm mn gp mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hb hc hd he hf"><p id="f754" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">参考，</p><div class="lr ls ez fb lt lu"><a href="https://github.com/tionx3na/ML-For-Beginners" rel="noopener  ugc nofollow" target="_blank"><div class="lv ab dw"><div class="lw ab lx cl cj ly"><h2 class="bd hj fi z dy lz ea eb ma ed ef hh bi translated">GitHub-tion x3na/ML-初学者:12周，25节课，50次测验，经典机器学习…</h2><div class="mb l"><h3 class="bd b fi z dy lz ea eb ma ed ef dx translated">🌍环游世界，通过世界文化探索机器学习🌍Azure Cloud倡导…</h3></div><div class="mc l"><p class="bd b fp z dy lz ea eb ma ed ef dx translated">github.com</p></div></div><div class="md l"><div class="mu l mf mg mh md mi io lu"/></div></div></a></div></div></div>    
</body>
</html>
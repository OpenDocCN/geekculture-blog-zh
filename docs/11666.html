<html>
<head>
<title>An Overview Of Encoder Transformers — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">编码器变压器概述—第1部分</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/an-overview-of-encoder-transformers-part-1-78524ca5a784?source=collection_archive---------5-----------------------#2022-04-03">https://medium.com/geekculture/an-overview-of-encoder-transformers-part-1-78524ca5a784?source=collection_archive---------5-----------------------#2022-04-03</a></blockquote><div><div class="ds hc hd he hf hg"/><div class="hh hi hj hk hl"><div class=""/><figure class="ev ex im in io ip er es paragraph-image"><div role="button" tabindex="0" class="iq ir di is bf it"><div class="er es il"><img src="../Images/3f2ef1055cfd06d897154899c7b19768.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*atXk9u8jG9iDeDRJGgdDPA.jpeg"/></div></div></figure><p id="9bc6" class="pw-post-body-paragraph iw ix ho iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hh bi translated">直到最近几年，利用递归神经网络(RNNs)和深度学习方法(如长短期记忆(LSTM))的架构被广泛用于文本翻译到文本分类等任务。</p><p id="6bf9" class="pw-post-body-paragraph iw ix ho iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hh bi translated">但是2017年开发了transformer架构，性能超过了这些方法。各种自然语言的变压器架构的兴起…</p></div></div>    
</body>
</html>
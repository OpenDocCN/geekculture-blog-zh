<html>
<head>
<title>Stop using the Elbow Method</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">停止使用肘法</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/stop-using-the-elbow-method-96bcfbbbe9fd?source=collection_archive---------2-----------------------#2021-10-31">https://medium.com/geekculture/stop-using-the-elbow-method-96bcfbbbe9fd?source=collection_archive---------2-----------------------#2021-10-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="70bd" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">剪影分析:使用K-Means寻找最佳聚类数的更精确方法</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/dca08f3e38a1ff3bbe70dd36586863e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pscH2P7a4yNTJ2cvb7IMuQ.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Photo by <a class="ae jn" href="https://unsplash.com/@davidpisnoy?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">David Pisnoy</a> on <a class="ae jn" href="https://unsplash.com/s/photos/colorful?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="6635" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">当使用K-Means 执行聚类时，我们面临的一个常见挑战是找到最佳的聚类数。自然，著名和流行的<strong class="jq hj">肘方法</strong>是大多数数据科学家用来解决这个特殊问题的技术。</p><p id="607b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在这篇文章中，我们将学习一种更精确和更少主观的方法来帮助我们找到最佳的聚类数，即<strong class="jq hj">剪影得分</strong>分析。</p><h1 id="7a53" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">肘法的缺陷</h1><p id="c8df" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">在<a class="ae jn" rel="noopener" href="/@alexandre.hsd/everything-you-need-to-know-about-clustering-with-k-means-722f743ef1c4">的另一篇文章</a>中，我提供了K-Means算法的详细解释，它的微妙之处，(质心初始化，数据标准化，和聚类数)，以及一些优点和缺点。在那里，我也解释何时以及如何使用肘法。</p><p id="7df4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">K-Means算法旨在最小化每个实例与其最近质心之间的均方距离，定义为<strong class="jq hj">惯性</strong>。然而，从这个定义中，出现了一个问题。</p><p id="05f9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">只要我们不断增加簇的数量，惯性就会一直减小，因为这些点会更靠近它们的质心。因此，在为K-Means选择正确的聚类数时，我们正在寻找给我们合理惯性的最小聚类数。这正是我们试图用肘法达到的目的。假设我们有以下数据:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es li"><img src="../Images/4216010eb5d238eb6eff38b291fab271.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wb-55MBlZe7vDhmOH_CR-g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Dataset</figcaption></figure><p id="df50" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">对于人类来说，可能很清楚数据来自5个不同的集群，但在处理高维数据时，我们无法轻松地将其可视化。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es li"><img src="../Images/34330aa9f1d54e62652761885dc11e0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D2brewu4r1zN1XC_tCYNPw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Elbow Method</figcaption></figure><p id="e296" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">使用肘方法，我们可能会选择<em class="lh"> k = 4 </em>，如左图所示。</p><p id="fdf1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="lh">注意，由于两个聚类彼此相对靠近，肘形法使我们认为这些聚类只是一个，因为如果我们在两个聚类之间放置质心，从数据点到它的相对距离将会很短。</em></p><p id="a086" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因此，我们需要一种更加精确、严格和可靠的方法来为我们的聚类任务定义最佳的聚类数。剪影得分进入。</p><h1 id="b060" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">剪影分数</h1><p id="b32f" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">轮廓分数是数据集所有实例的平均轮廓系数。轮廓系数衡量一个聚类中的点与相邻聚类中的点的接近程度，其范围从-1到1。</p><p id="34c0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">数学上，轮廓系数由下式给出</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lj"><img src="../Images/81d2a89380260a8b3923e8c30ff51337.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*Fl5uf_2iDyz39rF2BYJv4A.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Silhouette coefficient formula</figcaption></figure><p id="50b0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">其中<em class="lh"> a </em>是到同一聚类中其他实例的平均距离(即，平均聚类内距离)，而<em class="lh"> b </em>是平均最近聚类距离(即，到最近聚类中实例的平均距离，不包括实例自己的聚类)。</p><p id="a3f9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">解释是当<em class="lh">b&gt;T29】a</em>时，剪影系数更接近+1，这意味着实例可能接近聚类的中心。同时，如果<em class="lh"> b = a </em>，则剪影系数为0，表示该实例在两个聚类之间的判定边界上。最后，如果<em class="lh">a&gt;b</em>，那么，该实例非常接近另一个聚类中心，这意味着它可能被分配到错误的聚类。让我们绘制K均值聚类的轮廓分数，改变聚类数<em class="lh"> k </em>。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es li"><img src="../Images/63e8fecd39043d8ac6e9cc27ac676781.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1sKqD3AiwtjTsRCw3oYRqg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Silhouette Scores for different number of clusters</figcaption></figure><p id="f7f7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">该图比弯管法所用的图信息量大得多。很明显，即使<em class="lh"> k = 4 </em>不是一个坏的选择，分割数据的最佳聚类数是<em class="lh"> k = 5 </em>。可以使用Scikit-Learn计算轮廓分数:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lk ll l"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Computing the Silhouette Score</figcaption></figure><h1 id="ce65" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">剪影图</h1><p id="d314" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">更丰富的可视化是<strong class="jq hj">剪影图</strong>。它是在绘制每个实例的轮廓系数时获得的，并按其聚类和轮廓系数排序。每个簇的厚度告诉我们簇的大小，每个簇的宽度代表簇中实例的排序剪影系数(越宽越好)。</p><p id="fa4c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">一条额外的信息是黑色虚线，它告诉我们t1756he星团的平均轮廓系数(即轮廓分数)。我们可以看到下面的剪影图从3到6不等。</p><p id="58df" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">首先，仔细看看带有<strong class="jq hj"> <em class="lh"> k = 3 </em> </strong>的图。在此设置中，群集0和1的大小是群集2的两倍。此外，几乎所有来自簇0的剪影系数都在<em class="lh">剪影分数虚线的左侧。</em>这意味着<em class="lh"> k = 3 </em>不是一个好的选择，因为它告诉我们一些集群可能被合并了，而集群0的实例离其他集群太近了。</p><p id="47e8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">另一方面，在<strong class="jq hj"> <em class="lh"> k = 4 </em> </strong>和<strong class="jq hj"> <em class="lh"> k = 5 </em> </strong>的情况下，一切似乎更好。对于<em class="lh"> k </em>的两种选择，轮廓系数延伸到虚线之外，并且更接近于1。然而，当<em class="lh"> k = 4 </em>时，集群1的大小是其他集群的两倍，在右边的图中，我们明白了为什么。两个分类合并形成分类1，这意味着这是一个不平衡的分类，但我们事先知道数据集是平衡的。因此，<em class="lh"> k = 5 </em>，确实是一个更好的选择，因为我们得到了相似大小的集群。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lm"><img src="../Images/a72e06b4be50b1d08cba486c6adbee05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B7MLVLjnYJ9M7azlbkxK1Q.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Silhouette diagram for various <em class="ln">k</em></figcaption></figure><p id="532e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">最后，当<strong class="jq hj"> <em class="lh"> k = 6 </em> </strong>时，有几个实例的剪影系数较低。那是因为其中一个集群被切断了。</p><h1 id="2378" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">决策界限</h1><p id="b04e" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">另一个有见地的可视化是K-Means的决策边界图。尽管它仅限于二维数据，但理解K-Means如何分割数据是非常有用的。在下图中，您可以看到在<em class="lh"> k = 4 </em>的情况下，该算法无法区分集中在图的最右侧的两个聚类(所有实例都被分配给聚类1)。同时，用<em class="lh"> k = 5 </em>，我们得到了想要的解。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lo"><img src="../Images/57f347604e7d082ad76ea5938d75c40d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sF-EPhWtKuolpYRdKvC78w.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">K-Means decision boundaries</figcaption></figure></div><div class="ab cl lp lq gp lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="hb hc hd he hf"><p id="4f5d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">就这样，我希望你觉得这篇文章有用，我很想在评论区知道你的想法。本帖所有剧情源代码此处<a class="ae jn" href="https://github.com/alexandrehsd/Cluster-Analysis/blob/master/Silhouette%20Analysis.ipynb" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><h1 id="0505" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">参考资料:</h1><ol class=""><li id="d6f5" class="lw lx hi jq b jr lc ju ld jx ly kb lz kf ma kj mb mc md me bi translated"><a class="ae jn" href="https://www.amazon.com.br/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646" rel="noopener ugc nofollow" target="_blank">使用Scikit-Learn、Keras和Tensorflow进行机器实践学习:构建智能系统的概念、工具和技术</a></li><li id="bd1f" class="lw lx hi jq b jr mf ju mg jx mh kb mi kf mj kj mb mc md me bi translated"><a class="ae jn" href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py" rel="noopener ugc nofollow" target="_blank">利用K-Means聚类的剪影分析选择聚类数</a></li></ol></div></div>    
</body>
</html>
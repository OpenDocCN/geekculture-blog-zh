<html>
<head>
<title>Introduction to Deep Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度强化学习简介</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/introduction-to-deep-reinforcement-learning-268ebd44b23?source=collection_archive---------16-----------------------#2021-04-08">https://medium.com/geekculture/introduction-to-deep-reinforcement-learning-268ebd44b23?source=collection_archive---------16-----------------------#2021-04-08</a></blockquote><div><div class="dt gx gy gz ha hb"/><div class="hc hd he hf hg"><div class=""/><figure class="ew ey ih ii ij ik es et paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="es et ig"><img src="../Images/011829cc23849c0685384163067e6b25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bY2yg63DaohUoNUtOXgIfA.jpeg"/></div></div><figcaption class="ir is eu es et it iu bd b be z dy">Image credit: <a class="ae iv" href="http://prod-upp-image-read.ft.com/4bb1cd86-0a48-11e7-ac5a-903b21361b43" rel="noopener ugc nofollow" target="_blank">FT</a></figcaption></figure><p id="a820" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">在这篇博客中，我将介绍强化学习(RL)领域，如何以及何时使用这种形式的机器学习。我还会谈到你应该遵循的途径，以建立在RL领域的专业知识。</p><p id="ee2c" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk"> 1。简介</strong></p><p id="b31b" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">强化学习是机器学习的一个子课题。这是帮助人工智能成为现实的发展最快的学科之一。将深度学习与强化学习相结合已经导致了许多重大进步，这些进步越来越多地让机器更接近人类的行为方式。所有的智慧生物都是从一个小知识开始的。然而，随着他们与世界互动并获得经验，他们学会适应环境，变得更擅长做事。</p><p id="3a23" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">强化学习的现代概念是两种不同的思路通过各自的发展结合起来。</p><p id="80e8" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">-首先是最优控制的概念，尤其是理查德·贝尔曼在1950年提出的<strong class="iy hk">动态规划</strong>学科。这是所有关于规划通过空间的各种选择使用贝尔曼递归方程。</p><p id="3268" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">-第二条主线是<strong class="iy hk">通过试错学习</strong>，它起源于动物训练心理学。爱德华·桑戴克是第一个明确表达“试错法”概念的人。用他的话说:</p><p id="8aed" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">在20世纪80年代，这两个领域合并在一起，形成了现代强化学习领域。在过去十年中，随着强大的深度学习方法的出现，强化学习与深度学习相结合，正在产生非常强大的算法，这些算法可以在未来的时代实现人工智能。</p><p id="9d8c" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk"> 1.1机器学习分支</strong></p><p id="0e79" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">机器学习涉及从提供给系统的数据中学习，以便系统可以执行指定的任务。系统没有被明确告知如何完成任务。更确切地说，它与数据一起呈现，系统学习基于定义的目标执行一些任务。</p><p id="7f60" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">机器学习方法通常分为三大类，如下图1所示:</p><figure class="jv jw jx jy fe ik es et paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="es et ju"><img src="../Images/925d745ff713df7ab75e6ed3ff174fa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-COT4ehAQIZH7Tx3vLwAOA.png"/></div></div><figcaption class="ir is eu es et it iu bd b be z dy"><em class="jz">Figure 1: Branches of Machine Learning</em></figcaption></figure><p id="56f8" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">机器学习的三个分支在学习系统可用的“反馈”方面有所不同。</p><p id="21e9" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk"> 1.2监督学习</strong></p><p id="d489" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">在监督学习中，向系统呈现已标记的数据，目标是概括知识，以便可以标记新的未标记数据。考虑将猫和狗的图像连同哪个图像显示猫或狗的标签一起呈现给系统。</p><p id="2f69" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">输入数据表示为一组数据:</p><figure class="jv jw jx jy fe ik es et paragraph-image"><div class="es et ka"><img src="../Images/516ab1bb9f5debaf04ec8bb0c4b9ecf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/1*7LoWeND4Lu6X4l3Hu7Pkuw.gif"/></div></figure><figure class="jv jw jx jy fe ik es et paragraph-image"><div class="es et kb"><img src="../Images/e6a2e32e583b6d09807d0dede44e3180.png" data-original-src="https://miro.medium.com/v2/resize:fit:158/1*601M5eoBJDu8vGU0Z8xGiA.gif"/></div><figcaption class="ir is eu es et it iu bd b be z dy">pixel values of individual images</figcaption></figure><figure class="jv jw jx jy fe ik es et paragraph-image"><div class="es et kb"><img src="../Images/21e4365be990d29c9f5e6e42ad015478.png" data-original-src="https://miro.medium.com/v2/resize:fit:158/1*0PdY5H4WujTYD7EZ_hfznw.gif"/></div><figcaption class="ir is eu es et it iu bd b be z dy">lables of the respective images: 0 for cat image and 1 for dog image</figcaption></figure><p id="d784" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">该系统接受该输入，并学习从图像x到标签y的映射。一旦训练完毕，向该系统呈现新的图像x’，以根据该图像是猫还是狗来获得标签y’= 0或1的预测。这是一个分类问题，系统学习将输入分类到正确的类别。我们有类似的回归设置，我们希望根据输入值x的向量来预测连续的输出y。</p><figure class="jv jw jx jy fe ik es et paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="es et kc"><img src="../Images/eeafdc81e3a3fecdcd2d6568fd5ed0bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*esCoCKXVnoxoizqZhXoW5A.png"/></div></div><figcaption class="ir is eu es et it iu bd b be z dy"><em class="jz">Figure 2: Supervised Machine Learning</em></figcaption></figure><p id="0d19" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk"> 1.3无监督学习</strong></p><p id="138f" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">第二个分支是无监督学习。无监督学习没有标签。它只有输入D= x1，x2，…，xn，没有标签。系统使用这些数据来学习数据的隐藏结构，从而可以将数据聚类/分类到一些大的类别中。学习后，当系统呈现新的数据点x '时，它可以将新的数据点匹配到一个已学习的聚类。与监督学习不同，每个类别没有明确定义的含义。一旦数据被分类，基于一个分类中最常见的属性，我们可以赋予它一些意义。无监督学习的另一个用途是利用基础输入数据来学习数据分布，以便随后可以查询系统来产生新的合成数据点。</p><figure class="jv jw jx jy fe ik es et paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="es et kc"><img src="../Images/3ed65d1cd6feb71a0eb630c6ce87641f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SI3DmPq1HmXsxRfsxKK-_Q.png"/></div></div><figcaption class="ir is eu es et it iu bd b be z dy"><em class="jz">Figure 3: Unsupervised Machine Learning</em></figcaption></figure><p id="4fc8" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk"> 1.4强化学习(RL) </strong></p><p id="2518" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">监督学习是在一个<strong class="iy hk">老师</strong> — <em class="kd">的指导下学习，标签数据</em>告诉系统从输入x到输出y的映射是什么，而RL更像是在一个<strong class="iy hk">评论家</strong>的指导下学习。<strong class="iy hk">评论家</strong>向学习者(模型)反馈他的知识好坏。学习者使用这种反馈来迭代地改进其知识。</p><p id="ff4b" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">让我们先看一个例子。假设我们正在设计一辆可以自己驾驶的自动驾驶汽车。我们有一辆汽车，我们将称之为<strong class="iy hk"> <em class="kd">代理</em> </strong>，即一个系统或算法，它倾向于自己驾驶。它正在学习一种<strong class="iy hk"> <em class="kd">行为</em> </strong>驾驶。它当前的坐标、速度、运动方向当组合在一起成为一个矢量数字时就被称为它的<strong class="iy hk"> <em class="kd">当前状态</em> </strong>。代理使用其当前状态来决定是应用制动还是踩下油门踏板。它还利用这些信息来转动方向盘，以改变汽车的运动方向。“刹车/加速”和“驾驶汽车”的组合决定被称为<strong class="iy hk"> <em class="kd">动作</em> </strong>。特定当前状态到特定动作的映射称为<strong class="iy hk"> <em class="kd">策略</em> </strong>。代理人的行为在好的时候会产生一个快乐的结果，而在坏的时候会导致一个不快乐的结果。代理使用这种结果反馈来评估其行动的有效性。作为反馈的结果被称为<strong class="iy hk"> <em class="kd">奖励</em> </strong>，即代理人在特定状态下以特定方式行动所获得的奖励。基于当前状态及其动作，汽车达到一组新的坐标、速度和方向。这是代理根据其在上一步中的行为发现自己所处的<strong class="iy hk"> <em class="kd">新状态</em> </strong>。谁提供这个结果并决定新的状态？它是汽车的周围环境，是汽车/代理商无法控制的。代理不控制的所有其他事物被称为<strong class="iy hk"> <em class="kd">环境</em> </strong>。</p><figure class="jv jw jx jy fe ik es et paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="es et kc"><img src="../Images/b5ed40ed43dff1a634d9ab3ac3dd32e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pC5x6Pf7tIMTB2ybWoSDhw.png"/></div></div><figcaption class="ir is eu es et it iu bd b be z dy"><em class="jz">Figure 4: Reinforcement Learning</em></figcaption></figure><p id="c785" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">在强化学习中，主体没有系统的先验知识。它收集反馈，并使用反馈来计划/学习行动，以最大限度地实现特定目标。由于它最初没有足够的关于环境的信息，它必须探索以收集洞察力。一旦它收集了“足够”的知识，它需要利用这些知识来开始调整它的行为，以最大化它所追求的目标。难的是没有办法知道什么时候探索“够了”。如果代理即使在获得了完美的知识后还继续探索，那么它试图收集没有剩余的新信息就是在浪费资源。另一方面，如果代理过早地认为它已经收集了足够的知识，它可能会基于不完整的信息进行优化，并且可能表现不佳。这种<strong class="iy hk"> <em class="kd">何时探索、何时利用</em> </strong>的困境，是强化学习算法的核心重复主题。</p><p id="b397" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">2014年，DeepMind成功地将深度学习技术与强化学习相结合，训练了一个玩游戏的代理，它可以根据游戏的图像玩雅达利游戏，而无需事先了解雅达利游戏的规则。</p><p id="70a9" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">为了进一步激励，让我们看一些今天RL如何被使用的例子:</p><p id="857a" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">- <strong class="iy hk">自动驾驶汽车(AV) </strong>:自动驾驶汽车有像激光雷达、雷达、摄像机等传感器。，利用这些AVs感知它们附近的环境。将原始感觉数据和对象检测相结合以获得统一的场景表示，用于规划出到达目的地的路径。像超车、变道、自动泊车这样的行为也利用了强化学习的各个部分来将智能构建到行为中。另一种选择是手工制定各种各样的规则，这永远不可能是详尽的或灵活的。</p><p id="c2d8" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">- <strong class="iy hk">机器人</strong>:使用计算机视觉和自然语言处理或使用深度学习技术的语音识别，为自主机器人增加了类似人类的感知能力。此外，深度学习和强化学习相结合的方法已经导致教会机器人像人类一样走路、挑选和操纵物体，或者通过相机观察人类行为，并学习像人类一样表演。</p><p id="3843" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">- <strong class="iy hk">推荐系统</strong>:今天我们到处都能看到推荐系统。YouTube和脸书等视频共享/托管应用程序会根据我们的观看历史向我们推荐我们想看的视频。所有这些推荐引擎越来越受到基于强化学习的系统的驱动。这些系统不断地从用户对引擎给出的建议的反应中学习。根据建议采取行动的用户在给定的上下文中将这些行动强化为好的行动。</p><p id="0dfc" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">- <strong class="iy hk">金融和交易</strong>:由于其顺序行动优化重点，其中过去的状态和行动影响未来的结果；强化学习在时间序列分析中有着重要的用途，尤其是在金融和股票交易领域。许多自动化交易策略使用强化学习方法，根据过去行动的反馈不断改进和微调交易算法。</p><p id="7bd7" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">- <strong class="iy hk">玩游戏</strong>:最后，基于RL的代理能够在许多棋盘游戏中击败人类玩家。虽然设计能玩游戏的代理看起来有些浪费，但这是有原因的。游戏提供了一个更简单的理想化世界，使设计、训练和比较方法变得更容易。在这种理想化的环境/设置下学习的方法可以随后被增强，以使代理在真实世界的情况下表现良好。游戏提供了一个控制良好的环境来更深入地研究这个领域。</p><p id="8cc2" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk"> 2。RL中算法/方法的类型</strong></p><p id="d861" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">让我们简单看一下构成RL系统的核心要素:</p><p id="bf41" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk"> <em class="kd">政策</em> </strong>是什么形成了智能体。代理开始与环境交互以感知环境的当前状态，例如，机器人从系统获得视觉和其他感官输入。像智能实体一样，机器人使用当前信息来决定下一步做什么，即执行什么动作。策略将状态映射到动作。策略可以是确定性的，即，对于给定的环境状态，代理采取固定的动作。有时策略可以是随机的，即对于给定的状态，代理可以采取多种可能的行动。</p><p id="d8b0" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk"> <em class="kd">奖励</em> </strong>是指代理人试图达到的目标/目的。假设一个机器人试图从A点到b点，它感应到当前位置并采取行动。如果这个行动使它更接近目标B，我们期望回报是正的。如果它把机器人从B点带走，这是一个不利的结果，我们期望回报是负的。奖励是一个数值，表示代理所采取行动的好坏，是代理调整其行为的主要方式，即优化其正在学习的策略。</p><p id="81f9" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk"> <em class="kd">价值函数</em> </strong>类似于长期报酬，不仅受环境影响，还受代理人遵循的政策影响。价值因为奖励而存在。代理在遵循策略时累积奖励，并使用该累积奖励来评估州中的值。然后，它改变其政策，以增加国家的价值。</p><p id="3a00" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">最后一个组件是<strong class="iy hk"> <em class="kd">环境的模型</em> </strong>。在一些寻找最佳行为的方法中，代理使用与环境的交互来形成环境的内部模型。这种内部模型有助于代理人进行计划，即考虑一个或多个动作链来评估最佳动作序列。这种方法叫做<strong class="iy hk">基于模型的学习</strong>。与此同时，还有其他完全基于试错法的方法。这种方法不形成任何环境模型。因此这些被称为<strong class="iy hk">无模型方法</strong>。大多数代理使用基于模型和无模型方法的组合来寻找最优策略。</p><p id="ff14" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">在强化学习设置和贝尔曼方程的背景下，是时候看看强化学习世界中算法的前景了。图5示出了RL空间中各种类型的学习算法的高级景观。</p><figure class="jv jw jx jy fe ik es et paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="es et ke"><img src="../Images/3c42c7496efc50f19f99a4a33dabf523.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SsscZ6A1V7NSYY7B0jnjiA.png"/></div></div><figcaption class="ir is eu es et it iu bd b be z dy"><em class="jz">Figure 5: </em><a class="ae iv" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html" rel="noopener ugc nofollow" target="_blank"><em class="jz">Mind map of algorithms</em></a><em class="jz"> in reinforcement learning. It is a high level map showing only broad categorization.</em></figcaption></figure><p id="72b8" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">算法的第一个宽泛的分类可以基于模型的知识(或缺乏知识)来完成，即基于<strong class="iy hk"> <em class="kd">模型的</em> </strong>和无<strong class="iy hk"> <em class="kd">模型的</em> </strong>算法的分类。</p><p id="1162" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk"> <em class="kd">基于模型的</em> </strong>算法可以进一步分为两类——一类是给我们的模型，例如“围棋”或“象棋”游戏，第二类是代理需要探索和学习的模型。“学习模型”下的一些流行方法是<strong class="iy hk">世界模型</strong>，想象力增强代理<strong class="iy hk"> (I2A) </strong>，基于模型的RL与模型自由微调<strong class="iy hk"> (MBMF) </strong>和基于模型的价值探索<strong class="iy hk"> (MBVE) </strong>。</p><p id="ee25" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">回到<strong class="iy hk"> <em class="kd">无模型</em> </strong>设置，有一套算法直接优化策略，使长期累积回报最大化。这一类别下一些流行的是<strong class="iy hk">政策梯度</strong>、演员评论家及其变体(<strong class="iy hk"> A2C / A3C </strong>)、邻近政策优化(<strong class="iy hk"> PPO </strong>)和信任区域政策优化(<strong class="iy hk"> TRPO </strong>)。</p><p id="7505" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">还有另一组算法使用贝尔曼动态规划来学习随机策略的质量，然后使用价值函数来迭代地改进策略。<strong class="iy hk"> Q-learning </strong>构成了无模型贝尔曼驱动状态/动作值优化的主要部分。这种方法下的流行变体是深度Q网络(<strong class="iy hk"> DQN </strong>)及其各种变体，分类51原子DQN ( <strong class="iy hk"> C51 </strong>)，分位数回归DQN ( <strong class="iy hk"> QR-DQN </strong>)和后见之明经验回放(<strong class="iy hk">她的</strong>)。</p><p id="bfda" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">最后，有一组算法位于Q学习和策略优化之间。这一类别中最受欢迎的是深度确定性政策梯度(<strong class="iy hk"> DDPG </strong>)、孪生延迟DDPG ( <strong class="iy hk"> TD3 </strong>)和软演员评论家(<strong class="iy hk"> SAC </strong>)。</p><p id="2e54" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">以上分类只是为了帮助你理解不同的方法和流行的算法。然而，清单和分类并不详尽。强化学习领域正在迅速发展，新的方法也在不断增加。请将上述思维导图仅用作高级指导。</p><p id="f19f" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk"> 3。如何入门</strong></p><p id="1483" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">像机器学习的任何其他学科一样，掌握RL涉及两个方面:理论和算法实现。我将首先浏览我写的这本书，这本书形成了掌握RL的端到端学习产品。接下来，我将介绍一些来自各个顶级大学和研究机构的领先专家的替代资源、课程和书籍。</p><p id="dd07" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk"> 3.1跟随我的书——用Python进行深度强化学习</strong></p><p id="213b" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">你可以在这里查看详情。</p><p id="4bd8" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">我写了一本书，涵盖了结合深度学习的RL的所有方面，涵盖了从非常基础的算法到高级算法的理论和实现，如图5中的思维导图所示。它采用线性方法来深入研究每个分支下的各种RL设置和流行算法。它正准备在即将到来的四月出版。</p><p id="1db1" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">虽然这本书假设没有强化学习领域的先验知识，但它期望读者熟悉机器学习特别是监督学习的基础知识。你之前用Python编码过吗？你喜欢使用像NumPy和scikit-learn这样的库吗？你听说过深度学习，并在PyTorch或TensorFlow中探索过训练简单模型的基本构建模块吗？你应该对以上所有的问题都回答是，这样才能从本书中获得最大的收获。如果没有，我建议你先更新一下这些概念，不要太深，任何介绍性的在线教程或来自出版社的关于上述主题的书就足够了。</p><p id="04f9" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">像这样的学科，数学是不可避免的。不过，我已经尽力做到最小化了。这本书引用了大量的研究论文，对所采用的方法做了简短的解释。想要更深入理解该理论的读者应该浏览这些研究论文。这本书的目的是向从业者介绍这个领域中许多最新技术背后的动机和高层次的方法。然而，这绝不意味着提供对这些技术的完整的理论理解，最好通过阅读原始论文来获得。</p><p id="564e" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">这本书分为十章。</p><p id="4891" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk">章节</strong> — <em class="kd">强化学习简介</em>，是对主题的介绍，设置背景，用真实世界的例子激励读者。它还包括python和相关库的安装，以便能够运行本书附带的代码。</p><p id="3891" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk">第2章</strong> — <em class="kd">马尔可夫决策过程</em>，详细定义了我们要在RL中解决的问题。它深入探讨了报酬的构成、价值函数、模型和政策。它介绍了各种风格的马尔可夫过程，作为动态规划的一部分，由理查德·贝尔曼建立方程。</p><p id="63f1" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk">第3章</strong> — <em class="kd">基于模型的算法</em>，重点介绍给定模型的设置，以及代理计划其行动以获得最佳结果。它介绍了OpenAI Gym环境库，该环境库在整本书中用于编码和测试算法。最后，它探讨了规划的价值和策略迭代方法。</p><p id="4c2a" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk">第四章</strong> — <em class="kd">无模型方法</em>，讲述无模型学习方法。在这种设置下，代理不了解环境/模型。它与环境相互作用，并通过试错法使用奖励来学习最佳策略。它特别关注学习的蒙特卡罗(MC)方法和时间差异(TD)方法，首先单独研究这些方法，然后在n步回报和合格跟踪的概念下将两者结合起来。</p><p id="d230" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk">第5章</strong> — <em class="kd">函数逼近和深度学习</em>，转到查看系统状态从离散(直到第4章都是这种情况)变为连续的设置。接下来，它探索了使用参数化函数来表示状态并带来可扩展性的概念-首先使用手工制作的函数逼近的传统方法，然后使用基于深度学习的模型作为非线性函数逼近器。</p><p id="4f45" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk">第6章</strong>—<em class="kd">Deep Q Learning(DQN)</em>，深入探讨了DeepMind采用的方法，该方法成功演示了深度学习与强化学习一起使用，设计了可以学习玩视频游戏(如雅达利游戏)的代理。它探索了DQN是如何工作的，以及需要做哪些调整才能让它学习。接下来是对各种DQN风格的调查，包括PyTorch和TensorFlow中的详细代码示例。</p><p id="d4dd" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk">第7章</strong> — <em class="kd">策略梯度算法</em>，切换焦点探讨在无模型设置中直接学习好策略的方法。前面章节中的方法基于首先学习价值函数，然后使用这些价值函数来优化策略。在这一章中，我们首先讨论直接政策优化方法的理论基础。在建立基础之后，我们讨论各种方法，包括一些最近非常成功的算法，以及在PyTorch和TensorFlow中的实现。</p><p id="b5ac" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk">第8章</strong> — <em class="kd">结合政策梯度和Q-Learning </em>顾名思义，处理结合基于价值的DQN和政策梯度方法的方法，以便利用两种方法的优点。它还使我们能够设计可以在连续动作空间中操作的代理。我们特别关注三种非常流行的方法——深度确定性策略梯度(DDPG)、双延迟DDPG (TD3)和软行动者批评(SAC)。像以前一样，PyTorch和TensorFlow提供了全面的实现来帮助读者掌握这个主题。</p><p id="36a5" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk">第9章</strong>—<em class="kd"/>综合规划和学习，是关于结合第3章的基于模型的方法和第4章到第8章的无模型方法。它探讨了使这种整合成为可能的一般框架。最后，它解释了蒙特卡罗树搜索(MCTS)以及如何将其用于训练AlphaGo，以击败出口人类围棋选手。</p><p id="593c" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk">第10章</strong> — <em class="kd">进一步探索和后续步骤</em>，概述了强化学习的各种其他扩展，如可扩展的基于模型的方法、模仿和逆向学习、无导数方法、迁移和多任务学习以及元学习等概念。这里的覆盖范围是在30，000英尺，让读者接触到新的和相关的概念，而不会迷失在细节中。这一章最后谈到了读者应该继续探索和学习的方式，而不仅仅是本书所涵盖的内容。</p><p id="0af5" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk"> 3.2其他课程和链接</strong></p><p id="45d7" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">最后，我列出了一些课程和链接，可以帮助你进一步探索理论和高级主题。我在下面不分先后地列举了它们:</p><p id="a043" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">1.课本— <a class="ae iv" href="http://incompleteideas.net/book/the-book.html" rel="noopener ugc nofollow" target="_blank">强化学习:理查德·萨顿和安德鲁·g·巴尔托的介绍</a>。这是任何RL从业者的必读之作。这本书分为三个部分，我强烈建议通读第一和第二部分。标有(*)的部分在初读时可以跳过。如果你点击<a class="ae iv" href="http://incompleteideas.net/book/code/code2nd.html" rel="noopener ugc nofollow" target="_blank">这个</a>，你会看到书中包含的例子和练习的python和Matlab实现的链接。</p><p id="f28a" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">2.David Silver的视频讲座。这些讲座很精彩，也很容易理解RL的介绍，基础知识和一些高级主题。我强烈建议按顺序观看这些。</p><p id="be48" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">3.加州大学伯克利分校助理教授Sergey Levine的深度RL课程。到目前为止，你已经为2020年秋季课程准备了完整的视频讲座、幻灯片和python作业。它更深入地研究了推导，并且有重要的数学意义。然而，如果你有很好的线性代数、概率论和微积分背景，通读这些材料将是非常有益的。</p><p id="0b69" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">4.阿尔贝拉大学提供的Coursera强化学习专业化课程，萨顿和巴尔托都是该大学的教授。</p><p id="0c97" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">5.<a class="ae iv" href="https://sites.google.com/view/deep-rl-bootcamp/lectures" rel="noopener ugc nofollow" target="_blank">深度RL Bootcamp — 2017 </a>。这些是2017年在加州大学伯克利分校由Pieter Abbeel教授组织的为期2天的训练营的视频记录。一组很好的录像，与谢尔盖·莱文的课程内容有相当的重叠。</p><p id="72bf" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">6.<a class="ae iv" href="https://web.stanford.edu/class/cs234/" rel="noopener ugc nofollow" target="_blank">斯坦福的强化学习课程</a>。这又是对强化学习领域的一个极好的介绍。虽然该材料可用于目前正在进行的2021年冬季课程，包括演示文稿、笔记和作业，但视频不可免费观看。你可以在这里查看2019跑步<a class="ae iv" href="https://www.youtube.com/watch?v=FgzM3zpZ55o" rel="noopener ugc nofollow" target="_blank">的视频</a>。</p><p id="0998" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">7.<a class="ae iv" href="https://www.coursera.org/learn/practical-rl" rel="noopener ugc nofollow" target="_blank">莫斯科HSE大学的实用强化学习课程</a>。这是一门节奏非常快的课程，涵盖了推进主题的基础知识。然而，有时我发现材料的覆盖范围对我的喜好来说有点太高了。尽管他们的编码练习很棒。你也可以在github<a class="ae iv" href="https://github.com/yandexdataschool/Practical_RL" rel="noopener ugc nofollow" target="_blank">查看他们最新的课程版本。</a></p><p id="5ed8" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">8.<a class="ae iv" href="https://deepmind.com/learning-resources/reinforcement-learning-lectures-series-2018" rel="noopener ugc nofollow" target="_blank">高级深度学习和强化学习</a>由伦敦大学学院(UCL)的DeepMind科学家讲授。它还涵盖了基础和高级主题。该材料与David Silver的视频讲座中涵盖的材料有很好的重叠。</p><p id="c3b9" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">9.<a class="ae iv" href="https://spinningup.openai.com/" rel="noopener ugc nofollow" target="_blank">在Deep RL </a>中旋转，这是一个由OpenAI托管的带有python库的网页，是深入RL的另一个很好的资源。在我看来，在完成基础课程后，这份材料是巩固你理解的绝佳资源。或者，如果你对深度学习和PyTorch或TensorFlow感到舒服，你可以一头扎进去。</p><p id="25be" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy hk"> 4。结论</strong></p><p id="b205" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">强化学习正取得重大进展。我在本书的最后一章中介绍了更多关于基础RL的内容。有不断发展的学科，如模仿和逆向学习、无导数方法、迁移和多任务学习以及元学习。</p><p id="bba0" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">RL在医疗保健、自动驾驶汽车、机器人、金融和电子商务以及其他各种领域的应用越来越多。</p><p id="c5b1" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">在这篇博客中，我尝试着介绍了RL领域，并分享了可以用来掌握这一领域的各种在线资源的链接。</p><p id="8b14" class="pw-post-body-paragraph iw ix hj iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">我将非常高兴得到您的反馈，以便我可以进一步改进它。</p></div></div>    
</body>
</html>
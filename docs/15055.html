<html>
<head>
<title>Long for Symbolic Processing? Meanwhile, get to know your Tokenizer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">渴望符号处理？同时，了解你的标记器</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/longing-for-symbolic-processing-meanwhile-get-to-know-the-tokenizer-3cedc885f824?source=collection_archive---------10-----------------------#2022-10-10">https://medium.com/geekculture/longing-for-symbolic-processing-meanwhile-get-to-know-the-tokenizer-3cedc885f824?source=collection_archive---------10-----------------------#2022-10-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/1a57d33a8b5220883bc09cfcbea0a93e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*UwegZHFEQdI-WIC5f8EXaw.png"/></div><figcaption class="im in et er es io ip bd b be z dx">Generated by Stable Diffusion with prompt, “Word blocks in a salad bowl”</figcaption></figure><p id="5dac" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">关于用符号处理增强机器学习的热烈<a class="ae jo" href="https://twitter.com/ylecun/status/1065327226298789889" rel="noopener ugc nofollow" target="_blank">辩论</a>正在进行。在取得突破之前，我们必须将输入文本标记化，并将其转换为数字标识符(token-id)。毕竟，今天的计算机处理的是数字(尽管数字精度决定了要做的工作量)。</p><p id="287d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，我们来看一下简单的记号赋予器，这是一个将单词拆分成子单词，然后在子单词记号到数字的查找表的帮助下，将这些子单词转换成数字标识符的过程。</p><p id="4829" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">自然语言处理(NLP)用例的原型化越来越容易，这得益于从预训练模型中进行微调的范例，以及预训练(基础)模型的广泛可用性。使用如下所示的一行程序，可以更轻松地完成标记化实例化步骤:</p><figure class="jp jq jr js fd ij"><div class="bz dy l di"><div class="jt ju l"/></div></figure><h2 id="bb66" class="jv jw hi bd jx jy jz ka kb kc kd ke kf jb kg kh ki jf kj kk kl jj km kn ko kp bi translated">记号赋予器有两种(主要)风格</h2><p id="f1ca" class="pw-post-body-paragraph iq ir hi is b it kq iv iw ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn hb bi translated"><a class="ae jo" href="https://huggingface.co/docs/transformers/model_doc/bert" rel="noopener ugc nofollow" target="_blank"> BERT </a>、<a class="ae jo" href="https://huggingface.co/docs/transformers/model_doc/distilbert" rel="noopener ugc nofollow" target="_blank"> DistilBERT </a>和<a class="ae jo" href="https://huggingface.co/docs/transformers/model_doc/electra" rel="noopener ugc nofollow" target="_blank">伊莱克特</a>使用<strong class="is hj">文字块</strong>方法来标记化，而<a class="ae jo" href="https://huggingface.co/docs/transformers/model_doc/gpt2" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>和<a class="ae jo" href="https://huggingface.co/docs/transformers/model_doc/roberta" rel="noopener ugc nofollow" target="_blank">罗伯塔</a>使用<strong class="is hj">字节对编码</strong> (BPE)方法。</p><h2 id="3cff" class="jv jw hi bd jx jy jz ka kb kc kd ke kf jb kg kh ki jf kj kk kl jj km kn ko kp bi translated">为什么要费神做记号呢？</h2><p id="f055" class="pw-post-body-paragraph iq ir hi is b it kq iv iw ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn hb bi translated">毫无疑问，逐字符处理文本输入是有好处的。缺点是计算成本高，这会导致序列非常长，我们都知道，在Transformer模型中，相对于输入长度n，成本以O(n)的二次方增加。</p><h2 id="b3cd" class="jv jw hi bd jx jy jz ka kb kc kd ke kf jb kg kh ki jf kj kk kl jj km kn ko kp bi translated">不同之处，并举例说明</h2><p id="fea2" class="pw-post-body-paragraph iq ir hi is b it kq iv iw ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn hb bi translated">这里有一段代码突出了不同之处；第一，在为单词产生的子标记中，<code class="du kv kw kx ky b">hospitalized</code>以及第二，用于“完整”标记和子标记的约定；<code class="du kv kw kx ky b">##</code>词缀和BPE的一个有趣变体。</p><figure class="jp jq jr js fd ij"><div class="bz dy l di"><div class="jt ju l"/></div></figure><p id="3221" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面是运行上面脚本的输出。</p><figure class="jp jq jr js fd ij"><div class="bz dy l di"><div class="jt ju l"/></div></figure><h2 id="1e19" class="jv jw hi bd jx jy jz ka kb kc kd ke kf jb kg kh ki jf kj kk kl jj km kn ko kp bi translated">迭代单词片段子单词创建</h2><ol class=""><li id="74dd" class="kz la hi is b it kq ix kr jb lb jf lc jj ld jn le lf lg lh bi translated">首先，初始化词汇表，使其包含训练集中出现的每个字符。</li><li id="25b6" class="kz la hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">接下来，用上面的词汇集从训练文本构建一个语言模型(步骤1)。</li><li id="041f" class="kz la hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">从当前词汇集中选择两个条目，合并它们，并将新合并的单元添加回集合中。选择合并单元，使得一旦训练数据被添加到词汇表中，它就最大化训练数据的可能性。最大化训练数据的似然性等同于找到这样的符号对，其概率除以其第一个符号和其后的第二个符号的概率，在所有符号对中是最大的。例如，如果<code class="du kv kw kx ky b">"qu"</code>(高)除以<code class="du kv kw kx ky b">"q"</code>、<code class="du kv kw kx ky b">"u"</code>的概率大于任何其他符号对，则<em class="ln"> </em> <code class="du kv kw kx ky b">"q"</code>后跟<code class="du kv kw kx ky b">"u"</code>将被合并。</li><li id="5c98" class="kz la hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">重复步骤2和3，直到词汇集达到预设大小或可能性停止增加。</li></ol><h2 id="ebff" class="jv jw hi bd jx jy jz ka kb kc kd ke kf jb kg kh ki jf kj kk kl jj km kn ko kp bi translated">字节对编码与WordPiece有何不同</h2><p id="f934" class="pw-post-body-paragraph iq ir hi is b it kq iv iw ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn hb bi translated"><a class="ae jo" href="https://www.aclweb.org/anthology/P16-1162" rel="noopener ugc nofollow" target="_blank"> BPE </a>算法更直观一些，不同之处仅在于步骤3，它只是选择新的单词单元作为当前子单词单元集合中下一个最频繁出现的词对的合并。</p><h2 id="63f9" class="jv jw hi bd jx jy jz ka kb kc kd ke kf jb kg kh ki jf kj kk kl jj km kn ko kp bi translated">主要优势</h2><p id="1897" class="pw-post-body-paragraph iq ir hi is b it kq iv iw ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn hb bi translated">假设子词合并的数量有限，则不在词汇表(OOV)中的词将被拆分成更频繁的子词。</p><h2 id="988a" class="jv jw hi bd jx jy jz ka kb kc kd ke kf jb kg kh ki jf kj kk kl jj km kn ko kp bi translated"><strong class="ak">总结</strong></h2><p id="8fdb" class="pw-post-body-paragraph iq ir hi is b it kq iv iw ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn hb bi translated">我引入了符号化器，半开玩笑地提到了符号操作，这无疑会推动人工智能的发展，但还需要很多年。</p><p id="6e78" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们有两个主要的记号赋予器品种，<strong class="is hj">文字块</strong>和<strong class="is hj"> BPE </strong>，每一个都采用了稍微不同的方法(和稍微不同的子记号表示)，但是每一个都工作得很好。</p><p id="89e8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我的行动号召是了解你的标记器。</p><h2 id="e780" class="jv jw hi bd jx jy jz ka kb kc kd ke kf jb kg kh ki jf kj kk kl jj km kn ko kp bi translated">参考</h2><p id="f437" class="pw-post-body-paragraph iq ir hi is b it kq iv iw ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn hb bi translated"><a class="ae jo" href="https://blog.octanove.org/guide-to-subword-tokenization/" rel="noopener ugc nofollow" target="_blank">神经时代子词标记化方法完全指南</a></p><p id="8b24" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jo" href="https://stackoverflow.com/questions/55382596/how-is-wordpiece-tokenization-helpful-to-effectively-deal-with-rare-words-proble/55416944#55416944" rel="noopener ugc nofollow" target="_blank">词块标记化如何帮助有效处理NLP中的生僻字问题</a></p></div></div>    
</body>
</html>
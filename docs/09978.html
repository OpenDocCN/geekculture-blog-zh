<html>
<head>
<title>Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变形金刚(电影名)</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/transformers-231c4a430746?source=collection_archive---------1-----------------------#2022-01-08">https://medium.com/geekculture/transformers-231c4a430746?source=collection_archive---------1-----------------------#2022-01-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f15c0772c2e510d2d5aef9237505d621.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CHbkzNNYom6oLjq5.jpg"/></div></div></figure><p id="df25" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Transformer模型已经成为大多数NLP任务中的首选模型。许多基于变压器的模型，如伯特、罗伯塔、GPT系列等，被认为是自然语言处理中最先进的模型。虽然NLP在所有这些模型中都势不可挡，但变形金刚在计算机视觉领域也越来越受欢迎。变压器现在用于识别和构建图像、图像编码等等。虽然变压器模型正在接管人工智能领域，但对这些模型有一个低层次的理解也很重要。这个博客旨在让读者了解Transformer和基于Transformer的模型。这包括模型组件、训练细节、度量和损失函数、性能等。</p><h1 id="6616" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">先决条件</h1><ul class=""><li id="37ff" class="km kn hi is b it ko ix kp jb kq jf kr jj ks jn kt ku kv kw bi translated">嵌入</li><li id="9b75" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">序列建模</li><li id="8c91" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">rnn和LSTMs</li></ul><h1 id="8242" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">一点背景</h1><p id="1962" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">如今，基于神经网络的模型在计算机视觉和自然语言处理(NLP)领域越来越受欢迎。但回到21世纪初，NLP并不太受欢迎，人工智能的乐趣仅发生在计算机视觉中。通过ImageNet挑战赛和AlexNet、ResNet、GANs等模型，计算机视觉在当时越来越受欢迎。成为头条新闻。计算机视觉是如此的吸引人，以至于NLP在图片中并不重要。</p><p id="73a6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，与此同时，NLP也通过像Word2Vec这样的模型取得了一些进展。Word2Vec是谷歌在2013年发明的神经网络模型。该模型可以生成与上下文无关的文本数据嵌入。Word2Vec模型是NLP领域的一个关键突破，因为这是第一个可以基于单词的语义生成嵌入的模型。</p><p id="e2af" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，这还不够。传统神经网络的一个基本缺点是它们不能记忆东西。这对于机器翻译、图像字幕等序列应用来说非常重要。然而，2014-2015年流行的递归神经网络(RNN)克服了这一缺点。同样，RNN模型也提出了许多挑战，如消失梯度、爆炸梯度、处理长期依赖性等。</p><p id="ff2d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个问题后来被另一个被称为LSTMs的模型系列解决了。LSTM模型最初是在1997年发明的，但是在RNNs发明之后才流行起来。LSTMs可以被认为是自然语言处理领域的一个重大进展。LSTMs可以处理消失/爆炸梯度问题，以更聪明的方式记忆东西。LSTMs还有其他变体，仍被许多公司广泛使用。其中包括双向LSTM、GRUs等等。如果你想了解更多关于LSTMs的内容，我推荐你去看看我之前的博文。</p><div class="lf lg ez fb lh li"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/long-short-term-memory-networks-23119598b66b"><div class="lj ab dw"><div class="lk ab ll cl cj lm"><h2 class="bd hj fi z dy ln ea eb lo ed ef hh bi translated">长短期记忆网络</h2><div class="lp l"><h3 class="bd b fi z dy ln ea eb lo ed ef dx translated">介绍</h3></div><div class="lq l"><p class="bd b fp z dy ln ea eb lo ed ef dx translated">medium.com</p></div></div><div class="lr l"><div class="ls l lt lu lv lr lw io li"/></div></div></a></div><p id="24fc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正是在2017年，NLP取得了关键突破。谷歌发布了一篇名为“注意力是你所需要的”的研究论文，其中引入了一个叫做注意力的概念。注意力帮助我们只关注需要的特征，而不是所有的特征。注意机制导致了变压器和基于变压器的模型的发展。</p><h1 id="b110" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">为什么是变形金刚</h1><p id="f8c4" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">ltm很棒。但是也有一些限制。第一个是LSTMs很难训练。这主要是因为较长的训练时间以及训练期间的高记忆要求。LSTMs主要用于克服RNNs的消失/爆炸梯度问题。但事实证明，LSTMs只能在一定程度上解决这个问题。LSTM的另一个缺点是它很容易过度拟合，并且很难包含正则化技术。所以现在我们知道，LSTMs不能把我们从所有这些问题中完全解救出来。那现在怎么办？</p><p id="08ae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">变形金刚来了。变压器的发明可以被认为是NLP的“BAM”时刻。那么变形金刚提供了什么额外的好处呢？让我们首先对变压器有一个高层次的概述，然后我们可以在接下来的会话中深入了解它的组件及其工作原理</p><h1 id="3291" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">变形金刚(电影名)</h1><p id="b156" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">在很高的层面上，变压器可以被视为一个编码器-解码器模型，其中编码器将我们的输入转换成一个矢量。这些编码输入被提供给解码器，解码器试图将向量转换成自然语言或可解释语言。现在，如果我们考虑上述过程，我们可以将其与机器翻译(例如:将法语句子转换为英语)或图像字幕(为图像生成字幕)联系起来。当我们使用转换器进行机器翻译时，我们将输入的句子(一个法语句子)交给编码器。编码器会将文本转换成矢量。现在，这个向量被传递给解码器，在那里它把它转换成一个英语句子。同样的方法也适用于图像字幕，其中输入将是图像，输出将是描述图像的句子。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/f7f8e521311c00f568d1eee9c1c20b5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zJzXcVXYVXlQW9Ge"/></div></div></figure><p id="587e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">编码器和解码器模块实际上是编码器和解码器的堆栈，如图所示。输入进入第一个编码器堆栈。它将把输入转换成另一个向量Z，然后这个向量被传递给下一个组件，依此类推。来自最终编码器层的输出将被提供给解码器堆栈。所有编码器都具有相似的结构和相似的层，只是权重不同，并且是在训练过程中学习的。类似地，解码器也具有类似的结构。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/6b9a602e61c3b11ba5bc8c20afcd67aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KeozgL_ZRa_2icDA"/></div></div></figure><p id="3a05" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些都可以。但是与之前的型号相比，《变形金刚》有什么特别之处呢？</p><ul class=""><li id="40fc" class="km kn hi is b it iu ix iy jb md jf me jj mf jn kt ku kv kw bi translated">变形金刚能比LSTM更好地处理长期依赖</li><li id="27b7" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">解决消失/爆炸渐变的问题</li><li id="8c02" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">编码是双向的</li><li id="1a43" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">支持并行化</li></ul><p id="a7fd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">随着我们的前进，每一点都将变得更加清晰。</p><h1 id="429b" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">编码器堆栈</h1><p id="08cd" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">由于所有的编码器堆栈都是相似的，为了理解相同的内容，我们将采用单个编码器模块。现在，编码器需要对我们的输入进行编码。在编码时，它需要确保解码器模块获得所有相关信息进行解码。现在它是如何工作的？让我们来看看编码器的结构。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/48a29859949e5dbeb226a88444dc5325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FbVY_fmp_f30bLN8"/></div></div></figure><p id="e047" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从上图可以看出，一个编码器单元包含两个模块——自关注模块和前馈神经网络。我假设你已经熟悉前馈神经网络，所以我不会解释。让我们把注意力集中在自我关注层。</p><h2 id="d94f" class="mh jp hi bd jq mi mj mk ju ml mm mn jy jb mo mp kc jf mq mr kg jj ms mt kk mu bi translated">自我关注层</h2><p id="438b" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">如果你记得在RNNs和LSTMs中，为了产生当前的隐藏状态，我们需要一些关于先前事件的记忆。从过去或未来的事件中获取信息是非常重要的，因为它有助于我们了解背景，这反过来会使预测更加准确。在变压器编码器中，想法是相似的，但是有一些变化。</p><p id="4ec0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在编码器中，我们将输入的句子拆分成单词。现在从图3中，我们可以看到每个单词都被转换成了一个向量。在生成这些向量的同时，如果我们也能得到上下文，那不是很好吗？类似的过程也发生在自我关注层</p><p id="0bf8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如，考虑一个句子</p><p id="ce53" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这只动物没有过马路，因为它太累了</p><p id="005b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在想象我们需要翻译这句话。为此，首先，我们需要对它进行编码。现在，在编码时，我们将这个句子中的所有单词传递给编码器(同时传递)。现在，当我们开始编码时，我们知道，为了对一些单词进行编码，还需要一些上下文。例如，“它”这个词——不管它是指动物还是街道。现在，这就是我们使用自我关注机制的地方。最后，我们将得到一个注意力矩阵，其中包含每个单词在编码特定单词时的重要性。上面例子中的注意力矩阵看起来像这样。牢房越暗，就越需要注意。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mw"><img src="../Images/d655880b3366412b80505fe16843115b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*113vObGumgih6pvR"/></div></div></figure><p id="1e63" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，一旦我们得到了注意力矩阵，下一步就是计算向量Z。现在这个Z将是每个单词的编码向量表示。这个向量是如何计算的呢？</p><h2 id="6977" class="mh jp hi bd jq mi mj mk ju ml mm mn jy jb mo mp kc jf mq mr kg jj ms mt kk mu bi translated">注意矩阵和编码向量— Z</h2><p id="b3de" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">第一步，我们需要处理我们的输入(已经作为向量嵌入)，计算注意力矩阵。我们已经知道什么是注意力矩阵，现在让我们看看它是如何计算的。</p><h2 id="51dc" class="mh jp hi bd jq mi mj mk ju ml mm mn jy jb mo mp kc jf mq mr kg jj ms mt kk mu bi translated">注意力矩阵</h2><p id="34db" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">注意力矩阵是使用两个向量计算的——查询和关键字。还有一个向量叫价值向量，用在后期。对于输入句子中的每个单词，都有一个查询、键和值向量。现在，这些向量是通过将输入单词(嵌入)乘以一个矩阵来计算的，如下所示</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mx"><img src="../Images/0863e37cca097fb9d1a4d36fbabaa436.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*djOwDZe6zYorgX4T"/></div></div></figure><p id="5692" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如上图所示，每个输入单词向量(图中的x1)乘以一些矩阵，生成键、查询和值向量。用于乘法的矩阵通过反向传播来学习。假设我们已经将输入维数(输入字数)固定为L，权重矩阵的维数为<em class="mv"> dxd </em>。当我们为所有输入单词计算这些向量时，这些向量的维数将是<em class="mv"> 1xd。</em>现在，当我们连接所有输入单词的所有这些向量时，合成向量将是<em class="mv"> Lxd。</em></p><p id="1dbf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">好了，现在我们有了这三个向量。我们现在如何计算注意力矩阵？一旦我们得到这些向量，注意力矩阵的计算如下</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es my"><img src="../Images/43b7ac523f0cc52f0a4dd47c61aa968f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/0*wne4j8gK657YHj0L"/></div></figure><p id="d5f1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后的softmax层是为了确保注意力矩阵将包含0-1范围内的值。一个值，指示某个单词在对当前输入单词进行编码时的高度重要性。注意矩阵的顺序为<em class="mv"> LxL </em></p><p id="3642" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在下一步是计算矢量<strong class="is hj"> z </strong>。这个向量是通过将价值向量乘以关注矩阵来计算的。因此，这将产生一个维度为<em class="mv"> Lxd </em>的向量，这表示L个维度为d的向量。计算z向量的整个过程如下所示</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/076f20686236547572441851df9083b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/0*RmOJ71dj81ie1J-P"/></div></figure><p id="efa5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在对特定的单词进行编码时，查询、键和值向量将帮助我们更多地关注重要的单词(基于注意力权重),并从无关的单词中获取很少或没有信息。现在，这些向量被传递到下一个编码器单元，并重复相同的过程。</p><p id="ecf5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在是时候看看一个完整的编码器单元是什么样子了</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es na"><img src="../Images/ef41033559b14ea3f3249bbe8f84c4f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/0*-5geHqvb8x_nZ3a_"/></div></figure><p id="d1eb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以在这里看到更多的特征</p><ul class=""><li id="a8da" class="km kn hi is b it iu ix iy jb md jf me jj mf jn kt ku kv kw bi translated">位置编码—这是一种表示单词在句子中位置的方法</li><li id="ad90" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">归一化图层-图层归一化将归一化每个图层输出的分布</li><li id="bfb4" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">剩余连接-剩余连接或跳过连接将防止消失/爆炸梯度问题，并防止过度拟合的机会</li></ul><h1 id="1ef1" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">解码器堆栈</h1><p id="f95b" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">既然我们已经了解了编码器架构及其工作原理，那么就很容易理解模型的解码器部分了。</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nb"><img src="../Images/d5c8e1d50ffa406de040877626a90a20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MX109JPVDOniJkbD"/></div></div></figure><p id="64ac" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从上图中，我们可以看到编码器和解码器非常相似，只是在解码器端多了一个单元。这是编码解码器的注意力。让我们从解码者的角度来理解注意力机制</p><ul class=""><li id="263b" class="km kn hi is b it iu ix iy jb md jf me jj mf jn kt ku kv kw bi translated">自我关注——这与编码器的工作原理相似。但是这里有点不同。如果你还记得，编码器中的自我关注，对于当前单词，我们必须考虑所有单词对应的键、值和查询向量。这意味着，当前单词必须考虑过去和未来的单词，这使得这个过程是双向的。但是解码器的功能是预测下一个字。因此，在计算当前单词的注意力向量时，我们只能考虑过去的单词，因为未来的单词还不为我们所知，它是可以预测的。这使得解码器关注过程是单向的</li><li id="4379" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">编码器-解码器注意力-这一层将计算编码器和解码器之间的注意力，并告诉我们每个编码器矢量分量在预测下一个单词时有多重要</li></ul><h1 id="bf6a" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">输出层</h1><p id="9ddd" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">模型的最后一部分是获得所需格式的输出。例如，如果任务是机器翻译，那么输出应该是所需语言的句子，如英语或法语。现在，我们如何获得单词或句子形式的输出？</p><p id="96b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们假设，在我们的任务中，目标语言是英语，我们的词汇表中总共有1000个单词。现在，Transformer模型中的最终输出层是一个softmax层，其大小等于词汇表的大小。输出的值将从0到1，这实际上是每个单词输出的概率。例如，如果在时间t=1，输出向量是这样的</p><figure class="ly lz ma mb fd ij er es paragraph-image"><div class="er es nc"><img src="../Images/4eb4b7469120507db0930d9027b24519.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*dwOy5EOe17xxkHat4UYU6A.png"/></div></figure><p id="dee9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">那么t=1时的输出字将是“am”。像这样，每个输出字都会被预测，直到达到<eos>条件。</eos></p><h1 id="92ab" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">培训详情</h1><p id="944b" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">变形金刚接受了海量数据的训练。训练是通过反向传播算法完成的，使用的损失函数是交叉熵损失和Kullback-Leibler散度。</p><h1 id="68c4" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">变压器变化</h1><p id="3376" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">尽管变压器可以被认为是所有NLP工程师的一个重大“发现”时刻，但它也带来了一些挑战。转换器本身的自关注机制存在与二次记忆和二次计算时间需求相关的问题。一些其他类似的架构也被引入来缓解这些问题，包括<a class="ae nd" href="https://openai.com/blog/sparse-transformer/" rel="noopener ugc nofollow" target="_blank">稀疏变压器</a>、<a class="ae nd" href="https://arxiv.org/abs/2004.05150" rel="noopener ugc nofollow" target="_blank">长成形器</a>、<a class="ae nd" href="https://arxiv.org/abs/2009.14794" rel="noopener ugc nofollow" target="_blank">表演器</a>等。</p><h1 id="81fa" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">最后的想法</h1><p id="b5da" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">我希望你喜欢这篇文章。本博客试图涵盖变压器及其架构的基本概述。不过，我也跳过了一些细节，比如<a class="ae nd" href="https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853" rel="noopener" target="_blank">多头注意力</a>、<a class="ae nd" href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/" rel="noopener ugc nofollow" target="_blank">位置编码</a>等。，我强烈建议您阅读更多关于相同的内容。</p><p id="79d5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如有任何疑问或建议，欢迎通过<a class="ae nd" href="https://www.linkedin.com/in/vinitha-v-n-5a0560179/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae nd" href="https://twitter.com/Vinitha_vn" rel="noopener ugc nofollow" target="_blank"> Twitter </a>联系我</p></div></div>    
</body>
</html>
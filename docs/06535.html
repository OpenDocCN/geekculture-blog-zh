<html>
<head>
<title>Support Vector Machine (SVM) Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机(SVM)分类</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/support-vector-machine-svm-classification-6579184d78e5?source=collection_archive---------14-----------------------#2021-08-23">https://medium.com/geekculture/support-vector-machine-svm-classification-6579184d78e5?source=collection_archive---------14-----------------------#2021-08-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9cf0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SVM适用于复杂的中小型数据集的分类。目标是找到一个<strong class="ih hj">超平面</strong>，在所有数据点都位于超平面的正确一侧的约束下，最大化两个类之间的<strong class="ih hj">裕度</strong>(超平面和最近数据点之间的距离)。</p><p id="b037" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Scikit-learn </strong>库能够对数据集进行二元和多类SVM分类，分类器有<strong class="ih hj"> LinearSVC，SVC，NUSVC </strong>和<strong class="ih hj"> SGDClassifier </strong>。Scikit-learn网站上的用户指南表明，他们的实现是基于<strong class="ih hj"> libsvm </strong>的，这是用C++编写的，尽管它有一个通过PyPI和绑定的Python接口&amp;端口存在于少数其他编程语言中，即:Java、MATLAB和r。</p><p id="f500" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我意识到scikit-learn的SVM模型需要对几个超参数进行调整，但是没有提供精确数学公式的细节。我希望这篇文章能够弥合理论和实践之间的差距。</p><ol class=""><li id="b3e8" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><strong class="ih hj">硬保证金SVM </strong></li></ol><p id="6c26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">硬边距SVM严格规定所有数据点必须在边距线之间的区域之外。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es jm"><img src="../Images/c6d79f7fd55218992d80d720f59ff33d.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*CZHhFRPcQF6dkiUf3EfjfQ.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx">The vector <strong class="bd jy">w </strong>is <strong class="bd jy">orthogonal</strong> to the hyperplane. “negative hyperplane” and “positive hyperplane” are the margin lines. Hyperplane, also named the “Decision boundary” lies right in the middle of the margin lines. The data points that lie on the margin lines are called the support vectors. <strong class="bd jy">Support vectors </strong>determine the maximum margin of the separating hyperplane. If one of those 3 support vectors is removed and SVM is retrained, the new hyperplane would be different.</figcaption></figure><p id="3ad1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">超平面</strong>由以下等式定义:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es jz"><img src="../Images/5fd0ca206038a3aff6bf355185fde649.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*y-B97yKpKP2_2S1O-oSvJQ.png"/></div></figure><p id="9e62" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">裕量</strong>由以下等式定义:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es ka"><img src="../Images/a028a0cd1d4e9c5c826d376599a5ee5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*XeJsqL-Pmy20ir8D6rgOdw.png"/></div></figure><p id="1028" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">边距也是<strong class="ih hj">比例不变的</strong>，这是我们稍后将受益的一个重要属性:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es kb"><img src="../Images/320a630c9c09a734dd3344beecd26d42.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*LDKRYX1LLg1r16b11Y4MLQ.png"/></div></figure><p id="1185" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果超平面能够很好地分离数据集中的类，那么数据就是<strong class="ih hj">线性可分的</strong>。在这种情况下，</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es kc"><img src="../Images/f033fec07d903e8cf7188ee24097e09a.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*3YfMF4E6uNwNvFKYZFSJYw.png"/></div></figure><p id="a48d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于线性可分的数据集，有无限多的潜在超平面，但我们选择最大化边缘的超平面，因为边缘小的超平面更容易过拟合。这里我们有一个约束优化问题要解决。我们的目标是<strong class="ih hj">在所有数据点都位于超平面</strong>的正确一侧的约束下最大化余量:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es kd"><img src="../Images/8a7a39ea234e9d73031ab8c126c4a4a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*l3PmJN4Q2YzxJUxKBpuGBw.png"/></div></figure><p id="64cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，这有一个<strong class="ih hj">非凸目标</strong>，具有多个局部最小值，难以求解(当我们应用梯度下降时，非凸函数可能导致算法陷入局部最小值)。由于分离超平面是尺度不变的，我们可以任意固定w和b的尺度。我们用这个属性来设置</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es ke"><img src="../Images/cf4dfa7d71051c85f83dcf042132e7fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*eE-izJaK1z-vwCQC1rSkAg.png"/></div></figure><p id="6983" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于最接近超平面的点。在这种情况下，边距变为:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es kf"><img src="../Images/6a4b61fde1c9fe7626d3a442a2c3910c.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*QIK6LLJagRkbPP1xGNa6Ew.png"/></div></figure><p id="4389" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，为了最大化边际，我们必须最小化<strong class="ih hj"> ||w|| </strong>。因此，新的目标函数和约束变为:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es kg"><img src="../Images/d1c88fd7492c29d6973bc92c0b19d8af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*rVfZ9lkBoohvUM0FmYdo_g.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx">Our initial objective was to maximize the margin, which is 1/<strong class="bd jy">||w||</strong>, but this is <strong class="bd jy">non-convex</strong>. Maximizing 1/<strong class="bd jy">||w|| </strong>is same as minimizing <strong class="bd jy">||w||²/2</strong>, but this new objective function is <strong class="bd jy">convex </strong>that is why we changed the objective function here.</figcaption></figure><p id="10bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">给定一个<strong class="ih hj">原始问题</strong>(一个约束最小化问题)，我们可以表达一个<strong class="ih hj">对偶问题</strong>(一个约束最大化问题)。对偶问题的解决方案为原始问题的解决方案提供了一个T4下界。在<strong class="ih hj"> Slater的条件下，强对偶</strong>适用于这里的优化问题。<strong class="ih hj">对偶缺口</strong>变为0，对偶问题的解与原问题的解相同。SVM的约束优化问题满足Slater条件。</p><p id="8f49" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们可以使用<strong class="ih hj">拉格朗日乘数:α重写SVM的约束优化函数。</strong>这个目标函数被称为<strong class="ih hj">拉格朗日原始问题</strong>:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kh"><img src="../Images/58eafd731f0a74d574f072af01e0feab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*emrMJBmYPBNz7llHuMh5XQ.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx">We are minimizing this w.r.t. <strong class="bd jy">w </strong>and <strong class="bd jy">b</strong>, and maximizing w.r.t. <strong class="bd jy">α</strong>.</figcaption></figure><p id="e31d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以为<strong class="ih hj"> w </strong>和<strong class="ih hj"> b: </strong>解决这个拉格朗日原始问题</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es km"><img src="../Images/86978f5d282dc49cc56453671283b663.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*b1OAV-0WLSKvBTkUz-5fZw.png"/></div></figure><p id="ea45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代入<strong class="ih hj"> w </strong>和<strong class="ih hj"> b </strong>的值并简化，我们得到对偶表示:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es kn"><img src="../Images/fda4e6ccbb6cdeb8ee3fb578bbfe8389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*hFO30z9fGhJwFigwvcOWyA.png"/></div></figure><p id="8e66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最大化关于<strong class="ih hj"> α </strong>的对偶问题相当于最小化关于<strong class="ih hj"> w </strong>和<strong class="ih hj"> b </strong>的原始问题。对偶优化问题可以通过<strong class="ih hj"> QP(二次规划)解算器</strong>来解决。</p><p id="2045" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种形式的约束优化满足<strong class="ih hj">卡鲁什-库恩-塔克(KKT)条件</strong>，在这种情况下，要求以下性质成立:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es ko"><img src="../Images/9c70fff63c3c620879c5b8b7a6550bd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*NJcsHe7SuJmBsRWMOiG_-Q.png"/></div></figure><p id="ded0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kp"><img src="../Images/43ccdb14f4d1454f6c04b57c418648ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RiH8cd1epEJZSjNJoDrg8w.png"/></div></div></figure><p id="94b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，<strong class="ih hj">在训练之后，我们只需要存储支持向量</strong>并丢弃数据点的重要部分。一旦我们通过QP解算器找到拉格朗日乘数的最优解，我们就可以找到<strong class="ih hj"> w: </strong></p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es kq"><img src="../Images/584a6e1d0459b0b79146e68d70992e57.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*EskZujnXnLiX4B2E3381hg.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx">Please note that the Lagrange multipliers, <strong class="bd jy">α</strong>, are <strong class="bd jy">non-zero only for the support vectors</strong>. Thus, in the inference time, while making predictions, we only compute the dot product of the new input vector with the support vectors.</figcaption></figure><p id="4a5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们可以通过使用xi满足的任何支持向量来找到<strong class="ih hj"> b </strong>:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kr"><img src="../Images/3d29dcb0a86f39fd3d59dced4627197d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fJKi-0DJwpMSQ_Q6jNt6Mg.png"/></div></div></figure><p id="fb43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不使用任意的支持向量来寻找<strong class="ih hj"> b </strong>，一个数值上更稳定的解决方案是对所有的支持向量进行平均:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ks"><img src="../Images/1849e95a3cc1e083d8b746d13c64b623.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Qp_4mYp3Vyuwj7ugiKbhw.png"/></div></div></figure><p id="aa81" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在推理时间里，在进行预测时，我们评估以下符号:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kt"><img src="../Images/5f1c11726f55793247d2ad59a1d9a5a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qxrLECyYPosgdlrbVg1oHQ.png"/></div></div></figure><p id="5571" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意<strong class="ih hj">拉格朗日乘数α仅对于支持向量</strong>是非零的。因此，进行预测是非常便宜的，这需要计算新输入向量与支持向量的点积。</p><p id="ed39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。软保证金SVM </strong></p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ku"><img src="../Images/1ddd2f21d208b62437bb47e7273c13e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o_2ubtLPO3qQfaPIyJMS9g.png"/></div></div></figure><p id="2d89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">硬边距SVM严格规定所有数据点必须在边距线之间的区域之外。因此，它只适用于线性可分的数据。硬利润对异常值也很敏感。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es kv"><img src="../Images/74996fd250cf8d5ce2152f09669085e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*wejm53dZX9PPKLiM70UwTA.png"/></div></figure><p id="aaf9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在一些问题中，一个具有较宽边缘的超平面(<strong class="ih hj"> B1 </strong>)会错误地分类一些数据点，而一个具有较窄边缘的超平面(<strong class="ih hj"> B2 </strong>)会更适合数据。</p><p id="3b2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在软边距SVM中，允许数据点位于边距的错误一侧，但惩罚随着与该边界的距离增加而增加。</p><p id="48c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于每个数据点，引入一个非负的<strong class="ih hj">松弛变量</strong> : <strong class="ih hj"> ξ </strong>:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kw"><img src="../Images/6cab0e4829359c6d3fbc1e97f211ba51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3rUbA_5bpC0B0eqxRfTjcw.png"/></div></div></figure><p id="0434" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的目标是最大化利润，最小化松弛变量的值。最大化余量确保良好的泛化，而最小化松弛变量确保低训练误差。软裕度SVM的约束目标函数公式化为:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kx"><img src="../Images/6207cea242367ad880b7e7c48d14624e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8nrzC__Ttavq3JoR59z-5g.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx"><strong class="bd jy">C is a hyperparameter that controls the trade-off between maximizing the margin and minimizing the training error.</strong></figcaption></figure><p id="1823" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">超参数<strong class="ih hj"> C </strong>的最佳值应通过交叉验证进行微调。</p><p id="dc41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果C非常大<strong class="ih hj"> → </strong> SVM试图得到超平面右侧的所有点。</p><p id="89c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果C非常小<strong class="ih hj"> → </strong> SVM试图错误分类许多点，以最大限度地提高利润率。</p><p id="351f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">像以前一样我们可以先写出软裕度SVM的<strong class="ih hj">拉格朗日量</strong>，</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ca"><img src="../Images/3407dff1eceafb6c7f56b46129fbf605.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4xky9J5MwbEO8fCkwwOH7Q.png"/></div></div></figure><p id="56d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将导数w.r.t. <strong class="ih hj"> w </strong>，<strong class="ih hj"> b </strong>和<strong class="ih hj"> ξ </strong>设为0后，代入拉格朗日方程并简化，我们得到下面的<strong class="ih hj">对偶问题</strong>:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ky"><img src="../Images/5fc998e261fd1d6c2d964c3ff751119a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aPxTJWsamVxiRozE4BpQng.png"/></div></div></figure><p id="2b9b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个问题的<strong class="ih hj"> KKT双互补条件</strong>是:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es kz"><img src="../Images/47bd5e1e6d4b9b3af90857a1d470aaee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*Tq0IsY_j67aOVCXCf4v7qA.png"/></div></figure><p id="102f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们可以使用QP解算器为每个数据点找到拉格朗日乘数<strong class="ih hj"> α </strong>，</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es la"><img src="../Images/8fd797b11ab9b4f1612118128f32b592.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*m-NQGtGBqGmWOQ3oqpgQpQ.png"/></div></figure><p id="3998" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后<strong class="ih hj"> w </strong>和<strong class="ih hj"> b </strong>的最佳值可以由下式得到，</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es lb"><img src="../Images/de4d850dadd3ea470de7147a34dfa3d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*C6XTL5fbpl9tjvpNLwSPQQ.png"/></div></figure><p id="359f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中<strong class="ih hj"> S </strong>表示驻留在边缘超平面上的支持向量集，</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es lc"><img src="../Images/bfe2664c3845016048415d953b4a88b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*jKYkk4J2wfXFwFIypSyyuA.png"/></div></figure><p id="ca44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于软裕度SVM的约束目标是尽可能最小化<strong class="ih hj"> ξ </strong>，因此<strong class="ih hj">不等式约束</strong>也可以写成:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ld"><img src="../Images/5c68570732261cf6684be09e8e17169a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ojDn0mhjDJ9g14Ef62t5og.png"/></div></div></figure><p id="45a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这相当于遵循封闭形式，</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es le"><img src="../Images/c691f3e553541097c58340af08d11583.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*4kv5AxTCPdEPx4fwL254rA.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx"><strong class="bd jy">Hinge Loss</strong></figcaption></figure><p id="f9ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将其代入软边际SVM的目标，我们得到软边际SVM的无约束公式<strong class="ih hj">。无约束公式具有<strong class="ih hj"> L2正则化子</strong>和<strong class="ih hj">铰链损失</strong>。</strong></p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lf"><img src="../Images/642945f87f00908fd6b1c8c92f7e3c84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fGC7IYJDYwjoG0-JCf_R1g.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx">SVM parameters <strong class="bd jy">w </strong>and <strong class="bd jy">b </strong>can be optimized just as in logistic regression.</figcaption></figure><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lg"><img src="../Images/246b1f21e2f2e9a18d0cf4c33cc159dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vU53l7u0JowIqIeioMLuVA.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx"><strong class="bd jy">Hinge Loss</strong></figcaption></figure><p id="c6d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">铰链损耗为<strong class="ih hj">凸</strong>，因此上述优化问题可以通过<strong class="ih hj">梯度下降</strong>解决。</p><p id="96da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，铰链损耗的平坦区域导致<strong class="ih hj">稀疏解</strong>。</p><p id="ccd6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。非线性SVM </strong></p><p id="836d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">非线性SVM将数据从其原始输入空间转换到高维空间，其中数据是线性可分的。然后，学习的超平面被投影回其原始输入空间，产生非线性决策边界。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lh"><img src="../Images/a5492c3eb26439ec4ec0af9285e24b50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3T9p8A-4dSAnJDnEVPLZGA.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx"><strong class="bd jy">Adding polynomial features. </strong>A low polynomial degree might not deal with complex dataset, whereas a high polynomial degree might overfit. Using a suitable function <strong class="bd jy">ϕ(x)</strong>, we can transform any data sample <strong class="bd jy">x</strong> to <strong class="bd jy">ϕ(x)</strong>.</figcaption></figure><p id="4c7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">非线性SVM的目标函数是，</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es li"><img src="../Images/87a9cea5bcf92d1e04a910a0c5cd8511.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*o3sfjkg72szhTfiCyQjbgQ.png"/></div></figure><p id="6211" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将导数w.r.t. <strong class="ih hj"> w </strong>、<strong class="ih hj"> b </strong>和<strong class="ih hj"> ξ </strong>设为0后，代入拉格朗日方程并简化，得到如下<strong class="ih hj">对偶问题</strong>:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lj"><img src="../Images/42d8454823df626ae28c0461c9d62974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RY-tdaF-5jG7A2a-NV6KMQ.png"/></div></div></figure><p id="f23e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">超平面的最优<strong class="ih hj"> w </strong>，</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es lk"><img src="../Images/1a68a9ea44d1d196af59831d5f218f06.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*xagAXDh34pxKjeVvknm-kg.png"/></div></figure><p id="394f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此超平面的方程是，</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es ll"><img src="../Images/403f72912325a84e7e9fc82ebbeed1e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*zL6bksBftDRNfMj0z8oKsA.png"/></div></figure><p id="d27a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> b </strong>可以通过以下方式进行评估:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lm"><img src="../Images/20a78cf410677f1c5934b8cfc3c03dff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3PpzPuBf3jNAWekyTtkKsQ.png"/></div></div></figure><p id="3391" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">核</strong>给出了在某个特征空间中计算内积的方法，而无需显式计算特征空间<strong class="ih hj"> ϕ(x) </strong>。这允许我们隐式地使用高维甚至无限维的特征空间。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es ln"><img src="../Images/90875ad07b05424f910af1f53709a00b.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*T48nfbtV2GgKPzj47_-Wtw.png"/></div></figure><p id="68e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一些常见的<strong class="ih hj">基函数</strong>和它们的<strong class="ih hj">核</strong>，</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es lo"><img src="../Images/e9c4b6250730a85837b27f191854aad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*_4p8RD9zumLxf0r85sqlxg.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx">Common basis functions</figcaption></figure><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es lp"><img src="../Images/aa3d345ea1f454074ab0f6576f0d63d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*BonL_sSE1nYepDz9tVBb2g.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx">Common kernels</figcaption></figure><p id="ad61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">新的内核也可以通过使用更简单的内核来构建，</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es lq"><img src="../Images/752641bd0077d14a7ac2ff08645af549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*ndmeRpB2thBfliv9zjOz7A.png"/></div></figure><p id="26a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据<strong class="ih hj"> Mercer定理</strong>，检验一个函数是否为有效核的简单方法，</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lr"><img src="../Images/00b078db085f41f5342e0a2593e85916.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mz8ac4BsExb-84SHo348dA.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx"><strong class="bd jy">Mercer’s Theorem</strong></figcaption></figure><p id="b105" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4。使用SVM进行多类分类</strong></p><p id="cfed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SVM只支持二元分类，但可以扩展到多类分类。对于多类分类，有两种不同的方法:<strong class="ih hj">一对一方法</strong>、<strong class="ih hj">一对一方法</strong>。</p><p id="48d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一对一方法训练<strong class="ih hj"> n_class x (n_class-1) / 2 </strong>个SVM型号。</p><p id="6227" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一对多方法训练<strong class="ih hj"> n_class </strong>数量的模型。</p><p id="10f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 5。获取每个类别的类别概率分数</strong></p><p id="9218" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SVM没有直接提供类别概率估计，但是<strong class="ih hj"> Plott scaling </strong>提出将逻辑回归模型拟合到经过训练的SVM的输出，以获得每个类别的概率分数</p><p id="a216" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 6。Scikit的内置SVM模型-了解</strong></p><p id="4ae1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> a)线性SVC </strong></p><p id="bdc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它不支持内核技巧，但训练起来很有效率。它与样本数量<strong class="ih hj"> n </strong>和特征数量<strong class="ih hj"> d </strong>成线性比例。时间复杂度:<strong class="ih hj"> <em class="ls"> O(nd) </em> </strong> <em class="ls">。它使用一个对所有的方法进行多类分类。</em></p><p id="21e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> b) SGD分类器</strong></p><p id="5aa4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它不支持内核技巧；训练效率高，但不如线性SVC快。但是，它对于处理不适合内存的数据集或处理在线分类任务非常有用。它应用随机梯度下降来训练线性SVM分类器。时间复杂度:<strong class="ih hj"> <em class="ls"> O(nd) </em> </strong> <em class="ls">。它使用一对一的方法进行多类分类。</em></p><p id="6fc2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> c) SVC </strong></p><p id="45ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它基于libsvm库，使用QP解算器在<strong class="ih hj"> <em class="ls"> O(n d) </em> </strong>和<strong class="ih hj"> <em class="ls"> O(n d) </em> </strong>之间缩放。对于具有数万个样本的大型数据集来说，这是不切实际的。它实现了多类分类的一对一方法。</p><p id="a5e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> d) NuSVC </strong></p><p id="2ad3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">NuSVC实施v-SVM(SVM的替代方案)。参数v设置了误差分数的上限和支持向量分数的下限。它基于libsvm库，使用QP解算器在<strong class="ih hj"> <em class="ls"> O(n d) </em> </strong>和<strong class="ih hj"> <em class="ls"> O(n d) </em> </strong>之间缩放。对于具有数万个样本的大型数据集来说，这是不切实际的。它实现了多类分类的一对一方法。</p></div></div>    
</body>
</html>
<html>
<head>
<title>Beginners guide to Principal Component Analysis- Dimensionality Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析初学者指南-降维</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/beginners-guide-to-principal-component-analysis-dimensionality-reduction-109c742d3ff9?source=collection_archive---------24-----------------------#2021-06-16">https://medium.com/geekculture/beginners-guide-to-principal-component-analysis-dimensionality-reduction-109c742d3ff9?source=collection_archive---------24-----------------------#2021-06-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/1e19f58072686c6c69a585c8bdca425d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sShgHjFK8KS5wbeW"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Photo by <a class="ae iu" href="https://unsplash.com/@mjessier?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Myriam Jessier</a> on <a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="iv"><p id="4bcb" class="iw ix hi bd iy iz ja jb jc jd je jf dx translated">对于你的机器学习问题，你是否有太多的变量，并且对如何从这些变量中获得最佳效果感到困惑？</p></blockquote><p id="e3b1" class="pw-post-body-paragraph jg jh hi ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc jf hb bi translated">主成分分析可能是你必须考虑的一个。PCA背后的基本直觉是，你会用数学从一个大的池中提取重要的变量。基本上，它将高度相关的变量组合在一起，形成一个较小的数据集。换句话说，它被称为<strong class="ji hj">“主成分”</strong>，由数据中最大方差的变量组成。</p><p id="aabf" class="pw-post-body-paragraph jg jh hi ji b jj kd jl jm jn ke jp jq jr kf jt ju jv kg jx jy jz kh kb kc jf hb bi translated">第一步是从每个样本数据(变量)中减去平均值。</p><figure class="kj kk kl km fd ij er es paragraph-image"><div class="er es ki"><img src="../Images/b67b178dca55df1a2585bc177f31c661.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*kR51MXbPlOOfUKd-nvqL-w.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Normalized Data</figcaption></figure><p id="bf8d" class="pw-post-body-paragraph jg jh hi ji b jj kd jl jm jn ke jp jq jr kf jt ju jv kg jx jy jz kh kb kc jf hb bi translated">下一步是找出上述数据的协方差矩阵。应用下面的公式</p><figure class="kj kk kl km fd ij er es paragraph-image"><div class="er es kn"><img src="../Images/74c5859521abd8255f09d7fe184d5fd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*dK1Lun-63mrD-wpwq2vajg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Covariance formula</figcaption></figure><p id="dd61" class="pw-post-body-paragraph jg jh hi ji b jj kd jl jm jn ke jp jq jr kf jt ju jv kg jx jy jz kh kb kc jf hb bi translated">接下来，我们计算协方差矩阵，最后计算特征向量和特征值。一旦我们得到了本征参数，我们就可以进行重新定向和绘制值。这可以让我们了解变量之间的关系有多密切。</p><p id="900b" class="pw-post-body-paragraph jg jh hi ji b jj kd jl jm jn ke jp jq jr kf jt ju jv kg jx jy jz kh kb kc jf hb bi translated">尽管PCA被使用并且有时被重新发明，但它本质上是许多学科中的一种统计技术，因此它的大部分发展是由数学家完成的。</p><p id="fad8" class="pw-post-body-paragraph jg jh hi ji b jj kd jl jm jn ke jp jq jr kf jt ju jv kg jx jy jz kh kb kc jf hb bi translated">这意味着“<strong class="ji hj">保留尽可能多的多样性</strong>”转化为寻找新的变量，这些变量是原始数据集中那些变量的线性函数，连续地<strong class="ji hj">最大化方差，并且彼此不相关。</strong>寻找这样的新变量，主成分(PCs)在求解特征值/特征向量问题时减少。关于主成分分析的最早文献可以追溯到Pearson [1]和Hotelling [2]，但是直到几十年后电子计算机变得广泛可用，才使得在不小的数据集上使用主成分分析成为可能。从那时起，它的使用增加了十倍，并且在各种不同的学科中开发了许多变体。关于这个主题已经写了很多书[3，4]。主成分分析的主要用途是描述性的，而不是推断性的；一个例子将说明这一点。</p><p id="a896" class="pw-post-body-paragraph jg jh hi ji b jj kd jl jm jn ke jp jq jr kf jt ju jv kg jx jy jz kh kb kc jf hb bi translated">数学太多？我现在将从我的GitHub存储库中提供一个python笔记本，它对从<a class="ae iu" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank"> sklearn </a>导入的数据集执行PCA。</p><figure class="kj kk kl km fd ij er es paragraph-image"><div class="er es ko"><img src="../Images/7e8b09e50c4701cda047cc6942371147.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ahPYhtPhSJXrn_OC.png"/></div><figcaption class="iq ir et er es is it bd b be z dx"><em class="kp">Percentage of Variance for each by Principal component</em></figcaption></figure><figure class="kj kk kl km fd ij"><div class="bz dy l di"><div class="kq kr l"/></div></figure><pre class="kj kk kl km fd ks kt ku kv aw kw bi"><span id="a44d" class="kx ky hi kt b fi kz la l lb lc">dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])</span></pre><figure class="kj kk kl km fd ij"><div class="bz dy l di"><div class="kq kr l"/></div></figure><p id="5c3e" class="pw-post-body-paragraph jg jh hi ji b jj kd jl jm jn ke jp jq jr kf jt ju jv kg jx jy jz kh kb kc jf hb bi translated">转换成标准标量并初始化我们需要的组件数量(在本例中，我将变量减少到2个)</p><figure class="kj kk kl km fd ij"><div class="bz dy l di"><div class="kq kr l"/></div></figure><figure class="kj kk kl km fd ij"><div class="bz dy l di"><div class="kq kr l"/></div></figure><figure class="kj kk kl km fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/4e8a7ed02616b801cfa72593040c0ce3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*iMdz6LtDWOXtyyor-ANK7g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Correlation between the two variables</figcaption></figure><h1 id="79d3" class="le ky hi bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated"><strong class="ak">参考文献</strong></h1><ol class=""><li id="0cec" class="mb mc hi ji b jj md jn me jr mf jv mg jz mh jf mi mj mk ml bi translated">Pearson K. 1901关于最接近空间点系统的直线和平面。菲尔。玛格。 <strong class="ji hj"> 2 </strong>，559–572。(<a class="ae iu" href="http://dx.doi.org/10.1080/14786440109462720" rel="noopener ugc nofollow" target="_blank">doi:10.1080/14786440109462720</a>)<a class="ae iu" href="https://royalsocietypublishing.org/servlet/linkout?suffix=e_1_3_6_2_2&amp;dbid=16&amp;doi=10.1098%2Frsta.2015.0202&amp;key=10.1080%2F14786440109462720" rel="noopener ugc nofollow" target="_blank">交叉引用</a>，<a class="ae iu" href="http://scholar.google.com/scholar_lookup?hl=en&amp;volume=2&amp;publication_year=1901&amp;pages=559-572&amp;journal=Phil.+Mag.&amp;author=K+Pearson&amp;title=On+lines+and+planes+of+closest+fit+to+systems+of+points+in+space" rel="noopener ugc nofollow" target="_blank">谷歌学术</a></li><li id="461e" class="mb mc hi ji b jj mn jn mo jr mp jv mq jz mr jf mi mj mk ml bi translated">Hotelling H将复杂的统计变量分析成主要成分。<em class="mm"> J. Educ。心理学。</em> <strong class="ji hj"> 24 </strong>，417–441，498–520。(<a class="ae iu" href="http://dx.doi.org/10.1037/h0071325" rel="noopener ugc nofollow" target="_blank">doi:10.1037/h 0071325</a>)<a class="ae iu" href="https://royalsocietypublishing.org/servlet/linkout?suffix=e_1_3_6_3_2&amp;dbid=16&amp;doi=10.1098%2Frsta.2015.0202&amp;key=10.1037%2Fh0071325" rel="noopener ugc nofollow" target="_blank">交叉引用</a>，<a class="ae iu" href="http://scholar.google.com/scholar_lookup?hl=en&amp;volume=24&amp;publication_year=1933&amp;pages=417-441&amp;journal=J.+Educ.+Psychol.&amp;author=H+Hotelling&amp;title=Analysis+of+a+complex+of+statistical+variables+into+principal+components" rel="noopener ugc nofollow" target="_blank">谷歌学术</a></li><li id="64cf" class="mb mc hi ji b jj mn jn mo jr mp jv mq jz mr jf mi mj mk ml bi translated">杰克逊·JE。1991年<em class="mm">主要部件用户指南</em>。纽约州纽约市:威利。<a class="ae iu" href="https://royalsocietypublishing.org/servlet/linkout?suffix=e_1_3_6_4_2&amp;dbid=16&amp;doi=10.1098%2Frsta.2015.0202&amp;key=10.1002%2F0471725331" rel="noopener ugc nofollow" target="_blank">交叉引用</a>，<a class="ae iu" href="http://scholar.google.com/scholar_lookup?hl=en&amp;publication_year=1991&amp;author=JE+Jackson&amp;title=A+user%E2%80%99s+guide+to+principal+components" rel="noopener ugc nofollow" target="_blank">谷歌学术</a></li><li id="cca5" class="mb mc hi ji b jj mn jn mo jr mp jv mq jz mr jf mi mj mk ml bi translated">乔利弗。2002年<em class="mm">主成分分析</em>，第二版。纽约州纽约市:斯普林格出版社。<a class="ae iu" href="http://scholar.google.com/scholar_lookup?hl=en&amp;publication_year=2002&amp;author=IT+Jolliffe&amp;title=Principal+component+analysis" rel="noopener ugc nofollow" target="_blank">谷歌学术</a></li></ol></div></div>    
</body>
</html>
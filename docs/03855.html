<html>
<head>
<title>Principal Component Analysis: Losing out minimum information while performing dimension reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析:在执行降维时丢失最小信息</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/principal-component-analysis-losing-out-minimum-information-while-performing-dimension-reduction-85862b116d72?source=collection_archive---------29-----------------------#2021-06-16">https://medium.com/geekculture/principal-component-analysis-losing-out-minimum-information-while-performing-dimension-reduction-85862b116d72?source=collection_archive---------29-----------------------#2021-06-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/9ec9363613273537610e20c8fadc07ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BVQG0uYHaIWPCFH-"/></div></div></figure><div class=""/><p id="d289" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这个数据量不断增长的世界中，我们一直在寻找最佳的方法来存储可能存在于多维空间中的巨大数据，并对其进行压缩或缩减，以适应可用的存储空间。然而，我们希望在尝试减少或压缩原始数据时不要丢失太多信息。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es jo"><img src="../Images/f2927d27b7bc8ad826b8d2ad39ea68d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*4e52gpSQxNZhqpgG_k2J0w.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx">Data points distributed in a 2D plane</figcaption></figure><p id="2804" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设我们有以下分布在2D平面上的数据点。但是，我们的系统没有足够的空间来存储它，为了便于存储，我们希望将它简化为一维数据。</p><p id="3bda" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我们有以下选择:我们可以只保留数据点的x值，或者只保留y值。无论哪种方式，我们都会丢失一些关于数据的信息。有没有什么方法可以将数据点投影到新的行上，这样我们就不会丢失太多信息。我们要做的是找到数据点之间的最佳分离方向，因为它们是在原始维度上分离的，即信息损失最小的方向。试着想象一个场景，一群朋友正在拍集体照。摄影师尝试了几个角度，并在那个角度投射相机，在那里他可以清晰地捕捉到所有的脸，并且它们之间的重叠最小。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es jx"><img src="../Images/96e406d138797350483837084d806aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uqbr4tnCpa8ABEohBLDI2w.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx">Data points overlap when projected solely on X axis or Y axis, separation between points is maximized when projected on the new line</figcaption></figure><p id="cbc9" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这种将数据从多维空间投影到较低维度，而不丢失太多信息，最大化数据点之间的变化或分离的想法是通过称为<strong class="is hu">主成分分析(PCA) </strong>的过程实现的。在机器学习算法中，它是处理具有大量维度的庞大数据集时最重要的降维方法之一。</p><p id="ad0d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">如何找到最大变化的方向？</strong></p><figure class="jp jq jr js fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es jy"><img src="../Images/cee1b7223db5fd9698708ec6d64f1fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JGkRrwnceoRJ6Ri4fcE1Gw.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx">Original data points are shown in blue, projected data points shown in yellow</figcaption></figure><p id="7e05" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们的原始数据集分布在一个X-Y平面上，数据点是x1，x2等等，如蓝色圆圈所示。现在，我们希望将数据点投影到一个新行上。如果u是这条新直线的方向，那么从几何学上讲，新数据或<strong class="is hu">新嵌入的数据点</strong>由下式给出:</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es jz"><img src="../Images/dac49f991dfbba4f36e9507681ecf22a.png" data-original-src="https://miro.medium.com/v2/resize:fit:206/format:webp/1*P1m7TDzUcqpE8MmodHMuzQ.png"/></div></figure><p id="1ed6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">数据应该是<strong class="is hu">均值居中(标准化)</strong>，即数据的均值= 0。这是PCA的第一步。如果我们回忆一下方差公式，就可以理解为什么数据需要以平均值为中心。方差由下式给出</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es ka"><img src="../Images/c42458bd4d9b3c4fccdc4e48017e3080.png" data-original-src="https://miro.medium.com/v2/resize:fit:266/format:webp/1*EkFFGwfhxWoDlG8mncY0Zw.png"/></div></figure><p id="587f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果平均值为0，那么方差项减少到</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es kb"><img src="../Images/a1771407c03a3ff5462d353985ae5057.png" data-original-src="https://miro.medium.com/v2/resize:fit:186/format:webp/1*rjA-xncpF2Y7GRo8hJeNag.png"/></div></figure><p id="1ba4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设均值= 0，新嵌入数据的方差由下式给出</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es jz"><img src="../Images/e647af45715de786792fb33b907b5679.png" data-original-src="https://miro.medium.com/v2/resize:fit:206/format:webp/1*wXCFAl_ZuCA4Xly7GGxVMQ.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx">Variance of embedded data</figcaption></figure><p id="ac5a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们的主要目标是最大化这个方差，以便我们在这个新投影的数据点中丢失最少的信息。</p><p id="2f46" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">找出新嵌入数据的差异:</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es kc"><img src="../Images/a678957c14019238abc9748e654138b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*GD3LbDgq95S4y-oLrGcOqw.png"/></div></figure><p id="b282" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> S是样本数据的协方差矩阵</strong>。我们必须找到使上述表达式最大化的u，使得u是一个单位向量，即</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es kd"><img src="../Images/2931bc48cb9b59d64c9402d4163c061d.png" data-original-src="https://miro.medium.com/v2/resize:fit:134/format:webp/1*2ChA3Lf1urGKXx0p3gVOYA.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx">u needs to be a unit vector</figcaption></figure><p id="48f8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用拉格朗日方法求解上述表达式的argmax，我们得到如下结果:</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es ke"><img src="../Images/ade9d223abdf2db2e311167910fb515b.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*UYMo1HhWgaK9yNZwQFpeaA.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx">Using Lagrange method to solve for maximum value</figcaption></figure><p id="339a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">s是D*D矩阵，或数据的协方差矩阵，u是D*1向量。</p><p id="3255" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以我们得到的解决方案，就是协方差矩阵s的<strong class="is hu">本征分解</strong></p><p id="7f3b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">简单回顾一下:<strong class="is hu">特征向量</strong>是一个向量，当对其应用线性变换时，它会随标量而变化。向量缩放的因子称为<strong class="is hu">特征值</strong>。这里，u是本征向量，λ是本征值。</p><p id="7d74" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将投影数据的方向由本征向量u给出。现在，为了最大化嵌入数据的方差，这是PCA的主要目标，我们需要找到具有最大本征值或λ的本征向量。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es kf"><img src="../Images/c7b8d6a226e0b39e3ac777c84a9252db.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*4dEYvq0mi3B9M7ki6vo-AQ.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx">Maximizing the variance of embedded data, boils down to finding the largest eigen value obtained by the eigen decomposition of the covariance matrix of the data.</figcaption></figure><p id="c946" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">现在，选择哪个特征值？</strong></p><p id="5c40" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最佳嵌入是具有<strong class="is hu">最大特征值</strong>的协方差矩阵的特征向量。如果我们希望在一个一维上投影，选择顶部特征值，对于k维，选择具有顶部k个特征值的那些特征向量。</p><p id="03ca" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第一主成分:具有最大特征值的特征向量</p><p id="d6b7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第二主分量:具有下一个最大特征值的特征向量</p><p id="6def" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi">……</p><p id="7e67" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">分数或加载分数被定义为原始数据(以平均值为中心)乘以特征向量的权重或因子。我们可以这样形象化:设X1，X2，X3为输入数据的三维或特征，乘以特征向量[2 1 0]。使用矩阵向量乘法规则，我们可以看到，那些权重较小或为0的特征，在乘法之后，将具有较小的重要性或者将减少到0。因此，PCA有助于保存更重要的特征，并排除不太重要的特征。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es kg"><img src="../Images/b7845fe1b2791f18a21ed3c081c1b378.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*B19R-RS0qBsEaHk8jPJMuQ.png"/></div></figure><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es kh"><img src="../Images/23428a3eaca15636578ed14d86dcd24d.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*amDYwwyQEIBG2RVzhsZAqQ.png"/></div></figure><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es ki"><img src="../Images/6691d7ccd8adcaee95ad9ceb6efabe7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/format:webp/1*WQHTy4P5AS_kFyEDZVMERg.png"/></div></figure><p id="c125" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这就是二维数据在乘以一个本征向量后，现在被简化为一维数据的方式。因此，1和-3被称为第一主成分的加载分数。</p><p id="f0e8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">了解碎石地块</strong>:</p><p id="3376" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">碎石图是一个线形图，它告诉我们应该用多少个主成分来描述数据的最大变化。它描绘了当使用主成分时发生了多少损失，并且在该图中，主成分按照特征值排序。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es kj"><img src="../Images/048013d20215f2516db0da5d4a6d777b.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/0*gtNYFEcrURylY729.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx">Scree Plot</figcaption></figure><p id="57a5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面的scree图表明，当我们使用一个主成分时，产生的损失最高，随着我们继续增加主成分的数量，产生的损失不断减少，当我们使用全部12个主成分时，基本上是全部数据，我们没有产生信息损失。该图显示了元件数量为4时的陡峭波谷，之后保持相当恒定，这意味着如果我们使用具有前4个本征值的本征向量，我们将导致最小的损耗，此后，即使我们增加元件数量，损耗也将保持不变。因此，不是使用上述数据的所有12个维度，而是仅使用4个主成分，将捕获最大变化，如原始数据所述。</p><p id="ab1f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">r studio中的PCA模拟，使用白葡萄酒数据:</strong></p><p id="f674" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">白葡萄酒数据总共由12个变量组成，其中前11个是自变量，最后一列是第12个，“质量”是因变量，它取决于11个自变量。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es kk"><img src="../Images/1a5360b72fcd77a96b95cf29d4d83ec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*s1ClX9LZwlXNO0Wpi3QxDg.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx">PCA simulation in R</figcaption></figure><p id="cb66" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">主成分分析的总结产生以下输出，其描述了主成分描述了多少方差。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kl"><img src="../Images/5198f679de2b7115b5fcc21ca6e74811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iteurxcOAVZvN4oQ31cd8A.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx">First PC is built with the eigen vector having the largest eigen value: describes the highest variance, followed by second PC and so on, all 11 PCs cumulatively describe 100% or the total variance.</figcaption></figure><p id="f702" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">PCA的局限性:</strong></p><ol class=""><li id="0050" class="km kn ht is b it iu ix iy jb ko jf kp jj kq jn kr ks kt ku bi translated">在进行PCA之前，必须对数据进行标准化。</li><li id="d91e" class="km kn ht is b it kv ix kw jb kx jf ky jj kz jn kr ks kt ku bi translated">独立变量很难用主成分分析来解释。由于PCA涉及协方差矩阵S，如果数据之间没有协方差，例如，如果数据分布在均匀的圆上，那么PCA可能没有帮助。</li></ol></div></div>    
</body>
</html>
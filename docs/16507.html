<html>
<head>
<title>Optical Character Recognition with Hugging Face Spaces</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有拥抱面部空间的光学字符识别</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/optical-character-recognition-with-hugging-face-spaces-467714dfade2?source=collection_archive---------23-----------------------#2022-12-30">https://medium.com/geekculture/optical-character-recognition-with-hugging-face-spaces-467714dfade2?source=collection_archive---------23-----------------------#2022-12-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/60b5f6400e6cbf96c0051698e2018d70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mQYUJB1hVcYxcJfx.jpg"/></div></div></figure><h1 id="fecc" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">什么是拥抱脸枢纽？</h1><p id="2a3f" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">HuggingFace Hub是一个平台，允许开发人员存储和共享代码，以及在机器学习项目上进行合作。它托管基于Git的存储库，这是一种版本控制的存储，开发人员可以在其中保存他们所有的项目文件。开发人员可以在Hub上上传和访问自然语言处理、计算机视觉和音频任务的前沿模型。它还为各种领域和模态提供了各种数据集。最后，开发人员可以研究直接在浏览器中展示ML模型的交互式应用程序。<br/>要了解更多关于抱抱脸中枢的信息，查看<a class="ae km" href="https://huggingface.co/docs" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><h1 id="28ee" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">什么是拥抱脸空间？</h1><p id="d4e6" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">Spaces是一个中心平台，允许开发者快速创建和展示ML演示应用。它与两个Python软件开发工具包(SDK)兼容，即<a class="ae km" href="https://gradio.app/" rel="noopener ugc nofollow" target="_blank"> Gradio </a>和<a class="ae km" href="https://streamlit.io/" rel="noopener ugc nofollow" target="_blank"> Streamlit </a>，这两个工具使得在短时间内创建应用程序变得简单。此外，用户能够创建静态空间，即空间中托管的HTML、CSS和JavaScript网页。如果您想了解更多关于共享空间以及如何创建自己的共享空间的信息，请访问<a class="ae km" href="https://huggingface.co/docs/hub/spaces" rel="noopener ugc nofollow" target="_blank">共享空间文档</a>。您还可以升级您的空间，以便在GPU或其他加速硬件上运行。</p><p id="5e0a" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">让我们快速了解一下光学字符识别(OCR)。</p><h1 id="ce24" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">光学字符识别</h1><p id="8e94" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">光学字符识别(OCR)是一种深度学习方法，用于从扫描的文档和照片等图像中识别文本。它使用卷积神经网络分析图像并从中提取文本。之后，提取的文本被输入到一个OCR引擎，该引擎已经被训练来识别单词和字符。然后，OCR引擎的输出用于生成原始图像的文本版本。为了自动化数据输入和文档管理过程，通常使用OCR从图像中提取文本。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ks"><img src="../Images/5d656f7046d8c08bdefaeff9d9088813.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bBHDxzCZBS1X_qK_.png"/></div></div></figure><p id="98c9" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">有许多用于OCR的库和技术。在这里，我们将为文本识别实现3种OCR技术PaddleOCR、KerasOCR和EasyOCR。</p><p id="f73f" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">在本教程中，我们将了解如何在拥抱面部空间上托管OCR应用程序。为此，首先你需要在拥抱面部空间创建一个存储库，如下步骤所示。</p><h1 id="62b7" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">在拥抱脸上创建存储库的步骤(🤗)空间:</h1><p id="6cba" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated"><strong class="jq hj">第一步:</strong>在上创建帐户🤗枢纽，打造<a class="ae km" href="https://huggingface.co/spaces" rel="noopener ugc nofollow" target="_blank">新空间</a>。转到文件和版本。您将看到为项目生成的README.md文件。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kx"><img src="../Images/6fbdb6cdf865feda520e15d132664f15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ujQOkmDVOWjzhfKdce0OfA.png"/></div></div></figure><p id="1af8" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><strong class="jq hj">第二步:</strong>目前，我们已经在README.md文件中设置了如下元数据，如图所示。您可以根据需要替换元数据值并保存它们。有关元数据配置参考的更多信息，请访问🤗<a class="ae km" href="https://huggingface.co/docs/hub/spaces-config-reference" rel="noopener ugc nofollow" target="_blank">空间配置</a>。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ky"><img src="../Images/92da14a949a26694bd89a6e29819a987.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/0*2xtFb1Wr9qmeo8z_.png"/></div></figure><p id="b8d0" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><strong class="jq hj">步骤3: </strong>现在您可以创建新文件或者从您的本地系统上传项目文件，如下所示。您需要在requirement.txt文件中添加所有需要的库，🤗服务器将自动下载所有的库。另一种上传整个项目的方法是使用<a class="ae km" href="https://huggingface.co/docs/huggingface_hub/how-to-upstream" rel="noopener ugc nofollow" target="_blank"> huggingface_hub </a>，为此请确保您登录到🤗从你的系统里。然后你可以按照<a class="ae km" href="https://huggingface.co/docs/huggingface_hub/how-to-upstream" rel="noopener ugc nofollow" target="_blank"> huggingface_hub </a>的步骤上传你的本地文件夹到🤗空间。</p><p id="c865" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><strong class="jq hj">第四步:</strong>现在让我们从代码开始，我们将在app.py文件中编写我们的代码。</p><h1 id="97a7" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">让我们开始代码实现</h1><p id="8bc0" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">1.导入所有库</p><pre class="kt ku kv kw fd kz la lb bn lc ld bi"><span id="a271" class="le ir hi la b be lf lg l lh li">import os<br/>import cv2<br/>import json<br/>import easyocr<br/>import datasets<br/>import socket<br/>import requests<br/>import keras_ocr<br/><br/>import numpy as np<br/>import gradio as gr<br/>import pandas as pd<br/>import tensorflow as tf<br/>import re as r<br/><br/>from PIL import Image<br/>from datasets import Image<br/>from datetime import datetime<br/>from paddleocr import PaddleOCR<br/>from urllib.request import urlopen<br/>from huggingface_hub import Repository, upload_file</span></pre><p id="6e10" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">2.我们为这三种方法分别编写了OCR生成函数。</p><p id="fa48" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">Paddle OCR的代码:</p><pre class="kt ku kv kw fd kz la lb bn lc ld bi"><span id="f40f" class="le ir hi la b be lf lg l lh li">"""<br/>Paddle OCR<br/>"""<br/>def ocr_with_paddle(img):<br/>    finaltext = ''<br/>    ocr = PaddleOCR(lang='en', use_angle_cls=True)<br/>    # img_path = 'exp.jpeg'<br/>    result = ocr.ocr(img)<br/>    <br/>    for i in range(len(result[0])):<br/>        text = result[0][i][1][0]<br/>        finaltext += ' '+ text<br/>    return finaltext</span></pre><p id="ee3e" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">Keras OCR的代码:</p><pre class="kt ku kv kw fd kz la lb bn lc ld bi"><span id="5901" class="le ir hi la b be lf lg l lh li">"""<br/>Keras OCR<br/>"""<br/>def ocr_with_keras(img):<br/>    output_text = ''<br/>    pipeline=keras_ocr.pipeline.Pipeline()<br/>    images=[keras_ocr.tools.read(img)]<br/>    predictions=pipeline.recognize(images)<br/>    first=predictions[0]<br/>    for text,box in first:<br/>        output_text += ' '+ text<br/>    return output_text</span></pre><p id="936e" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">轻松OCR的代码:</p><pre class="kt ku kv kw fd kz la lb bn lc ld bi"><span id="052f" class="le ir hi la b be lf lg l lh li">"""<br/>easy OCR<br/>"""<br/># gray scale image<br/>def get_grayscale(image):<br/>    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)<br/><br/># Thresholding or Binarization<br/>def thresholding(src):<br/>    return cv2.threshold(src,127,255, cv2.THRESH_TOZERO)[1]<br/>    <br/>def ocr_with_easy(img):<br/>    gray_scale_image=get_grayscale(img)<br/>    thresholding(gray_scale_image)<br/>    cv2.imwrite('image.png',gray_scale_image)<br/>    reader = easyocr.Reader(['th','en'])<br/>    bounds = reader.readtext('image.png',paragraph="False",detail = 0)<br/>    bounds = ''.join(bounds)<br/>    return bounds</span></pre><p id="1089" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">3.为所有OCR方法创建了一个公共函数，该函数将输入作为图像，并从输入图像返回生成的文本。</p><pre class="kt ku kv kw fd kz la lb bn lc ld bi"><span id="ba0a" class="le ir hi la b be lf lg l lh li">"""<br/>Generate OCR<br/>"""<br/>def generate_ocr(Method,input_image):<br/>    text_output = ''<br/>    if (input_image).any():<br/>        print("Method___________________",Method)<br/>        if Method == 'EasyOCR':<br/>            text_output = ocr_with_easy(input_image)<br/>        if Method == 'KerasOCR':<br/>            text_output = ocr_with_keras(input_image)<br/>        if Method == 'PaddleOCR':<br/>            text_output = ocr_with_paddle(input_image)<br/>        <br/>        flag(Method,input_image,text_output,ip_address,location)<br/>        return text_output<br/>    else:<br/>        raise gr.Error("Please upload an image!!!!")</span></pre><p id="c1b4" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">4.完成所有这些功能后，让我们使用gradio应用程序将我们的代码与用户界面集成起来。</p><h1 id="5308" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">格拉迪欧</h1><p id="c181" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">Gradio对于开发人员来说是一个有用的工具，因为它允许他们快速而轻松地为他们的机器学习模型构建交互式用户界面。这对于向其他人展示模型的功能，或者收集用户对模型性能的反馈特别有用。此外，由于Gradio使用Jupyter笔记本，开发人员可以轻松地与其他人分享他们的工作，这使其成为一个很好的协作工具。如果你想了解更多关于Gradio应用的信息，请点击<a class="ae km" href="https://gradio.app/getting_started/" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/54f941b40339a983b267e9f54ebc6430.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YROLVtNrQG5hJM--.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx">This is the UI for our demo using Gradio app</figcaption></figure><p id="756f" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">基本上，我们可以用两种方式启动Gradio演示，使用<code class="du ln lo lp la b">gr.blocks</code>和<code class="du ln lo lp la b">gr.interface.</code></p><p id="3f41" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">Gradio中有三个主要参数:<br/> 1。功能:处理用户界面主要功能的进程<br/> 2。输入:输入部件<br/> 3的类型。输出:输出组件的类型</p><p id="9dad" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">代码的最后一部分涉及启动界面。它由各种组件组成，如功能、输入、输出、标题、描述等等。这个<a class="ae km" href="https://www.gradio.app/docs/" rel="noopener ugc nofollow" target="_blank">链接</a>包含了所有的接口组件。</p><pre class="kt ku kv kw fd kz la lb bn lc ld bi"><span id="d437" class="le ir hi la b be lf lg l lh li">image = gr.Image(shape=(300, 300))<br/>method = gr.Radio(["PaddleOCR","EasyOCR", "KerasOCR"],value="PaddleOCR",elem_id="radio_div")<br/>output = gr.Textbox(label="Output",elem_id="opbox")<br/><br/>demo = gr.Interface(<br/>    generate_ocr,<br/>    [method,image],<br/>    output,<br/>    title="Optical Character Recognition",<br/>    css=".gradio-container {background-color: #C0E1F2} #radio_div {background-color: #ADA5EC; font-size: 40px;} #btn {background-color: #94D68B; font-size: 20px;} #opbox {background-color: #ADA5EC;}",<br/>    article="""&lt;p style='text-align: center;'&gt;Feel free to give us your &lt;a href="https://www.pragnakalp.com/contact/" target="_blank"&gt;feedback&lt;/a&gt; and contact us at <br/>                    &lt;a href="letstalk@pragnakalp.com" target="_blank"&gt;letstalk@pragnakalp.com&lt;/a&gt; And don't forget to check out more interesting <br/>                    &lt;a href="https://www.pragnakalp.com/services/natural-language-processing-services/" target="_blank"&gt;NLP services&lt;/a&gt; we are offering.&lt;/p&gt;<br/>                    &lt;p style='text-align: center;'&gt;Developed by :&lt;a href="https://www.pragnakalp.com" target="_blank"&gt; Pragnakalp Techlabs&lt;/a&gt;&lt;/p&gt;"""<br/>    <br/>)<br/>demo.launch()</span></pre><h1 id="9317" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">在Hugging Face Hub数据集上保存数据和日志</h1><p id="8c99" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">创建应用程序后。如果您想记录用户输入和结果，那么您可以遵循以下步骤。这里，我们使用了拥抱脸数据集来存储日志。</p><p id="802f" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><strong class="jq hj">步骤1: </strong>要保存/存储日志或数据，请在🤗<a class="ae km" href="https://huggingface.co/datasets" rel="noopener ugc nofollow" target="_blank">数据集</a>。详细信息可以参考<a class="ae km" href="https://huggingface.co/docs/datasets/index" rel="noopener ugc nofollow" target="_blank">数据集文档</a>。</p><p id="55bc" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><strong class="jq hj">步骤2: </strong>要与数据集建立连接，请遵循下面的代码片段。</p><pre class="kt ku kv kw fd kz la lb bn lc ld bi"><span id="5ae7" class="le ir hi la b be lf lg l lh li">HF_TOKEN = os.environ.get("HF_TOKEN")<br/>DATASET_NAME = "OCR-img-to-text"<br/><br/>DATASET_REPO_URL = f"https://huggingface.co/datasets/pragnakalp/{DATASET_NAME}"<br/>HF_TOKEN = os.environ.get("HF_TOKEN")<br/>DATASET_REPO_ID = "pragnakalp/OCR-img-to-text"<br/>print("is none?", HF_TOKEN is None)<br/>REPOSITORY_DIR = "data"<br/>LOCAL_DIR = 'data_local'<br/>os.makedirs(LOCAL_DIR,exist_ok=True)<br/><br/>repo = Repository(<br/>    local_dir="ocr_data", clone_from=DATASET_REPO_URL, use_auth_token=HF_TOKEN<br/>)<br/>repo.git_pull()</span></pre><p id="7320" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">在这里，HF_TOKEN被称为用户访问令牌的<a class="ae km" href="https://huggingface.co/docs/hub/security-tokens" rel="noopener ugc nofollow" target="_blank">🤗，以下哪种是对应用程序或笔记本电脑进行身份验证的最常用方法🤗服务。注意:保存令牌时，将角色保持在“写入”模式。生成访问令牌后，将其复制并保存到您的空间的设置→存储库机密，保留名称为“HF_TOKEN”。</a></p><p id="000b" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">DATASET_REPO_ID将是您的数据集路径。<br/> REPOSITORY_DIR将是您保存数据的文件夹名称。</p><p id="f0ab" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><strong class="jq hj">第三步:</strong>写一个保存数据的函数。</p><pre class="kt ku kv kw fd kz la lb bn lc ld bi"><span id="3bc7" class="le ir hi la b be lf lg l lh li">"""<br/>Save generated details<br/>"""<br/>def dump_json(thing,file):<br/>    with open(file,'w+',encoding="utf8") as f:<br/>        json.dump(thing,f)<br/><br/>def flag(Method,input_image,text_output,ip_address,location):<br/>    try:<br/>        print("saving data------------------------")<br/>        adversarial_number = 0<br/>        adversarial_number = 0 if None else adversarial_number<br/>        metadata_name = datetime.now().strftime('%Y-%m-%d %H-%M-%S')<br/>        SAVE_FILE_DIR = os.path.join(LOCAL_DIR,metadata_name)<br/>        os.makedirs(SAVE_FILE_DIR,exist_ok=True)<br/>        image_output_filename = os.path.join(SAVE_FILE_DIR,'image.png')<br/>        try:<br/>            Image.fromarray(input_image).save(image_output_filename)<br/>            <br/>        except Exception:<br/>            raise Exception(f"Had issues saving PIL image to file")<br/>        # Write metadata.json to file<br/>        json_file_path = os.path.join(SAVE_FILE_DIR,'metadata.jsonl')<br/>        metadata= {'id':metadata_name,'method':Method,<br/>                   'File_name':'image.png','generated_text':text_output,<br/>                    'ip_address': ip_address,'loc': location}        <br/>        dump_json(metadata,json_file_path)  <br/>            <br/>        # Simply upload the image file and metadata using the hub's<br/>          upload_file<br/>        # Upload the image<br/>        repo_image_path = os.path.join(REPOSITORY_DIR,os.path.join<br/>                          (metadata_name,'image.png'))<br/>        <br/>        _ = upload_file(path_or_fileobj = image_output_filename,<br/>                    path_in_repo =repo_image_path,<br/>                    repo_id=DATASET_REPO_ID,<br/>                    repo_type='dataset',<br/>                    token=HF_TOKEN<br/>                ) <br/>        # Upload the metadata<br/>        repo_json_path = os.path.join(REPOSITORY_DIR,os.path.join<br/>                        (metadata_name,'metadata.jsonl'))<br/>        _ = upload_file(path_or_fileobj = json_file_path,<br/>                    path_in_repo =repo_json_path,<br/>                    repo_id= DATASET_REPO_ID,<br/>                    repo_type='dataset',<br/>                    token=HF_TOKEN<br/>                )        <br/>        adversarial_number+=1<br/>        repo.git_pull()    <br/>        return "*****Logs save successfully!!!!"<br/>    except Exception as e:<br/>        return "Error whils saving logs --&gt;"+ str(e)</span></pre><p id="3453" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">您可以在下图中看到日志数据集预览。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/720e2b93e280ab25c7be3dacca34c8f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*z_uvAyJKg9VOjp0-.png"/></div></div></figure></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><p id="5ea0" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><em class="lx">最初发表于2022年12月30日</em> <a class="ae km" href="https://www.pragnakalp.com/optical-character-recognition-with-hugging-face-spaces/" rel="noopener ugc nofollow" target="_blank">带有拥抱人脸空间的光学字符识别</a> <em class="lx">。</em></p></div></div>    
</body>
</html>
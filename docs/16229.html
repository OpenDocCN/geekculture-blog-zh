<html>
<head>
<title>How to deal with vanishing and exploding gradients</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何处理消失和爆炸渐变</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/how-to-deal-with-vanishing-and-exploding-gradients-in-neural-networks-24eb00c80e84?source=collection_archive---------2-----------------------#2022-12-17">https://medium.com/geekculture/how-to-deal-with-vanishing-and-exploding-gradients-in-neural-networks-24eb00c80e84?source=collection_archive---------2-----------------------#2022-12-17</a></blockquote><div><div class="ds hc hd he hf hg"/><div class="hh hi hj hk hl"><h2 id="ab75" class="hm hn ho bd b fp hp hq hr hs ht hu dx hv translated" aria-label="kicker paragraph">梯度下降优化算法挑战</h2><div class=""/><blockquote class="iu iv iw"><p id="7455" class="ix iy iz ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hh bi translated">消失和爆炸梯度是在训练基于深度神经网络的模型时可能发生的已知问题。它们会带来不稳定性，并导致具有许多层的模型无法在给定的数据集上学习。本文提供了识别和处理消失和爆炸渐变的详细概述。</p></blockquote><figure class="jx jy jz ka fd kb er es paragraph-image"><div role="button" tabindex="0" class="kc kd di ke bf kf"><div class="er es jw"><img src="../Images/7cd9445034ac325dfbef792527b804e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mi4IFHW_M7VYognZ"/></div></div><figcaption class="ki kj et er es kk kl bd b be z dx">Photo by <a class="ae km" href="https://unsplash.com/@julianhochgesang?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Julian Hochgesang</a> on <a class="ae km" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div>    
</body>
</html>
<html>
<head>
<title>How to Collapse Any Complex Data in Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何折叠Apache Spark中的任何复杂数据</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/working-with-complex-data-in-apache-spark-made-simple-d791841646a?source=collection_archive---------10-----------------------#2021-08-31">https://medium.com/geekculture/working-with-complex-data-in-apache-spark-made-simple-d791841646a?source=collection_archive---------10-----------------------#2021-08-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="07a3" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">一个Python函数，可以将JSON这样的复杂数据折叠成单独的列</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/37e071f180977ffde79d2d4084ce1cc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6Kd1wAHSpiz26CMM"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Photo by <a class="ae jt" href="https://unsplash.com/@goumbik?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Lukas Blazek</a> on <a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="7f80" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">最近更新—2022年3月15日—更新了示例代码，以支持名称中带有空格的属性。</p><p id="c20d" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">Apache Spark本身支持复杂的数据类型，在某些情况下，如JSON，如果有合适的数据源连接器，它可以很好地表示数据。顶级键值对出现在它们自己的列中，而更复杂的分层数据则使用转换为复杂数据类型的列来持久化。在select子句中使用点符号，可以选择复杂对象中的单个数据点。例如:</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="1b51" class="ky ig hi ku b fi kz la l lb lc">from pyspark.sql.functions import col</span><span id="2301" class="ky ig hi ku b fi ld la l lb lc">jsonStrings = ['{"car":{"color":"red", "model":"jaguar"},"name":"Jo","address":{"city":"Houston",' + \<br/>      '"state":"Texas","zip":{"first":1234,"second":4321}}}']<br/>otherPeopleRDD = spark.sparkContext.parallelize(jsonStrings)<br/>source_json_df = spark.read.json(otherPeopleRDD)</span><span id="8f9c" class="ky ig hi ku b fi ld la l lb lc">source_json_df.select(col("car.color"), col("car.model")).show()</span></pre><p id="6419" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">这将返回以下数据帧:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es le"><img src="../Images/3cfb418c16e43146805d00064a5e00a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*eZ_4rHnP1wCqb0OMcwG6NQ.png"/></div></figure><p id="456e" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">这种机制很简单，而且很有效。但是，如果数据很复杂，有多个级别，跨越大量的属性和/或列，每个属性和/或列都与不同的模式对应，并且数据的消费者无法处理(例如，像大多数BI工具一样，它们喜欢从Oracle、MySQL等关系数据库生成报告)，那么问题就会接踵而至。手动写出Select语句的方法也是劳动密集型的，并且难以维护(从编码的角度来看)。</p><p id="f965" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">为了简化复杂数据的处理，本文将提供一个函数，用于将多级复杂层次列转换为非层次列。本质上，一个没有复杂数据类型列的数据帧。所有嵌套属性都被分配了以其原始位置命名的自己的列。例如:</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="e0f5" class="ky ig hi ku b fi kz la l lb lc">car.color</span><span id="332a" class="ky ig hi ku b fi ld la l lb lc">becomes</span><span id="a13c" class="ky ig hi ku b fi ld la l lb lc">car_color</span></pre><h1 id="c175" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">入门，方法</h1><p id="c8a0" class="pw-post-body-paragraph ju jv hi jw b jx lf jz ka kb lg kd ke kf lh kh ki kj li kl km kn lj kp kq kr hb bi translated">假设我们需要转换下面的JSON，它已经使用spark.read.json加载到Spark中:</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="f7bc" class="ky ig hi ku b fi kz la l lb lc">{<br/>  "car":{<br/>    "color":"red", <br/>    "model":"jaguar"<br/>  },<br/>  "name":"Jo",<br/>  "address":{<br/>    "city":"Houston",<br/>    "state":"Texas",<br/>    "zip":{<br/>      "first":1234,<br/>      "second":4321<br/>    }<br/>  }<br/>}</span></pre><p id="52be" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">第一项任务是创建一个函数，该函数可以解析绑定到Dataframe的模式。该模式通过在数据帧本身上找到的同名属性来访问。</p><p id="7167" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">接下来，我们将遍历模式，创建所有可用属性的列表，记录它们的整个祖先路径。我们的目标是以数组的形式创建元数据，其中每个元素都是每个值的完整祖先分支。由于复杂数据是分层的，所以需要一个递归函数来遍历所有的树分支。最后，我们将处理元数据以创建列对象的集合，使用点标记约定来选择每个属性，然后使用alias属性来分配一个惟一的名称。如上所述，我们将使用每个属性的路径作为别名。</p><h1 id="f040" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">解析模式</h1><p id="f47d" class="pw-post-body-paragraph ju jv hi jw b jx lf jz ka kb lg kd ke kf lh kh ki kj li kl km kn lj kp kq kr hb bi translated">Apache Spark模式是StructType和StructField对象的组合，StructType表示每个分支的顶级对象，包括根。StructType拥有通过Fields属性访问的StructFields集合。每个StructField对象都用三个属性进行实例化，即名称、数据类型及其为空性。例如，如果我们对上面创建的数据帧运行以下代码:</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="8d86" class="ky ig hi ku b fi kz la l lb lc">schema = source_json_df.schema<br/>print(schema)</span></pre><p id="4523" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">输出将是:</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="805c" class="ky ig hi ku b fi kz la l lb lc">StructType([<br/>  StructField("car", StructType(<br/>    StructField("color", StringType(), True),<br/>    StructField("model", StringType(), True)<br/>  ), True),<br/>  StructField("name", StringType(), True),<br/>  StructField("address", StructType(<br/>    StructField("city", StringType(), True),<br/>    StructField("state", StringType(), True),<br/>    StructField("zip", StructType(<br/>      StructField("first", IntegerType(), True),<br/>      StructField("second", IntegerType(), True)<br/>    ), True)<br/>  ), True)<br/>])</span></pre><p id="c840" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">Apache Spark支持许多不同的数据类型，例如字符串和整数，以及StructType本身。当需要新的分支时，StructField的数据类型设置为StructType，如上面的示例所示。</p><p id="d4ee" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">递归函数是一种调用自身的函数，它非常适合遍历像我们的模式这样的树结构。例如:</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="6361" class="ky ig hi ku b fi kz la l lb lc">from pyspark.sql.types import StructType</span><span id="12e2" class="ky ig hi ku b fi ld la l lb lc">def get_all_columns_from_schema(schema, depth=None):<br/>  if depth is None: depth = 0<br/>  for field in schema.fields:<br/>    field_name = ""<br/>    for i in range(depth):<br/>      field_name += "--"<br/>      <br/>    field_name += field.name<br/>    print(field_name)<br/>    if isinstance(field.dataType, StructType):    <br/>      get_all_columns_from_schema(field.dataType, depth+1)   <br/>      <br/>#<br/>get_all_columns_from_schema(source_json_df.schema)</span></pre><p id="020b" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">假设我们使用上面声明的source_json_df数据帧，如果我们对它执行这段代码，我们将看到下面的输出:</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="6717" class="ky ig hi ku b fi kz la l lb lc">address<br/>--city<br/>--state<br/>--zip<br/>----first<br/>----second<br/>car<br/>--color<br/>--model<br/>name</span></pre><p id="911a" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">递归解决了一个问题，但Python引发了另一个问题。不幸的是，Python不支持通过引用传递函数属性。当你把一个变量传递给一个函数时，Python会复制一个变量，不会保留对原始变量的引用。每次我们迭代get_all_columns_from_schema时，Python都会复制两个参数schema和depth，这样当我们将depth增加1时，depth的原始副本保持不变，只有函数的下一次迭代接收到的实例会被更新。</p><p id="6277" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">这是一个问题，因为get_all_columns_from_schema的每次迭代都不知道在它之前发生了什么。虽然我们可以为每个分支创建一个数组，但是我们无法将所有分支数组整理成一个列表，并返回给正在执行的代码。将创建select语句的代码。为了克服这个Python限制，我们需要将解析函数包装在另一个函数中(或者一个类，但函数更简单)，并使用父函数的上下文作为元数据数组的容器。</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="7e41" class="ky ig hi ku b fi kz la l lb lc">from pyspark.sql.types import StructType</span><span id="b44c" class="ky ig hi ku b fi ld la l lb lc">def get_all_columns_from_schema(source_schema):<br/>  branches = []<br/>  def inner_get(schema, ancestor=None):<br/>    if ancestor is None: ancestor = []<br/>    for field in schema.fields:<br/>      branch_path = ancestor+[field.name]     <br/>      if isinstance(field.dataType, StructType):    <br/>        inner_get(field.dataType, branch_path) <br/>      else:<br/>        branches.append(branch_path)<br/>        <br/>  inner_get(source_schema)<br/>        <br/>  return branches</span></pre><p id="7f8a" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">主外部函数get_all_columns_from_schema现在将dataframe模式作为单个输入参数。该函数从声明一个列表开始，这个列表对于内部函数来说实际上是全局的。这是以数组形式保存所有分支的列表。递归函数是在get_all_columns_from_schema中声明的，它与上面演示的函数相同，只是做了一些小的调整(用一个列表更改深度计数器，以持久化单个分支的所有祖先节点)。此外，对print的调用被替换为对外部函数拥有的分支列表的追加。</p><p id="444a" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">如果我们对数据帧的模式运行此代码，get_all_columns_from_schema将返回以下列表:</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="2819" class="ky ig hi ku b fi kz la l lb lc">[<br/>  ['address', 'city'], <br/>  ['address', 'state'], <br/>  ['address', 'zip', 'first'], <br/>  ['address', 'zip', 'second'], <br/>  ['car', 'color'], <br/>  ['car', 'model'], <br/>  ['name']<br/>]</span></pre><h1 id="f55d" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">折叠结构化列</h1><p id="b659" class="pw-post-body-paragraph ju jv hi jw b jx lf jz ka kb lg kd ke kf lh kh ki kj li kl km kn lj kp kq kr hb bi translated">现在我们已经有了所有分支的元数据，最后一步是创建一个数组来保存我们想要选择的dataframe列，遍历元数据列表，并在为每个分支值分配唯一的别名之前，创建使用每个分支值的点标记地址初始化的列对象。</p><p id="80ba" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">为了克服属性名中可能有一个空格的可能性，我们用反引号将列括起来。</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="e56c" class="ky ig hi ku b fi kz la l lb lc">from pyspark.sql.functions import col<br/><br/>  _columns_to_select = []<br/>  _all_columns = get_all_columns_from_schema(source_json_df.schema)<br/>  <br/>  for column_collection in _all_columns:<br/>    _select_column_collection = ['`%s`' % list_item for \ <br/>         list_item in column_collection]    <br/>  <br/>    if len(column_collection) &gt; 1:<br/>      _columns_to_select.append(col('.'.join( \ <br/>         _select_column_collection)).alias('_'.join( \ <br/>         column_collection)))<br/>    else:<br/>      _columns_to_select.append(col(_select_column_collection[0]))</span></pre><p id="bb4b" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">我们首先用get_all_columns_from_schema的输出初始化一个数组，然后进行循环迭代，并测试每个元素的项目长度。如果长度大于1，那么它是一个分支，否则它是一个常规的非层次列的名称。在Pythons字符串上使用join方法，我们将数组成员连接在一起，首先创建点符号字符串来选择分支值，然后声明新列的别名。</p><p id="1e63" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">新数组_columns_to_select现在包含了完全折叠所有复杂数据类型所需的一切，为每个单独的值创建一列。正在执行:</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="f8cc" class="ky ig hi ku b fi kz la l lb lc">collapsed_df = source_json_df.select(_columns_to_select)<br/>collapsed_df.show()</span></pre><p id="ef62" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">输出以下数据帧:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lk"><img src="../Images/faddb592d38a8b24b368dc7531288e79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hrgrj8Ae3aovONMb0ylSsg.png"/></div></div></figure><p id="ab78" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">Github的源代码可以在https://github.com/jamesshocking/collapse-spark-dataframe的<a class="ae jt" href="https://github.com/jamesshocking/collapse-spark-dataframe" rel="noopener ugc nofollow" target="_blank">找到</a></p></div></div>    
</body>
</html>
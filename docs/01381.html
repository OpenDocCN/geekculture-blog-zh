<html>
<head>
<title>The Math Behind: Everything About Principle Component Analysis (PCA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">背后的数学:关于主成分分析(PCA)的一切</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/the-math-behind-everything-about-principle-component-analysis-pca-d6f0baff5681?source=collection_archive---------6-----------------------#2021-04-08">https://medium.com/geekculture/the-math-behind-everything-about-principle-component-analysis-pca-d6f0baff5681?source=collection_archive---------6-----------------------#2021-04-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="9d4c" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">PCA降低了许多空间中的数据点的维数。一些现成的代码和库允许编码人员轻松地创建PCA，但是，您知道什么是PCA以及它在数学上是如何工作的吗？</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/60d07f161da4a7a48a1cbc0d6f6324b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*nQovAnvFEyVwfF3YTguqdw.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx">Photo by <a class="ae jj" href="https://unsplash.com/@evieshaffer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Evie S.</a> on <a class="ae jj" href="https://unsplash.com/s/photos/dimension?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="7a4c" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><a class="ae jj" href="https://www.dezyre.com/data-science-in-python-tutorial/principal-component-analysis-tutorial#:~:text=The%20main%20idea%20of%20principal,up%20to%20the%20maximum%20extent." rel="noopener ugc nofollow" target="_blank">主成分分析(PCA) </a>的主要思想是降低由许多变量组成的数据集的维数，这些变量或多或少相互关联，同时最大限度地保留数据集中存在的变化</p><p id="c486" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">直观地说，主成分分析允许我们用较低维度的图片来查看数据点的位置。当用户从信息最丰富的角度看时，它可以被称为这些数据点的投影或“阴影”，如上面菱形图片的阴影。</p><p id="aab9" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">如果有人问你什么是PCA，这些是你应该记住的关于PCA的第一件事。</p><ul class=""><li id="0d5e" class="kg kh hi jm b jn jo jq jr jt ki jx kj kb kk kf kl km kn ko bi translated">减少尺寸</li><li id="1525" class="kg kh hi jm b jn kp jq kq jt kr jx ks kb kt kf kl km kn ko bi translated">试图保持数据集中的最大方差</li><li id="7130" class="kg kh hi jm b jn kp jq kq jt kr jx ks kb kt kf kl km kn ko bi translated">有时，新的维度无法解释。</li><li id="aff7" class="kg kh hi jm b jn kp jq kq jt kr jx ks kb kt kf kl km kn ko bi translated">允许创建易于可视化的</li></ul><h1 id="3cef" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">PCA在数学上是如何工作的？</h1><p id="4e0d" class="pw-post-body-paragraph jk jl hi jm b jn lm ij jp jq ln im js jt lo jv jw jx lp jz ka kb lq kd ke kf hb bi translated">在网上，PCA的数学部分很少一步一步讲解。关于它的资源很少，而且大部分都不够详细。<strong class="jm hj"> <em class="lr">我相信数学直觉能让我们把长期的拼图块</em> </strong> <em class="lr">。</em>因此，现在您可以通过一个示例了解PCA的分步计算。<br/>假设我们有一个关于5名学生不同体育课考试成绩的数据集，我们将尝试降低该数据集的维度。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ls"><img src="../Images/88691bb322bae86e2da42691d98ca52f.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*Iw7ymECVO4MnNnC9aFRKKw.png"/></div></figure><h1 id="12e0" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">1-获取数据集的特征，不考虑标注。</h1><p id="8d33" class="pw-post-body-paragraph jk jl hi jm b jn lm ij jp jq ln im js jt lo jv jw jx lp jz ka kb lq kd ke kf hb bi translated">主成分分析在特征维度上工作，在这种情况下是运动课程。因此，我们不应该考虑标签列和唯一的学生id。该过程仅与数据集的特征相关。在实施所有步骤后，每个学生在PCA选定维度的新分数将是一个结果。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lt"><img src="../Images/63e26d9eb25dfda23ef61ab831b7a232.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*Xtp3pAFk7nl_73mnU_Q2kg.png"/></div></figure><h1 id="d399" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">2-数据集的规范化</h1><p id="0f1e" class="pw-post-body-paragraph jk jl hi jm b jn lm ij jp jq ln im js jt lo jv jw jx lp jz ka kb lq kd ke kf hb bi translated"><strong class="jm hj"> <em class="lr"> PCA是一种将分量方向改变到最大方差方向的做法。</em> </strong>基本上，原始数据会被分配到方差最大化的不同方向。如果不归一化数据集，PCA技术可能会偏向特定的特征。</p><p id="e008" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">标准化分两步完成:</p><ul class=""><li id="60a7" class="kg kh hi jm b jn jo jq jr jt ki jx kj kb kk kf kl km kn ko bi translated">首先，从每个数据点减去变量各自的平均值。这会产生一个平均值为零的数据集。</li><li id="0633" class="kg kh hi jm b jn kp jq kq jt kr jx ks kb kt kf kl km kn ko bi translated">其次，减去平均值后，将每个数据点除以标准差。(我不会在本文中实现它)</li></ul><p id="33dc" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">您可以看到每列的平均值如下:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lu"><img src="../Images/2acafc9d441eb6a448790d165cc17ded.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*oYziU0T9Kabpg6-pP9lZ0g.png"/></div></figure><h1 id="68f2" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated"><strong class="ak"> 3-计算协方差矩阵</strong></h1><blockquote class="lv lw lx"><p id="6ba6" class="jk jl lr jm b jn jo ij jp jq jr im js ly ju jv jw lz jy jz ka ma kc kd ke kf hb bi translated">在我们开始之前，我们将快速地看一下协方差和方差之间的区别。<a class="ae jj" href="https://datascienceplus.com/understanding-the-covariance-matrix/" rel="noopener ugc nofollow" target="_blank">方差</a>衡量单个随机变量的变化(比如人口中一个人的身高)，而协方差是衡量两个随机变量一起变化的程度(比如人口中一个人的身高和体重)。</p></blockquote><p id="3123" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">数据点的归一化与协方差矩阵相关联。因为在协方差公式中，需要从每个属性的每个数据点中减去每个列(特征)的平均值。如果应用归一化，平均值将为零。如果不应用归一化的第一步，它将通过协方差矩阵公式来完成。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mb"><img src="../Images/b2fa01e99b5d76223708dd357f330206.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*Yn5KnQU_DELAMWmSeeYH3Q.png"/></div></figure><p id="4bcb" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">协方差计算应该发生在两个特征的每个组合中。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mc"><img src="../Images/4cbc0158b9ae52417f12cadea1064c76.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*vQl9jvcT7zHmid4jaNvlfw.png"/></div></figure><p id="b5c3" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">如果我们说，我们有d维(d个特征)，协方差矩阵将在dxd维。</p><ul class=""><li id="3011" class="kg kh hi jm b jn jo jq jr jt ki jx kj kb kk kf kl km kn ko bi translated">这是一个正方形矩阵</li><li id="21a3" class="kg kh hi jm b jn kp jq kq jt kr jx ks kb kt kf kl km kn ko bi translated">它是对角对称矩阵，因此cov(X，Y) = cov(Y，X)</li></ul><p id="33c8" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">cov(X，X)表示X特征的方差。数字越大表示方差越大。因此，矩阵的对角线部分表示每个特征的方差。矩阵中每隔一个元素表示两个不同变量的协方差。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es md"><img src="../Images/9ad11736009ea83bfc6addec41525b81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*9PTUlZ9GbAiGN1oDISez8g.png"/></div></figure><p id="ac2f" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">在我们的例子中，你可以看到A矩阵和每一列的平均值。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es me"><img src="../Images/f9462f313ca853f27d5ee5b2a4fe2a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*01rlB-HmBGb-kSNGywG__g.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mf"><img src="../Images/c6527e6e8c0f8afd9403c85731ad5d0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:310/format:webp/1*FNVGTSREEhdh1kw60u8c2Q.png"/></div></figure><p id="da10" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">因此，实现为协方差矩阵的一个值:</p><p id="6795" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">Cov(X，Y)=[(6–5)*(4–5.6)+(8–5)*(3–5.6)+(5–5)*(8–5.6)+(4–5)*(5–5.6)+(2–5)*(8–5.6)]/(5–1)</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mg"><img src="../Images/c4a481ce183e63d95d498c2a27cd7f28.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*55j5n0I1WGDpwHoiLOINyA.png"/></div></figure><h2 id="9c4f" class="mh kv hi bd kw mi mj mk la ml mm mn le jt mo mp lg jx mq mr li kb ms mt lk mu bi translated">这个协方差矩阵告诉我们什么？</h2><ul class=""><li id="3156" class="kg kh hi jm b jn lm jq ln jt mv jx mw kb mx kf kl km kn ko bi translated">可变性最高的是跑步(+7.7)，可变性最低的是网球(+5.0)。</li><li id="0360" class="kg kh hi jm b jn kp jq kq jt kr jx ks kb kt kf kl km kn ko bi translated">排球和跑步之间的协方差是正的(如0.35)，这意味着得分趋向于以正的方式协变。随着网球分数的上升，跑步分数也会上升，反之亦然。</li><li id="bfe1" class="kg kh hi jm b jn kp jq kq jt kr jx ks kb kt kf kl km kn ko bi translated">网球和排球之间的协方差是负的(as -4)，这意味着分数倾向于以负的方式协变。随着网球分数的上升，排球分数会下降，反之亦然。</li><li id="375f" class="kg kh hi jm b jn kp jq kq jt kr jx ks kb kt kf kl km kn ko bi translated">如果任何两个特征的协方差为零，这意味着相关的两个课程的分数之间没有可预测的关系。</li></ul><h1 id="1426" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">4-计算特征值</h1><p id="15a4" class="pw-post-body-paragraph jk jl hi jm b jn lm ij jp jq ln im js jt lo jv jw jx lp jz ka kb lq kd ke kf hb bi translated">特征值是用于变换(拉伸)特征向量的标量。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es my"><img src="../Images/e368b33cdd4164960693fdcb76b0bcf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:212/format:webp/1*kSiWO5IaB5qP15U1lmE9uw.png"/></div></figure><p id="6fd1" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">A是方阵(它是协方差矩阵A)，v是特征向量，λ是作为特征值(与矩阵的特征向量相关联)的缩放器。<br/>当你把问题的右边移到左边时，你需要解下面的方程来得到λ特征值。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lt"><img src="../Images/fbea8b0eaac2c9b4ccc560bee14bf73a.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*mDJEbfRl-rjHO-0TPKWa6g.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mz"><img src="../Images/5db6bab1f7ae62ef46d4a01cc860d6f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*RvXs182Rr8DXLqn_mU1Gsw.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es na"><img src="../Images/46f1a0aa4c175b66e2b46d31e3c49800.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*ZPbYwqiI26gDFLdS79hSrg.png"/></div></figure><p id="76b2" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">现在，我们需要通过求解上面的方程来获得λ值。如何解决3X3维的行列式在下图中，但更多信息请阅读<a class="ae jj" href="https://www.mathsisfun.com/algebra/matrix-determinant.html" rel="noopener ugc nofollow" target="_blank">此链接</a>。</p></div><div class="ab cl nb nc gp nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="hb hc hd he hf"><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es ni"><img src="../Images/76093ad11e8c28e0f2d4e234b68ac483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gn0bSpWt2JkKrjgb7wN8LQ.png"/></div></div></figure></div><div class="ab cl nb nc gp nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="hb hc hd he hf"><p id="5db5" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">因此，最终我们的方程几乎等于下面的方程:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es nn"><img src="../Images/bf01081b818a6d9f0e9316d71cabe8e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*miy5Xn5ds_U7wbFS7XtIyQ.png"/></div></div></figure><p id="0b2f" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">解完这个方程后，你会得出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es no"><img src="../Images/99a476ce076ee2feaf132faf0bcf460d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_MUankqMVGc_065FB_j3PQ.png"/></div></div></figure><h1 id="4976" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">5-计算特征向量</h1><blockquote class="lv lw lx"><p id="12d1" class="jk jl lr jm b jn jo ij jp jq jr im js ly ju jv jw lz jy jz ka ma kc kd ke kf hb bi translated">直观地说，<a class="ae jj" href="https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643" rel="noopener" target="_blank">特征向量是一个向量</a>，当对其应用线性变换时，其方向保持不变。</p></blockquote><p id="c7bb" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">找到特征值后，现在是找特征向量的时候了。你可能记得我们一开始的等式是:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es my"><img src="../Images/e368b33cdd4164960693fdcb76b0bcf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:212/format:webp/1*kSiWO5IaB5qP15U1lmE9uw.png"/></div></figure><p id="b635" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">现在，我们知道了A和λ。现在的问题是，哪个向量与(A — λ)矩阵点乘后给出零？<br/>对于每一个λ值，我们都会找到不同的特征向量。让我们开始:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es np"><img src="../Images/427e2c6da68502d02ae9d957b5f6631e.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*l7jnwUaVLUIhia-Ep-y9Bw.png"/></div></figure><p id="6bf2" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">(A- λ。I) = B，其中B矩阵等于A负特征值矩阵。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nq"><img src="../Images/b0a711fc7efacc9fc0212114ca275f8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/format:webp/1*z0rmWjKMxl8NUaxwmNk5TA.png"/></div></figure><p id="257d" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">通过解这个方程，我们会找到特征值对应的特征向量。特征值= [λ = 11.0，λ = 6.4，λ = 0.58 ]，对应的特征向量将为:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es nr"><img src="../Images/88eec4e0c1bba5195df3ef6e22d4588d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ViPelsuBN4kd0ulm1rLJ7w.png"/></div></div></figure><h1 id="b0cd" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">6- <strong class="ak">分类选择</strong></h1><p id="a401" class="pw-post-body-paragraph jk jl hi jm b jn lm ij jp jq ln im js jt lo jv jw jx lp jz ka kb lq kd ke kf hb bi translated"><strong class="jm hj">按照对应的特征值递减对特征向量进行排序，选择特征值最大的前n个特征向量。</strong></p><p id="8b68" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">特征向量只定义空间中新维度的方向。因此特征向量是单位向量。我们需要根据这些向量来形成我们的数据集。</p><blockquote class="ns"><p id="9143" class="nt nu hi bd nv nw nx ny nz oa ob kf dx translated">我们使用主成分分析的目标是创建更少的维度，因此我们将根据我们期望的维度数量选择一些特征向量。</p></blockquote><p id="0478" class="pw-post-body-paragraph jk jl hi jm b jn oc ij jp jq od im js jt oe jv jw jx of jz ka kb og kd ke kf hb bi translated">将特征值对应的特征值从高到低排序。例如，如果你想把维数从三个减少到两个，那么取对应于前两个最高特征值的前两个特征向量。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es oh"><img src="../Images/008a289767598f3762358de28d800238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UStIKk2hekY7AUI1w0zD3A.png"/></div></div></figure><h1 id="9ba2" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated"><strong class="ak"> 6。用特征向量矩阵创建最终数据点。</strong></h1><p id="296a" class="pw-post-body-paragraph jk jl hi jm b jn lm ij jp jq ln im js jt lo jv jw jx lp jz ka kb lq kd ke kf hb bi translated">最终目标是将真实数据集转换到新的子空间上，这样我们就可以将所有学生的信息转换到二维空间中。我们需要点乘<strong class="jm hj">标准化的</strong>数据集和通过取最高特征值创建的特征向量矩阵。在矩阵乘法中，第一个矩阵的列数应该等于第二个矩阵的行数。</p><p id="fbfd" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">(3列x 5行)矩阵将点乘以(2列，3行)矩阵，以获得新维度的转换数据集。</p><p id="9196" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj"> <em class="lr">(规格化一个矩阵)。(特征向量矩阵)=变换数据集</em> </strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es oi"><img src="../Images/7082642b0a4b92fe4e1c4fbda65f5889.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YLYnh-ighGYfXktUWSE1Cg.png"/></div></div></figure><ul class=""><li id="af31" class="kg kh hi jm b jn jo jq jr jt ki jx kj kb kk kf kl km kn ko bi translated">红色方块表示根据特征向量，哪一行乘以哪一列得到新维度中的第一个元素。</li></ul><p id="ec89" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">总之，您可以在二维空间中看到由主成分分析确定的每个学生的最新数据集，以及来自原始表格的相同标签。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es oj"><img src="../Images/970bad5e1a4a1144ee9665345150938d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JYnYbyY1cBcMwYX5jv8ZsQ.png"/></div></div></figure><p id="215f" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">如果你有任何反馈，请给我留言！</p><p id="8e2f" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><em class="lr">注:实数显示到小数点后两位。</em></p></div><div class="ab cl nb nc gp nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="hb hc hd he hf"><p id="06a6" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><em class="lr">我的这篇帖子最初发表于2021年4月8日https://www.datadriveninvestor.com</em><a class="ae jj" href="https://www.datadriveninvestor.com/2021/04/08/the-math-behind-everything-about-principle-component-analysis-pca/#" rel="noopener ugc nofollow" target="_blank"><em class="lr"/></a><em class="lr">。</em></p></div></div>    
</body>
</html>
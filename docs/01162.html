<html>
<head>
<title>Optimization Algorithm for Deep Neural Networks: Conjugate Gradient Method</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度神经网络的优化算法:共轭梯度法</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/optimization-algorithm-for-deep-neural-networks-conjugate-gradient-method-d17e25068775?source=collection_archive---------8-----------------------#2021-03-30">https://medium.com/geekculture/optimization-algorithm-for-deep-neural-networks-conjugate-gradient-method-d17e25068775?source=collection_archive---------8-----------------------#2021-03-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="edbd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度神经网络成为解决图像分类、文本分类、自然语言处理、计算机视觉等不同问题的标准方法之一</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/6742b39a3f267dd3202dfb89b1d4227c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*emeIyvLUCm1rv-ewPU7fRA.gif"/></div><figcaption class="jl jm et er es jn jo bd b be z dx"><strong class="bd jp">Figure 1</strong> <em class="jq">Image classification example 1</em></figcaption></figure><p id="08b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度学习问题需要一些特征来解决，如损失函数或优化器。为了求解，我们必须像往常一样首先定义一个损失函数。在这个操作之后，优化算法准备好最小化损失函数。让我们继续优化部分。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jr"><img src="../Images/a2bd95fe82f3aff95edf723c1a7eea36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*KJu3jZwHxsCbc3RTfTBPkQ.gif"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx"><strong class="bd jp">Figure 2 </strong><em class="jq">Image classification example 2</em></figcaption></figure><p id="2eac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">优化器有时是算法或方法。它们是在深度神经网络训练阶段改变一些属性如权重、学习速率的非常有效的方法。选择正确/最佳的优化技术对于减少损失非常重要。其主要思想是以最少的损失提供最准确的结果。但是，容易吗？</p><p id="1ec1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">目前，最大的问题之一是处理大型稀疏数据矩阵。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jw"><img src="../Images/601f882ddf488edcdd6841a28e9f4325.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*5ookQNjZ0FhUHOYc9wQJMw.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx"><strong class="bd jp">Figure 3 </strong><em class="jq">Sparse matrix</em></figcaption></figure><p id="da2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">寻找这些矩阵的逆也是另一个难题。经典的迭代方法不能解决这些矩阵。幸运的是，Krylov子空间方法是处理这类问题的有用方法。Krylov子空间方法的基础是20世纪10大算法思想之一。它们用在算法中，通过变换计算近似解。这些算法通过使用降维(从n维向量空间到更低的m维向量空间，m小于n或等于n)找到x的近似值。利用这些算法，不需要显式地估计矩阵A的逆矩阵。</p><p id="e997" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Arnoldi，Lanczos，共轭梯度，广义最小剩余法是流行的Krylov子空间方法。我们将集中讨论共轭梯度法。</p><p id="093a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">共轭梯度法是一种优化线性和非线性系统的数学方法。Hestenes和Stiefel把这种方法介绍给我们用于最小化凸二次函数，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jx"><img src="../Images/0da5786aeaa15682833dfd0e89a5af42.png" data-original-src="https://miro.medium.com/v2/resize:fit:288/format:webp/1*P5CRhFVW0h2StS9xfGtpDQ.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jy"><img src="../Images/459e46eaf9e561fca90a71f7ebc7e473.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*U7g2FQfmj6dkpJixtMLUkw.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx"><strong class="bd jp">Figure 4 </strong><em class="jq">Conjugate Gradient Algorithm Visualization</em></figcaption></figure><p id="72da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一种处理对称稀疏线性系统的有效方法，并且易于使用。一般来说，共轭梯度法是求解带有s.p.d矩阵的线性方程组的最佳迭代格式，无需多重网格分量。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jz"><img src="../Images/438d2a16a3cba58684a5425b268d5954.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*yUjFrRd1cEaEfr9opJZf7g.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx">Ax=b</figcaption></figure><p id="39a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，算法为每一步选择一个初始位置并沿着一个方向行走。CG算法将下一个方向向量确定为前一个方向向量的共轭版本。在每一步中顺序确定相关方向。在步骤I，在找到当前负梯度向量之后，该算法将先前方向向量的线性组合加到它上面。</p><p id="153e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它们如何选择下一步的搜索方向和步长属于不同的方法。但是，我们特别知道CG算法的两个重要优点。其中一个是寻找新方向向量的简单公式。另一个是统一进度。</p><p id="d607" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">算法:共轭梯度法</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ka"><img src="../Images/624436f0bdb27f9a80dd4e3239905584.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*Xr8Uy3EzsTaW5oesScCasg.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kb"><img src="../Images/784c63ab0ba40c7cc72a998d056648fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/format:webp/1*5q-T12yNd4f2z5RYAOI1OQ.png"/></div></figure><p id="2cbe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">共轭梯度成本</strong></p><p id="d6d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">o一次CG迭代</p><p id="863c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kc">一个矩阵向量乘法(第6行)</em></p><p id="d8f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kc">(第8、9、11行)中向量的三次加法</em></p><p id="acf3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kc">向量与标量的三次乘法运算(第8、9、11行)</em></p><p id="efd1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kc">向量的两个内积(第7，10行)</em></p><p id="f300" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">o存储四个向量</p><p id="c335" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">MATLAB中共轭梯度法的代码</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kd"><img src="../Images/dd8d6064bf1f7d3efd02541cfb061717.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*C1rOOqH5dtStFhSyXm66Xg.png"/></div></figure><p id="c0ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">共轭梯度法的缺点是它只对对称正定矩阵有用。对于非对称矩阵，还有其他方法/途径，我将在后面提到。然而，该算法是性能最好的Krylov子空间方法之一。</p><p id="9411" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献</strong></p><p id="f609" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ke" href="https://d2l.ai/chapter_optimization/optimization-intro.html" rel="noopener ugc nofollow" target="_blank">https://d2l.ai/chapter_optimization/optimization-intro.html</a></p><p id="dd15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ke" href="https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6" rel="noopener" target="_blank">https://towards data science . com/optimizer-for-training-neural-network-59450d 71 caf 6</a></p><p id="8c6a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ke" href="http://www.maths.lth.se/na/courses/NUM115/NUM115-05/krylov.pdf" rel="noopener ugc nofollow" target="_blank">http://www . maths . lth . se/na/courses/num 115/num 115-05/krylov . pdf</a></p><p id="f1d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ke" href="https://arxiv.org/pdf/1811.09025.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1811.09025.pdf</a></p><p id="16b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ke" href="https://en.wikipedia.org/wiki/Conjugate_gradient_method" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Conjugate_gradient_method</a></p><p id="3361" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ke" href="https://web.cs.iastate.edu/~cs577/handouts/conjugate-gradient.pdf" rel="noopener ugc nofollow" target="_blank">https://web . cs . I astate . edu/~ cs 577/讲义/conjugate-gradient.pdf </a></p><p id="6526" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ke" href="https://www.wias-berlin.de/people/john/LEHRE/lehre.html" rel="noopener ugc nofollow" target="_blank">https://www.wias-berlin.de/people/john/LEHRE/lehre.html</a></p></div></div>    
</body>
</html>
<html>
<head>
<title>Gradient descent in Matlab/Octave</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Matlab/Octave中的梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/gradient-descent-in-matlab-octave-954160e2d3fa?source=collection_archive---------6-----------------------#2021-06-14">https://medium.com/geekculture/gradient-descent-in-matlab-octave-954160e2d3fa?source=collection_archive---------6-----------------------#2021-06-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="237a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你已经读过一些关于线性回归的知识。在机器学习的世界里，这是最常用的方程式之一，而且理由充分。</p><p id="f530" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">线性回归</strong>是一种基于监督<strong class="ih hj">学习</strong>的<strong class="ih hj">机器学习</strong>算法。它执行一个<strong class="ih hj">回归</strong>任务。<strong class="ih hj">回归</strong>基于独立变量对目标预测值进行建模。… <strong class="ih hj">线性回归</strong>根据给定的自变量(x)执行预测因变量(y)的任务。</p><p id="7385" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么，在Matlab / octave中运行梯度下降快速教程预测房价怎么样？</p><p id="7d2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">听起来不错吧？</p><p id="7add" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你需要一些预读，这里有一些好文章:</p><ol class=""><li id="8de8" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><a class="ae jm" href="https://lionbridge.ai/datasets/10-open-datasets-for-linear-regression/" rel="noopener ugc nofollow" target="_blank">线性回归数据集网站</a></li><li id="e287" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated"><a class="ae jm" href="http://www.philender.com/courses/multivariate/notes/matoctave.html" rel="noopener ugc nofollow" target="_blank">矩阵运算</a></li><li id="0c3e" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated"><a class="ae jm" href="https://www.codeproject.com/Articles/879043/Implementing-Gradient-Descent-to-Solve-a-Linear-Re" rel="noopener ugc nofollow" target="_blank">对我帮助很大的好文章</a></li></ol><p id="e70c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你喜欢视频，那就关注我在Youtube上的5集系列。<a class="ae jm" href="https://www.youtube.com/playlist?list=PLOEB0iByIwznDBU8aF9hVXALjwj5Odrnm" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/playlist?list = plo EB 0 ibyiwzndbu 8 af 9 hvxaljwj 5 odrnm</a></p><p id="ba8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你也可以在这里下载源代码:<a class="ae jm" href="https://github.com/shaunenslin/gradientdescentmatlab" rel="noopener ugc nofollow" target="_blank">https://github.com/shaunenslin/gradientdescentmatlab</a></p><p id="4d77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降允许运行几千个θ，直到我们得到最低成本，从而得到最佳θ来进行预测。</p><p id="e556" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jm" href="https://alykhantejani.github.io/images/gradient_descent_line_graph.gif" rel="noopener ugc nofollow" target="_blank">来源</a></p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es js"><img src="../Images/322c42205f701bd1f6b25d1bb46b19d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*AsfV2NelG1Ta5F-0kr727w.gif"/></div></div></figure></div><div class="ab cl ke kf gp kg" role="separator"><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj"/></div><div class="hb hc hd he hf"><h1 id="7a58" class="kl km hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">公式</h1><p id="4d5a" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">除了梯度下降，我们将使用下面的公式。我们的假设函数用于预测线性回归的结果。在下面的数据中，我们有3个特征，因此我们的假设是:</p><pre class="jt ju jv jw fd lo lp lq lr aw ls bi"><span id="b456" class="lt km hi lp b fi lu lv l lw lx">hø(x) = ø0 + ø1x + ø2x + ø3x</span></pre><p id="8ec4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你没有接触过假设方程，看看这里的<a class="ae jm" rel="noopener" href="/@amittheaj.jain/hypothesis-testing-in-linear-regression-8dd85406290a"/>。</p><h1 id="57ce" class="kl km hi bd kn ko ly kq kr ks lz ku kv kw ma ky kz la mb lc ld le mc lg lh li bi translated"><strong class="ak">关于数据</strong></h1><p id="0edc" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">我们将使用一个精选的数据集。它将有3个完全相同的自变量(x ),因变量(y)总是1.5 * x。这使我们可以在进行过程中检查我们的值，并且它们很容易计算。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es md"><img src="../Images/9485b62a75b46418355114881faa4a81.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*SbmJpz7cHeuPjlBG7vP2iw.png"/></div></figure></div><div class="ab cl ke kf gp kg" role="separator"><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj"/></div><div class="hb hc hd he hf"><h1 id="be1c" class="kl km hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">第一步:加载数据集</strong></h1><p id="3f9c" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">我们首先需要加载数据集，并将其分割成X/Y轴。让我们归一化我们的X值，使数据范围在-1和0之间。这将有助于大量的梯度下降，并允许更大的学习率和更快地获得我们的最低成本θ。最后，我们添加一列1来帮助我们的假设计算，并使计算每个θ的成本成为简单的矩阵算术计算。</p><pre class="jt ju jv jw fd lo lp lq lr aw ls bi"><span id="ef02" class="lt km hi lp b fi lu lv l lw lx">% load dataset<br/>ds = load(“realestat3.txt”)</span><span id="3ea7" class="lt km hi lp b fi me lv l lw lx">% split x/y<br/>n = size(ds,2)-1;<br/>x = ds(:,1:n);<br/>y = ds(:,n+1);<br/>m = length(y);</span><span id="5fa5" class="lt km hi lp b fi me lv l lw lx">% normalise<br/>[x, maxs, mins] = normalize(x, n);</span><span id="e75f" class="lt km hi lp b fi me lv l lw lx">% add column with ones — help hypothesis<br/>xo = [ones(m,1),x];</span></pre><h1 id="6096" class="kl km hi bd kn ko ly kq kr ks lz ku kv kw ma ky kz la mb lc ld le mc lg lh li bi translated">步骤2:正常化</h1><p id="be6a" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">归一化很容易通过一些简单的矩阵运算来完成。注意第2行注释中的公式。这个函数将我们所有的特征(独立变量)归一化到-1到0之间。我们还返回每个特性的最大值和最小值的向量，以备后用。</p><pre class="jt ju jv jw fd lo lp lq lr aw ls bi"><span id="f66e" class="lt km hi lp b fi lu lv l lw lx">function [x, maxs, mins] = normalize(x, n)<br/> % n = (x-max) / (max-min)<br/> maxs = max(x);<br/> mins = min(x);<br/> x = (x-max(x)) ./ (max(x)-min(x));<br/>end</span></pre><h1 id="09f1" class="kl km hi bd kn ko ly kq kr ks lz ku kv kw ma ky kz la mb lc ld le mc lg lh li bi translated">第三步:梯度下降</h1><p id="4f08" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">首先，梯度下降需要三样东西</p><ol class=""><li id="013f" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">学习率——我们猜测为0.01</li><li id="fe89" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated">重复次数——我们猜测为1500次</li><li id="014f" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated">我们需要以下θ，我们将从零开始<br/>-θ0-截距<br/>-θ1-第一特征的θ1<br/>-θ2-第二特征的θ2<br/>-θ3-第三特征的θ3</li></ol><p id="84a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降现在将学习率应用于每个特征的成本导数函数。参见下面的公式。如果这对你来说是陌生的，就看看上面的一些文章或者看看我的youtube系列。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es mf"><img src="../Images/ec4dff760ad546a031115a235dd3dc1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*ln6skhp2-dYx-mrLzxRA7g.png"/></div></figure><p id="0d05" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，让我们创建下面的代码，接下来我们将看看实际的功能。一旦我们运行了梯度下降，我们将得到我们的最佳θ以及我们通过梯度下降时每个θ的成本。</p><pre class="jt ju jv jw fd lo lp lq lr aw ls bi"><span id="c83e" class="lt km hi lp b fi lu lv l lw lx">% gradient descent<br/>repeat = 1500;<br/>lrate = 0.1;<br/>thetas = zeros(n+1, 1);<br/>[best, costs] = gradientdescent(repeat, lrate, thetas, xo, y, m, n);</span></pre><p id="83ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用矩阵运算，我们可以很容易地执行梯度下降。同样，如果下面的内容对你来说是希腊式的，那么请观看我的YouTube系列以获得更深入的理解。</p><pre class="jt ju jv jw fd lo lp lq lr aw ls bi"><span id="4008" class="lt km hi lp b fi lu lv l lw lx">function [thetas, costs] = gradientdescent(repeat, lrate, thetas, xo, y, m, n)<br/> costs = zeros(repeat,1);<br/> for r = 1:repeat<br/>   hc = xo * thetas — y;<br/>   temp = sum(hc .* xo);<br/>   thetas = thetas — (lrate * (1/m)) * temp’;<br/>   costs(r) = cost(thetas, xo, y);<br/> end<br/>end</span></pre><h1 id="c80e" class="kl km hi bd kn ko ly kq kr ks lz ku kv kw ma ky kz la mb lc ld le mc lg lh li bi translated">第四步:成本函数</h1><p id="4d87" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">您希望梯度下降调用您的成本函数，这样对于每次重复，您可以计算成本并确保成本下降。请参见下面的等式，我们将在下面付诸实施。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es mg"><img src="../Images/442609c67e74baca086a16db646b7cfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5x0lp4bgSo9PSw4WivljHA.png"/></div></div></figure><p id="fd6b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">再次，观看我的youtube系列，看看学习率的变化以及它如何影响梯度下降达到最低成本的速度。</p><pre class="jt ju jv jw fd lo lp lq lr aw ls bi"><span id="6b20" class="lt km hi lp b fi lu lv l lw lx">function j = cost(thetas, xo, y)<br/>  hc = xo * thetas — y;<br/>  m = length(y);<br/>  j = (hc’ * hc) / (2 * m);<br/>end</span></pre><h1 id="0bdc" class="kl km hi bd kn ko ly kq kr ks lz ku kv kw ma ky kz la mb lc ld le mc lg lh li bi translated">第五步:预测我们的结果</h1><p id="f7af" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">最后，回到我们的主程序，让我们:</p><ol class=""><li id="2df6" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">画出我们的成本，这样我们就可以得到一个可视化的成本队列。</li><li id="7d33" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated">获得我们想要预测的特征矩阵(x)。</li><li id="a207" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated">使用之前的质量/分钟数对预测进行标准化。</li><li id="9fba" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated">执行我们的假设:h = 0+1 * x1+2 * x2+3 * x3。<br/>(我们加了一个“1”列，这样简单的矩阵乘法就可以做上面的等式)</li></ol><pre class="jt ju jv jw fd lo lp lq lr aw ls bi"><span id="88b4" class="lt km hi lp b fi lu lv l lw lx">% plot costs<br/>plot(costs, 1:repeat);</span><span id="e06d" class="lt km hi lp b fi me lv l lw lx">% predict a value<br/>p = [6;6;6];<br/>% normalise the dependent variables<br/>pn = (p-maxs’)./(maxs’-mins’)<br/>% add a 1 for easy hypothesis calc<br/>pn = [1;pn];<br/>r = pn’ * best</span></pre><p id="c282" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面运行，您应该得到“9”的结果</p><p id="22e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您想了解所有这些内容，请访问</p><blockquote class="mh mi mj"><p id="3351" class="if ig mk ih b ii ij ik il im in io ip ml ir is it mm iv iw ix mn iz ja jb jc hb bi translated"><a class="ae jm" href="https://www.youtube.com/playlist?list=PLOEB0iByIwznDBU8aF9hVXALjwj5Odrnm" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/playlist?list = plo EB 0 ibyiwzndbu 8 af 9 hvxaljwj 5 odrnm</a></p></blockquote></div></div>    
</body>
</html>
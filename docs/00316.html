<html>
<head>
<title>Web Scraping With No Effort. Python: BeautifulSoup, Grequests.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">网页抓取不费吹灰之力。Python: BeautifulSoup，Grequests。</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/web-scraping-without-efforts-python-beautifulsoup-grequests-7e7d7886355a?source=collection_archive---------2-----------------------#2021-01-13">https://medium.com/geekculture/web-scraping-without-efforts-python-beautifulsoup-grequests-7e7d7886355a?source=collection_archive---------2-----------------------#2021-01-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="d016" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">逐步指南</h2><div class=""/><div class=""><h2 id="b71c" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">如何用漂亮的组和异步HTTP请求(Grequests)构建一个web scraper</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/573d7f56dcae8a459babdd9a9251ea93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tC_0fXFphGC1w_p-x1Ntow.jpeg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Photo by <a class="ae jw" href="https://unsplash.com/@sapegin?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Artem Sapegin</a> on <a class="ae jw" href="https://unsplash.com/s/photos/coding?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="0dac" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">引言。</h1><p id="7370" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">这是我的第一个关于网络抓取的教程。我将解释(用完整的代码示例)如何使用BeautifulSoup和Grequests Python库创建一个web scraper。</p><p id="8dfd" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">假设你有一个NLP任务——从食谱网站收集文本数据，并进行二进制分类:配料/说明。让我们从食谱网站<a class="ae jw" href="https://www.loveandlemons.com/recipes/" rel="noopener ugc nofollow" target="_blank">https://www.loveandlemons.com/</a>搜集数据。为此，我们将使用最流行的、初学者友好的库:BeautifulSoup和Grequests。</p><h1 id="384f" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated"><strong class="ak">定义</strong>。</h1><p id="4543" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated"><a class="ae jw" href="https://pypi.org/project/beautifulsoup4/" rel="noopener ugc nofollow" target="_blank"><strong class="kr hs"><em class="lq">beautiful soup</em></strong></a><strong class="kr hs"><em class="lq"/></strong>是开源且完全免费使用的库，使得从网页中抓取信息变得非常容易。它位于HTML或XML解析器的顶端，为迭代、搜索和修改解析树提供了Pythonic习惯用法。BeautifulSoup没有发送web请求的功能。我们将使用Grequests模块。除了发送web请求，BeautifulSoup也没有文档解析器。我们必须从诸如“html”这样的选项中进行选择。parser '，HTML5lib，XML Parser，还有一些其他的。</p><p id="6fad" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated"><a class="ae jw" href="https://pypi.org/project/grequests/" rel="noopener ugc nofollow" target="_blank"><strong class="kr hs"><em class="lq">Grequests</em></strong></a><strong class="kr hs"><em class="lq"/></strong>也是一个开源的免费库，它允许你用Gevent的requests来进行异步HTTP请求。大多数教程使用请求模块，以手工方式实现异步请求。Grequests在盒子里已经内置了异步方法，非常方便。</p><h1 id="8e45" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">包安装。</h1><p id="b408" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">这并不难做到:</p><ol class=""><li id="c1cf" class="lr ls hi kr b ks ll kv lm ky lt lc lu lg lv lk lw lx ly lz bi translated">在IDE解释器设置中查找(我使用Pycharm，但是新的包安装在任何IDE中都是一样的)</li><li id="bbd9" class="lr ls hi kr b ks ma kv mb ky mc lc md lg me lk lw lx ly lz bi translated">按加号键添加新的包/库</li><li id="16fc" class="lr ls hi kr b ks ma kv mb ky mc lc md lg me lk lw lx ly lz bi translated">输入您要安装的软件包的名称</li><li id="8caf" class="lr ls hi kr b ks ma kv mb ky mc lc md lg me lk lw lx ly lz bi translated">按安装包。</li></ol><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mf"><img src="../Images/568997b8151647f93fe7f123ab2d1462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gmfiWEbKf0QXUlfu2O2ixw.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Pic.1 How to install package Grequests in Pycharm using Anaconda virtual environments</figcaption></figure><p id="d95b" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">当我们安装好所有的包后，我们可以考虑从网站<strong class="kr hs"> <em class="lq"> : </em> </strong>收集数据的步骤</p><ol class=""><li id="2f49" class="lr ls hi kr b ks ll kv lm ky lt lc lu lg lv lk lw lx ly lz bi translated">将所有食谱链接收集到一个列表中</li><li id="a9f0" class="lr ls hi kr b ks ma kv mb ky mc lc md lg me lk lw lx ly lz bi translated">从每个页面收集数据</li><li id="0d0c" class="lr ls hi kr b ks ma kv mb ky mc lc md lg me lk lw lx ly lz bi translated">将数据保存到文件中</li></ol><h1 id="659a" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated"><strong class="ak">步骤一。收集所有链接。</strong></h1><p id="5faf" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">这很容易做到。首先，构造一个请求列表，使用“grequests.map()”并行发送。然后得到一个响应列表。</p><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mg mh l"/></div><figcaption class="js jt et er es ju jv bd b be z dx"><a class="ae jw" href="https://gist.github.com/Galina-Blokh/d0528413523e333a261e02ac415987ce" rel="noopener ugc nofollow" target="_blank">How to use grequest</a></figcaption></figure><p id="73af" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">因为我们得到了一个响应列表，所以我们能够用bs4和html.parser遍历它。这样，我们就得到一个链接列表。</p><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mg mh l"/></div><figcaption class="js jt et er es ju jv bd b be z dx"><a class="ae jw" href="https://gist.github.com/Galina-Blokh/4db00fd3926ee9eaad35f9c67f8cbd64" rel="noopener ugc nofollow" target="_blank">Parsing the results of all the requests by iterating through the res_list with bs4 and link pattern</a></figcaption></figure><p id="1fd7" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">我们完成了步骤1。只有四行代码加上导入——简单！</p><h1 id="5970" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">第二步。从每个页面收集数据。</h1><p id="848d" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">首先，定义一个默认字典来存储我们正在收集的数据。然后重复两行，构建一个请求列表，并从步骤1中的所有菜谱页面获得响应。注意:我们应该制作一组链接以减少重复:</p><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mg mh l"/></div><figcaption class="js jt et er es ju jv bd b be z dx"><a class="ae jw" href="https://gist.github.com/Galina-Blokh/d17b0ec042591821cb34c0cb1e41e554" rel="noopener ugc nofollow" target="_blank">Get all recipe responses at one time</a></figcaption></figure><p id="9a40" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">然后开始迭代响应，对类“成分”和类“说明”的每个响应元素应用bs4搜索功能。用try-except语句包围解析器，因为我们可以捕获AttributeError异常(当我们试图解析None类型对象时):</p><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mg mh l"/></div><figcaption class="js jt et er es ju jv bd b be z dx"><a class="ae jw" href="https://gist.github.com/Galina-Blokh/5457feb40ba41bfaac266b642599cf4d" rel="noopener ugc nofollow" target="_blank">Data search in each response with bs4</a></figcaption></figure><p id="402f" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">这里，在下面的图片上，目标类在网页HTML中搜索。它显示了您获取目标搜索键的位置:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mi"><img src="../Images/0a04cf30068927d0d55dfebe8ddcff18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xng2OyfhtIIk29GPKlefdg.png"/></div></div></figure><p id="2c0c" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">当你捕捉所有的文本元素(在同一个for循环中)时，你必须把所有的结果转换成一个文本列表，把它放入一个默认的字典中。仅此而已。第二步结束了:</p><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mg mh l"/></div><figcaption class="js jt et er es ju jv bd b be z dx"><a class="ae jw" href="https://gist.github.com/Galina-Blokh/57d286e0ea8307f4cd1d99bfcffcca6c" rel="noopener ugc nofollow" target="_blank">Using list comprehension extract text data from a parsed response</a></figcaption></figure><h1 id="8c13" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">第三步。将数据保存到文件中。</h1><p id="eeeb" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">这是最快也是最容易的一步。Pickle转储数据的速度比CSV快得多，占用的空间也更少。在这里，我们采取泡菜格式，但你可以自由使用任何你喜欢的。</p><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="mg mh l"/></div><figcaption class="js jt et er es ju jv bd b be z dx"><a class="ae jw" href="https://gist.github.com/Galina-Blokh/118705d88d22d9a140a88f35974c8541" rel="noopener ugc nofollow" target="_blank">Save data into pickle file</a></figcaption></figure><p id="409c" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">试着用熊猫打开这个文件——你会看到一组有两栏:“食谱”和“说明”。数据帧中1009行中的每一行都是来自1009个具有配方的独特页面的数据。每个单元格中的每一项都是段落列表。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mj"><img src="../Images/f9e0c8c9fd88e3cc0a7b4ffd724a90a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MxQPeDxteWjbwkxoH_RL4g.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Pic.2 Data set loaded from pickle file</figcaption></figure><h1 id="369a" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">结论。</h1><p id="07cb" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">嗯，我们在短时间内做了一件大事。NLP目标的下一步应该是:预处理数据，将其分成段落，为每个文本段落给出标签(用于文本分类)，将文本转换为序列并建立模型。这可能是一篇文章的主题。</p><p id="ae6c" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">在本教程中，您了解了异步HTTP请求的web抓取是如何快速而简单的。它不需要太多的时间和直观。BeautifulSoup和Grequests对于数据科学来说是非常方便和强大的工具。我希望这些知识能帮你节省谷歌搜索的时间，避免陷入错误。</p><p id="3592" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated"><em class="lq">完整项目上</em> <a class="ae jw" href="https://github.com/Galina-Blokh/ai_assignment_aidock/tree/refator" rel="noopener ugc nofollow" target="_blank"> <em class="lq"> Github </em> </a></p></div></div>    
</body>
</html>
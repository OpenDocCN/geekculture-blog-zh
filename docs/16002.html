<html>
<head>
<title>Principal Component Analysis (PCA) in Feature Engineering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征工程中的主成分分析</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/principal-component-analysis-pca-in-feature-engineering-472afa39c27d?source=collection_archive---------8-----------------------#2022-12-05">https://medium.com/geekculture/principal-component-analysis-pca-in-feature-engineering-472afa39c27d?source=collection_archive---------8-----------------------#2022-12-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/da52ec6f60eca748ee40ee8e544a0a63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LchxMOwEAZuItruX"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Photo by <a class="ae iu" href="https://unsplash.com/@gtomassetti?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Giorgio Tomassetti</a> on <a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="b3f9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">文章将解释<strong class="ix hj">主成分分析(PCA) </strong>和代码实现的概念和用途。</p><p id="eb4c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">主成分分析(PCA) </strong>是一种<strong class="ix hj">统计程序</strong>，使用一种技术将一组相关变量转换为一组不相关变量。在<a class="ae iu" href="https://www.kaggle.com/datasets/rodolfomendes/abalone-dataset" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj">鲍鱼</strong> </a>数据集中，有<strong class="ix hj">高度</strong>和<strong class="ix hj">直径等特征。鲍鱼是一种类似牡蛎的生物。这里的概念是将上述变量转换成一组称为<strong class="ix hj">变化轴的其他变量。</strong></strong></p><p id="511c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">长轴</strong>可称为<strong class="ix hj">【尺寸】</strong>部件:小高度小直径(左下)与大高度大直径(右上)相对。<strong class="ix hj">短轴</strong>可称为<strong class="ix hj">【形状】</strong>部件:小高度大直径(扁平形状)与大高度小直径(圆形)形成对比。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/3f536a6751c7af0e55ac8a67f92b3031.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*5zMQm0kn_QoCs7y8Uhmu6w.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Axes of variation defined (Image from Kaggle course)</figcaption></figure><p id="6f93" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，与其用鲍鱼的<strong class="ix hj">高度</strong>和<strong class="ix hj">直径</strong>来描述它们，不如用它们的<strong class="ix hj">大小</strong>和<strong class="ix hj">形状</strong>来描述它们。事实上，这就是PCA的全部思想:我们用它的<strong class="ix hj">变化轴</strong>来描述它，而不是用<strong class="ix hj">原始特征</strong>来描述数据。变化轴成为新的特征。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jy"><img src="../Images/de8ff27801c5fbba1d03dd936fbfaeca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dSKf14hqp6WW-7uGtCt3kg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">New axes after rotation of dataset in feature space (image from Kaggle)</figcaption></figure><p id="1c2b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> PCA </strong>也告诉<strong class="ix hj">每个成分的变化量</strong>。它显示了与<strong class="ix hj">形状</strong>组件相比，<strong class="ix hj">尺寸</strong>特征对变化的影响程度。PCA通过每个组成部分的<strong class="ix hj">解释方差的百分比使这一点变得精确。</strong></p><h2 id="be11" class="jz ka hi bd kb kc kd ke kf kg kh ki kj jg kk kl km jk kn ko kp jo kq kr ks kt bi translated"><strong class="ak">特征工程的PCA</strong></h2><p id="1cc0" class="pw-post-body-paragraph iv iw hi ix b iy ku ja jb jc kv je jf jg kw ji jj jk kx jm jn jo ky jq jr js hb bi translated">有两种方法可以将<strong class="ix hj"> PCA </strong>用于<strong class="ix hj">特征工程</strong>。</p><p id="c4a9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第一种方法是把它作为一种描述技巧。由于组件告诉了变化，您可以<strong class="ix hj">计算组件的</strong> <a class="ae iu" rel="noopener" href="/p/360ec43353a0"> <strong class="ix hj">【互信息】</strong> </a> <strong class="ix hj">分数</strong>，并查看哪种变化是<strong class="ix hj">对目标变量</strong>最有预测性的。这可以为要创建的各种特征提供思路——比如说，如果尺寸<strong class="ix hj">重要，则是高度和直径的乘积<strong class="ix hj">;如果尺寸<strong class="ix hj">重要，则是高度和直径的比值</strong><strong class="ix hj"/>。甚至可以在一个或多个高分组件上尝试<a class="ae iu" rel="noopener" href="/p/112c61d55231"> <strong class="ix hj">集群</strong> </a>。</strong></strong></p><p id="d514" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">第二种方式</strong>是<strong class="ix hj">使用部件本身</strong>作为特征。因为组件直接公开了数据的变化结构，所以它们通常比原始特征更能提供信息。以下是一些使用案例:</p><ul class=""><li id="08c3" class="kz la hi ix b iy iz jc jd jg lb jk lc jo ld js le lf lg lh bi translated"><strong class="ix hj">降维</strong>:当<strong class="ix hj">特征高度冗余</strong>(特别是<em class="li">多重共线</em>)时，PCA会<strong class="ix hj">将冗余</strong>分割成一个或多个<strong class="ix hj">接近零方差</strong>分量，然后您可以丢弃这些分量，因为它们包含很少或没有信息。</li><li id="7c7c" class="kz la hi ix b iy lj jc lk jg ll jk lm jo ln js le lf lg lh bi translated"><strong class="ix hj">异常检测</strong>:在<strong class="ix hj">低方差分量</strong>中经常会出现与原始特征不一致的异常变化。在异常或异常值检测任务中，这些组件可能是<strong class="ix hj">高信息量的</strong>。</li><li id="b9d0" class="kz la hi ix b iy lj jc lk jg ll jk lm jo ln js le lf lg lh bi translated"><strong class="ix hj">降噪</strong>:传感器读数的集合将总是具有共同的背景噪声。PCA有时可以将(信息)信号收集到<strong class="ix hj">更少数量的特征</strong>中，而<strong class="ix hj">不处理噪声</strong>，从而<strong class="ix hj">提高信噪比</strong><strong class="ix hj"/>。</li><li id="02b9" class="kz la hi ix b iy lj jc lk jg ll jk lm jo ln js le lf lg lh bi translated"><strong class="ix hj">去相关</strong>:一些ML算法与<strong class="ix hj">高度相关的特征</strong>斗争。PCA <strong class="ix hj">将相关的特征转换成不相关的成分</strong>，这对于算法来说更容易处理。</li></ul><p id="b3f3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">PCA基本上直接访问数据的<strong class="ix hj">相关结构。</strong></p><p id="4506" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">PCA <strong class="ix hj">最佳实践</strong>:</p><p id="23c3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用PCA时，需要记住一些事情</p><ul class=""><li id="be13" class="kz la hi ix b iy iz jc jd jg lb jk lc jo ld js le lf lg lh bi translated">PCA <strong class="ix hj">仅适用于数字特征</strong>，</li><li id="e84c" class="kz la hi ix b iy lj jc lk jg ll jk lm jo ln js le lf lg lh bi translated">PCA<strong class="ix hj">对比例</strong>敏感。在应用PCA之前对数据进行标准化是一个很好的做法，</li><li id="c1b2" class="kz la hi ix b iy lj jc lk jg ll jk lm jo ln js le lf lg lh bi translated">最好<strong class="ix hj">移除或限制异常值</strong>，因为它们会在结果中产生偏差。</li></ul><p id="6526" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">PCA的代码实现</strong></p><p id="8fa9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们看看PCA的一些代码实现，以及它如何解决一些现实生活中的问题。<a class="ae iu" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data" rel="noopener ugc nofollow" target="_blank"><strong class="ix hj">Ames</strong></a><strong class="ix hj"/>房价数据集用作用例，我们也可以将代码用于其他问题陈述。</p><pre class="ju jv jw jx fd lo lp lq bn lr ls bi"><span id="7ccd" class="lt ka hi lp b be lu lv l lw lx">#List of installed libraries<br/><br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>from sklearn.decomposition import PCA<br/>from sklearn.feature_selection import mutual_info_regression<br/>from sklearn.model_selection import cross_val_score<br/>from xgboost import XGBRegressor<br/><br/># Set Matplotlib defaults<br/>plt.style.use("seaborn-whitegrid")<br/>plt.rc("figure", autolayout=True)<br/>plt.rc(<br/>    "axes",<br/>    labelweight="bold",<br/>    labelsize="large",<br/>    titleweight="bold",<br/>    titlesize=14,<br/>    titlepad=10,<br/>)<br/><br/>#Standardization and applying PCA to create principal components<br/><br/>def apply_pca(X, standardize=True):<br/>    # Standardize<br/>    if standardize:<br/>        X = (X - X.mean(axis=0)) / X.std(axis=0)<br/>    # Create principal components<br/>    pca = PCA()<br/>    X_pca = pca.fit_transform(X)<br/>    # Convert to dataframe<br/>    component_names = [f"PC{i+1}" for i in range(X_pca.shape[1])]<br/>    X_pca = pd.DataFrame(X_pca, columns=component_names)<br/>    # Create loadings<br/>    loadings = pd.DataFrame(<br/>        pca.components_.T,  # transpose the matrix of loadings<br/>        columns=component_names,  # so the columns are the principal components<br/>        index=X.columns,  # and the rows are the original features<br/>    )<br/>    return pca, X_pca, loadings<br/><br/><br/>def plot_variance(pca, width=8, dpi=100):<br/>    # Create figure<br/>    fig, axs = plt.subplots(1, 2)<br/>    n = pca.n_components_<br/>    grid = np.arange(1, n + 1)<br/>    # Explained variance<br/>    evr = pca.explained_variance_ratio_<br/>    axs[0].bar(grid, evr)<br/>    axs[0].set(<br/>        xlabel="Component", title="% Explained Variance", ylim=(0.0, 1.0)<br/>    )<br/>    # Cumulative Variance<br/>    cv = np.cumsum(evr)<br/>    axs[1].plot(np.r_[0, grid], np.r_[0, cv], "o-")<br/>    axs[1].set(<br/>        xlabel="Component", title="% Cumulative Variance", ylim=(0.0, 1.0)<br/>    )<br/>    # Set up figure<br/>    fig.set(figwidth=8, dpi=100)<br/>    return axs<br/><br/>#Calculate Mutual Information scores<br/><br/>def make_mi_scores(X, y):<br/>    X = X.copy()<br/>    for colname in X.select_dtypes(["object", "category"]):<br/>        X[colname], _ = X[colname].factorize()<br/>    # All discrete features should now have integer dtypes<br/>    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]<br/>    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)<br/>    mi_scores = pd.Series(mi_scores, name="MI Scores", index=X.columns)<br/>    mi_scores = mi_scores.sort_values(ascending=False)<br/>    return mi_scores<br/><br/># Create model scores<br/><br/>def score_dataset(X, y, model=XGBRegressor()):<br/>    # Label encoding for categoricals<br/>    for colname in X.select_dtypes(["category", "object"]):<br/>        X[colname], _ = X[colname].factorize()<br/>    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)<br/>    score = cross_val_score(<br/>        model, X, y, cv=5, scoring="neg_mean_squared_log_error",<br/>    )<br/>    score = -1 * score.mean()<br/>    score = np.sqrt(score)<br/>    return score<br/><br/><br/>df = pd.read_csv("ames.csv")</span></pre><p id="4c5e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上面的代码中，我们计算了数据集的<strong class="ix hj"> PCA成分、互信息分数和模型分数</strong>。在下一段代码中，我们从数据集中提取了<strong class="ix hj"> 4个特征</strong>，并找到了<strong class="ix hj">与<strong class="ix hj">目标(销售价格)</strong>的关联</strong>。</p><pre class="ju jv jw jx fd lo lp lq bn lr ls bi"><span id="e1a9" class="lt ka hi lp b be lu lv l lw lx">features = [<br/>    "GarageArea",<br/>    "YearRemodAdd",<br/>    "TotalBsmtSF",<br/>    "GrLivArea",<br/>]<br/><br/>print("Correlation with SalePrice:\n")<br/>print(df[features].corrwith(df.SalePrice))<br/><br/><br/>Correlation with SalePrice:<br/><br/>GarageArea      0.640138<br/>YearRemodAdd    0.532974<br/>TotalBsmtSF     0.632529<br/>GrLivArea       0.706780<br/>dtype: float64</span></pre><p id="fcbe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将使用主成分分析来观察特征之间的<strong class="ix hj">相关性，并建议可以用新特征</strong>有效建模的<strong class="ix hj">关系。</strong></p><pre class="ju jv jw jx fd lo lp lq bn lr ls bi"><span id="ffc5" class="lt ka hi lp b be lu lv l lw lx">X = df.copy()<br/>y = X.pop("SalePrice")<br/>X = X.loc[:, features]<br/><br/># `apply_pca`, defined above, reproduces the code from the tutorial<br/>pca, X_pca, loadings = apply_pca(X)<br/>print(loadings)<br/><br/><br/>                   PC1       PC2       PC3       PC4<br/>GarageArea    0.541229  0.102375 -0.038470  0.833733<br/>YearRemodAdd  0.427077 -0.886612 -0.049062 -0.170639<br/>TotalBsmtSF   0.510076  0.360778 -0.666836 -0.406192<br/>GrLivArea     0.514294  0.270700  0.742592 -0.332837</span></pre><h2 id="fa4c" class="jz ka hi bd kb kc kd ke kf kg kh ki kj jg kk kl km jk kn ko kp jo kq kr ks kt bi translated"><strong class="ak">解释组件载荷</strong></h2><p id="20a5" class="pw-post-body-paragraph iv iw hi ix b iy ku ja jb jc kv je jf jg kw ji jj jk kx jm jn jo ky jq jr js hb bi translated">第一个组件<strong class="ix hj"> PC1 </strong>看起来像是本文前一部分中的“大小”组件:所有的特征<strong class="ix hj">都具有相同的符号(正)</strong>，这表明该组件正在描述这些特征的值大的<strong class="ix hj">房屋和值小的</strong>房屋之间的对比。</p><p id="7a8d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对第三个组件<strong class="ix hj"> PC3 </strong>的解释有点棘手。特性<code class="du ly lz ma lp b">GarageArea</code>和<code class="du ly lz ma lp b">YearRemodAdd</code>都有接近零的负载，所以让我们忽略它们。这个组件主要是关于<code class="du ly lz ma lp b">TotalBsmtSF</code>和<code class="du ly lz ma lp b">GrLivArea</code>的。它描述了居住面积很大但地下室很小(或不存在)的房子和相反情况的对比:地下室很大的小房子。</p><h2 id="d02e" class="jz ka hi bd kb kc kd ke kf kg kh ki kj jg kk kl km jk kn ko kp jo kq kr ks kt bi translated"><strong class="ak">创建新特征</strong></h2><p id="50e2" class="pw-post-body-paragraph iv iw hi ix b iy ku ja jb jc kv je jf jg kw ji jj jk kx jm jn jo ky jq jr js hb bi translated">在这一部分，我们将尝试创建新的特性来测试现有的特性，以确定是否能找到更好的<a class="ae iu" href="https://www.quora.com/What-is-the-difference-between-an-RMSE-and-RMSLE-logarithmic-error-and-does-a-high-RMSE-imply-low-RMSLE" rel="noopener ugc nofollow" target="_blank"><strong class="ix hj">【RMSLE】</strong></a><strong class="ix hj">【均方根对数误差】</strong>。它是使用<strong class="ix hj"> score_dataset() </strong>函数计算的。代码如下。</p><pre class="ju jv jw jx fd lo lp lq bn lr ls bi"><span id="410f" class="lt ka hi lp b be lu lv l lw lx"># Solution 1: Inspired by loadings<br/>X = df.copy()<br/>y = X.pop("SalePrice")<br/><br/>X["Feature1"] = X.GrLivArea + X.TotalBsmtSF<br/>X["Feature2"] = X.YearRemodAdd - X.TotalBsmtSF<br/><br/>score = score_dataset(X, y)<br/>print(f"Your score: {score:.5f} RMSLE")<br/><br/><br/># Solution 2: Uses components<br/>X = df.copy()<br/>y = X.pop("SalePrice")<br/><br/>X = X.join(X_pca)<br/>score = score_dataset(X, y)<br/>print(f"Your score: {score:.5f} RMSLE")<br/><br/><br/><br/>Your score: 0.13478 RMSLE<br/>Your score: 0.13707 RMSLE</span></pre><p id="07ec" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">代码的第一部分有<strong class="ix hj">创建的特征(特征1和特征2) </strong>并计算得分。在下一部分中，<strong class="ix hj"> PCA成分</strong>用于计算分数。以上两种情况下的RMSLE值分别为<strong class="ix hj"> 0.134和0.137 </strong>。当然，人们可以尝试现有特征的不同组合，并检查<strong class="ix hj"> RMSLE值是否可以降低到0.140 </strong>以下，因为这是没有PCA或新创建特征的数据集的分数。</p><p id="dc34" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这篇关于PCA的文章到此结束，我希望我写得很清楚，这里是<a class="ae iu" href="https://www.kaggle.com/code/ryanholbrook/principal-component-analysis/tutorial" rel="noopener ugc nofollow" target="_blank"><strong class="ix hj"/></a>。创造一个投入生产的好模型需要大量的实践，所有这些<strong class="ix hj">特征工程的步骤对于实现目标</strong>是必不可少的。</p><p id="7a9a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">到那时，请查看我的其他<a class="ae iu" rel="noopener" href="/@abhi2652254"> <strong class="ix hj">文章</strong> </a>，说<a class="ae iu" href="https://www.linkedin.com/in/obhinaba17/" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj">嗨</strong> </a> <strong class="ix hj">。</strong>还有，看看我的<a class="ae iu" href="https://github.com/abhigyan631" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj"> GitHub </strong> </a> <strong class="ix hj">。如果你喜欢我的作品，你可以给我捐赠几杯咖啡，这样我就可以在写作的道路上不断提高内容的质量。</strong></p></div></div>    
</body>
</html>
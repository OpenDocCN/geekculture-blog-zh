<html>
<head>
<title>A COMPARATIVE ANALYSIS OF VARIOUS MACHINE LEARNING MODELS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">各种机器学习模型的比较分析</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/a-comparative-analysis-of-various-machine-learning-models-3d8e08aa4df4?source=collection_archive---------22-----------------------#2021-08-05">https://medium.com/geekculture/a-comparative-analysis-of-various-machine-learning-models-3d8e08aa4df4?source=collection_archive---------22-----------------------#2021-08-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="194e" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">利用压力、焦虑的感知水平预测印度学校教师的性别&amp;新冠肺炎疫情中他们的人口统计学特征</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/629c7553df81da7b32d2e3d255675909.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_43nynLJpvCUJsRw"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Photo by <a class="ae jt" href="https://unsplash.com/@gagliardiphotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Giovanni Gagliardi</a> on <a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><ol class=""><li id="6346" class="ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="jw hj">简介</strong></li></ol><p id="ff68" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">由于这种疾病的未知性质，COVID 19疫情引发了所有年龄段的人的恐惧。当局已经采取了各种预防措施，包括社交距离、封锁、旅行限制以及关闭学校和大学。印度也采取了这些预防措施。它影响了几乎所有的经济体，即使在小规模的部门也有直接的影响。在社会不同部门工作的人面临着精神压抑和恐惧的感觉，这是由于突然的封锁场景和由于同样原因死亡人数的增加。技术工人阶级在通过网络方式改变他们的大部分工作时遇到了极大的麻烦，这可能会导致他们的压力和焦虑感。在新的正常情况下引入新的在线教学模式显然导致了心理偏差，关键是他们中的大多数人无法有效地利用工作时间，因为他们缺乏以正常方式轻松工作的机会。</p><p id="9287" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">这种劳动中最重要的阶层是教师。由于在线教学模式的突然转变，他们往往会有额外的精神压力。学校和大学被关闭，教育被转移到互联网上。印度学校的老师习惯于传统的教学方法，在线教育的转变对他们来说是一个新的领域。在COVID 19疫情，进行了一项在线研究，以评估学校教师的压力和焦虑水平。数据取自印度。</p><p id="b495" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">现在，来看这里的分析，主要意图是对各种机器学习模型进行比较分析，以使用他们在新冠肺炎时间获得的焦虑水平、他们感知的压力水平、年龄、他们有孩子的数量以及他们每天教学的最小小时数来预测教师的性别。做这样的分析是很有可能的，因为有几项研究讨论了一个人将如何对待或处理与他们的性别有关的情况。因此，这可以成为利用这些因素预测一个人性别的另一种方法。有几种机器学习模型可用于预测机器学习问题中的特定类别。确定适用于特定情况的最佳模型是一项具有挑战性的任务，因为每个数据都是唯一的，并且在不同的模型中可能表现不同。从逻辑上讲，选择一个特定的机器学习问题进行分析可能会得出某些结论，但在每种情况下都建议进行试错，以找出有关问题的最佳模型。</p><p id="a67c" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 2。数据</strong></p><p id="cbf8" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 2.1。用于分析的数据和变量的来源</strong></p><p id="f7ff" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">用于分析的数据取自https://data.mendeley.com/datasets/yhmb6psmpm/1的<a class="ae jt" href="https://data.mendeley.com/datasets/yhmb6psmpm/1" rel="noopener ugc nofollow" target="_blank">。研究人员在印度进行了数据收集，数据集包含有关教师的年龄、性别、他们的教学类别、他们参与教学的小时数、他们的婚姻状况、他们有多少个孩子、在covid19疫情情景中感知的压力水平以及感知的焦虑水平的信息。分析中采用的变量有年龄、压力水平、焦虑水平、他们授课的小时数以及他们有多少个孩子。关于用于分析的变量类型的信息如下，以及相同的深入细节。(如果你想对不同的机器学习模型进行比较分析，你可以使用任何数据集！)</a></p><p id="4e70" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">让我们导入必要的库并为分析设置数据框。我已将数据直接保存在工作中，文件名指定为Data_Covid.csv。</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="a5e2" class="lg ig hi lc b fi lh li l lj lk">import pandas as pd</span><span id="81ae" class="lg ig hi lc b fi ll li l lj lk">import numpy as np</span><span id="5d59" class="lg ig hi lc b fi ll li l lj lk">from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler</span><span id="a413" class="lg ig hi lc b fi ll li l lj lk">import matplotlib.pyplot as plt</span><span id="f8d4" class="lg ig hi lc b fi ll li l lj lk">#visualising the data</span><span id="d0e7" class="lg ig hi lc b fi ll li l lj lk">df=pd.read_csv('/content/Data_covid.csv')</span><span id="c4c5" class="lg ig hi lc b fi ll li l lj lk">df</span></pre><p id="a423" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 2.2。选择特定变量进行分析的原因</strong></p><p id="19bc" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">如引言中所述，我们必须预测学校教师的性别。使用基本的人口统计特征，如年龄、婚姻状况和孩子数量来预测性别是很有挑战性的。但是一些与心理学和经济学相关的文献表明，他们的性别也与他们的行为模式有关。我们知道在疫情新冠肺炎，由于不确定性的发生，每个人都变得高度紧张和焦虑。同样，对于学校教师来说，由于在线上课和在线处理学术活动的额外压力，也发生了同样的事情。因此，对他们来说，感知到的压力水平和焦虑水平也可能因性别而异。因此，在这里，感知压力水平和感知焦虑水平这两个变量被用来预测性别，此外，一些人口统计学特征，如他们在covid19疫情情景中的教学时数、他们的年龄和孩子数量，也被考虑在内，以预测相关印度学校教师的性别。</p><p id="d74e" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">现在绘制如下相关图，以了解是否有任何变量彼此之间的相关性更高。</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="c162" class="lg ig hi lc b fi lh li l lj lk">#BUILDING PEARSON CORRELATION PLOT</span><span id="efba" class="lg ig hi lc b fi ll li l lj lk">import seaborn as sns</span><span id="8ea0" class="lg ig hi lc b fi ll li l lj lk">def correlation_map(df):</span><span id="aca2" class="lg ig hi lc b fi ll li l lj lk">_ , ax = plt.subplots(figsize =(14, 12))</span><span id="09e3" class="lg ig hi lc b fi ll li l lj lk">colormap = sns.diverging_palette(220, 10, as_cmap = True)</span><span id="adf7" class="lg ig hi lc b fi ll li l lj lk">_ = sns.heatmap(</span><span id="61da" class="lg ig hi lc b fi ll li l lj lk">df.corr(),</span><span id="8571" class="lg ig hi lc b fi ll li l lj lk">cmap = colormap,</span><span id="3376" class="lg ig hi lc b fi ll li l lj lk">square=True,</span><span id="fedf" class="lg ig hi lc b fi ll li l lj lk">cbar_kws={'shrink':.9 },</span><span id="a3dc" class="lg ig hi lc b fi ll li l lj lk">ax=ax,</span><span id="c3dd" class="lg ig hi lc b fi ll li l lj lk">annot=True,</span><span id="ccc8" class="lg ig hi lc b fi ll li l lj lk">linewidths=0.1,vmax=1.0, linecolor='white',</span><span id="ee2a" class="lg ig hi lc b fi ll li l lj lk">annot_kws={'fontsize':12 }</span><span id="d470" class="lg ig hi lc b fi ll li l lj lk">)</span><span id="feb0" class="lg ig hi lc b fi ll li l lj lk">plt.title('Pearson Correlation of Features', y=1.05, size=15)</span><span id="39d1" class="lg ig hi lc b fi ll li l lj lk">correlation_map(DATA)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lm"><img src="../Images/9bbc3e2e3314adbadb7f3b496f299603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XefUJDQtyMlZhRc7"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Image by Author(Data Analysis)</figcaption></figure><p id="fe22" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">从图中可以明显看出，没有任何自变量彼此之间有太多的相关性。所以，变量的选择是有希望的。</p><p id="3c3b" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 2.3。数据清洗和数据预处理(特征缩放)</strong></p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="43ad" class="lg ig hi lc b fi lh li l lj lk">DATA.info()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ln"><img src="../Images/e63d24cbd69cdf66362d8fe91b60d337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/0*t00h9k4jTMK-9ObC"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Image by Author(Data Analysis)</figcaption></figure><p id="9c26" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">已经发现，用于分析的所有变量都不为空。因此，不需要进一步的数据清理，因为我们在将要用于分析的变量中没有任何“NaN”、“None”或空格。变量Gender是一个对象类型，所有其他变量都是int64类型。因此，为了支持使用变量进行分析，我使用了一个标签编码器将“性别”变量从object类型转换为int64类型。</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="7c5a" class="lg ig hi lc b fi lh li l lj lk">#assigning the independent variables,X and the dependent variable Y.</span><span id="f56e" class="lg ig hi lc b fi ll li l lj lk">X=DATA.iloc[:,1:6]</span><span id="8805" class="lg ig hi lc b fi ll li l lj lk">Y=DATA.iloc[:,0:1]</span><span id="cea2" class="lg ig hi lc b fi ll li l lj lk">Y</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lo"><img src="../Images/40ee0e50d853f769c1b5a5b8312a42d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/0*q1R7-aS70sKUi_j_"/></div></figure><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="3762" class="lg ig hi lc b fi lh li l lj lk">labelencoder=LabelEncoder()</span><span id="0668" class="lg ig hi lc b fi ll li l lj lk">Y=labelencoder.fit_transform(Y)</span><span id="0d53" class="lg ig hi lc b fi ll li l lj lk">pd.DataFrame(Y,columns= ['Gender_labelled'])</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lp"><img src="../Images/d3437e02fbf2369d2e2e7a1bb36e1c64.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/0*GQqRUetnyz0qgq4U"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Image by Author(Data Analysis)</figcaption></figure><p id="256c" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">因此，现在所有的变量都是int64类型。</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="2782" class="lg ig hi lc b fi lh li l lj lk">DATA.describe()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lq"><img src="../Images/ed3298616f8b1a019fcbc8fc85c8afbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jOuveE8cJqZOS58u"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Image by Author(Data Analysis)</figcaption></figure><p id="019e" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">图3给出了用于分析的变量的统计描述，它给出了关于数据的深刻见解。大部分变量都不是那么正态分布的，所以作为直觉的一面，逻辑回归可能是合适的，但是我们要检查所有的机器学习方法，确认。因此，我们有了干净的数据，变量也作为预处理进行了最小最大缩放。在最小最大值缩放中，数据以这样的方式缩放，其值通常在0和1之间。在标准缩放中，数据以均值为0、标准差为1的方式进行缩放。关于我们的数据集，MinMax是一个很好的选择，因为我们希望保留我们的零，因为它显示了一个类别。所以，我更喜欢在整体模型中使用它。然而，当我们转向机器学习模型时，我们使用MinMaxScaler和标准Scaler通过反复试验来检查模型。</p><p id="5ad2" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 2.4。首先通过直觉寻找最佳拟合的数据可视化</strong></p><p id="9a9b" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">这是通过使用X轴上的每个自变量和Y轴上的因变量来完成的。这基本上是为了了解什么可能是最合适的。</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="112c" class="lg ig hi lc b fi lh li l lj lk">plt.scatter(x=DATA["Stress_Score"],y=DATA["Gender"])</span><span id="f29e" class="lg ig hi lc b fi ll li l lj lk">plt.xlabel("Stress_Score")</span><span id="d472" class="lg ig hi lc b fi ll li l lj lk">plt.ylabel("Gender")</span><span id="821e" class="lg ig hi lc b fi ll li l lj lk">plt.title('scatter plot between Stress Score  and their Gender')</span><span id="ea59" class="lg ig hi lc b fi ll li l lj lk">plt.show</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/7a24afc3cad709808e248b00af3c8f1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/0*u2Dd7x26ZQs_sJLd"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Image by Author(Data Analysis)</figcaption></figure><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="4b24" class="lg ig hi lc b fi lh li l lj lk">plt.scatter(x=DATA["Anxiety_Score"],y=DATA["Gender"])</span><span id="6843" class="lg ig hi lc b fi ll li l lj lk">plt.xlabel("Anxiety_Score")</span><span id="a285" class="lg ig hi lc b fi ll li l lj lk">plt.ylabel("Gender")</span><span id="3037" class="lg ig hi lc b fi ll li l lj lk">plt.title('scatter plot between Anxiety Score  and their Gender')</span><span id="b985" class="lg ig hi lc b fi ll li l lj lk">plt.show</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/d79ef4db416d98881cf5a9e1366e9817.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/0*3cDemJZWp8E5W8mB"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Image by Author(Data Analysis)</figcaption></figure><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="f1b4" class="lg ig hi lc b fi lh li l lj lk">plt.scatter(x=DATA["No_of_children"],y=DATA["Gender"])</span><span id="a746" class="lg ig hi lc b fi ll li l lj lk">plt.xlabel("No_of_children")</span><span id="b122" class="lg ig hi lc b fi ll li l lj lk">plt.ylabel("Gender")</span><span id="61f5" class="lg ig hi lc b fi ll li l lj lk">plt.title('scatter plot between No_of_children they have and their Gender')</span><span id="55a2" class="lg ig hi lc b fi ll li l lj lk">plt.show</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/648fa6e56c108be54466c558c63f8b94.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/0*rswWwWO0ZO0ihbiN"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Image by Author(Data Analysis)</figcaption></figure><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="c2fd" class="lg ig hi lc b fi lh li l lj lk">plt.scatter(x=DATA["Age"],y=DATA["Gender"])</span><span id="9a8a" class="lg ig hi lc b fi ll li l lj lk">plt.xlabel("Age")</span><span id="553e" class="lg ig hi lc b fi ll li l lj lk">plt.ylabel("Gender")</span><span id="6c2e" class="lg ig hi lc b fi ll li l lj lk">plt.title('scatter plot between Age and Gender')</span><span id="881f" class="lg ig hi lc b fi ll li l lj lk">plt.show</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/3dbd52dfae63b9f6189e47ce7f4fad61.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/0*Kb55KREkMdhNzeXD"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Image by Author(Data Analysis)</figcaption></figure><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="1e7a" class="lg ig hi lc b fi lh li l lj lk">plt.scatter(x=DATA["Usual Hrs involved in teaching"],y=DATA["Gender"])</span><span id="c9f3" class="lg ig hi lc b fi ll li l lj lk">plt.xlabel("Usual Hrs involved in teaching")</span><span id="b7ac" class="lg ig hi lc b fi ll li l lj lk">plt.ylabel("Gender")</span><span id="7ae2" class="lg ig hi lc b fi ll li l lj lk">plt.title('scatter plot between Usual Hrs involved in teaching and their Gender')</span><span id="77cc" class="lg ig hi lc b fi ll li l lj lk">plt.show</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lt"><img src="../Images/a80d63c74266bbecce5256915a0855c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/0*u_NjCoL1ZOkmdgVs"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Image by Author(Data Analysis)</figcaption></figure><p id="3c54" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">毫无疑问，因为我们的因变量“性别”是一个二元变量，所以所有的图都以这样一种方式显示，它们都促进了一条sigmoid曲线来拟合它们。此外，我们知道，对于分类问题，拟合一条类似s形曲线的非线性曲线应该很好。我们将会看到这个思考过程是对还是错。</p><p id="0a79" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 2.5。将用于分析的变量存储到相关数据帧</strong></p><p id="dae3" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">数据已经被清理和预处理。因此我们有-</p><p id="8aa4" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj">因变量</strong> 🡪学校教师的性别</p><p id="552d" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj">自变量</strong> 🡪 x1。感知压力水平</p><p id="6288" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">x2。感知焦虑水平</p><p id="8c9d" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">x3。学校老师的年龄</p><p id="e969" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">x4。他们授课的小时数</p><p id="aead" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">x5。他们有几个孩子(几个孩子的母亲/父亲)</p><p id="3d7f" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">因变量和自变量都是非空的，类型为int64。现在，因变量被存入变量a的数据框，而自变量Y被存入x。</p><p id="4c7f" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 2.6。将数据集分成训练集和测试集。</strong></p><p id="f44d" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">数据被分成训练和测试，以执行数据的测试和训练。X被分成X_train和X_test，Y被分成Y_train和Y_test，因为测试大小为整个数据的20%,而训练大小是数据的剩余80%。下面，我们可以看到分割X和Y后得到的X_train，Y_train，X_test，Y_test的大小。</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="75e3" class="lg ig hi lc b fi lh li l lj lk">from sklearn.model_selection import train_test_split</span><span id="c9cf" class="lg ig hi lc b fi ll li l lj lk">#I'm giving a 20% for testing</span><span id="fee8" class="lg ig hi lc b fi ll li l lj lk">X_scale=MinMaxScaler().fit_transform(X)</span><span id="a69f" class="lg ig hi lc b fi ll li l lj lk">X_train, X_test, Y_train, Y_test =train_test_split(X_scale,Y,test_size=0.2,random_state=0)</span><span id="5962" class="lg ig hi lc b fi ll li l lj lk">n=X_train.shape</span><span id="5cac" class="lg ig hi lc b fi ll li l lj lk">print("shape of X_train is:-",n)</span><span id="9aa4" class="lg ig hi lc b fi ll li l lj lk">r=X_test.shape</span><span id="4910" class="lg ig hi lc b fi ll li l lj lk">print("shape of X_test is:-",r)</span><span id="78f4" class="lg ig hi lc b fi ll li l lj lk">k=Y_train.shape</span><span id="c50d" class="lg ig hi lc b fi ll li l lj lk">print("shape of Y_train is:-",k)</span><span id="dada" class="lg ig hi lc b fi ll li l lj lk">f=Y_test.shape</span><span id="23bf" class="lg ig hi lc b fi ll li l lj lk">print("shape of Y_test is:-",f)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/3c705ebe2efc7dcbfe25565f0dcab3cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/0*dXjc0EYNyQH9XFg1"/></div><figcaption class="jp jq et er es jr js bd b be z dx">By Author(Data Analysis)</figcaption></figure><p id="d1f4" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 3。方法学&amp;机器学习模型的分析(注:最后完成所有模型的误差分析)</strong></p><p id="5129" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 3.1。线性回归分析</strong></p><p id="578c" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> <em class="lv"> 3.1.1 .方法论</em> </strong></p><p id="434b" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">为了进行线性回归，使用scikit-learn/sklearn库的线性模型中的linear regression包。基本上，线性回归分别为x1、x2、x3、x4、x5拟合具有诸如β1、β2、β3、β4、β5的系数的线性模型，以最小化误差的残差平方和。(误差是数据集中观察到的目标与线性近似预测的目标之间的差异。).因此，将线性回归模型定义为线性回归函数。然后将X_train和Y_train值拟合到线性回归模型上。然后使用定义的模型预测相应的X测试值的Y值，之后确定其准确性。</p><p id="e10e" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> <em class="lv"> 3.1.2 .代码</em> </strong></p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="32d1" class="lg ig hi lc b fi lh li l lj lk">from sklearn.linear_model import LinearRegression</span><span id="6015" class="lg ig hi lc b fi ll li l lj lk">from sklearn.metrics import mean_squared_error, r2_score</span><span id="48c4" class="lg ig hi lc b fi ll li l lj lk">from sklearn import preprocessing</span><span id="a5ef" class="lg ig hi lc b fi ll li l lj lk">#now linear regression</span><span id="50f2" class="lg ig hi lc b fi ll li l lj lk">reg=LinearRegression()</span><span id="d6ab" class="lg ig hi lc b fi ll li l lj lk">reg.fit(X_train,Y_train)</span><span id="44aa" class="lg ig hi lc b fi ll li l lj lk">#for getting our predicted Y value from X_test value,the following code</span><span id="ba6b" class="lg ig hi lc b fi ll li l lj lk">Y_pred=reg.predict(X_test)</span><span id="794f" class="lg ig hi lc b fi ll li l lj lk">#the cofficient of x or the slope of the line</span><span id="64ed" class="lg ig hi lc b fi ll li l lj lk">print("Linear Regression Analysis results")</span><span id="571f" class="lg ig hi lc b fi ll li l lj lk">print("----------------------------------")</span><span id="f3e3" class="lg ig hi lc b fi ll li l lj lk">coefficient=reg.coef_</span><span id="4257" class="lg ig hi lc b fi ll li l lj lk">print("The coefficients for x1,x2,x3,x4 &amp; x4 are",coefficient)</span><span id="8528" class="lg ig hi lc b fi ll li l lj lk">#then we need to find the R squared score and mean squared error for checking how good the model is</span><span id="ff53" class="lg ig hi lc b fi ll li l lj lk">r2_value=r2_score(Y_test,Y_pred)</span><span id="0f34" class="lg ig hi lc b fi ll li l lj lk">MeanSquareError=mean_squared_error(Y_test,Y_pred)</span><span id="6b56" class="lg ig hi lc b fi ll li l lj lk">print("R squared Value of the model is",r2_value)</span><span id="636e" class="lg ig hi lc b fi ll li l lj lk">print('Accuracy on the Test set in (%) is ', round(reg.score(X_test, Y_test)*100, 2))</span><span id="4f47" class="lg ig hi lc b fi ll li l lj lk">print("Mean Squared Error of the model is",MeanSquareError)</span></pre><p id="36d3" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> <em class="lv"> 3.1.3 .结果&amp;线性回归分析</em> </strong></p><p id="364c" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj">图6 </strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/a5a35f674a7baf305ffbafeaf8c6bee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7pyxX1ViD_Qj5K-4"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Image by Author(Data Analysis)</figcaption></figure><p id="49ac" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">精确度很差(精确度=20.14)，这是意料之中的，因为在我们的例子中，Y变量是一个二元变量，因此拟合一条线是一个坏主意。所以，在这个数据中由于二元因变量，线性回归表现不好。因此，线性回归不适用于这个数据，这也是我们在看到自变量对因变量的曲线图时的第一个观察结果。因为y是二进制的，所以所有的图看起来肯定像是sigmoid曲线可以是最佳拟合的图，但是为了清楚每个变量，以不同的方式显示也是很重要的。因此，我们可以离开线性回归，看看有什么进一步的ML方法来完美地拟合数据。</p><p id="9a1c" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 3.2。逻辑回归分析</strong></p><p id="bda4" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 3.2.1。<em class="lv">方法论</em> </strong></p><p id="ef44" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">使用scikit-learn库的线性模型中的Logistic regression包进行逻辑回归。基本上，逻辑回归拟合具有某些系数的s形曲线。因此，将逻辑回归模型定义为逻辑回归函数。注意，在使用这个LogisticRegression函数<strong class="jw hj">时，默认情况下应用正则化。</strong>此外，默认情况下会应用“l2”惩罚。然后将X_train和Y_train值拟合到逻辑回归模型中，其中“lbfgs”解算器用作默认值。然后使用定义的模型预测相应的X测试值的Y值，之后确定其准确性。</p><p id="5641" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 3.2.2代码</strong></p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="2415" class="lg ig hi lc b fi lh li l lj lk">from sklearn.linear_model import LogisticRegression</span><span id="0df1" class="lg ig hi lc b fi ll li l lj lk">from sklearn.metrics import classification_report, confusion_matrix, accuracy_score</span><span id="3e1f" class="lg ig hi lc b fi ll li l lj lk"># we can also add penalty</span><span id="d237" class="lg ig hi lc b fi ll li l lj lk">LR = LogisticRegression(random_state=0, solver = 'lbfgs')</span><span id="6a4a" class="lg ig hi lc b fi ll li l lj lk">LR.fit(X_train, Y_train)</span><span id="3c31" class="lg ig hi lc b fi ll li l lj lk">Y_pred = LR.predict(X_test)</span><span id="fc3e" class="lg ig hi lc b fi ll li l lj lk">print("Logistic Regression Analysis results")</span><span id="e2ae" class="lg ig hi lc b fi ll li l lj lk">print("----------------------------------")</span><span id="68b4" class="lg ig hi lc b fi ll li l lj lk">print('The Coefficients for the model are \n', LR.coef_)</span><span id="8a53" class="lg ig hi lc b fi ll li l lj lk">print('Accuracy in(%) on Test set is', round(LR.score(X_test, Y_test)*100, 2))</span><span id="81e4" class="lg ig hi lc b fi ll li l lj lk">result = confusion_matrix(Y_test, Y_pred)</span><span id="18cf" class="lg ig hi lc b fi ll li l lj lk">print("Confusion Matrix:")</span><span id="5eb1" class="lg ig hi lc b fi ll li l lj lk">print(result)</span><span id="d269" class="lg ig hi lc b fi ll li l lj lk">result1 = classification_report(Y_test, Y_pred)</span><span id="04b4" class="lg ig hi lc b fi ll li l lj lk">print("\nClassification Report:")</span><span id="07b8" class="lg ig hi lc b fi ll li l lj lk">print (result1)</span><span id="aaf8" class="lg ig hi lc b fi ll li l lj lk">result2 = accuracy_score(Y_test, Y_pred)</span><span id="db1a" class="lg ig hi lc b fi ll li l lj lk">print("\nAccuracy:",result2)</span><span id="1ded" class="lg ig hi lc b fi ll li l lj lk">#confusion matrix</span><span id="64eb" class="lg ig hi lc b fi ll li l lj lk">labels = sorted(DATA['Gender'].unique())</span><span id="3a37" class="lg ig hi lc b fi ll li l lj lk">sns.heatmap(</span><span id="e561" class="lg ig hi lc b fi ll li l lj lk">confusion_matrix(Y_test, Y_pred),</span><span id="eb17" class="lg ig hi lc b fi ll li l lj lk">annot=True,</span><span id="78cc" class="lg ig hi lc b fi ll li l lj lk">xticklabels=labels,</span><span id="d86d" class="lg ig hi lc b fi ll li l lj lk">yticklabels=labels</span><span id="ef08" class="lg ig hi lc b fi ll li l lj lk">)</span></pre><p id="2649" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj">3 . 2 . 3<em class="lv">。结果&amp;Logistic回归分析</em> </strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/7dbe540c53cf253e83002c4f8fbc4b34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sfwqwrGxuATpWLh-"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/9509a7943e8d53a8cfcc4e36e84e4c3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9Jywws2xlfnn-iKe"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/3c3ef0702c0dd01a47f7da86d662f331.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/0*jpKFT54_nP4cJ2h6"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Images by Author(Data Analysis)</figcaption></figure><p id="e5b8" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">很明显，逻辑回归是一个很好的预测模型，可以使用感受到的压力水平、焦虑水平、年龄、孩子数量、他们在教学中花费的时间来预测学校教师的性别，因为测试准确度为83.33，远远好于线性回归。其中86名女性被预测为女性，9名男性被预测为男性。所以，这个模型非常好，非常准确。</p><p id="c622" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 3.3。支持向量机分析</strong></p><p id="30e7" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 3.3.1。<em class="lv">方法论</em>方法论</strong></p><p id="4c51" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">SVM算法通常使用将输入数据空间转换为适当格式的内核来实现。SVM采用了一种被称为内核技巧的方法，在这种方法中，内核将低维输入空间转换为高维空间。内核，用基本的术语来说，通过给不可分离的问题增加更多的维度，使它们变得可分离。我使用的SVC函数就是支持向量分类。我试验了四种内核，如“线性”、“多边形”、“rbf”和“sigmoid”。然后将X_train和Y_train值拟合到支持向量分类器模型上。然后使用定义的模型预测相应的X测试值的Y值，之后确定其准确性。</p><p id="b1ce" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 3.3.2。代码</strong></p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="38bf" class="lg ig hi lc b fi lh li l lj lk">from sklearn.svm import SVC</span><span id="e6d2" class="lg ig hi lc b fi ll li l lj lk">svm = SVC(kernel = 'linear',gamma = 'scale')</span><span id="4fcd" class="lg ig hi lc b fi ll li l lj lk">svm.fit(X_train, Y_train)</span><span id="4637" class="lg ig hi lc b fi ll li l lj lk">Y_pred = svm.predict(X_test)</span><span id="4037" class="lg ig hi lc b fi ll li l lj lk">print('Results of Support Vector Classification')</span><span id="7102" class="lg ig hi lc b fi ll li l lj lk">print('----------------------------------------')</span><span id="f4b2" class="lg ig hi lc b fi ll li l lj lk">print('Accuracy on Test with linear kernal: ', round(svm.score(X_test, Y_test)*100, 2))</span><span id="e579" class="lg ig hi lc b fi ll li l lj lk">svm = SVC(kernel = 'poly',gamma = 'scale')</span><span id="50e2" class="lg ig hi lc b fi ll li l lj lk">svm.fit(X_train, Y_train)</span><span id="819c" class="lg ig hi lc b fi ll li l lj lk">Y_pred = svm.predict(X_test)</span><span id="850e" class="lg ig hi lc b fi ll li l lj lk">print('Accuracy on Test with polynomial kernal: ', round(svm.score(X_test, Y_test)*100, 2))</span><span id="810d" class="lg ig hi lc b fi ll li l lj lk">svm = SVC(kernel = 'rbf',gamma = 'scale')</span><span id="4b7b" class="lg ig hi lc b fi ll li l lj lk">svm.fit(X_train, Y_train)</span><span id="c2a6" class="lg ig hi lc b fi ll li l lj lk">Y_pred = svm.predict(X_test)</span><span id="4837" class="lg ig hi lc b fi ll li l lj lk">print('Accuracy on Test with rbf kernal: ', round(svm.score(X_test, Y_test)*100, 2))</span><span id="bb57" class="lg ig hi lc b fi ll li l lj lk">svm = SVC(kernel = 'sigmoid',gamma = 'scale')</span><span id="1256" class="lg ig hi lc b fi ll li l lj lk">svm.fit(X_train, Y_train)</span><span id="a366" class="lg ig hi lc b fi ll li l lj lk">Y_pred = svm.predict(X_test)</span><span id="a4d1" class="lg ig hi lc b fi ll li l lj lk">print('Accuracy on Test with sigmoid kernal: ', round(svm.score(X_test, Y_test)*100, 2))</span></pre><p id="31d6" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj">3 . 3 . 3<em class="lv">。结果&amp;支持向量机</em>分析</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/6f04177d36c27a4fc6a581a1ca8cdd52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KyyRpEgpf4UWyCh9"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Image by Author( Data Analysis)</figcaption></figure><p id="7ac0" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">以上是支持向量分类的结果。我尝试了四种不同的内核，上面给出了每个内核的精度。因此，支持向量分类模型在“rbf”(径向基函数)核中工作得最好，测试准确率为82.41 %。因此，rbf核在模型中表现良好。因此，以下是具有rbf核的支持向量分类模型的混淆矩阵和分类报告，其显示了相当好的迹象，因为85个雌性被正确预测，11个雄性被正确预测。</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="613c" class="lg ig hi lc b fi lh li l lj lk"># Confusion Matrix</span><span id="fefc" class="lg ig hi lc b fi ll li l lj lk">result = confusion_matrix(Y_test, Y_pred)</span><span id="296f" class="lg ig hi lc b fi ll li l lj lk">print("Confusion Matrix of the model with rbf kernal:")</span><span id="07ba" class="lg ig hi lc b fi ll li l lj lk">print(result)</span><span id="9115" class="lg ig hi lc b fi ll li l lj lk"># Classification report</span><span id="76d6" class="lg ig hi lc b fi ll li l lj lk">result1 = classification_report(Y_test, Y_pred)</span><span id="d75c" class="lg ig hi lc b fi ll li l lj lk">print("\nClassification Report of the model with rbf kernal:")</span><span id="443c" class="lg ig hi lc b fi ll li l lj lk">print (result1)</span><span id="651a" class="lg ig hi lc b fi ll li l lj lk"># Accuracy score</span><span id="cb16" class="lg ig hi lc b fi ll li l lj lk">result2 = accuracy_score(Y_test, Y_pred)</span><span id="388a" class="lg ig hi lc b fi ll li l lj lk">print("\nAccuracy:",result2)</span><span id="7170" class="lg ig hi lc b fi ll li l lj lk">#confusion matrix</span><span id="15b2" class="lg ig hi lc b fi ll li l lj lk"># Transform to df for easier plotting</span><span id="472a" class="lg ig hi lc b fi ll li l lj lk">outcome_labels = sorted(DATA['Gender'].unique())</span><span id="28ce" class="lg ig hi lc b fi ll li l lj lk">sns.heatmap(</span><span id="b7b8" class="lg ig hi lc b fi ll li l lj lk">confusion_matrix(Y_test, Y_pred),</span><span id="5feb" class="lg ig hi lc b fi ll li l lj lk">annot=True,</span><span id="e9f4" class="lg ig hi lc b fi ll li l lj lk">xticklabels=outcome_labels,</span><span id="a5a1" class="lg ig hi lc b fi ll li l lj lk">yticklabels=outcome_labels</span><span id="5eeb" class="lg ig hi lc b fi ll li l lj lk">)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/fd4c577703892b70dcfb102ef4442613.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nHVzSXlNMi0DqHz6"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/3e859a881326f0dd9751fc3846e3e92c.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/0*x2j9wFJtzpOtHV5w"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Images by Author(Data Analysis)</figcaption></figure><p id="c751" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">核函数用于将原始数据集(线性/非线性)映射到更高维的空间，以便将其转换成线性数据集。使用rbf核的SVM工作良好，因为它允许SVM变得非线性而不是线性。85名女性被预测为女性，11名男性被预测为男性。准确率和召回率也不错。</p><p id="ca3b" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 3.4。随机森林分类器</strong></p><p id="8c4b" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 3.4.1。<em class="lv">方法论</em> </strong></p><p id="f434" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">使用随机森林分类器，因为我们的上下文是一个分类问题。从sklearn库中，使用具有RandomForestClassifier的集成包。随机森林是一种元估计器，它使用平均来提高预测准确性，并通过在给定数据集的各个子样本上拟合多个决策树分类器来控制过拟合。RandomForestClassifier函数被定义为具有“n_estimators”的模型，森林中的树的数量被定义为50。然后将X_train和Y_train值拟合到模型中，并预测相应X测试值的Y值。</p><p id="a245" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">3.4.2。代码</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="c1bd" class="lg ig hi lc b fi lh li l lj lk">from sklearn.ensemble import RandomForestClassifier</span><span id="19eb" class="lg ig hi lc b fi ll li l lj lk">RandomForest = RandomForestClassifier(n_estimators = 50)</span><span id="ad67" class="lg ig hi lc b fi ll li l lj lk">RandomForest.fit(X_train, Y_train)</span><span id="dad4" class="lg ig hi lc b fi ll li l lj lk">Y_pred = RandomForest.predict(X_test)</span><span id="24d3" class="lg ig hi lc b fi ll li l lj lk">print("Result of Random Forest classifier model")</span><span id="bbea" class="lg ig hi lc b fi ll li l lj lk">print("----------------------------------")</span><span id="6657" class="lg ig hi lc b fi ll li l lj lk">result2 = accuracy_score(Y_test, Y_pred)</span><span id="ce10" class="lg ig hi lc b fi ll li l lj lk">print("\nAccuracy of the Random Forest Classifier:",result2)</span><span id="c7ab" class="lg ig hi lc b fi ll li l lj lk">result = confusion_matrix(Y_test, Y_pred)</span><span id="29ab" class="lg ig hi lc b fi ll li l lj lk">print("Confusion Matrix:")</span><span id="fbaf" class="lg ig hi lc b fi ll li l lj lk">print(result)</span><span id="927f" class="lg ig hi lc b fi ll li l lj lk"># Classification report</span><span id="e805" class="lg ig hi lc b fi ll li l lj lk">result1 = classification_report(Y_test, Y_pred)</span><span id="15e7" class="lg ig hi lc b fi ll li l lj lk">print("\nClassification Report:")</span><span id="6250" class="lg ig hi lc b fi ll li l lj lk">print (result1)</span><span id="4b7d" class="lg ig hi lc b fi ll li l lj lk">#confusion matrix</span><span id="c5b1" class="lg ig hi lc b fi ll li l lj lk"># Transform to df for easier plotting</span><span id="05d1" class="lg ig hi lc b fi ll li l lj lk">print("Confusion matrix_random forest classifier")</span><span id="5a47" class="lg ig hi lc b fi ll li l lj lk">labels = sorted(DATA['Gender'].unique()</span><span id="ec7c" class="lg ig hi lc b fi ll li l lj lk">sns.heatmap(</span><span id="17ee" class="lg ig hi lc b fi ll li l lj lk">confusion_matrix(Y_test, Y_pred),</span><span id="6e27" class="lg ig hi lc b fi ll li l lj lk">annot=True,</span><span id="1900" class="lg ig hi lc b fi ll li l lj lk">xticklabels=labels,</span><span id="b957" class="lg ig hi lc b fi ll li l lj lk">yticklabels=labels</span><span id="e7e3" class="lg ig hi lc b fi ll li l lj lk">)</span></pre><p id="d7f2" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 3.4.3。<em class="lv">结果&amp;随机森林分类器</em>分析</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/41cc54fd03bfca7e13d853b4d994e978.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Hs3l7fAfQaOeYfxT"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/1483445f236e9190b6f143a3e83046fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/0*JSo4N40oBJyTaffd"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Images by Author</figcaption></figure><p id="f024" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">随机森林分类器也表现良好，准确率达80%。因此，具有50个决策树的随机森林分类器在相关数据集中表现良好。我们知道随机森林算法是一个集合算法，它基本上是几个决策树算法的组合。从训练集的随机选择的子集，随机森林分类器生成一组决策树。这就是为什么它给出了更准确的结果，这是分类问题中最受欢迎的技术。(我还尝试对数据进行随机森林回归，它给出了非常差的测试精度，甚至低于线性回归给出的精度。)</p><p id="d9e7" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 3.5。安</strong></p><p id="04d9" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 3.5.1 .人工神经网络方法</strong></p><p id="5b64" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">人工神经网络也称为ann，是基于相互连接的神经元/节点工作的网络。输入通过权重和偏差与节点进行转换。这里，Keras用于构建神经网络。</p><p id="8c08" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">在Keras的模型包中，“顺序”用于构建模型。已经进行了几次试验和错误，但是最终的模型正在报告中展示。X_train和Y_train值使用最小最大缩放器进行缩放。对于“顺序”最终模型，有四层，其中一层是输入层，两层是隐藏层，一层是输出层。对于输入层，对权重的初始化进行了统一的内核初始化。那么输入维度是5，因为我们有5个独立变量/特征。使用“relu”激活函数将输入层中的节点数设置为32，以避免sigmoid(如果存在)的消失梯度问题。两个隐含层的节点数分别为16个和8个，都具有“relu”激活功能。最后一层用1作为节点数和“tanh”激活函数。神经网络模型然后以“adam”作为优化器(它被发现工作良好)，以“二进制交叉熵”作为损失函数(因为我们的输出变量具有值1和0。然后，通过以0.2的验证分割运行40个时期，用X_train和Y_train值来拟合模型(我们放入模型进行测试的一部分数据)。因此，整个数据的20%用于测试我们放入模型的数据。通过用X_test预测Y值，用X_test和Y_test检验准确性。</p><p id="bb84" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"><em class="lv">3.5.1.1在做安时的一些观察包括:- </em> </strong></p><ul class=""><li id="0d85" class="ju jv hi jw b jx jy jz ka kb kc kd ke kf kg kh ly kj kk kl bi translated">已经进行了大量的反复试验，并且还发现标准缩放对于人工神经网络没有给出好的结果，但是最小-最大缩放给出了好的结果。所以，这基本上是有利于数据集的。</li><li id="c3ba" class="ju jv hi jw b jx lz jz ma kb mb kd mc kf md kh ly kj kk kl bi translated">此外，还发现，与在所有层中放置相同数量的节点的正常过程相比，以降序方式给出节点的数量，在输入层中开始最高，在最后层中最低，给出了良好的准确性。这可能是因为将节点数量按降序排列起到了一种退出正则化机制的作用。因此，为此，我尝试使用Keras中的dropout函数在一些层中进行Dropout。但是，人们发现这两种特性结合在一起的效果并不好。因此，我只是将各层的节点按降序排列，作为提高模型准确性的策略。</li><li id="d0b2" class="ju jv hi jw b jx lz jz ma kb mb kd mc kf md kh ly kj kk kl bi translated">此外，发现tanh比sigmoid激活功能工作得更好。这是可能的，因为双曲正切函数关于原点是对称的(我们知道双曲正切的范围是从-1到+1，而sigmoid的范围是从0到1)，其中输入是归一化的，并且更可能产生基本上是下一层输入的输出，并且平均接近于零。因此，tanh被用于神经网络。</li></ul><p id="ade3" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 3.5.2。代码</strong></p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="da46" class="lg ig hi lc b fi lh li l lj lk">from keras.models import Sequential</span><span id="efec" class="lg ig hi lc b fi ll li l lj lk">from keras.layers import Dense</span><span id="eca0" class="lg ig hi lc b fi ll li l lj lk">from keras.utils import to_categorical</span><span id="d1b5" class="lg ig hi lc b fi ll li l lj lk">import tensorflow as tf</span><span id="4678" class="lg ig hi lc b fi ll li l lj lk">from keras.layers import Dropout</span><span id="5707" class="lg ig hi lc b fi ll li l lj lk">def plot_training_history(history, model):</span><span id="1f74" class="lg ig hi lc b fi ll li l lj lk">figure = plt.figure()</span><span id="7d27" class="lg ig hi lc b fi ll li l lj lk">plt.subplot(1, 2, 1)</span><span id="8aa7" class="lg ig hi lc b fi ll li l lj lk">plt.plot(history.history['accuracy'])</span><span id="92eb" class="lg ig hi lc b fi ll li l lj lk">plt.plot(history.history['val_accuracy'])</span><span id="f430" class="lg ig hi lc b fi ll li l lj lk">plt.title('model accuracy')</span><span id="3359" class="lg ig hi lc b fi ll li l lj lk">plt.ylabel('accuracy')</span><span id="a7f9" class="lg ig hi lc b fi ll li l lj lk">plt.xlabel('epoch')</span><span id="9454" class="lg ig hi lc b fi ll li l lj lk">plt.legend(['training', 'validation'], loc='best')</span><span id="e656" class="lg ig hi lc b fi ll li l lj lk">plt.tight_layout()</span><span id="819b" class="lg ig hi lc b fi ll li l lj lk">plt.subplot(1, 2, 2)</span><span id="dcb0" class="lg ig hi lc b fi ll li l lj lk">plt.plot(history.history['loss'])</span><span id="d2f2" class="lg ig hi lc b fi ll li l lj lk">plt.plot(history.history['val_loss'])</span><span id="6d99" class="lg ig hi lc b fi ll li l lj lk">plt.title('model loss')</span><span id="3507" class="lg ig hi lc b fi ll li l lj lk">plt.ylabel('loss')</span><span id="2d23" class="lg ig hi lc b fi ll li l lj lk">plt.xlabel('epoch')</span><span id="be27" class="lg ig hi lc b fi ll li l lj lk">plt.legend(['training', 'validation'], loc='best')</span><span id="da3a" class="lg ig hi lc b fi ll li l lj lk">plt.tight_layout()</span><span id="1e76" class="lg ig hi lc b fi ll li l lj lk">figure.tight_layout()</span><span id="d6d7" class="lg ig hi lc b fi ll li l lj lk">plt.show()</span><span id="340e" class="lg ig hi lc b fi ll li l lj lk">loss, accuracy  = model.evaluate(X_test, Y_test, verbose=False)</span><span id="0d93" class="lg ig hi lc b fi ll li l lj lk">print(f'Test loss: {loss:.3}')</span><span id="a0f3" class="lg ig hi lc b fi ll li l lj lk">print(f'Test accuracy: {accuracy:.3}')</span></pre><p id="893b" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">模型结构</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="ce05" class="lg ig hi lc b fi lh li l lj lk">model_first_part1 = Sequential()</span><span id="6a91" class="lg ig hi lc b fi ll li l lj lk">#if needed change kernal</span><span id="8f95" class="lg ig hi lc b fi ll li l lj lk">model_first_part1.add(Dense(units=32,kernel_initializer='uniform', activation='relu', input_dim=5))</span><span id="00c7" class="lg ig hi lc b fi ll li l lj lk">model_first_part1.add(Dense(units=16, activation='relu'))</span><span id="97fe" class="lg ig hi lc b fi ll li l lj lk">model_first_part1.add(Dense(units=8, activation='relu'))</span><span id="9820" class="lg ig hi lc b fi ll li l lj lk">model_first_part1.add(Dense(units=1, activation='tanh'))</span><span id="2981" class="lg ig hi lc b fi ll li l lj lk">#compiling the model</span><span id="ddc1" class="lg ig hi lc b fi ll li l lj lk">model_first_part1.compile(optimizer="adam", loss='binary_crossentropy', metrics=['accuracy'])</span><span id="b74a" class="lg ig hi lc b fi ll li l lj lk">#lets see how much time epochs take and the model accuracy and everthing, for that lets do fitting</span><span id="abdf" class="lg ig hi lc b fi ll li l lj lk">history_first_part1 = model_first_part1.fit(X_scale, Y, epochs=40, verbose=True,shuffle=True,validation_split=.2)</span><span id="352e" class="lg ig hi lc b fi ll li l lj lk">plot_training_history(history_first_part1,model_first_part1)</span></pre><p id="0c39" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">追求结果</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="4209" class="lg ig hi lc b fi lh li l lj lk"># Transform to df for easier plotting</span><span id="844c" class="lg ig hi lc b fi ll li l lj lk">pred=model_first_part1.predict_classes(X_test)</span><span id="6741" class="lg ig hi lc b fi ll li l lj lk">print("Confusion matrix_ANN")</span><span id="dd12" class="lg ig hi lc b fi ll li l lj lk">labels = sorted(DATA['Gender'].unique())</span><span id="3e63" class="lg ig hi lc b fi ll li l lj lk">sns.heatmap(</span><span id="8980" class="lg ig hi lc b fi ll li l lj lk">confusion_matrix(Y_test, pred),</span><span id="3d44" class="lg ig hi lc b fi ll li l lj lk">annot=True,</span><span id="3dcd" class="lg ig hi lc b fi ll li l lj lk">xticklabels=labels,</span><span id="213b" class="lg ig hi lc b fi ll li l lj lk">yticklabels=labels</span><span id="35c9" class="lg ig hi lc b fi ll li l lj lk">)<br/>print("Result of ANN")</span><span id="345f" class="lg ig hi lc b fi ll li l lj lk">print("-------------------------------------------------")</span><span id="ff34" class="lg ig hi lc b fi ll li l lj lk">result2 = accuracy_score(Y_test, pred)</span><span id="bc34" class="lg ig hi lc b fi ll li l lj lk">print("\nAccuracy of the ANN:",result2)</span><span id="15f0" class="lg ig hi lc b fi ll li l lj lk">print("")</span><span id="1bba" class="lg ig hi lc b fi ll li l lj lk">result = confusion_matrix(Y_test, pred)</span><span id="3c07" class="lg ig hi lc b fi ll li l lj lk">print("Confusion Matrix:")</span><span id="fc79" class="lg ig hi lc b fi ll li l lj lk">print(result)</span><span id="3632" class="lg ig hi lc b fi ll li l lj lk">print("")</span><span id="740c" class="lg ig hi lc b fi ll li l lj lk">print("Classification report of ANN")</span><span id="3609" class="lg ig hi lc b fi ll li l lj lk">print(classification_report(Y_test,pred))</span></pre><p id="2d79" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> <em class="lv"> 3.5.3。结果&amp;安</em>分析</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es me"><img src="../Images/ef3aa8fe037a0a95c8a91a1b2080b514.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/0*MkGVuHVCvsqT0zMJ"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/df7854cb2f37da050e7b7f0834735116.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/0*ydzlQJGbRyaagLAW"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Images by Author</figcaption></figure><p id="451b" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">在人工神经网络中，它也给出了76.9%的良好测试精度。所以，这是一个很好的预测模型。这是多次试验和错误的结果，在混淆矩阵中，我们可以看到在测试集中有79只雌性和7只雄性被正确预测，这是一个好现象。最小-最大缩放的使用也产生了良好的准确性。</p><p id="cc7b" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">3.6。无监督学习(K均值聚类)</p><p id="619c" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 3.6.1。<em class="lv">方法论</em> </strong></p><p id="2871" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">对于无监督学习分析，我在这里使用了KMeans聚类。同样的，来自sklearn。集群，则使用KMeans函数。对于聚类，聚类的数目被定义为2，因为我们需要用给定的五个特征将其聚类为男性或女性。然后用特征拟合模型，并打印标签或预测标签。通过检查原始的Y标签，已经找出了准确性。然而，正如通常所说的那样，预期Kmeans聚类给出的准确度较低。</p><p id="cf51" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">3.6.2。代码</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="da8b" class="lg ig hi lc b fi lh li l lj lk">from sklearn.cluster import KMeans</span><span id="061b" class="lg ig hi lc b fi ll li l lj lk">model = KMeans(n_clusters=2,random_state=1)</span><span id="f63d" class="lg ig hi lc b fi ll li l lj lk">model.fit(X_train,Y_train)</span><span id="82eb" class="lg ig hi lc b fi ll li l lj lk">model.cluster_centers_</span><span id="05ba" class="lg ig hi lc b fi ll li l lj lk">K=model.labels_</span><span id="661c" class="lg ig hi lc b fi ll li l lj lk">pred=model.predict(X_test)</span><span id="a93d" class="lg ig hi lc b fi ll li l lj lk">print("Result of KMeans Clustering Model with 2 clusters")</span><span id="67fc" class="lg ig hi lc b fi ll li l lj lk">print("-------------------------------------------------")</span><span id="2105" class="lg ig hi lc b fi ll li l lj lk">result2 = accuracy_score(Y_test, pred)</span><span id="c181" class="lg ig hi lc b fi ll li l lj lk">print("\nAccuracy of the KMeans Clutering Model:",result2)</span><span id="ed21" class="lg ig hi lc b fi ll li l lj lk">print("")</span><span id="867c" class="lg ig hi lc b fi ll li l lj lk">result = confusion_matrix(Y_test, pred)</span><span id="2346" class="lg ig hi lc b fi ll li l lj lk">print("Confusion Matrix:")</span><span id="fce0" class="lg ig hi lc b fi ll li l lj lk">print(result)</span><span id="2e7b" class="lg ig hi lc b fi ll li l lj lk">print("")</span><span id="9ebc" class="lg ig hi lc b fi ll li l lj lk">print("Classification report of KMeans Clutering Model")</span><span id="a3a9" class="lg ig hi lc b fi ll li l lj lk">print(classification_report(Y_test,pred))</span><span id="72d4" class="lg ig hi lc b fi ll li l lj lk">print("Confusion matrix_unsupervised learning")</span><span id="2239" class="lg ig hi lc b fi ll li l lj lk">import seaborn as sns</span><span id="c5e9" class="lg ig hi lc b fi ll li l lj lk">labels = sorted(DATA['Gender'].unique())</span><span id="10dc" class="lg ig hi lc b fi ll li l lj lk">sns.heatmap(</span><span id="9cab" class="lg ig hi lc b fi ll li l lj lk">confusion_matrix(Y_test, pred),</span><span id="f3b7" class="lg ig hi lc b fi ll li l lj lk">annot=True,</span><span id="e094" class="lg ig hi lc b fi ll li l lj lk">xticklabels=labels,</span><span id="d668" class="lg ig hi lc b fi ll li l lj lk">yticklabels=labels</span><span id="0c7b" class="lg ig hi lc b fi ll li l lj lk">)</span></pre><p id="ae49" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 3.6.3。<em class="lv">结果&amp;无监督学习的分析(KMeans聚类)</em> </strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/d4876be27379970787433b63f0f1a3ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_tc_uI1miszcAf24"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/9c37a8f02361bed32ad3bc5b4c0d9f74.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/0*OeT8eZrL9lpuLWZ_"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Images by Author(Data Analysis)</figcaption></figure><p id="21d3" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">在聚类方面，与SVM、逻辑回归和人工神经网络相比，准确率为52。这就是为什么，因为我们所拥有的数据并没有给出准确的关系或对应关系来将它们分成两个类别0和1。</p><p id="fcbc" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 4。所有方法的总体对比分析</strong></p><p id="0b05" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">比较数据集<strong class="jw hj">上执行的模型的准确性。位置</strong>是根据各位置精度手动给定的<strong class="jw hj"> </strong>等级。</p><p id="4377" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj">型号</strong>———<strong class="jw hj">测试精度/F1分数</strong>——<strong class="jw hj">位置</strong></p><p id="c022" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">SVM(带rbf核)———88.89————<strong class="jw hj">I</strong></p><p id="1235" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">物流学——87.96——87.96——87.97<strong class="jw hj">II</strong></p><p id="6ce7" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">随机森林分类—84.25————<strong class="jw hj">ⅲ</strong></p><p id="21b2" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">安—————79.69———<strong class="jw hj">四</strong></p><p id="7dc3" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">无监督学习———-51.85—————<strong class="jw hj">V</strong></p><p id="f393" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">线性回归———20.14————<strong class="jw hj">VI</strong></p><p id="05b4" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">上面的插图清楚地描述了用于预测学校教师性别的所有机器学习方法的排名。因此，很明显，支持向量分类是最好的，因为我们拥有的数据肯定适合分类问题，而像“rbf”这样的核最适合处理这样的非线性问题。所有适用于非线性数据的方法，主要是分类问题，在这个数据集中工作良好。此外，SVM是唯一可以分离不可线性分离的数据的线性模型。</p><p id="f380" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">这就是为什么除了线性回归至少有50%的准确率。无监督学习预计不会给出很好的精度，这基本上取决于聚类，但由于我们有标签，精度发现帮助我们理解它并不总是最好的解决方案。但是，在大多数真实的情况下，我们被迫使用相同的。ANN可以用不同的方式即兴创作，这个结果相当不错。因此，它是最大似然法中一个值得推荐的方法。</p><p id="ba95" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 5。机器学习模型的误差分析</strong></p><p id="e083" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">误差计算为预测Y值(使用X测试值)和原始Y测试值之间的差值。误差= Y _预测—Y _测试</p><p id="8dd3" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">代码:下面的代码将适用于每个模型中的绘图错误。但是请确保您为模型使用了相应的Y_test和Y_pred值。我刚刚给了SVM。您可以用类似的方式简单地完成剩下的工作。</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="5fbd" class="lg ig hi lc b fi lh li l lj lk">err=Y_pred-Y_test</span><span id="256e" class="lg ig hi lc b fi ll li l lj lk">plt.plot(Y_pred-Y_test,color="green",label='ERROR')</span><span id="6ed5" class="lg ig hi lc b fi ll li l lj lk">plt.title('Error in Support Vector classification with rbf kernal' )</span><span id="6243" class="lg ig hi lc b fi ll li l lj lk">plt.legend()</span><span id="49f0" class="lg ig hi lc b fi ll li l lj lk">plt.show()</span></pre><p id="73d7" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> I <em class="lv"> —精度= 88.8</em>T5】</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mf"><img src="../Images/c853fd5fc03411c5136466df315da517.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/0*3_vjuxOqf4drYlH1"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Image by Author( Data Analysis)</figcaption></figure><p id="e395" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> <em class="lv"> II —精度=87.96 </em> </strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mf"><img src="../Images/6fd4c08d00bdb32f254ffbfdca37547b.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/0*sZjDRPC0TN7XjpMv"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Image by Author( Data Analysis)</figcaption></figure><p id="873f" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">所以，如上所述，误差水平并不高。从零到1或-1的偏差很大。但是，无论如何，这个误差相对来说比图中显示的要小。</p><p id="0386" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> <em class="lv">三—精度=84.25 </em> </strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mf"><img src="../Images/d87fe5e097ee05ce9d9ba0d4d8f6d942.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/0*KVftQQaiGkWaW-FR"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Image by Author( Data Analysis)</figcaption></figure><p id="6b41" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> <em class="lv">四—精度=80.55 </em> </strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mf"><img src="../Images/0836676947409b5f77cc1b1427b93ae2.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/0*apFkjTB6Zi_QuhtK"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Image by Author( Data Analysis)</figcaption></figure><p id="cb73" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">与前两个图相比，第三和第四个图从零开始变得非常糟糕。我们的精度高于80，我们还可以看到图中的变化，因为它几次偏离零。</p><p id="8abe" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> <em class="lv"> V —精度=51.85 </em> </strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mf"><img src="../Images/9eda05a1344461ca97a54cabb3f85f6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/0*u-rhxYnruU6DaT6H"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Image by Author( Data Analysis)</figcaption></figure><p id="0963" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> <em class="lv"> VI —精度=20.14 </em> </strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mg"><img src="../Images/b6394037c68aa01aa41a1c233c2193ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/0*3dbMcVdQMcvq_P1l"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Image by Author( Data Analysis)</figcaption></figure><p id="045f" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">现在，来看第5和第6张图表，很明显，我们有一个高偏差图，正如我们在线性回归中所知道的，因此这种方式下的值不会正确地线性拟合。(即使预测值也在连续的范围内，所以在类似这样的问题中，最好避免使用线性回归。因此，每当我们有这类数据时，使用支持向量机(带rbf核)或逻辑回归总是好的。</p><p id="2e30" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">6。结论</p><p id="a6e6" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><em class="lv">使用感知压力水平、感知焦虑水平和人口统计学特征预测学校教师性别的最佳预测模型是</em>支持向量回归(具有rbf核)。</p><p id="75a5" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><em class="lv">使用感知压力水平、感知焦虑水平和人口统计学特征预测学校教师性别的最差预测模型是</em>线性回归。</p><p id="603d" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> 7。最后备注</strong></p><p id="fddf" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated"><strong class="jw hj"> H </strong> ope本文可以帮助您深入了解如何处理数据集，在其中执行各种机器学习方法，并得出有趣的结论，尤其是在预测分析领域。</p><p id="9be7" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">完成代码和细节:</p><p id="08d9" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">我的Github页面:</p><div class="mh mi ez fb mj mk"><a href="https://github.com/gopikasr/Machine-Learning/blob/main/ANALYSIS.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab dw"><div class="mm ab mn cl cj mo"><h2 class="bd hj fi z dy mp ea eb mq ed ef hh bi translated">Machine-Learning/analysis . ipynb at main gopikasr/Machine-Learning</h2><div class="mr l"><h3 class="bd b fi z dy mp ea eb mq ed ef dx translated">各种机器学习模型的比较分析-Machine-Learning/Analysis . ipynb at main…</h3></div><div class="ms l"><p class="bd b fp z dy mp ea eb mq ed ef dx translated">github.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my jn mk"/></div></div></a></div><p id="cd91" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">我的Colab笔记本</p><div class="mh mi ez fb mj mk"><a href="https://colab.research.google.com/drive/1xrXU6wbmOE30U_t9hKMrP4E6di8CuK_v?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab dw"><div class="mm ab mn cl cj mo"><h2 class="bd hj fi z dy mp ea eb mq ed ef hh bi translated">谷歌联合实验室</h2><div class="mr l"><h3 class="bd b fi z dy mp ea eb mq ed ef dx translated">编辑描述</h3></div><div class="ms l"><p class="bd b fp z dy mp ea eb mq ed ef dx translated">colab.research.google.com</p></div></div><div class="mt l"><div class="mz l mv mw mx mt my jn mk"/></div></div></a></div><p id="2e4a" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">8.参考</p><p id="6c6f" class="pw-post-body-paragraph km kn hi jw b jx jy ko kp jz ka kq kr kb ks kt ku kd kv kw kx kf ky kz la kh hb bi translated">马赫什瓦里，普拉奇；萨哈，ShilpasreeVaish，Hina (2020年)，“COVID 19疫情学校教师的压力和焦虑水平”，V1门德利数据，doi: 10.17632/yhmb6psmpm.1</p></div></div>    
</body>
</html>
<html>
<head>
<title>U-NET Implementation from Scratch using TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用TensorFlow从头开始实现U-NET</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/u-net-implementation-from-scratch-using-tensorflow-b4342266e406?source=collection_archive---------0-----------------------#2021-07-07">https://medium.com/geekculture/u-net-implementation-from-scratch-using-tensorflow-b4342266e406?source=collection_archive---------0-----------------------#2021-07-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="3dc4" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated"><em class="ix">底层概念和循序渐进的Python代码解释</em></h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es iy"><img src="../Images/172a193be0785c4483c92ef150e41776.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bOTsD_gsqbu0_DCx2vVmgA.jpeg"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx"><em class="ix">Fig-1: Here’s</em> how a self-driving car sees the world with U-Net! (<a class="ae jo" href="https://analyticsindiamag.com/" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><h1 id="4425" class="jp jq hi bd jr js jt ju jv jw jx jy jz io ka ip kb ir kc is kd iu ke iv kf kg bi translated">介绍</h1><p id="c98c" class="pw-post-body-paragraph kh ki hi kj b kk kl ij km kn ko im kp kq kr ks kt ku kv kw kx ky kz la lb lc hb bi translated">拉里·罗伯茨在他的博士论文(cir。1960)讨论了提取3D几何信息的可能性，并被认为为围绕计算机视觉的研究奠定了基础。从那以后，研究人员取得了巨大的进步，特别是在过去十年里，计算机视觉成为了面部识别、医学成像、自动驾驶汽车等现实世界人工智能应用的前沿。</p><p id="bd90" class="pw-post-body-paragraph kh ki hi kj b kk ld ij km kn le im kp kq lf ks kt ku lg kw kx ky lh la lb lc hb bi translated">在这篇博客中，我的目的是深入研究一个叫做U-Net的巨大的计算机视觉模型。该博客提供了关于U-Net架构中使用的操作的见解，如卷积、最大池化、转置卷积、跳过连接，还解释了如何使用TensorFlow从头实现这些概念。</p><p id="91b2" class="pw-post-body-paragraph kh ki hi kj b kk ld ij km kn le im kp kq lf ks kt ku lg kw kx ky lh la lb lc hb bi translated">在这篇博客结束的时候，你应该已经创建了下面的架构(图2)来将图像像素分类成片段(如图1)。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es li"><img src="../Images/a4510ce9851254e421c62ec7224da802.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PIuZCIWEf-vsyF_GaGbEdg.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx">Fig-2: The flow of U-Net architecture implemented in this blog</figcaption></figure><h1 id="5021" class="jp jq hi bd jr js jt ju jv jw jx jy jz io ka ip kb ir kc is kd iu ke iv kf kg bi translated">博客的内容</h1><ol class=""><li id="ff8c" class="lj lk hi kj b kk kl kn ko kq ll ku lm ky ln lc lo lp lq lr bi translated">U-Net概述</li><li id="1946" class="lj lk hi kj b kk ls kn lt kq lu ku lv ky lw lc lo lp lq lr bi translated">了解U-Net中使用的关键操作</li><li id="5a6c" class="lj lk hi kj b kk ls kn lt kq lu ku lv ky lw lc lo lp lq lr bi translated">处理数据</li><li id="9244" class="lj lk hi kj b kk ls kn lt kq lu ku lv ky lw lc lo lp lq lr bi translated">定义U-Net架构</li><li id="122b" class="lj lk hi kj b kk ls kn lt kq lu ku lv ky lw lc lo lp lq lr bi translated">训练模型</li><li id="d417" class="lj lk hi kj b kk ls kn lt kq lu ku lv ky lw lc lo lp lq lr bi translated">评估模型</li><li id="d256" class="lj lk hi kj b kk ls kn lt kq lu ku lv ky lw lc lo lp lq lr bi translated">预测！</li></ol></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><h1 id="12a9" class="jp jq hi bd jr js me ju jv jw mf jy jz io mg ip kb ir mh is kd iu mi iv kf kg bi translated">1.U-Net概述</h1><p id="9909" class="pw-post-body-paragraph kh ki hi kj b kk kl ij km kn ko im kp kq kr ks kt ku kv kw kx ky kz la lb lc hb bi translated">U-Net架构由Olaf Ronneberger、Philipp Fischer和Thomas Brox于2015年推出，用于肿瘤检测，但此后发现它在多个行业都很有用。作为一个图像分割工具，该模型旨在将每个像素分类为一个输出类，创建一个类似图1的输出。</p><p id="f526" class="pw-post-body-paragraph kh ki hi kj b kk ld ij km kn le im kp kq lf ks kt ku lg kw kx ky lh la lb lc hb bi translated">许多神经网络以前曾试图执行“图像分割”，但U-Net击败了它的前辈，计算成本更低，并使信息损失最小化。让我们进一步深入了解U-Net是如何做到这一点的。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mj"><img src="../Images/342f2c88bc2f14bea027a44026df8a50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lvXoKMHoPJMKpKK7keZMEA.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx">Fig-3: U-Net Architecture — Guess why is it called a ‘U’ Net?</figcaption></figure><h1 id="a763" class="jp jq hi bd jr js jt ju jv jw jx jy jz io ka ip kb ir kc is kd iu ke iv kf kg bi translated">2.了解U-Net中使用的关键操作</h1><p id="1e82" class="pw-post-body-paragraph kh ki hi kj b kk kl ij km kn ko im kp kq kr ks kt ku kv kw kx ky kz la lb lc hb bi translated">在我们创建一个U-Net之前，让我们了解一下在这个架构中使用的关键操作(图3的右下角)</p><h1 id="9de4" class="jp jq hi bd jr js jt ju jv jw jx jy jz io ka ip kb ir kc is kd iu ke iv kf kg bi translated">2.1卷积(Conv)</h1><p id="7297" class="pw-post-body-paragraph kh ki hi kj b kk kl ij km kn ko im kp kq kr ks kt ku kv kw kx ky kz la lb lc hb bi translated">如果我们只使用完全连接的层来创建高分辨率图像的网络，模型将变得极其计算昂贵。因此，被称为“卷积”的数学运算是计算机视觉故事中的白衣骑士。卷积保留所有输入像素的影响，但保持它们只是松散连接，以降低计算成本。<br/>要执行卷积运算，对整个输入图像矩阵重复以下步骤:</p><ul class=""><li id="8f29" class="lj lk hi kj b kk ld kn le kq mk ku ml ky mm lc mn lp lq lr bi translated"><strong class="kj hj">步骤1: </strong>取一个比输入图像矩阵I小的滤波矩阵K，与重叠的元素进行逐元素相乘，然后相加，在输出矩阵中创建一个单值。</li><li id="17c8" class="lj lk hi kj b kk ls kn lt kq lu ku lv ky lw lc mn lp lq lr bi translated"><strong class="kj hj">步骤2: </strong>根据定义的步幅将过滤器移动到右边的列，并重复步骤1。<br/> <em class="mo">示例:如果我们从第1列开始操作，并且stride是3，那么我们将移动到第4列并重复步骤1。</em></li></ul><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mp"><img src="../Images/f66d173d01269184c895b3c11375ee30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_GfG3qc4C93q1BOmLBJqTA.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx">Fig-4: Example of Convolution Operation (<a class="ae jo" href="https://www.researchgate.net/figure/An-example-of-convolution-operation-in-2D-2_fig3_324165524" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="8a7f" class="pw-post-body-paragraph kh ki hi kj b kk ld ij km kn le im kp kq lf ks kt ku lg kw kx ky lh la lb lc hb bi translated">卷积运算的量纲变化:<br/> <strong class="kj hj">输入矩阵</strong> : A x B x C其中高度为A；宽度是B，通道/深度是C(例如RGB图像将有3个通道)<br/> <strong class="kj hj">滤波器矩阵</strong> : D x E x C x G其中滤波器的高度是D；宽度是E，C是通道数/深度(与输入图像相同)，G是应用的过滤器数<br/> <strong class="kj hj">输出矩阵</strong> : H x W x G其中高度和宽度可以使用下面的公式计算，G是应用于输入的过滤器数</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mq"><img src="../Images/ec1cf2eead53104a9a5b516a51c5e08e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bLNdR0ul0JXF1yCmtEO5Ug.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx">Fig-5: Formula for computing height and width of output matrix from Conv. Operation</figcaption></figure><blockquote class="mr ms mt"><p id="3402" class="kh ki mo kj b kk ld ij km kn le im kp mu lf ks kt mv lg kw kx mw lh la lb lc hb bi translated">滤波器矩阵的元素充当“权重”参数，并且在训练模型期间被优化。请参考<a class="ae jo" href="https://towardsdatascience.com/conv-nets-for-dummies-a-bottom-up-approach-c1b754fb14d6" rel="noopener" target="_blank">这篇文章</a>了解更多关于Conv操作和通讯的信息</p></blockquote><h1 id="e7da" class="jp jq hi bd jr js jt ju jv jw jx jy jz io ka ip kb ir kc is kd iu ke iv kf kg bi translated">2.2转置卷积(上conv)</h1><p id="5dc3" class="pw-post-body-paragraph kh ki hi kj b kk kl ij km kn ko im kp kq kr ks kt ku kv kw kx ky kz la lb lc hb bi translated">为了给图像中的每个像素分配一个类别，图像分割需要将缩小的图像(由于卷积)放大到更接近原始图像的大小。这可以使用完全连接的层来完成，但是它在计算上变得非常昂贵。为了解决这个问题，U-Net使用转置卷积运算，通过使用比输入更大的滤波器来增加输入图像的维度。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mx"><img src="../Images/7ef4758a721806c66ef7842252ef8493.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*AlQAQ-ht2v7JD4B3bqKqUw.gif"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx">Fig-6: A Conv2DTranspose with 3x3 filter and stride of 2x2 applied to a 2x2 input to give a 5x5 output (<a class="ae jo" rel="noopener" href="/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8">source</a>)</figcaption></figure><blockquote class="mr ms mt"><p id="aee8" class="kh ki mo kj b kk ld ij km kn le im kp mu lf ks kt mv lg kw kx mw lh la lb lc hb bi translated">请参考本文<a class="ae jo" rel="noopener" href="/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8">了解转置卷积的更多信息</a></p></blockquote><h1 id="4bc5" class="jp jq hi bd jr js jt ju jv jw jx jy jz io ka ip kb ir kc is kd iu ke iv kf kg bi translated">2.3联营(最大联营)</h1><p id="fbec" class="pw-post-body-paragraph kh ki hi kj b kk kl ij km kn ko im kp kq kr ks kt ku kv kw kx ky kz la lb lc hb bi translated">池的用途与卷积相同，即减少参数数量并提高计算速度。该层也无意中允许一点正规化。通常在池中执行两个操作——平均或最大。在这两种方法中，我们基于滤波器大小“f”、步长“s”创建输入子集，然后将这些函数(最大值或平均值)应用于输入矩阵。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es my"><img src="../Images/9bf99ae45a571ff461ed23df137ac05b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*3ut6e8jYNvYZ7keOFOAWtA.png"/></div><figcaption class="jk jl et er es jm jn bd b be z dx">Fig-7: Example of max-pool with a stride of (2,2)</figcaption></figure><blockquote class="mr ms mt"><p id="92a8" class="kh ki mo kj b kk ld ij km kn le im kp mu lf ks kt mv lg kw kx mw lh la lb lc hb bi translated">与卷积不同，在汇集操作中不生成权重参数</p></blockquote><h1 id="b042" class="jp jq hi bd jr js jt ju jv jw jx jy jz io ka ip kb ir kc is kd iu ke iv kf kg bi translated">2.4跳过连接(复制和裁剪)</h1><p id="d612" class="pw-post-body-paragraph kh ki hi kj b kk kl ij km kn ko im kp kq kr ks kt ku kv kw kx ky kz la lb lc hb bi translated">U-Net中的跳过连接从早期层(图3的LHS层)复制图像矩阵，并将其用作后期层(RHS层)的一部分。这使得模型能够保留来自更丰富矩阵的信息，并防止信息丢失。许多流行的计算机视觉架构使用跳过连接来使输出更加丰富。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mz"><img src="../Images/f37bc7f446523fe9f6825e30e118f755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W8I288AMM0lidb04tPISNw.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx">Fig-8: Example of skip connections sharing image information from initial layers to later layers<strong class="bd jr"> </strong>(<a class="ae jo" href="https://arxiv.org/pdf/1603.04992.pdf" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><h1 id="0d1b" class="jp jq hi bd jr js jt ju jv jw jx jy jz io ka ip kb ir kc is kd iu ke iv kf kg bi translated">3.处理数据</h1><p id="e1c1" class="pw-post-body-paragraph kh ki hi kj b kk kl ij km kn ko im kp kq kr ks kt ku kv kw kx ky kz la lb lc hb bi translated">既然我们已经掌握了一些基本概念，让我们开始实现这个模型，并使用牛津-IIIT Pet数据集获得一些实践知识。该数据集中的文件大小不一，我们将使用resize、shape将它们全部转换为一致的所需大小。我们还将通过将像素值除以256来归一化图像矩阵。请注意,‘mask’矩阵中的值代表类，因此，我们不会对它们进行规范化。</p><pre class="iz ja jb jc fd na nb nc nd aw ne bi"><span id="91fe" class="nf jq hi nb b fi ng nh l ni nj"><strong class="nb hj">for</strong> file <strong class="nb hj">in</strong> img:<br/>    index = img.index(file)<br/>    path = os.path.join(path1, file)<br/>    single_img = Image.open(path).convert('RGB')<br/>    single_img = single_img.resize((i_h,i_w))<br/>    single_img = np.reshape(single_img,(i_h,i_w,i_c)) <br/>    single_img = single_img/256.<br/>    X[index] = single_img<br/>        <br/>    single_mask_ind = mask[index]<br/>    path = os.path.join(path2, single_mask_ind)<br/>    single_mask = Image.open(path)<br/>    single_mask = single_mask.resize((m_h, m_w))<br/>    single_mask = np.reshape(single_mask,(m_h,m_w,m_c)) <br/>    single_mask = single_mask - 1 <br/>    y[index] = single_mask</span></pre><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nk"><img src="../Images/53931f2a3d3f07edb5176b5695b4c77c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IFUqS8RACTP7qvtJ5omODA.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx">Fig-9: The output has 3 classes — background, pet, outline</figcaption></figure><p id="3ef1" class="pw-post-body-paragraph kh ki hi kj b kk ld ij km kn le im kp kq lf ks kt ku lg kw kx ky lh la lb lc hb bi translated">恭喜你！我们的图像文件夹已经被转换为X <em class="mo"> (dims: #图像，img高度，img宽度，img通道)</em>和y <em class="mo"> (dims: #蒙版，蒙版高度，蒙版宽度，蒙版通道)。我们现在可以继续设计U-Net的架构了！</em></p><blockquote class="mr ms mt"><p id="af40" class="kh ki mo kj b kk ld ij km kn le im kp mu lf ks kt mv lg kw kx mw lh la lb lc hb bi translated">X中的图像数量应该等于y中的掩模数量，数据集的其他维度可以不同。</p></blockquote><h1 id="d369" class="jp jq hi bd jr js jt ju jv jw jx jy jz io ka ip kb ir kc is kd iu ke iv kf kg bi translated">4.定义U-Net架构</h1><p id="5e1c" class="pw-post-body-paragraph kh ki hi kj b kk kl ij km kn ko im kp kq kr ks kt ku kv kw kx ky kz la lb lc hb bi translated">在编写U-Net架构时，我把它分成了两个部分——编码器和解码器。它们可以进一步被分成一系列重复的编码器微块和解码器微块。</p><blockquote class="mr ms mt"><p id="bbc8" class="kh ki mo kj b kk ld ij km kn le im kp mu lf ks kt mv lg kw kx mw lh la lb lc hb bi translated">要设计一个U-Net，我们必须设计可重复使用的迷你模块，并简单地将它们串在一起。</p></blockquote><h1 id="4d65" class="jp jq hi bd jr js jt ju jv jw jx jy jz io ka ip kb ir kc is kd iu ke iv kf kg bi translated">4.1编码器</h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nl"><img src="../Images/2ce3e47b554c52fa940c1c5c77089d17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xbG7s8urH-QcsHokMsj85Q.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx">Fig-10: The first 4 blocks are typically called ‘contraction’ section and the last one is called ‘bottleneck’</figcaption></figure><p id="b112" class="pw-post-body-paragraph kh ki hi kj b kk ld ij km kn le im kp kq lf ks kt ku lg kw kx ky lh la lb lc hb bi translated">我们将开发一个编码器迷你块的功能，这将允许我们动态地创建所有的编码器层。如果我们看一下上面的图表，每个微型块中有两个conv 3x3操作和一个最大池操作(后者在“瓶颈”块中不存在)。</p><p id="970a" class="pw-post-body-paragraph kh ki hi kj b kk ld ij km kn le im kp kq lf ks kt ku lg kw kx ky lh la lb lc hb bi translated">下面的函数允许我们实现相同的操作选项，如批量标准化、删除，以使模型更加健壮。我们使用“初始化”和ReLU来获得最佳结果。在我们应用max pool之前，我们正在保存跳过连接的信息，稍后我们将在解码器中使用该信息。</p><pre class="iz ja jb jc fd na nb nc nd aw ne bi"><span id="1fa2" class="nf jq hi nb b fi ng nh l ni nj"><strong class="nb hj">def</strong> EncoderMiniBlock(inputs, n_filters=32, dropout_prob=0.3, max_pooling=<strong class="nb hj">True</strong>):<br/>    conv = Conv2D(n_filters, <br/>                  3, <em class="mo"> # filter size</em><br/>                  activation='relu',<br/>                  padding='same',<br/>                  kernel_initializer='HeNormal')(inputs)<br/>    conv = Conv2D(n_filters, <br/>                  3,  <em class="mo"># filter size</em><br/>                  activation='relu',<br/>                  padding='same',<br/>                  kernel_initializer='HeNormal')(conv)<br/>  <br/>    conv = BatchNormalization()(conv, training=<strong class="nb hj">False</strong>)</span><span id="f041" class="nf jq hi nb b fi nm nh l ni nj">    <strong class="nb hj">if</strong> dropout_prob &gt; 0:     <br/>        conv = tf.keras.layers.Dropout(dropout_prob)(conv)<br/>    <strong class="nb hj">if</strong> max_pooling:<br/>        next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2,2))(conv)    <br/>    <strong class="nb hj">else</strong>:<br/>        next_layer = conv</span><span id="4620" class="nf jq hi nb b fi nm nh l ni nj">skip_connection = conv    <br/>    <strong class="nb hj">return</strong> next_layer, skip_connection</span></pre><blockquote class="mr ms mt"><p id="5dc4" class="kh ki mo kj b kk ld ij km kn le im kp mu lf ks kt mv lg kw kx mw lh la lb lc hb bi translated">为了完成编码器，我们将堆叠这些小块，在每个后续块中滤波器的数量加倍(如图10所示)</p></blockquote><h1 id="0fa9" class="jp jq hi bd jr js jt ju jv jw jx jy jz io ka ip kb ir kc is kd iu ke iv kf kg bi translated">4.2解码器</h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nn"><img src="../Images/bd4c70d5d1d6390ee344ef4850085522.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LDJ4y5QJc_eCr7quukcKkg.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx">Fig-11: The decoder is typically known as the ‘expansion’ section</figcaption></figure><p id="a9f1" class="pw-post-body-paragraph kh ki hi kj b kk ld ij km kn le im kp kq lf ks kt ku lg kw kx ky lh la lb lc hb bi translated">解码器首先使用转置卷积增加图像尺寸，然后将结果与来自跳过连接的信息(存储在编码器代码块中)合并。再进行两次卷积运算，我们的微型模块就准备好了。请注意，我们在卷积中使用“相同”填充，以确保我们的图像大小不会减少。</p><pre class="iz ja jb jc fd na nb nc nd aw ne bi"><span id="65f1" class="nf jq hi nb b fi ng nh l ni nj"><strong class="nb hj">def</strong> DecoderMiniBlock(prev_layer_input, skip_layer_input, n_filters=32):<br/>    up = Conv2DTranspose(<br/>                 n_filters,<br/>                 (3,3),<br/>                 strides=(2,2),<br/>                 padding='same')(prev_layer_input)</span><span id="79fa" class="nf jq hi nb b fi nm nh l ni nj">    merge = concatenate([up, skip_layer_input], axis=3)</span><span id="913f" class="nf jq hi nb b fi nm nh l ni nj">    conv = Conv2D(n_filters, <br/>                 3,  <br/>                 activation='relu',<br/>                 padding='same',<br/>                 kernel_initializer='HeNormal')(merge)<br/>    conv = Conv2D(n_filters,<br/>                 3, <br/>                 activation='relu',<br/>                 padding='same',<br/>                 kernel_initializer='HeNormal')(conv)<br/>    <strong class="nb hj">return</strong> conv</span></pre><p id="4270" class="pw-post-body-paragraph kh ki hi kj b kk ld ij km kn le im kp kq lf ks kt ku lg kw kx ky lh la lb lc hb bi translated">在堆叠4个小块之后，我们将使用conv 1x1操作来补足编译后的解码器，该操作将小块输出转换为期望的尺寸。</p><blockquote class="mr ms mt"><p id="d846" class="kh ki mo kj b kk ld ij km kn le im kp mu lf ks kt mv lg kw kx mw lh la lb lc hb bi translated">输出图层中使用的过滤器数量将等于输出类的数量。因此，我们的输出将具有以下维度:H * W * #类</p></blockquote><h1 id="d046" class="jp jq hi bd jr js jt ju jv jw jx jy jz io ka ip kb ir kc is kd iu ke iv kf kg bi translated">5.训练模型</h1><p id="ac9a" class="pw-post-body-paragraph kh ki hi kj b kk kl ij km kn ko im kp kq kr ks kt ku kv kw kx ky kz la lb lc hb bi translated">在编译了上一节中显示的所有小块之后，我们现在需要为模型决定一个优化器、损失函数和准确性度量。然后我们可以使用model.fit()进行训练。下面，我使用了Adam optimizer和稀疏分类交叉熵。</p><blockquote class="mr ms mt"><p id="9d44" class="kh ki mo kj b kk ld ij km kn le im kp mu lf ks kt mv lg kw kx mw lh la lb lc hb bi translated">如果输出标签是一次性编码的，请使用分类交叉熵而不是稀疏分类交叉熵</p></blockquote><pre class="iz ja jb jc fd na nb nc nd aw ne bi"><span id="6764" class="nf jq hi nb b fi ng nh l ni nj">unet.compile(optimizer=tf.keras.optimizers.Adam(),            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<strong class="nb hj">True</strong>), metrics=['accuracy'])</span><span id="2d0c" class="nf jq hi nb b fi nm nh l ni nj">results = unet.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_valid, y_valid))</span></pre><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es no"><img src="../Images/c1290f3f91880810201bd8fe505d1f71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y_gchPp1p9HVy2nk16r0AQ.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx">Fig-12: Sample output of model.fit()</figcaption></figure><h1 id="1825" class="jp jq hi bd jr js jt ju jv jw jx jy jz io ka ip kb ir kc is kd iu ke iv kf kg bi translated">6.评估模型</h1><ul class=""><li id="6d54" class="lj lk hi kj b kk kl kn ko kq ll ku lm ky ln lc mn lp lq lr bi translated">首先，我们将检查我们的模型是否以正确的速率学习<strong class="kj hj">。我们可以通过绘制每个时期的“损失函数”来做到这一点。如果学习率太大，“训练损失”会振荡，否则，我们会看到持续减少的损失。</strong></li><li id="ffb8" class="lj lk hi kj b kk ls kn lt kq lu ku lv ky lw lc mn lp lq lr bi translated">其次，我们将寻找<strong class="kj hj">高偏差或欠拟合</strong>，即，如果训练和验证精度都非常低。这意味着模型还没有训练好，需要调整。解决高偏差的一些选择是——更大的网络、更多的训练迭代或增加更多的功能。更好的优化算法和更好的权重初始化也可能有所帮助。</li><li id="fe1f" class="lj lk hi kj b kk ls kn lt kq lu ku lv ky lw lc mn lp lq lr bi translated">最后，我们将检查<strong class="kj hj">高方差或过拟合</strong>，即训练精度高但验证精度低。这意味着该模型与训练数据非常接近，并且不够通用，不足以预测新的数据值。为了解决这个问题，我们可以使用<strong class="kj hj">正则化</strong>，这将缩小权重的影响或向我们的训练集添加更多的例子。</li></ul><blockquote class="mr ms mt"><p id="57b5" class="kh ki mo kj b kk ld ij km kn le im kp mu lf ks kt mv lg kw kx mw lh la lb lc hb bi translated">评估后，调整模型以获得符合上述标准的最佳结果</p></blockquote><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es np"><img src="../Images/a884f26bd04f3f3a22eaa271ac1e5bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GxlWmGn0JPkKHnFdpdQ6wg.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx">Fig-13: (a) The train loss is consistently decreasing (b) The validation accuracy is not significantly different from the train accuracy</figcaption></figure><h1 id="99ed" class="jp jq hi bd jr js jt ju jv jw jx jy jz io ka ip kb ir kc is kd iu ke iv kf kg bi translated">7.预测！</h1><p id="e4a8" class="pw-post-body-paragraph kh ki hi kj b kk kl ij km kn ko im kp kq kr ks kt ku kv kw kx ky kz la lb lc hb bi translated">既然我们已经检查了are model在数字中的表现，我们还可以使用model.predict()来可视化它的预测。不要忘记确保输入的维度与训练模型的输入维度相匹配。此外，要可视化预测的遮罩，请调整其轴以匹配输出尺寸。</p><pre class="iz ja jb jc fd na nb nc nd aw ne bi"><span id="31d3" class="nf jq hi nb b fi ng nh l ni nj"><strong class="nb hj">def</strong> VisualizeResults(index):<br/>    img = X_valid[index]<br/>    img = img[np.newaxis, ...]<br/>    <strong class="nb hj">pred_y = unet.predict(img)</strong><br/>    pred_mask = tf.argmax(pred_y[0], axis=-1)<br/>    pred_mask = pred_mask[..., tf.newaxis]<br/>    fig, arr = plt.subplots(1, 3, figsize=(15, 15))<br/>    arr[0].imshow(X_valid[index])<br/>    arr[0].set_title('Processed Image')<br/>    arr[1].imshow(y_valid[index,:,:,0])<br/>    arr[1].set_title('Actual Masked Image ')<br/>    arr[2].imshow(pred_mask[:,:,0])<br/>    arr[2].set_title('Predicted Masked Image ')</span></pre><p id="70f4" class="pw-post-body-paragraph kh ki hi kj b kk ld ij km kn le im kp kq lf ks kt ku lg kw kx ky lh la lb lc hb bi translated">下图比较了实际遮罩与U-Net模型的预测遮罩。尝试使用我们创建的模型来预测您选择的图像的轮廓和背景！</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nq"><img src="../Images/a35dc4c06f95e9c6a664341cdbb8abe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ek2ACP6oyGLlv2LxGCo9rA.png"/></div></div><figcaption class="jk jl et er es jm jn bd b be z dx">Fig-14: Comparison of actual mask vs predicted mask from U-Net model</figcaption></figure></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><h1 id="dffc" class="jp jq hi bd jr js me ju jv jw mf jy jz io mg ip kb ir mh is kd iu mi iv kf kg bi translated">结论</h1><p id="39e6" class="pw-post-body-paragraph kh ki hi kj b kk kl ij km kn ko im kp kq kr ks kt ku kv kw kx ky kz la lb lc hb bi translated">在转置卷积和跳跃连接的帮助下，U-Net已经超越了它的前辈，并被证明是多个行业中有用的计算机视觉工具。我希望这篇博客是一个很好的起点，让你尝试为自己的应用程序制作一个U-Net模型。我也强烈推荐阅读最初发表的论文<a class="ae jo" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank"> U-Net:生物医学图像分割的卷积网络</a>。本博客中引用的代码存储在<a class="ae jo" href="https://github.com/VidushiBhatia/U-Net-Implementation/blob/main/U_Net_for_Image_Segmentation_From_Scratch_Using_TensorFlow_v4.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上，我很乐意回答任何问题。</p><blockquote class="mr ms mt"><p id="9726" class="kh ki mo kj b kk ld ij km kn le im kp mu lf ks kt mv lg kw kx mw lh la lb lc hb bi translated"><strong class="kj hj"> Pro提示</strong>:通过跟踪每一步的输入和输出数据维度，可以解决很多错误。编码快乐！</p></blockquote></div></div>    
</body>
</html>
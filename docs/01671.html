<html>
<head>
<title>Practical Understanding of Principal Component Analysis (PCA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对主成分分析的实际理解</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/practical-understanding-of-principal-component-analysis-pca-61015666354?source=collection_archive---------21-----------------------#2021-04-19">https://medium.com/geekculture/practical-understanding-of-principal-component-analysis-pca-61015666354?source=collection_archive---------21-----------------------#2021-04-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="eb49" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">很多时候，我过去学习的基础数学在应用于深度学习时变得不清楚，PCA就是其中之一。在这篇文章中，我想强调PCA在研究中的实用性，并涵盖一些数学背景。此外，我已经在<a class="ae jd" href="https://colab.research.google.com/drive/1jp_VnLPZBVZyzT8RrECTh9HtU1QnYgds?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>中实现了整个过程，供任何想在实践中跟进的人使用。</p><div class="je jf ez fb jg jh"><a href="https://colab.research.google.com/drive/1jp_VnLPZBVZyzT8RrECTh9HtU1QnYgds?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="ji ab dw"><div class="jj ab jk cl cj jl"><h2 class="bd hj fi z dy jm ea eb jn ed ef hh bi translated">主成分分析和特征脸</h2><div class="jo l"><h3 class="bd b fi z dy jm ea eb jn ed ef dx translated">林炳成</h3></div><div class="jp l"><p class="bd b fp z dy jm ea eb jn ed ef dx translated">colab.research.google.com</p></div></div><div class="jq l"><div class="jr l js jt ju jq jv jw jh"/></div></div></a></div><h1 id="12d1" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">PCA的定义</h1><blockquote class="kv kw kx"><p id="d195" class="if ig ky ih b ii ij ik il im in io ip kz ir is it la iv iw ix lb iz ja jb jc hb bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">主成分分析</strong> ( <strong class="ih hj"> PCA </strong> ) </a>是计算主成分，并用它们对数据进行<a class="ae jd" href="https://en.wikipedia.org/wiki/Change_of_basis" rel="noopener ugc nofollow" target="_blank">基</a>变换的过程，有时只使用前几个主成分，忽略其余的。</p></blockquote><p id="7abb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据上面维基百科的定义，主成分是方差最大的向量。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es lc"><img src="../Images/dfe2a4e96ab4f88a7257629902edb36e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*66dyWHm8KT6B-UupCMhahA.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx">Principal components of 2D data</figcaption></figure><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es ln"><img src="../Images/159f242918281a9101add0ac4b76f6be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*YJQeqXG7JlQ5bgCXlRQBAw.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx">Principal components of 3D data</figcaption></figure><p id="f284" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们可以用给定数量的彼此正交的主分量来有效地描述这种2D或3D数据分布。这样，我们可以在许多领域利用主成分分析，包括特征提取或降维。</p><h1 id="516b" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">如何找到主成分</h1><p id="9c21" class="pw-post-body-paragraph if ig hi ih b ii lo ik il im lp io ip iq lq is it iu lr iw ix iy ls ja jb jc hb bi translated">主分量通过协方差矩阵的特征分解获得。后面将解释执行特征分解的原因。首先，我们将详细说明协方差矩阵的含义。</p><h2 id="1186" class="lt jy hi bd jz lu lv lw kd lx ly lz kh iq ma mb kl iu mc md kp iy me mf kt mg bi translated">协方差矩阵</h2><p id="7f14" class="pw-post-body-paragraph if ig hi ih b ii lo ik il im lp io ip iq lq is it iu lr iw ix iy ls ja jb jc hb bi translated">根据定义，协方差矩阵是每个平均特征对的内积的期望值。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mh"><img src="../Images/9844b164c25621dd24c2eda615fb112e.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/0*9I7eKUFl1Hbno6BO"/></div></figure><p id="ad67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">协方差矩阵背后的含义可以被认为是特征对变化的相似性度量。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mi"><img src="../Images/1d8eca2d16b72e1f9f0b02e7b3f03738.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*7X7rFN4ItaOK5AnQinWAQg.png"/></div></figure><p id="a70f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">换句话说，对于给定的2D数据，协方差矩阵显示了数据在该方向上的分散程度。</p><h2 id="64b0" class="lt jy hi bd jz lu lv lw kd lx ly lz kh iq ma mb kl iu mc md kp iy me mf kt mg bi translated">特征分解</h2><p id="6d99" class="pw-post-body-paragraph if ig hi ih b ii lo ik il im lp io ip iq lq is it iu lr iw ix iy ls ja jb jc hb bi translated">我们现在可以通过查看协方差矩阵来了解数据是如何分布的。那么，我们如何找到投影数据方差最大的主成分呢？</p><p id="d58a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们有一个投影单位向量<em class="ky"> e </em>，</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mj"><img src="../Images/d609c958db0d1b151e69fdb5f013638e.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/0*YHD0bJAHAvhXmlc9"/></div></figure><p id="397c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设每个数据的平均值为0，投影数据的方差变成如下:</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mk"><img src="../Images/007637a32651ecfe35b7ef8ab3f7258f.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/0*Q-E7G32PPI0RuV7V"/></div></figure><p id="0b2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以应用拉格朗日乘数法来寻找使方差最大化的投影单位向量。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es ml"><img src="../Images/d9f7698bd022ee06bcab1370a4e0709e.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/0*7ESxJ4Qp-oZ5oYiu"/></div></figure><blockquote class="mm"><p id="4484" class="mn mo hi bd mp mq mr ms mt mu mv jc dx translated">因此，当投影到协方差矩阵的特征向量上时，投影数据的方差变得最大，并且其方差是特征值。</p></blockquote><h1 id="2008" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki mx kk kl km my ko kp kq mz ks kt ku bi translated">特征脸</h1><p id="ee26" class="pw-post-body-paragraph if ig hi ih b ii lo ik il im lp io ip iq lq is it iu lr iw ix iy ls ja jb jc hb bi translated">就人脸识别的发展而言，特征脸在很久以前就出现了。不考虑特征脸的应用，我只想指出它的PCA方面。我们可以将相同的概念应用于人脸图像，而不是寻找随机n维数据的主成分。所以，给定人脸图像的宽x高尺寸，我们可以认为是(宽x高)-维向量。</p><blockquote class="kv kw kx"><p id="058d" class="if ig ky ih b ii ij ik il im in io ip kz ir is it la iv iw ix lb iz ja jb jc hb bi translated">下面所有的人脸图像都来自<a class="ae jd" href="https://github.com/NVlabs/ffhq-dataset" rel="noopener ugc nofollow" target="_blank">https://github.com/NVlabs/ffhq-dataset</a>、<br/>一种基于风格的生成式对抗网络生成器架构<br/>泰罗·卡拉斯(英伟达)、萨穆利·莱恩(英伟达)、提莫·艾拉(英伟达)<a class="ae jd" href="https://arxiv.org/abs/1812.04948" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1812.04948</a></p></blockquote><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es na"><img src="../Images/9e669107d95973401c41b0b728afab66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*Qeyb0Vxp6yXgzhdg7CCgAA.png"/></div></figure><p id="b8e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，将有(宽×高)个主成分，每个主成分描述在数据中发现的重要面部特征。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="er es nb"><img src="../Images/59fbe0ecbea3645722ddf6c94103429f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uK5esbNu221p7HJA4YlXNg.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx">Eigenvectors in descending order by eigenvalues</figcaption></figure><p id="8cf4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于每个主成分都包含了数据中独特的人脸特征，因此我们只需要很少的特征向量就可以重建出特定的人脸。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es ng"><img src="../Images/0d6d82463cb81a310241a05c3e56b13d.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/0*Gm4Q5F0ExDR3wN5R"/></div></figure><p id="c4af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从平均人脸开始，将每个特征向量的数量加上它与目标图像的相似程度，将再现近似的人脸图像。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="er es nb"><img src="../Images/4299927c6b6c8525daa2574d798c173a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FBWaGTkwbUcJpcwlzRpgmw.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx">Reconstructed face images by Eigenface</figcaption></figure></div></div>    
</body>
</html>
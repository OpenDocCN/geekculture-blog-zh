<html>
<head>
<title>Logistic Regression Using Gradient Descent: Intuition and Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用梯度下降的逻辑回归:直觉和实现</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/logistic-regression-using-gradient-descent-intuition-and-implementation-36a8498afdcb?source=collection_archive---------21-----------------------#2021-05-17">https://medium.com/geekculture/logistic-regression-using-gradient-descent-intuition-and-implementation-36a8498afdcb?source=collection_archive---------21-----------------------#2021-05-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/abc06c5666a38c623792314287efc8a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HHnsKOQveYnGkP_i"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx">Photo by <a class="ae hv" href="https://unsplash.com/@isaacmsmith?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Isaac Smith</a> on <a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="e478" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是我正在从事的系列文章的第三部分，其中我们将讨论和定义介绍性的机器学习算法和概念。在这篇文章的最后，你会找到这个系列的所有前几篇文章。我建议你在深入研究这个之前先阅读一下<a class="ae hv" href="https://ali-h-khanafer.medium.com/linear-regression-using-gradient-descent-intuition-and-implementation-522d43453fc3" rel="noopener">线性回归:直觉和实现</a>。因为我在那里介绍了一些与逻辑回归非常相关的概念，我会在很多场合引用它们。</p><p id="7eb1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我们将详细介绍逻辑回归背后的理论，并使用Scikit-Learn的逻辑回归类来查看它的实际应用。</p><p id="1a2d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们开始吧。</p><h1 id="1b49" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">个案研究</h1><p id="95dd" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">理解逻辑回归概念的最好方法是通过一个例子。因此，在阅读本文的其余部分时，想象自己处于以下场景中:</p><blockquote class="kw kx ky"><p id="66f9" class="iv iw kz ix b iy iz ja jb jc jd je jf la jh ji jj lb jl jm jn lc jp jq jr js hb bi translated">你是一名居住在纽约市的数据科学家，纽约的新冠肺炎病例数量正在快速上升。你的朋友发烧了，但是如果他只是得了流感，他不想在测试上浪费250美元。他让你训练一个模型，根据他的症状预测他是否患有新冠肺炎。</p></blockquote><h1 id="517e" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">逻辑回归</h1><p id="585e" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">尽管名字如此，逻辑回归<strong class="ix hz">并不是</strong>用于回归，而是用于分类。在分类中，目标是根据属性将数据分类到离散的组中。在我们的运行示例中，我们有两个不同的可能组:阳性组和阴性组。逻辑回归是一种监督学习算法，因为它从预先存在和标记的数据中学习，以便对新的、传入的数据进行分类。</p><p id="4789" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">逻辑回归不是给我们一个绝对值，而是输出一个概率。该模型给出了一个介于0和1之间的值。例如，值0.75表示患者有75%的几率患有新冠肺炎。更正式地说，我们希望找到:</p><figure class="le lf lg lh fd hk er es paragraph-image"><div class="er es ld"><img src="../Images/1587e0ba936853faf32bb7cd837dc7bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*yjN5ypTAGyxslqkem6asjg.png"/></div></figure><p id="f8b2" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将看一下<code class="du li lj lk ll b">f(x)</code>只能取两个可能值的情况，然后我们将把发言权留给您，让您来决定我们如何扩展这个模型来处理多个类。</p><p id="9af3" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="kz">线性</em>回归可用于将患者分类为测试阳性还是阴性？技术上来说，是的。这是个好主意吗？肯定不是。原因如下:</p><ol class=""><li id="41bc" class="lm ln hy ix b iy iz jc jd jg lo jk lp jo lq js lr ls lt lu bi translated">在线性回归中，因变量可以取连续数量的值。在我们的场景中，我们需要我们的模型在离散数量的值之间做出决定。</li><li id="b6dc" class="lm ln hy ix b iy lv jc lw jg lx jk ly jo lz js lr ls lt lu bi translated">为了让线性回归很好地工作，在因变量和自变量之间需要有线性相关性。</li></ol><h2 id="344b" class="ma ju hy bd jv mb mc md jz me mf mg kd jg mh mi kh jk mj mk kl jo ml mm kp mn bi translated">直觉</h2><p id="1cee" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">术语“逻辑”来自逻辑功能的使用:</p><figure class="le lf lg lh fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mo"><img src="../Images/22f2e5268638079adc979530b5e73ce6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ViZOnToBxMvVVDozZjQfbA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 1:</strong> Logistic Function</figcaption></figure><p id="aeee" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">就像指数函数一样，逻辑函数用于模拟人口的指数增长，只是它考虑了影响人口承载能力的不同因素。我们可以操作这个函数，使它输出一个介于0和1之间的值。<code class="du li lj lk ll b">L</code>被定义为曲线的最大值。我们希望这个值等于1。我们不想改变增长率，所以<code class="du li lj lk ll b">k</code>也等于1。<code class="du li lj lk ll b">x_0</code>被定义为中点的x值。将其设置为零，我们得到中点0.5，等式简化为:</p><figure class="le lf lg lh fd hk er es paragraph-image"><div class="er es mp"><img src="../Images/6ba5cea0c5cdaeda1c729422681ff3e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*SEXIs96ckbkT-wUx1e5Meg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 2: </strong>Simplified<strong class="bd jv"> </strong>Logistic Function</figcaption></figure><p id="8202" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这给了我们下面的曲线:</p><figure class="le lf lg lh fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mq"><img src="../Images/dc81251e61cd897767124f21263c0065.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ELkJx6TzS91Xx3iISpPsKg.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 1: </strong>Logistic Regression Curve with L ,k, and x_0 = 1</figcaption></figure><p id="5dca" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">关于这条曲线，有几点很重要:</p><ol class=""><li id="dbdf" class="lm ln hy ix b iy iz jc jd jg lo jk lp jo lq js lr ls lt lu bi translated">随着<code class="du li lj lk ll b">x</code>接近正无穷大，<code class="du li lj lk ll b">f</code>接近1</li><li id="2e0e" class="lm ln hy ix b iy lv jc lw jg lx jk ly jo lz js lr ls lt lu bi translated">随着<code class="du li lj lk ll b">x</code>接近负无穷大，<code class="du li lj lk ll b">f</code>接近零</li><li id="6f68" class="lm ln hy ix b iy lv jc lw jg lx jk ly jo lz js lr ls lt lu bi translated">在<code class="du li lj lk ll b">y=1</code>和<code class="du li lj lk ll b">y=0</code>有渐近线</li><li id="f5af" class="lm ln hy ix b iy lv jc lw jg lx jk ly jo lz js lr ls lt lu bi translated"><code class="du li lj lk ll b">f(0)=0.5</code></li></ol><p id="e1d6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">给定<code class="du li lj lk ll b">x</code>的一个值，这个函数将吐出一个介于0和1之间的值。那么我们的<code class="du li lj lk ll b">x</code>是什么？</p><p id="cca3" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在关于线性回归的<a class="ae hv" href="https://ali-h-khanafer.medium.com/linear-regression-using-gradient-descent-intuition-and-implementation-522d43453fc3" rel="noopener">文章</a>中，我们提出了一个处理线性相关变量的通用公式:</p><figure class="le lf lg lh fd hk er es paragraph-image"><div class="er es mr"><img src="../Images/cbfec9d2bd4c2f9bc7c7888ee2b45734.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/0*ElEjfubOfzkjc_Rc.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 3:</strong> Multivariate Linear Regression</figcaption></figure><p id="75aa" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">还是那句话，这个方程输出的是连续数，所以不能用在分类问题上。然而，它很有用，因为它描述了不同特性之间的关系。如果我们能使<code class="du li lj lk ll b">h</code>输出的值被压缩到0到1之间，那么我们的问题就解决了。</p><p id="bae1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为简单起见，将<code class="du li lj lk ll b">x_0 = 1</code>设在<strong class="ix hz">等式3 </strong>中，那么我们可以用矢量形式表示为:</p><figure class="le lf lg lh fd hk er es paragraph-image"><div class="er es ms"><img src="../Images/18521a8e969900d8a004b71c9b49fd6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*HT3SWXqcsTPxXMnxbgCYfw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 4:</strong> Multivariate Linear Regression In Vector Form</figcaption></figure><p id="e4f7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中<code class="du li lj lk ll b">Theta=[Theta_0,Theta_1,...,Theta_j]</code>、<code class="du li lj lk ll b">x = [x_0,x_1,...x_j</code>和<code class="du li lj lk ll b">Theta^T</code>是行向量<code class="du li lj lk ll b">Theta</code>的转置。这可用于我们的物流功能，以获得:</p><figure class="le lf lg lh fd hk er es paragraph-image"><div class="er es mt"><img src="../Images/61d925912107ee42770f968996c41918.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*iM2rx8GzOrYj-D4SGk4W2Q.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 5:</strong> Logistic Function With x = h</figcaption></figure><p id="5d36" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是我们用于逻辑回归的方程。根据图1 中的<strong class="ix hz">曲线和<strong class="ix hz">公式5 </strong>我们可以得出结论:</strong></p><ol class=""><li id="bfc3" class="lm ln hy ix b iy iz jc jd jg lo jk lp jo lq js lr ls lt lu bi translated">如果<code class="du li lj lk ll b">h &gt;= 0</code>则<code class="du li lj lk ll b">p(f(x) = 1 | x) &gt;= 0.5</code></li><li id="117d" class="lm ln hy ix b iy lv jc lw jg lx jk ly jo lz js lr ls lt lu bi translated">如果<code class="du li lj lk ll b">h &lt; 0</code>则<code class="du li lj lk ll b">p(f(x) = 1 | x) &gt; 0.5</code></li></ol><p id="4f49" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们的例子中，如果<code class="du li lj lk ll b">h&gt;=0</code>，我们将预测值为1。否则，我们预测值为零。</p><p id="249a" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">和线性回归一样，现在剩下的就是找到使<code class="du li lj lk ll b">h</code>最小的<code class="du li lj lk ll b">Thetas</code>。</p><h1 id="5ab1" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">梯度下降</h1><p id="a0ff" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在本系列的第二部分中，我们强调了梯度下降不仅仅用于线性回归的事实。事实上，我们展示的算法不够通用。这里有一种更好的描述算法的方式:</p><figure class="le lf lg lh fd hk er es paragraph-image"><div class="er es mu"><img src="../Images/7a34538a2b74cb70a3a107a4cdf0c75f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*2zvrgQe7ThXMhtK1djLLqw.png"/></div></figure><p id="8c0d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中<code class="du li lj lk ll b">J</code>是任何<a class="ae hv" href="https://en.wikipedia.org/wiki/Cost_function" rel="noopener ugc nofollow" target="_blank">成本函数</a>，即传达您的参数执行情况的函数。</p><p id="655c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于线性回归，我们的成本函数是MSE。对于逻辑回归，我们不能再用这个了。为逻辑回归得出成本函数的步骤超出了本文的范围。因此，我们将提供等式，并留给您去深入挖掘:</p><figure class="le lf lg lh fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mv"><img src="../Images/db675a251b5adb0d10a63acb855420b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WJUwUDkq1ioixOHisAQh7g.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 6:</strong> Logistic Regression Cost Function</figcaption></figure><p id="36ef" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中<code class="du li lj lk ll b">Theta</code>、<code class="du li lj lk ll b">x</code>和<code class="du li lj lk ll b">y</code>为向量，<code class="du li lj lk ll b">x^(i)</code>为特征向量<code class="du li lj lk ll b">x</code>中的第I个条目，<code class="du li lj lk ll b">h(x^(i))</code>为第I个预测值，<code class="du li lj lk ll b">y^(i)</code>为类别向量<code class="du li lj lk ll b">y</code>中的第I个条目，即第I个实际值。在上面定义的算法中插入这个将会给我们一个最小化<code class="du li lj lk ll b">J</code>的<code class="du li lj lk ll b">Theta</code>向量。</p><h1 id="0737" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">履行</h1><p id="0463" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">让我们看看如何使用Scikit-learn的<a class="ae hv" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">逻辑回归</a>类和内置的<a class="ae hv" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer" rel="noopener ugc nofollow" target="_blank">乳腺癌数据集</a>类来为我们的模型找到最佳参数。我们将使用肿瘤的半径来预测肿瘤是恶性还是良性。</p><p id="eab0" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，我们导入我们需要的库:</p><pre class="le lf lg lh fd mw ll mx my aw mz bi"><span id="e98d" class="ma ju hy ll b fi na nb l nc nd"># Scikit learn's built-in Breast Cancer dataset<br/>from sklearn.datasets import load_breast_cancer</span><span id="3042" class="ma ju hy ll b fi ne nb l nc nd"># Library for scikit-learn compatible arrays and matrices<br/>import numpy as np</span><span id="b4ee" class="ma ju hy ll b fi ne nb l nc nd"># Library for plotting nice graphs<br/>import matplotlib.pyplot as plt</span></pre><p id="54eb" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后将我们的特征(肿瘤半径)与我们的目标变量(恶性或良性)分开:</p><pre class="le lf lg lh fd mw ll mx my aw mz bi"><span id="33ca" class="ma ju hy ll b fi na nb l nc nd"># Loads sklearn's Breast Cancer dataset<br/>dataset = load_breast_cancer()</span><span id="92ca" class="ma ju hy ll b fi ne nb l nc nd"># Set x as the tumour radius<br/>X = dataset.data[:100,0]</span><span id="d8aa" class="ma ju hy ll b fi ne nb l nc nd"># Set y as the tumour type (malignant (0) or benign (1))<br/>y = dataset.target[:100]</span></pre><p id="9723" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后将它们分开，这样20%的数据用于测试，其余的用于训练我们的模型:</p><pre class="le lf lg lh fd mw ll mx my aw mz bi"><span id="f1ef" class="ma ju hy ll b fi na nb l nc nd"># Split data into 20% testing and 80% training<br/>from sklearn.model_selection import train_test_split</span><span id="4a19" class="ma ju hy ll b fi ne nb l nc nd">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)</span></pre><p id="3d3b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的数据现在可以用于我们的逻辑回归模型了。我们需要做的第一件事是使用我们的训练集来训练它。请记住，逻辑回归是一种<em class="kz">监督学习算法，</em>意味着它从以前的数据中学习，以预测新的传入数据的值:</p><pre class="le lf lg lh fd mw ll mx my aw mz bi"><span id="9c82" class="ma ju hy ll b fi na nb l nc nd"># Train the model with our training set using logistic regression<br/>from sklearn.linear_model import LogisticRegression</span><span id="1798" class="ma ju hy ll b fi ne nb l nc nd"># Run Gradient Descent to get the values of Theta<br/>regressor = LogisticRegression()</span><span id="d728" class="ma ju hy ll b fi ne nb l nc nd">regressor.fit(X_train.reshape(-1,1),y_train)</span></pre><p id="f099" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以看到获得的<code class="du li lj lk ll b">Thetas</code>和<code class="du li lj lk ll b">Theta_0</code>的值:</p><pre class="le lf lg lh fd mw ll mx my aw mz bi"><span id="8c05" class="ma ju hy ll b fi na nb l nc nd">print(regressor.coef_) # Theta Vector<br/>print(regressor.intercept_) # Theta_0<br/>&gt;&gt; [[-0.8996107]] <br/>&gt;&gt; [11.83182617]</span></pre><p id="6a70" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们通过绘制图表来看看我们的模型相对于我们的测试集的表现如何:</p><pre class="le lf lg lh fd mw ll mx my aw mz bi"><span id="4f00" class="ma ju hy ll b fi na nb l nc nd">plt.scatter(X_test,regressor.predict(X_test.reshape(-1,1)),color='red')</span><span id="fd70" class="ma ju hy ll b fi ne nb l nc nd">plt.title('Tumour Type vs Size')</span><span id="7c31" class="ma ju hy ll b fi ne nb l nc nd">plt.xlabel('Tumor Radius')</span><span id="f41c" class="ma ju hy ll b fi ne nb l nc nd">plt.ylabel('Malignant or Benign')</span><span id="13a5" class="ma ju hy ll b fi ne nb l nc nd">plt.show()</span></pre><figure class="le lf lg lh fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nf"><img src="../Images/65105193acf2b988b7d50baefa4c704a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FQl2tSlwF8LfRK9XG_m7OQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 2: </strong>Logistic<strong class="bd jv"> </strong>Regression Predictions On Test Data</figcaption></figure><h1 id="30a3" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结论</h1><p id="1094" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在本文中，我们介绍了逻辑回归背后的理论，以及如何使用梯度下降算法来查找参数，从而为我们的数据点提供最佳拟合模型。我们还研究了如何使用Scikit Learn的逻辑回归类在我们选择的数据集上轻松使用该模型。</p><p id="835b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">尽管本文提供了大量信息，但还有很多我们没有涉及到的内容。以下是一些需要你思考的事情:</p><ul class=""><li id="f8c8" class="lm ln hy ix b iy iz jc jd jg lo jk lp jo lq js ng ls lt lu bi translated">为什么不能用MSE作为代价函数？如果我们对分类问题使用MSE运行梯度下降会发生什么？试着画出<code class="du li lj lk ll b">J</code>的曲线，看看结果是什么。</li><li id="a0a1" class="lm ln hy ix b iy lv jc lw jg lx jk ly jo lz js ng ls lt lu bi translated">我们如何扩展本文中使用的逻辑，将逻辑回归用于多类分类？例如，将产品分类为水果、蔬菜或其他。这里，有三个不同的类。</li><li id="80c1" class="lm ln hy ix b iy lv jc lw jg lx jk ly jo lz js ng ls lt lu bi translated">尝试在分类问题上使用线性回归，看看结果会有多差。</li><li id="b681" class="lm ln hy ix b iy lv jc lw jg lx jk ly jo lz js ng ls lt lu bi translated">试着理解我们如何得到逻辑回归的成本函数。</li><li id="f522" class="lm ln hy ix b iy lv jc lw jg lx jk ly jo lz js ng ls lt lu bi translated">为什么我们的<code class="du li lj lk ll b">Theta</code>向量中只有一个值？</li></ul><h1 id="4d5a" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">过去的文章</h1><ol class=""><li id="9d65" class="lm ln hy ix b iy kr jc ks jg nh jk ni jo nj js lr ls lt lu bi translated"><strong class="ix hz">第一部分:</strong> <a class="ae hv" href="https://ali-h-khanafer.medium.com/data-pre-processing-ee81bbe5cc77" rel="noopener">数据预处理</a></li><li id="c4c7" class="lm ln hy ix b iy lv jc lw jg lx jk ly jo lz js lr ls lt lu bi translated"><strong class="ix hz">第二部分:</strong> <a class="ae hv" href="https://ali-h-khanafer.medium.com/linear-regression-using-gradient-descent-intuition-and-implementation-522d43453fc3" rel="noopener">使用梯度下降的线性回归:直觉和实现</a></li></ol><h1 id="9768" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">参考</h1><ol class=""><li id="0e82" class="lm ln hy ix b iy kr jc ks jg nh jk ni jo nj js lr ls lt lu bi translated"><a class="ae hv" href="https://math.libretexts.org/Bookshelves/Calculus/Book%3A_Calculus_(OpenStax)/08%3A_Introduction_to_Differential_Equations/8.4%3A_The_Logistic_Equation#:~:text=dPdt%3DrP(1%E2%88%92PK,problem%20for%20P(t).&amp;text=If%20r%3E0%2C%20then%20the,grows%20rapidly%2C%20resembling%20exponential%20growth." rel="noopener ugc nofollow" target="_blank">自由文本的逻辑方程式</a></li><li id="6f83" class="lm ln hy ix b iy lv jc lw jg lx jk ly jo lz js lr ls lt lu bi translated"><a class="ae hv" href="https://www.coursera.org/learn/machine-learning?page=1" rel="noopener ugc nofollow" target="_blank">吴恩达的机器学习Coursera课程</a></li><li id="5c76" class="lm ln hy ix b iy lv jc lw jg lx jk ly jo lz js lr ls lt lu bi translated"><a class="ae hv" href="https://en.wikipedia.org/wiki/Cost_curve" rel="noopener ugc nofollow" target="_blank">维基成本曲线</a></li></ol></div></div>    
</body>
</html>
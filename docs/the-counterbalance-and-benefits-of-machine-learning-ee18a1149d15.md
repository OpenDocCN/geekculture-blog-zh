# 机器学习的平衡和好处

> 原文：<https://medium.com/geekculture/the-counterbalance-and-benefits-of-machine-learning-ee18a1149d15?source=collection_archive---------42----------------------->

![](img/6b8a2fa4bf816b5ce35730353711c5e0.png)

机器学习会一直存在下去；它的使用将继续渗透到我们基础设施的所有部门。它在很大程度上也将是无形的，危险在于我们对它的依赖也将是无形的。一些人张开双臂欢迎这一点，另一些人认为这是我们最终世界末日的前兆。事实是，就像许多如此复杂和困难的事情一样，双方都是正确的。

对我来说，有趣的问题不是这应该或不应该发生，而是我们应该做什么，我们如何依靠和限制它的影响，以及我们应该在哪里这样做。

为了开始理解如何处理这样的问题，我认为理解机器学习方法什么时候最有用，什么时候不有用是有益的。对于那些*不*有用的情况，答案通常是*而不是*现状，而是一种承认好处但防止灾难的替代或补充技术。

## 当你关心什么而不是为什么时，机器学习是很棒的

机器学习的一个伟大的、可能无害的应用是在推荐系统中。这些是引擎，基于很多(很多！)的数据，这些数据有时会给你神奇的建议，告诉你该读什么书，或者看什么节目。对于这样的系统，最重要的标准是推荐是否是一个好的推荐。对你来说，幕后进行的巫术没有结果的功效重要。*为什么*这个系统给我这些结果没有结果好不好重要。

另一个例子来自我们前段时间合作的一位教授；当时，我们正在考虑如何利用机器学习来增强 [QVscribe](https://qracorp.com/qvscribe) 的一些功能。TL；我们想弄清楚如何使用机器学习来分析需求或 RFP 的文本，以评估项目超出预算或时间的风险。他提到了他之前和一个学生一起做的一个可能相关的项目。他们试图分析上市公司的年度报告，以预测其未来的股票价格相对于前一年是上涨还是下跌。你可以想象，许多公司都有一个好主意，如果他们在未来一年的情况会更好或更差，在他们的年度报告中，他们会希望总是听起来积极。但是你也可以想象，有一些线索是训练有素的机器学到的！—眼睛可以察觉。现在，如果我是一个喜欢打赌的人，我只关心预测，那么这听起来很棒！但是，对于评估需求或 RFP 文档中的风险，仅仅预测灾难是不够的，即使我们的预测是正确的。为了有用，我们至少需要说，*为什么*文档很差，我们真的应该提出纠正措施。对于这一点，机器学习并没有那么有用。通常，对于自然语言处理类型的任务，“线索”是像单词的相对分组，或者甚至是作为一个单词的结尾和下一个单词的开头一起出现的特定音节。换句话说，机器学习经常发现并利用相关性来做出决策或预测。很多时候这正是正确的做法。其他时候，特别是如果因果关系是你所寻求的，那么机器学习就没那么有益了。

## 相关性与因果关系

这里还有一个更普通的例子来区分因果关系和相关性。我想知道今天是否会下雨。为此，我向窗外望去，看到人们走在繁忙的城市街道上。⁴:我注意到，每当我看到人们穿着橡胶靴或拿着雨伞，就更有可能下雨，所以每当我看到窗外的雨伞和橡胶靴，我就抓起雨衣。一切都好。现在，如果我*真正*关心的是下雨的原因，那么我不应该认为穿胶靴的人会下雨。这两个事件绝对有很强的相关性，但是为了理解是什么导致了降雨，我需要更多的东西。我需要一个雨的模型，我需要更深层次的概念，以及这些概念之间的关系。我需要意义。

简而言之，一方面是本体和语义，另一方面是统计和机器学习。揭示因果关系比揭示相关性要困难得多。特别是当数据是非结构化的时候，推断因果关系——因为这些特殊的问题，你的 RFP 有很高的项目失败风险——在很大程度上仍然是人类专业知识的领域。工具变得越来越好，但这是一个更难解决，也更有价值的问题。

## 当“为什么”至少和“什么”一样重要时，机器学习是危险的

机器学习需要数据。很多数据。它需要数据，因为它本质上是一项统计工作。大量的数据，大量的计算能力，任务就是发现模式。找到模式，你就有了预测能力。

但如果数据是倾斜的——对于任何有价值的东西来说，事情几乎总是倾斜和混乱的——那么这种倾斜就会被放大并融入机器学习的结果中。这可能很糟糕，因为机器学习算法预测某事的原因并不为人所知。数据经常是有偏差的，因为数据集中的值经常被无意中引入。这些可能是不言而喻的隐含假设，在某些情况下可能明显正确，而在其他情况下则明显错误。

因此，举例来说，一个算法可能试图预测犯罪将在哪里发生，但它*实际上*预测的可能是*逮捕*可能发生的地方。犯罪和逮捕根本不是一回事。预测犯罪可能有客观的外表，但以逮捕数据为条件的这些预测有各种各样的社会偏见和不对称。这可能会让一个糟糕的问题变得更糟。还是那句话，本体论 vs 统计学。价值观很重要。

## 无价值就是无意义

一种反对意见可能是，我们需要创建一个“好的”学习数据语料库。但这极难做到；现在还不知道(还？)一般怎么做。而且通常这甚至不是正确的事情——或者就像物理学家喜欢说的，“这甚至不是错误的”价值观产生偏见；他们偏爱某样东西，他们把某些主观标准提升到其他标准之上。但是价值观还不错。价值观是必须的。没有价值观，就真的没有意义。如果我们重视意义，我们就需要价值观。这是我认为经常被忽视的重要一点。如果你所关心的只是相关性——或者是*什么*事情一起出现，而不是*为什么*事情一起出现——那么你可以依靠对“好”数据集的纯粹统计分析。我们所说的“好”是指不受人类价值观或偏见影响的数据集。这很困难，但却至关重要。这很重要，因为任何包含的偏差(值)都会被技术放大。这是一种几乎总是坏事的腐败。

或者，如果你想揭开因果关系，如果你是在*为什么*而不是*什么*，那么你真的需要一个模型，或者一个世界理论。至少在今天，计算机不擅长建立模型。但是人类很擅长这个。当你建立模型的时候，你绝对想要价值。你*想让*说某些东西比其他东西更有价值。物理学在上个世纪取得的巨大成功——它们确实对社会产生了变革——实际上是因为我们已经变得非常擅长构建世界模型。我们*选择*某些项目或自由度是重要的，而其他的我们忽略或边缘化。我们是故意这样做的，而且非常明确。

## 凡事适可而止

关键是把机器学习描绘成既不完全好也不完全坏。你认为机器学习是一种可以很好利用的强大工具，但不是万能的良药。你需要一个平衡。

抗衡，是一种计算的绝对主义。有些事情是系统绝对不能做的。有些情况，如果遇到了，一定会在特定的时间内导致特定的行为。简而言之，机器必须做它应该做的事情，而不是做它不应该做的事情。

这是一种非常不同的机器学习方法，它是对更常见的神经网络的补充。在关键的安全社区中，以及在某种程度上在半导体行业中，这被称为形式方法。

在下一篇文章中，我将深入探讨这种互补的方法，在这种方法中，某些属性和行为是明确强制的(或明确禁止的)。我将尝试描述为了构建安全、可靠和健壮的自治系统，这两种方法是如何结合在一起的。

[1]研究制衡机器学习的另一个非常重要的轴是社会和社会政策轴。我在这里不讨论这些；已经有很多非常有资格的人写了这方面的文章。

[2]我这里的*不是指*数据隐私问题。这确实总是非常重要的。相反，我的意思是用于产生推荐的系统的特定特征对于推荐本身来说是次要的。

[3]征求建议书:一种招标文件，有时非常详细，概述政府或大公司想做什么或想买什么。

[4]其实我的窗外是一片森林，一条河流，但依然。
<html>
<head>
<title>Sparse Sabmanifold Convolutions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">稀疏sab流形卷积</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/3d-sparse-sabmanifold-convolutions-eaa427b3a196?source=collection_archive---------1-----------------------#2021-08-27">https://medium.com/geekculture/3d-sparse-sabmanifold-convolutions-eaa427b3a196?source=collection_archive---------1-----------------------#2021-08-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5fa0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">没有稀疏子流形3D卷积，用于实时激光雷达点云处理的CNN是不完整的。</p><h1 id="755d" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">回旋</h1><p id="c517" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">卷积是计算机视觉深度学习中处理数据最常见的操作。卷积神经网络(CNN)类似于基于深度学习的计算机视觉是如此普遍。</p><p id="e7d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">卷积是具有可学习参数的过滤器(矩阵/向量),用于从输入数据中提取低维特征。它们具有保留输入数据点之间的空间或位置关系的属性。卷积神经网络通过在相邻层的神经元之间实施局部连接模式来利用空间局部相关性。</p><p id="333f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">卷积运算不限于图像。卷积运算可应用于1D序列数据、2D相机图像或3D激光雷达点云。稀疏卷积在激光雷达信号处理中起着重要的作用。</p><blockquote class="kg kh ki"><p id="71db" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">一个解释稀疏子流形3D卷积的 <a class="ae kn" href="https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1" rel="noopener" target="_blank"> <em class="hi">博客</em> </a> <em class="hi">已经在TDS上可用。然而，当我浏览博客时，我觉得它本身是不完整的。在这篇博客中，我试图将理解稀疏子流形卷积概念所需的所有信息放在一起。</em></p></blockquote><h1 id="5b68" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">2D卷积</h1><p id="c310" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">在图像数据集上，CNN架构中主要使用2D卷积滤波器。2D卷积的主要思想是卷积滤波器在2个方向(x，y)上移动，以从图像数据中计算低维特征。输出形状也是二维矩阵。</p><h1 id="ea05" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">三维卷积</h1><p id="06fd" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">3D卷积将三维过滤器应用于数据集，过滤器沿3个方向(x，y，z)移动以计算低级要素制图表达。它们的输出形状是三维体积空间，例如立方体或长方体。它们有助于视频中的事件检测、3D医学图像、激光雷达点云处理等。</p></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="a828" class="jd je hi bd jf jg kv ji jj jk kw jm jn jo kx jq jr js ky ju jv jw kz jy jz ka bi translated">当我们已经有3D卷积时，为什么还要有子流形稀疏卷积？</h1><p id="c077" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">3D卷积运算利用了三维过滤器。这些3D过滤器很快增加了模型参数，使得模型过于庞大和缓慢，无法做任何有用的事情。</p><p id="170d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">而且由于图片、视频等常用数据大多可以看成是规则的密集网格，卷积也是基于这种密集结构的特点。当数据结构稀疏时，直接对这些数据应用传统的稠密卷积会由于在空的空间中进行无效的计算而浪费大量的计算资源，所以利用稀疏性来处理这些数据是非常重要的。</p><p id="262f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随着卷积层的加深，数据的稀疏性无法保持。例如，如果输入数据包含单个<strong class="ih hj"> <em class="kj">活性位点* </em> </strong>，那么在应用3×3卷积后，将有3×3活性位点。应用相同大小的第二个卷积将产生5×5个活性位点，依此类推。</p><blockquote class="kg kh ki"><p id="0e29" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">活动站点是指实际包含一些数据的空间中的点。想象在一张白纸上画一个圆。只有存在圆边界的纸张部分是活动的。参考下面的图1，同性恋区域可以被认为是一张2D纸，白色圆圈边界是2D平面上的活动点。</p></blockquote><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es la"><img src="../Images/be8d1a8065c7075340060ff2bf9a2e6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*DKiW87IAIQ6CLOYVYhEjQg.jpeg"/></div><figcaption class="li lj et er es lk ll bd b be z dx"><strong class="bd jf"><em class="lm">Figure 1: Submanifold expansion </em></strong>[Source: <a class="ae kn" href="https://arxiv.org/abs/1706.01307" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1706.01307</a>]</figcaption></figure><p id="8dc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上面的图1所示，输入数据是二维空间中的一维曲线。但是经过传统卷积后，提取的特征不再稀疏。“子流形稀疏卷积网络”的作者将这种现象称为子流形扩展问题。子流形是指稀疏的输入数据，比如二维空间的一维曲线，二维曲面，三维空间的点云。它们并不占据它们所在的整个空间。</p><p id="1a31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更好地理解这些子流形稀疏3D卷积的工作，有必要回顾一下常规卷积是如何操作以及它们是如何实现的。</p></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="801c" class="jd je hi bd jf jg kv ji jj jk kw jm jn jo kx jq jr js ky ju jv jw kz jy jz ka bi translated">如何实现常规/普通卷积</h1><p id="6864" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">为了理解正则卷积和稀疏卷积之间的区别，让我们简单地看一下正则卷积是如何实现的。</p><p id="fd3f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">是的，我知道你只需要调用torch.nn.Conv2D或tf.nn.conv2d，然后有一些DL向导魔法和puff，卷积就完成了。但是理解卷积算法实际上是如何工作的，以及它们在大多数标准DL库中是如何实现的，真的很有趣，也很重要。</p><blockquote class="kg kh ki"><p id="a5db" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">我推荐浏览一下<a class="ae kn" href="https://hackmd.io/@bouteille/B1Cmns09I" rel="noopener ugc nofollow" target="_blank">这个博客</a>，它很好地解释了卷积是如何从头开始实现的。所有很酷的gif都是从那个博客上摘下来的。</p></blockquote><p id="bd20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们需要做回旋，快。为了做到这一点，您不能进行嵌套循环实现。GPU不太喜欢循环，卷积在使用循环的CPU上非常慢(相信我)。所以…我们需要对卷积进行矢量化，以便它们可以利用由一些非常聪明的人(不像我)编写的GEMM(通用矩阵乘法)算法。</p><p id="ab19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤1:将我们的输入图像转换成一个矩阵(im2col) </strong></p><ul class=""><li id="a985" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc ls lt lu lv bi translated">首先，我们制作一个3D输入图像2D。怎么会？很酷的GIF描述了这是如何做到的。这个过程被称为im2col。</li></ul><figure class="lb lc ld le fd lf er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es lw"><img src="../Images/717fffe13f24b00330a2f52d8c271984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*hLiXaLRi4LmZfydDWWiV6w.gif"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx"><strong class="bd jf"><em class="lm">Figure 2: im2col<br/></em></strong>[Source: <a class="ae kn" href="https://hackmd.io/@bouteille/B1Cmns09I" rel="noopener ugc nofollow" target="_blank">https://hackmd.io/@bouteille/B1Cmns09I</a>]</figcaption></figure><ul class=""><li id="0f08" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc ls lt lu lv bi translated">如果你有<strong class="ih hj"> <em class="kj"> M </em> </strong>个图像(<strong class="ih hj"> <em class="kj"> M </em> </strong> &gt; 1)，你将有相同的矩阵但是水平堆叠<strong class="ih hj"> <em class="kj"> M </em> </strong>次。</li><li id="7a6b" class="ln lo hi ih b ii mb im mc iq md iu me iy mf jc ls lt lu lv bi translated">对中间特征地图也进行类似的操作。</li></ul><p id="16db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤2:重塑我们的内核(展平)</strong></p><ul class=""><li id="4c0e" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc ls lt lu lv bi translated">就像图像一样，但是对于卷积核。</li></ul><figure class="lb lc ld le fd lf er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mg"><img src="../Images/3c013f36b2cec46972f5221db65ca2ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*x4N2VWY-VYyI1oSJIxzKtQ.gif"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx"><strong class="bd jf"><em class="lm">Figure 3: im2col convolution kernel<br/></em></strong>[Source: <a class="ae kn" href="https://hackmd.io/@bouteille/B1Cmns09I" rel="noopener ugc nofollow" target="_blank">https://hackmd.io/@bouteille/B1Cmns09I</a>]</figcaption></figure><ul class=""><li id="59fc" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc ls lt lu lv bi translated">如您所见，每个过滤器都被展平，然后堆叠在一起。因此，对于<strong class="ih hj"> <em class="kj"> X </em> </strong>滤镜，我们将把<strong class="ih hj"> <em class="kj"> X </em> </strong>滤镜展平并叠加在一起。</li></ul><p id="0273" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤3:整形输入和内核之间的矩阵乘法</strong></p><ul class=""><li id="210a" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc ls lt lu lv bi translated">解释GEMM需要一个博客本身。为了让它超级容易理解，它是一个非常快速，易于并行化的矩阵乘法算法。</li></ul><figure class="lb lc ld le fd lf er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mh"><img src="../Images/d0d932dfeec9c9e235c180800a0f70c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*gZFsagPszm3z0AWBaZ6EEA.gif"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx"><strong class="bd jf"><em class="lm">Figure 4: convolution operation using GEMM<br/></em></strong>[Source: <a class="ae kn" href="https://hackmd.io/@bouteille/B1Cmns09I" rel="noopener ugc nofollow" target="_blank">https://hackmd.io/@bouteille/B1Cmns09I</a>]</figcaption></figure><ul class=""><li id="e3b2" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc ls lt lu lv bi translated">完成这一计算后，我们进行与步骤1相反的操作。这个过程通常被称为col2im。</li></ul></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="af34" class="jd je hi bd jf jg kv ji jj jk kw jm jn jo kx jq jr js ky ju jv jw kz jy jz ka bi translated">稀疏卷积如何工作</h1><p id="f67a" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">传统卷积使用FFT或im2col(如上所示)来构建计算流水线。这使得这些卷积中涉及的操作易于并行化。</p><p id="c7b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与传统卷积不同，子流形稀疏卷积处理不能直接应用GEMM的稀疏数据。它收集所有的原子操作w.r.t卷积核元素，并将它们作为计算指令保存在规则手册中，然后并行执行以获得加速。</p><p id="7139" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">子流形稀疏卷积有两个部分:</p><ol class=""><li id="b70e" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc mi lt lu lv bi translated"><strong class="ih hj">卷积运算后不添加到活动位置的子流形卷积</strong></li><li id="83d0" class="ln lo hi ih b ii mb im mc iq md iu me iy mf jc mi lt lu lv bi translated"><strong class="ih hj">稀疏卷积</strong>与常规卷积非常相似，但通过利用输入数据的稀疏性节省了大量计算。</li></ol><blockquote class="kg kh ki"><p id="dcd3" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">还记得子流形膨胀问题(博客开头提到的)吗？为了避免这种情况，我们需要考虑稀疏性，子流形卷积有助于实现这一点。</p></blockquote><p id="995d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">稀疏输入数据</strong></p><p id="bd5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了解释稀疏卷积的概念，以2D稀疏图像处理为例。由于稀疏信号用数据表和索引表表示，2D和3D稀疏信号没有本质区别。</p><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es mj"><img src="../Images/56ee17351511232f0af6dec17d50ca04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*jt0uaxF_RTW-mQAKpmy9YA.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx"><strong class="bd jf"><em class="lm">Figure 5: Sparse data representation<br/></em></strong>[Source: <a class="ae kn" href="https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1" rel="noopener" target="_blank">https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1</a>]</figcaption></figure><p id="7e0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上图所示，我们有一个带有3个通道的5x5图像。除了两点P1和P2，所有的像素都是(0，0，0)。这种非零元素也被称为<strong class="ih hj">有效输入位置。<br/> </strong>在稠密形式下，输入张量具有NCHW阶的形状【1x3x5x5】。稀疏形式下，数据表为[[0.1，0.1，0.1]，[0.2，0.2，0.2]]，索引表为[[1，2]，[2，3]]，YX顺序。</p><p id="2a8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">稀疏卷积核</strong></p><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es mk"><img src="../Images/429e2d942a617fc04864f26aed55b795.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*p3IIFn_wjRLTW_3AIwICEg.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx"><strong class="bd jf"><em class="lm">Figure 6: Sparse convolution kernel<br/></em></strong>[Source: <a class="ae kn" href="https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1" rel="noopener" target="_blank">https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1</a>]</figcaption></figure><p id="cba4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">稀疏卷积的卷积核与传统卷积的卷积核相同。上图就是一个例子，内核大小为3x3。深色和浅色代表两种滤镜。</p><p id="e722" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">稀疏卷积输出</strong></p><p id="641a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">稀疏卷积的输出与传统卷积完全不同。根据所应用的卷积类型，稀疏卷积有两种输出定义，即子流形卷积输出和稀疏卷积输出。</p><ol class=""><li id="72f1" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc mi lt lu lv bi translated"><strong class="ih hj">稀疏卷积输出:</strong>这个类似于常规卷积输出定义。就像普通卷积一样，只要内核覆盖一个输入站点，就计算输出站点。稀疏卷积和常规卷积的区别在于，与常规卷积不同，稀疏卷积放弃了对非活动区域的任何计算。换句话说，如果卷积核覆盖输入上没有任何活动点的区域，则不会为该区域计算输出。</li><li id="9fa1" class="ln lo hi ih b ii mb im mc iq md iu me iy mf jc mi lt lu lv bi translated"><strong class="ih hj">子流形卷积输出:</strong>对于该卷积，当且仅当输入处的位置是活动的时，才会计算输出，即只有当内核中心覆盖输入位置时，才会计算卷积输出。</li></ol><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es ml"><img src="../Images/665a02b096ac9da42a2573005343f6b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*ICx6ExAopJ8OGNW7mArabQ.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx"><strong class="bd jf"><em class="lm">Figure 7: Sparse and Submanifold convolution output<br/></em></strong>[Source: <a class="ae kn" href="https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1" rel="noopener" target="_blank">https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1</a>]</figcaption></figure><p id="0de2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图说明了这两种输出之间的区别。A1代表有效输出位置，即来自P1的卷积结果。类似地，A2代表从P2计算的活动输出站点。A1A2代表活动输出站点，是P1和P2输出的总和。深色和浅色代表不同的输出通道。</p><p id="6f98" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以在稠密形式下，输出张量具有NCHW阶的形状[1x2x3x3]。在稀疏形式中，输出有两个列表，一个数据列表和一个索引列表，它们类似于输入表示。</p><p id="0fa2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">稀疏卷积，就像规则卷积增加活性位点一样，那么我们为什么要使用它们呢？</p><p id="3855" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在网络中仅使用子流形卷积的一个潜在问题是，网络可能不会接收到分类输入数据所需的所有信息，特别是，两个相邻的连通分量被完全独立地对待。这个问题可以通过使用某种形式的池或步长卷积来解决。这种操作在稀疏卷积网络中是必不可少的，因为它们允许信息在输入中不相连的组件之间流动。组件在空间上越接近，组件在中间表示中“通信”所需的步进操作就越少。</p><p id="1a02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">换句话说，子流形卷积独立地处理输入中的所有活动位置，并为每个活动位置产生输出，因此如果只使用子流形卷积，将会丢失活动输入位置之间的空间关联。当使用稀疏卷积时，生成的输出中使用了多个输入活动部位，这增加了活动部位之间的空间关联。</p></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="8e40" class="jd je hi bd jf jg kv ji jj jk kw jm jn jo kx jq jr js ky ju jv jw kz jy jz ka bi translated">如何实现稀疏卷积</h1><p id="39d1" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">传统卷积通常使用im2col(如上图)将卷积重写为稠密矩阵乘法问题。然而，稀疏卷积使用索引生成算法和规则手册来调度和执行所有原子操作，而不是im2col。涉及的步骤如下所述。</p><p id="f40a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">哈希表生成</strong></p><p id="5413" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们创建一个哈希映射，包含关于活动输入站点和相应活动输出站点的信息，按照实现考虑稀疏或子流形卷积定义。</p><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es mm"><img src="../Images/b2574b8c934698c4572b2bd9b4dfd344.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*34mI2annqTA5dmfcCyjRIw.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx"><strong class="bd jf"><em class="lm">Figure 8: Input-Output hash map generation<br/></em></strong>[Source: <a class="ae kn" href="https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1" rel="noopener" target="_blank">https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1</a>]</figcaption></figure><p id="0150" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上图中，Hashin是输入哈希表。它存储所有输入活动站点，其中中的V <em class="kj">是哈希表的索引，而</em>中的关键字<em class="kj">描述了活动输入站点的空间位置。<br/>输出位置描述了根据稀疏卷积规则，在稀疏输入数据上卷积卷积核时产生的输出【参见图5和图7】。P <em class="kj"> out </em>保存这些输出活动点的空间位置。<br/>最后，生成一个输出哈希表Hash <em class="kj"> out </em>，它保存所有的输出空间位置以及关键字。</em></p><p id="69f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的图8中，哈希表中的V值是哈希键，键是哈希值。这随着索引生成算法的实现方式而改变。</p><p id="c599" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">规则书生成</strong></p><p id="e7ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦生成了哈希表，我们就有了关于活动输入站点以及它们将生成的相应活动输出站点的信息。现在第二步是建立规则手册。Rulebook的目的类似于im2col，即将卷积运算从数学形式(迭代滑动窗口运算)转换为高效的可编程形式。Rulebook收集卷积中涉及的所有原子操作，并将它们关联到相应的内核元素。</p><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es mm"><img src="../Images/323d9a82cc2145d5235ebe81b3eb0780.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fDWFf4Ei1a955g82xYw33A.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx"><strong class="bd jf"><em class="lm">Figure 9: Rulebook generation<br/></em></strong>[Source: <a class="ae kn" href="https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1" rel="noopener" target="_blank">https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1</a>]</figcaption></figure><p id="b26e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上图中，中的P <em class="kj">是描述活动输入点空间位置的输入列表。P <em class="kj"> out </em>保存通过用卷积核卷积活动输入位点而生成的活动输出位点的空间位置。Rulebook使用此信息从卷积计算过程的w.r.t .内核元素中收集所有原子操作。</em></p><p id="58de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上图中，单通道内核模板描绘了单通道3x3卷积内核。该模板的每个网格中填充的索引描述了卷积核所跨越的相对空间位置，卷积核的中心位于其原点(参见图10)。<br/>卷积运算时，卷积核中心位于空间位置(1，1)。在这种情况下，活动输入位置P1将在空间位置(0，0)生成活动输出位置，卷积核的索引(+1，0)与活动输入位置P1相互作用。当所描述的卷积核索引与活动输入站点交互时，输出的计算是一个原子操作，并被重新编码到规则手册中。</p><figure class="lb lc ld le fd lf er es paragraph-image"><div class="er es mj"><img src="../Images/27a3b72d691e7126990e2ca40ed00166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*a1LeruF99NobuZEqQjkRJg.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx"><strong class="bd jf"><em class="lm">Figure 10: Sparse Convolution Atomic Operation</em></strong></figcaption></figure><p id="f8ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图9中最右边的表格是规则手册。规则手册的第一列保存如上所述的卷积核元素索引。规则手册的第二列是关于这个卷积核元素涉及多少原子操作的计数器和索引。第三和第四列分别是这个原子操作中涉及的输入和输出哈希表键。</p><p id="1a97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">计算流水线</strong></p><p id="7a07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面的图11描述了稀疏卷积的完整计算流水线。</p><figure class="lb lc ld le fd lf er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mn"><img src="../Images/9e0122698552435482b59ef06df524ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Zl2A6HpNAFIVe8c_NbENg.png"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx"><strong class="bd jf"><em class="lm">Figure 11: Sparse Convolution Computation pipeline </em></strong>[Source: <a class="ae kn" href="https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1" rel="noopener" target="_blank">https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1</a>]</figcaption></figure><p id="0591" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">红色和蓝色箭头表示两个计算实例。红色箭头处理内核元素(-1，-1)的第一个原子操作。从Rulebook中，我们知道这个原子操作的输入来自位置(2，1)的P1，输出来自位置(2，1)。类似地，蓝色箭头表示另一个原子操作，它共享同一个输出站点。红色箭头实例和蓝色箭头实例的结果可以相加。</p><p id="c421" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">规则书中的计算可以在GPU中并行展开。</p></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="cee7" class="jd je hi bd jf jg kv ji jj jk kw jm jn jo kx jq jr js ky ju jv jw kz jy jz ka bi translated">结论</h1><p id="485b" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">是的，稀疏子流形卷积很难理解，但它们是有效的。第二次推出了第一个基于GPU的规则书生成算法，将使用3D卷积的CNN引入实时推理领域，从那时起，这种CNN就一直在发展。</p><p id="eeeb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">子流形稀疏3D卷积现在正被几乎所有其他激光雷达3D对象检测模型使用，以高精度实时进行3D对象检测，这是自动驾驶汽车所必须的。</p></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><h1 id="e2e4" class="jd je hi bd jf jg kv ji jj jk kw jm jn jo kx jq jr js ky ju jv jw kz jy jz ka bi translated">参考</h1><ul class=""><li id="ac6b" class="ln lo hi ih b ii kb im kc iq mo iu mp iy mq jc ls lt lu lv bi translated"><a class="ae kn" href="https://arxiv.org/abs/1711.10275" rel="noopener ugc nofollow" target="_blank">利用子流形稀疏卷积网络的3D语义分割</a></li><li id="5202" class="ln lo hi ih b ii mb im mc iq md iu me iy mf jc ls lt lu lv bi translated"><a class="ae kn" href="https://arxiv.org/abs/1706.01307" rel="noopener ugc nofollow" target="_blank">子流形稀疏卷积网络</a></li><li id="974d" class="ln lo hi ih b ii mb im mc iq md iu me iy mf jc ls lt lu lv bi translated"><a class="ae kn" href="https://www.researchgate.net/publication/328158485_SECOND_Sparsely_Embedded_Convolutional_Detection" rel="noopener ugc nofollow" target="_blank">第二:稀疏嵌入卷积检测</a></li><li id="6ea6" class="ln lo hi ih b ii mb im mc iq md iu me iy mf jc ls lt lu lv bi translated"><a class="ae kn" href="https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1" rel="noopener" target="_blank">稀疏卷积是如何工作的？</a></li><li id="5319" class="ln lo hi ih b ii mb im mc iq md iu me iy mf jc ls lt lu lv bi translated"><a class="ae kn" href="https://hackmd.io/@bouteille/B1Cmns09I" rel="noopener ugc nofollow" target="_blank">带Numpy的卷积神经网络</a></li></ul></div></div>    
</body>
</html>
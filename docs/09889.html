<html>
<head>
<title>Text Feature Extraction (3/3): Word Embeddings Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本特征提取(3/3):单词嵌入模型</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/text-feature-extraction-3-3-word-embeddings-model-e98f3d270dce?source=collection_archive---------14-----------------------#2022-01-03">https://medium.com/geekculture/text-feature-extraction-3-3-word-embeddings-model-e98f3d270dce?source=collection_archive---------14-----------------------#2022-01-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/15cfaa085cb6eecdd8128c7f338c53a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x9UAQ8ucKTnwSHjgWk2UfA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx"><a class="ae iu" href="https://www.namesnack.com/guides/nature-photography-business-names" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="862f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jt translated"><span class="l ju jv jw bm jx jy jz ka kb di">在</span>文本特征提取系列的前几部分中，我们已经看到在NLP中可以对基于文本的数据进行建模。已经讨论了特征提取技术，例如<a class="ae iu" href="https://shachikaul35.medium.com/text-feature-extraction-1-3-bag-of-words-model-649dbeeade79" rel="noopener"> BagOfWords </a>和TF-IDF。所以，在这里我们将学习另一种叫做单词嵌入的技术。单词嵌入已经成为一种非常重要和有趣的方法，在构建文本模型中有多种用途。让我们了解更多！</p><h1 id="f686" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">单词嵌入</h1><h2 id="9874" class="la kd hi bd ke lb lc ld ki le lf lg km jg lh li kq jk lj lk ku jo ll lm ky ln bi translated">目标</h2><p id="5580" class="pw-post-body-paragraph iv iw hi ix b iy lo ja jb jc lp je jf jg lq ji jj jk lr jm jn jo ls jq jr js hb bi translated">它旨在识别文本的语义和上下文信息，这在BOW和TFIDF中是不可能的。</p><h2 id="8d43" class="la kd hi bd ke lb lc ld ki le lf lg km jg lh li kq jk lj lk ku jo ll lm ky ln bi translated">详细研究</h2><ul class=""><li id="e356" class="lt lu hi ix b iy lo jc lp jg lv jk lw jo lx js ly lz ma mb bi translated"><strong class="ix hj">直觉:</strong>相似的语境(词)往往具有相似的向量空间或嵌入，并且彼此位置更近。</li><li id="2d69" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated"><strong class="ix hj">什么？</strong> <br/> -将<strong class="ix hj">文本转换成低维空间的实数向量</strong>的技术。这些低维的向量叫做<strong class="ix hj">嵌入</strong>。<br/> -单词嵌入，又名单词向量，用于表示单词<br/> -它将每个单词表示为具有特定维度的数字向量。这些数字维持了间接存储语义信息的各种其他词向量之间的相似性。女孩= [..,..,..]而女人= [..,..,..]密切相关。</li><li id="e7c2" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated"><strong class="ix hj">将单词的</strong>映射到向量空间</li><li id="bf67" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated">由于矢量，算术运算现在在文本中是可能的。<br/>例如，国王-男孩+女孩=王后<br/>国王向量(皇室+男性)减去男孩(正常+男性)再加上女孩(正常+女性)结果变成王后(皇室+女性)</li><li id="3c4f" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated">让我们看看<strong class="ix hj">这个矢量看起来</strong>怎么样。<br/>具有特定维度的向量如果使用英语语料库(en_core_web_lg)，假设有300个维度。这300个是英语单词的各种可能的特征。每一个特征都是人类无法解释但由ML模型获得单词的某一特性。因此，无法准确描述。<br/>例如，对于Queen，300-D向量，其中数字表示特征的大小，如图1所示</li></ul><figure class="mi mj mk ml fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/e3e447d7c041bcd52bec1583dea463d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*v7SWSAMFxYvlXlnjG59sng.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Fig-1</figcaption></figure><ul class=""><li id="8dfb" class="lt lu hi ix b iy iz jc jd jg mm jk mn jo mo js ly lz ma mb bi translated"><strong class="ix hj">单词嵌入为什么会出现？</strong> <br/>我们来调出一个场景<br/> Vocab words = 5000 <br/>一个矢量皇后投射在Vocab中的第5个索引上。通常，BOW和TFIDF中文本到文本的转换使用字数。<br/>皇后= [0，0，0，0，1，0..00..00..0]与5000-D <br/>这导致整个稀疏向量只有1位非零值。因此，出现了单词嵌入。<br/>所以确切原因:<br/> 1。<strong class="ix hj">稀疏性:</strong>单词嵌入使用特定特征维数的实值向量，而不是0。嵌入向量的大小更小。<br/> 2。<strong class="ix hj">语义&amp;上下文:</strong>通过考虑更接近目标词的上下文词来学习，这在TF-IDF以前的模型中是不可能的。<br/> 3。<strong class="ix hj">关系和相似度:</strong>向量允许执行算术运算，这样更容易找到相似的单词。<br/> 4。每个单词不被视为一个特征，而是一个固定维度的向量。因此，LSTM/GRU的算法并不复杂，也不容易训练。</li><li id="5d13" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated"><strong class="ix hj">应用:</strong>情感分析，语音识别，信息检索</li><li id="6967" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated"><strong class="ix hj">利用单词嵌入的</strong>:<br/>—预测目标/上下文单词<br/> —降维</li></ul><h2 id="70fa" class="la kd hi bd ke lb lc ld ki le lf lg km jg lh li kq jk lj lk ku jo ll lm ky ln bi translated">词嵌入伞</h2><p id="10cf" class="pw-post-body-paragraph iv iw hi ix b iy lo ja jb jc lp je jf jg lq ji jj jk lr jm jn jo ls jq jr js hb bi translated">单词嵌入是一个总括术语，具有预训练模型(Word2Vec，GloVe)或嵌入层的概念。</p><figure class="mi mj mk ml fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/a49c0c8fa000ad736e9dd08babef6270.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*_qHRQD2SY3Hol-lF2w6rPg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Fig-2</figcaption></figure><h2 id="4f28" class="la kd hi bd ke lb lc ld ki le lf lg km jg lh li kq jk lj lk ku jo ll lm ky ln bi translated">预训练模型</h2><p id="c1d6" class="pw-post-body-paragraph iv iw hi ix b iy lo ja jb jc lp je jf jg lq ji jj jk lr jm jn jo ls jq jr js hb bi translated">-使用预训练的词向量来构建模型<br/> -这些词向量来自于从庞大的语料库中训练嵌入<br/> - Sklearm的<strong class="ix hj"> Gensim </strong>库提供预训练的模型和API，如Word2Vec、GloVe、FastText等。</p><h2 id="7d98" class="la kd hi bd ke lb lc ld ki le lf lg km jg lh li kq jk lj lk ku jo ll lm ky ln bi translated">1.1 Word2Vec</h2><ul class=""><li id="27c7" class="lt lu hi ix b iy lo jc lp jg lv jk lw jo lx js ly lz ma mb bi translated">Google的神经网络嵌入模型，它训练网络从大量文本中学习单词之间的关系</li><li id="e4f8" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated">它是在1000亿个谷歌新闻数据集上训练出来的，有300万个单词</li><li id="56e1" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated">它将单词之间的关系嵌入到一个低维向量空间中</li><li id="dfc6" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated">产生单词嵌入</li><li id="5048" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated"><strong class="ix hj">输入、隐藏和输出层的2层神经网络</strong></li><li id="7c52" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated">症结:这里就像任何神经网络，目的不仅仅是预测概率，而是获得隐含层的训练权重(向量)。</li><li id="4f08" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated">由模型学习的隐藏层的权重被用作单词嵌入</li></ul><figure class="mi mj mk ml fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/7e7d8854a53ea092d94ed890b52b9f5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*BecfA72_331Ob6W0bnJrZg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Fig-3</figcaption></figure><ul class=""><li id="d20b" class="lt lu hi ix b iy iz jc jd jg mm jk mn jo mo js ly lz ma mb bi translated">它根据word2vec变体体系结构预测上下文单词和目标单词的概率</li></ul><figure class="mi mj mk ml fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/a3ad53ed2786cb31553671a82cb563df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*s8a8Dh0g1k-Vm55TG7DPzQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Fig-4, Word2Vec Variants Architecture: <a class="ae iu" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><ul class=""><li id="b5c9" class="lt lu hi ix b iy iz jc jd jg mm jk mn jo mo js ly lz ma mb bi translated">在训练之后，我们得到词汇表中每个单词的训练向量，并且彼此更接近(相似)的向量被更接近地放置在向量空间中。</li><li id="46fe" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated">我们来看看W2V车型的变种。</li></ul><figure class="mi mj mk ml fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/5f08ee9171414874d49a38c8557f6598.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*P769v-ZSIIobHs-eAvIk-w.png"/></div></figure><h2 id="9c07" class="la kd hi bd ke lb lc ld ki le lf lg km jg lh li kq jk lj lk ku jo ll lm ky ln bi translated">1.2手套(全局向量)</h2><ul class=""><li id="77e0" class="lt lu hi ix b iy lo jc lp jg lv jk lw jo lx js ly lz ma mb bi translated">我的研究为我找到了一个完美的定义</li></ul><p id="61e3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="mt">“GloVe是一个基于计数、</em> <strong class="ix hj"> <em class="mt">的无监督学习模型</em> </strong> <em class="mt">，它使用</em> <strong class="ix hj"> <em class="mt">共现</em> </strong> <em class="mt">统计在一个</em> <strong class="ix hj"> <em class="mt">全局级别</em> </strong> <em class="mt">对单词的向量表示进行建模。”</em></p><ul class=""><li id="0284" class="lt lu hi ix b iy iz jc jd jg mm jk mn jo mo js ly lz ma mb bi translated">原始论文<a class="ae iu" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank">中的定义https://nlp.stanford.edu/pubs/glove.pdf</a>指出—</li></ul><blockquote class="mu"><p id="9a51" class="mv mw hi bd mx my mz na nb nc nd js dx translated"><em class="ne"> GloVe基于单词-单词共现矩阵的概率比，结合了基于计数的模型的直觉，从而捕获word2vec等方法使用的线性结构。</em></p></blockquote><ul class=""><li id="51d5" class="lt lu hi ix b iy nf jc ng jg nh jk ni jo nj js ly lz ma mb bi translated"><strong class="ix hj">为什么这样命名？</strong> <br/>因为矩阵的数学是围绕考虑整个数据集的<strong class="ix hj">全局级</strong>属性，而在Word2Vec模型中考虑局部属性。这意味着Word2Vec的模型使用窗口长度得出考虑单词的统计数字，而GloVe考虑整个数据集来创建共现矩阵。</li><li id="9140" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated">由斯坦福大学研究人员发现的这种单词嵌入算法的工作原理是基于统计学(概率和计数)使用共现矩阵来描述单词之间的关系。</li><li id="5b22" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated"><strong class="ix hj">数学简介<br/> </strong> GloVe使用共现矩阵，其中每个值代表这两个词如何一起出现。它将共现概率视为区分两个相关词的比率，而不是概率本身。</li><li id="ccff" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated">它是在同现矩阵上训练的<strong class="ix hj">，该矩阵表示一对单词在语料库中一起出现的频率。</strong></li><li id="475b" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated">让我们理解一些符号和共生矩阵</li></ul><figure class="mi mj mk ml fd ij er es paragraph-image"><div class="er es nk"><img src="../Images/70bf982e2c1f4e4f20b6dc91db209111.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*mlyEWNkrp4ww3flvGiLnaQ.png"/></div></figure><p id="a1fa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">来自大型语料库的概率和比率</p><figure class="mi mj mk ml fd ij er es paragraph-image"><div class="er es nl"><img src="../Images/3d644f70941c0c8de901de9de2b37cb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*WYsyim29Jm21ULgUZ8Jo6Q.png"/></div></figure><p id="ca54" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上表类似于固体在冰中的概率较高，而在蒸汽中的概率较低。因为分子是较高的比率，两者都大于1，这与单词gas相反。</p><ul class=""><li id="c48e" class="lt lu hi ix b iy iz jc jd jg mm jk mn jo mo js ly lz ma mb bi translated">你可以从<a class="ae iu" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/projects/glove/</a>下载预先训练好的单词向量</li></ul><p id="aa1e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">查看我的</strong> <a class="ae iu" href="https://github.com/shachi01/NLP/blob/main/glove_v1.0.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj"> GitHub回购</strong> </a> <strong class="ix hj">实现手套。</strong></p><h2 id="dfc9" class="la kd hi bd ke lb lc ld ki le lf lg km jg lh li kq jk lj lk ku jo ll lm ky ln bi translated">2.嵌入层</h2><ul class=""><li id="89e4" class="lt lu hi ix b iy lo jc lp jg lv jk lw jo lx js ly lz ma mb bi translated">不使用预训练模型，你可以使用Keras嵌入层训练你自己的单词嵌入。</li><li id="b719" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated">Keras有一个嵌入层，用于在文本数据集上训练神经网络。它是一种在输入层之后指定的隐藏层，输入层采用整数编码的数据。</li><li id="c918" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated">通常，任何文本数据都使用某种编码技术(比如一次性编码)转换成数字。这将为每个分类值创建虚拟特征。对于大文本数据似乎不可行。Hecne，嵌入层将每个单词转换成定长向量。它将高维数据映射到低维向量空间。</li><li id="9dd0" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated"><strong class="ix hj">嵌入图层的用途:</strong> <br/> 1 .嵌入层可以单独使用，如在另一个模型中的<strong class="ix hj">2。有助于t <strong class="ix hj">转移学习</strong>；用于加载预训练的嵌入模型<br/> 3。作为<strong class="ix hj">深度学习模型</strong>的一部分进行自我学习</strong></li><li id="5a36" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated">简单实现参见<a class="ae iu" rel="noopener" href="/analytics-vidhya/understanding-embedding-layer-in-keras-bbe3ff1327ce">博客</a></li></ul><h1 id="c0f1" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">参考</h1><p id="90a9" class="pw-post-body-paragraph iv iw hi ix b iy lo ja jb jc lp je jf jg lq ji jj jk lr jm jn jo ls jq jr js hb bi translated"><a class="ae iu" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/projects/glove/</a><br/>T16】https://jalammar.github.io/illustrated-word2vec/<br/>T19】https://www.youtube.com/watch?v=Fn_U2OG1uqI</p></div><div class="ab cl nm nn gp no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="hb hc hd he hf"><p id="0e8a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="mt">如果你喜欢这位作者的博客，请随意关注，因为这位作者向你保证会带来更多有趣的人工智能相关的东西。<br/> </em>谢谢，<br/>学习愉快！😄</p><p id="78fc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="mt">可以通过</em></strong><a class="ae iu" href="https://www.linkedin.com/in/kaul-shachi" rel="noopener ugc nofollow" target="_blank"><strong class="ix hj"><em class="mt">LinkedIn</em></strong></a><strong class="ix hj"><em class="mt">取得联系。</em> </strong></p></div></div>    
</body>
</html>
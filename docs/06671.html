<html>
<head>
<title>Scribble Stadium — Bridging the Analog and Digital</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Scribble体育场——模拟与数字的桥梁</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/scribble-stadium-bridging-the-analog-and-digital-39865975a954?source=collection_archive---------40-----------------------#2021-08-25">https://medium.com/geekculture/scribble-stadium-bridging-the-analog-and-digital-39865975a954?source=collection_archive---------40-----------------------#2021-08-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/954751c62fe10fb92c0d85a14704386b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bpuQ907woQbHq8rI-YZ8MA.png"/></div></div></figure><figure class="hs ht hu hv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hr"><img src="../Images/b70a184bde550125a13b8f84a7854c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PhtTtledspMhVJ3gsD26Bg.png"/></div></div></figure><div class=""/><p id="7e04" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Scribble Stadium(原名为<a class="ae jt" href="https://www.storysquad.education/" rel="noopener ugc nofollow" target="_blank"> Story Squad </a>)是<a class="ae jt" href="https://www.linkedin.com/in/graig-peterson-225a421b6/" rel="noopener ugc nofollow" target="_blank"> Graig Peterson </a>的创意，他是一名六年级教师，他想把孩子们从“完美”的数字世界拉出来，让他们回到更混乱/更有创意的纸笔模拟世界。</p><p id="ac01" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为此，他设计了Scribble Stadium，这是一款网络应用程序，通过使用少量的屏幕时间，最大限度地利用创作者草草写下原创故事和图画的时间，将年轻的孩子从“不情愿的读者”变成有趣的世界构建游戏的作者和插画师。</p><p id="0cc8" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">虽然创意发生在模拟世界，但在用户将他们手工制作的故事和图画加载到应用程序后，这款游戏在数字世界发挥了它的所有荣耀。计算机视觉(CV)和光学字符识别(OCR)的工具和技术有助于弥合这两个世界之间的差距。</p><p id="0891" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">OCR用于将手写故事的图像转换或“转录”为文本。</p><figure class="hs ht hu hv fd hk er es paragraph-image"><div class="er es ju"><img src="../Images/91d5828e25b513171b44b2cda3236457.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uqkP4O__iITWZ0_-I-vfYg.png"/></div></figure><p id="b731" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这项技术的一个商业表现是Google Cloud Vision的DOCUMENT_TEXT_DETECTION API，它将图像数据作为输入，并返回转录的文本。</p><p id="3f89" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">另一个选择是谷歌维护的开源宇宙魔方项目，最初由惠普开发。这个项目已经发展了多年，从4.0版本开始，提供了一个基于LSTM神经网络的新OCR引擎——为问题空间训练网络的可用数据越多，它就变得越好。</p><p id="ad92" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">目前，Scribble Stadium使用Cloud Vision API来执行转录，但计划是转移到基于Tesseract的转录模型。虽然成本是一个原因，但它不是移动的主要驱动力-这将是一个信念，从长远来看，Scribble Stadium提供的儿童手写数据将有助于培训Tesseract，并使其成为一个优秀的OCR平台，用于转录儿童手写故事的特定问题空间。</p><p id="ee17" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们团队的任务是促进这一过渡，我们意识到这涉及到两个主要任务</p><ul class=""><li id="f610" class="jv jw hy ix b iy iz jc jd jg jx jk jy jo jz js ka kb kc kd bi translated">用新数据训练宇宙魔方语言模型</li><li id="1f65" class="jv jw hy ix b iy ke jc kf jg kg jk kh jo ki js ka kb kc kd bi translated">构建工具来计算OCR性能指标</li></ul><p id="881e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">本博客的其余部分将重点关注OCR性能指标任务。</p><h1 id="71f5" class="kj kk hy bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">计算OCR性能指标—比较假设和参考抄本</h1><p id="f82e" class="pw-post-body-paragraph iv iw hy ix b iy lh ja jb jc li je jf jg lj ji jj jk lk jm jn jo ll jq jr js hb bi translated">为了确定Tesseract引擎何时足以取代Google的Cloud Vision API，我们需要能够计算和比较OCR性能指标。这意味着能够测量OCR引擎生成的转录本(又名<em class="lm">假设</em>转录本)和人类生成的转录本(又名<em class="lm">参考</em>转录本)之间的差异。我们实现的方法包括使用NLP技术来计算和比较<em class="lm">假设</em>和<em class="lm">参考</em>转录本的向量。</p><h2 id="1f20" class="ln kk hy bd kl lo lp lq kp lr ls lt kt jg lu lv kx jk lw lx lb jo ly lz lf ma bi translated">抄本矢量化</h2><p id="7532" class="pw-post-body-paragraph iv iw hy ix b iy lh ja jb jc li je jf jg lj ji jj jk lk jm jn jo ll jq jr js hb bi translated">对于抄本矢量化，我们使用<em class="lm"> TfIdf(词频逆文档频率)</em>方法。</p><p id="092e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">假设有一个<em class="lm">词汇</em>中的<strong class="ix hz"> <em class="lm"> M个</em> </strong>术语，每一个文档都将由一个<strong class="ix hz"> <em class="lm"> M个</em> </strong>元素的向量来表示。</p><p id="2e7a" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于文档<em class="lm"> j </em>，向量的元素<em class="lm"> i </em>计算如下</p><figure class="hs ht hu hv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mb"><img src="../Images/4252209b1286c32c5901d0b36d52f00e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Co00Ttks463HJHyTiTjMoA.jpeg"/></div></div><figcaption class="mc md et er es me mf bd b be z dx">tfidf value for term <strong class="bd kl">i</strong> in document <strong class="bd kl">j</strong></figcaption></figure><p id="b0ce" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了确定<strong class="ix hz"><em class="lm"/></strong>、词汇<em class="lm">中的各个术语、</em>和<strong class="ix hz"> <em class="lm"> dfᵢ </em> </strong>的值，我们首先用<em class="lm">参考</em>转录本拟合<em class="lm"> TfIdf </em>模型。</p><p id="7fbb" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦完成，就可以使用上面定义的公式计算每个文档的<em class="lm"> TfIdf </em>向量。</p><p id="806b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这种矢量化方法(与所有单词包方法一样)的一个问题是，生成的向量具有非常高的维数，并且非常稀疏(许多零元素)，在Scribble Stadium的情况下，我们最终得到22，296个维度。为了解决这个问题，我们使用自动编码器降低了由<em class="lm"> TfIdf </em>矢量器生成的矢量的维数</p><h2 id="8d74" class="ln kk hy bd kl lo lp lq kp lr ls lt kt jg lu lv kx jk lw lx lb jo ly lz lf ma bi translated">使用自动编码器降维</h2><p id="98f2" class="pw-post-body-paragraph iv iw hy ix b iy lh ja jb jc li je jf jg lj ji jj jk lk jm jn jo ll jq jr js hb bi translated"><em class="lm">自动编码器</em>是一种神经网络，其中<em class="lm">输入</em>层连接到<em class="lm">编码器</em>层，后者汇聚到<em class="lm">瓶颈</em>层。这然后连接到<em class="lm">解码器</em>层和<em class="lm">输出</em>层，它们反映了<em class="lm">瓶颈</em>的另一侧。</p><figure class="hs ht hu hv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mg"><img src="../Images/82958206e27ceb49363c20caeac5b785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*juQaNfKukT_M_sdueAaqOA.jpeg"/></div></div><figcaption class="mc md et er es me mf bd b be z dx">Input layer connected to Encoder layers of 32, 16, and 8 neurons that go through a Bottleneck layer of 4 neurons to connect to Decoder layers of 8, 16, and 32 neurons and then to the Output layer.</figcaption></figure><p id="8037" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lm">自动编码器</em>被训练来获取它们的输入，通过<em class="lm">编码器</em>层向下压缩到<em class="lm">瓶颈</em>层，并通过<em class="lm">解码器</em>层重建它，以匹配<em class="lm">输出</em>层的输入。</p><p id="6bf1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦<em class="lm">自动编码器</em>被训练完毕，通过分解<em class="lm">自动编码器</em>并仅使用<em class="lm">输入</em>、<em class="lm">编码器</em>和<em class="lm">瓶颈</em>层，构建出<em class="lm">降维器</em>—<em class="lm">瓶颈</em>层成为<em class="lm">降维器</em>的<em class="lm">输出</em>层。</p><p id="e3c9" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在Scribble Stadium的情况下，<em class="lm">自动编码器</em>的<em class="lm">输入</em>层的大小为22296，其中3个<em class="lm">编码器</em>层分别为512个神经元、256个神经元和128个神经元，一个<em class="lm">瓶颈</em> / <em class="lm">输出</em>层为64个神经元。</p><p id="6dd5" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在使用<em class="lm">参考</em>转录本的<em class="lm"> TfIdf </em>向量进行训练之后，<em class="lm">自动编码器</em>被分解以创建能够将22，296维压缩到64维的<em class="lm">降维器</em>。</p><h2 id="8b39" class="ln kk hy bd kl lo lp lq kp lr ls lt kt jg lu lv kx jk lw lx lb jo ly lz lf ma bi translated">用余弦相似度比较矢量</h2><p id="a506" class="pw-post-body-paragraph iv iw hy ix b iy lh ja jb jc li je jf jg lj ji jj jk lk jm jn jo ll jq jr js hb bi translated">现在我们已经得到了合理大小的非稀疏向量，我们使用<em class="lm">余弦相似度</em>的概念来比较它们，其中我们主要计算向量之间角度的余弦。</p><figure class="hs ht hu hv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mh"><img src="../Images/f2495a66d57ef9003704a464869c2951.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CmE2yb1BRPq8hLU7TgtvSQ.png"/></div></div></figure><p id="7097" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">值的范围在0和1 <em class="lm"> </em>之间—值为</p><ul class=""><li id="1172" class="jv jw hy ix b iy iz jc jd jg jx jk jy jo jz js ka kb kc kd bi translated"><em class="lm"> 1 </em>为<em class="lm">余弦(0) </em>意味着矢量(文档)的完美对齐(匹配)</li><li id="bda9" class="jv jw hy ix b iy ke jc kf jg kg jk kh jo ki js ka kb kc kd bi translated"><em class="lm"> 0 </em>为<em class="lm">余弦(90) </em>意味着向量(文档)之间的正交性(没有关系)</li></ul><h2 id="29a2" class="ln kk hy bd kl lo lp lq kp lr ls lt kt jg lu lv kx jk lw lx lb jo ly lz lf ma bi translated">从余弦相似性生成误差/性能度量</h2><p id="9da5" class="pw-post-body-paragraph iv iw hy ix b iy lh ja jb jc li je jf jg lj ji jj jk lk jm jn jo ll jq jr js hb bi translated">一旦我们计算出<em class="lm">余弦相似度</em>，我们就有了一个在<em class="lm">【0，1】</em>范围内的小数部分</p><figure class="hs ht hu hv fd hk er es paragraph-image"><div class="er es mi"><img src="../Images/8fe6c2eb519622cd0199b86e4ca956a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*FVIgdx_4wmm2e3FvYXkThw.png"/></div><figcaption class="mc md et er es me mf bd b be z dx">Mean Cosine Similarity values for the various Transcript types</figcaption></figure><p id="e446" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这些数字不容易比较。</p><p id="2323" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">理想情况下，我们希望将这些小数转换成整数值，这样可以更容易地比较OCR性能。</p><p id="4ffe" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最有效的公式是</p><figure class="hs ht hu hv fd hk er es paragraph-image"><div class="er es mj"><img src="../Images/5b097d4471486d1a626b09be3bcb58b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*eo5m7OXu6zrwJAoupBt57A.jpeg"/></div></figure><p id="77ad" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为在将参考转录本与其自身进行比较的情况下，这给了我们一个零误差值(或至多低个位数)。</p><figure class="hs ht hu hv fd hk er es paragraph-image"><div class="er es mk"><img src="../Images/abe681e23c4dd1f50f28e3a2161ac893.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*NRzIowlerCp5Ro4odurZAg.png"/></div><figcaption class="mc md et er es me mf bd b be z dx">Mean Error values for the various Transcript types</figcaption></figure><h1 id="4ecf" class="kj kk hy bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">OCR性能—运行数字</h1><p id="ef8d" class="pw-post-body-paragraph iv iw hy ix b iy lh ja jb jc li je jf jg lj ji jj jk lk jm jn jo ll jq jr js hb bi translated">制造了两件工具:</p><ul class=""><li id="db3d" class="jv jw hy ix b iy iz jc jd jg jx jk jy jo jz js ka kb kc kd bi translated"><em class="lm">google _ tessera CT _转录本— </em>用于使用Google或tesseract OCR引擎生成<em class="lm">假设</em>转录本。</li><li id="6e11" class="jv jw hy ix b iy ke jc kf jg kg jk kh jo ki js ka kb kc kd bi translated"><em class="lm"> ocr_performance </em> —用于将<em class="lm">假设</em>副本与<em class="lm">参考</em>副本进行比较，并生成每幅图像的计算误差值以及平均误差值的CSV文件。</li></ul><h2 id="ed5d" class="ln kk hy bd kl lo lp lq kp lr ls lt kt jg lu lv kx jk lw lx lb jo ly lz lf ma bi translated">用参考转录进行验证</h2><p id="6f23" class="pw-post-body-paragraph iv iw hy ix b iy lh ja jb jc li je jf jg lj ji jj jk lk jm jn jo ll jq jr js hb bi translated">我们需要做的第一件事是验证所有的工具，确保当我们比较参考抄本和它们自己时，我们得到的错误值为0。</p><p id="7975" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">那有效！</p><h2 id="5218" class="ln kk hy bd kl lo lp lq kp lr ls lt kt jg lu lv kx jk lw lx lb jo ly lz lf ma bi translated">要打败的人——谷歌转录</h2><p id="4b2c" class="pw-post-body-paragraph iv iw hy ix b iy lh ja jb jc li je jf jg lj ji jj jk lk jm jn jo ll jq jr js hb bi translated">下一步是看看谷歌的云视觉API表现如何。</p><p id="da56" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">运行这个工具给我们一个663的错误值。</p><h2 id="d243" class="ln kk hy bd kl lo lp lq kp lr ls lt kt jg lu lv kx jk lw lx lb jo ly lz lf ma bi translated">具有LSTM模型的宇宙魔方—小型训练集</h2><p id="485d" class="pw-post-body-paragraph iv iw hy ix b iy lh ja jb jc li je jf jg lj ji jj jk lk jm jn jo ll jq jr js hb bi translated">对宇宙魔方引擎的最初评估使用了一个LSTM模型(名为<strong class="ix hz"><em class="lm">【ssq】</em></strong>),该模型在可用数据的一个小子集上进行训练。</p><p id="92f2" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如所料，它的表现比谷歌差得多，错误值为4428。</p><h2 id="2475" class="ln kk hy bd kl lo lp lq kp lr ls lt kt jg lu lv kx jk lw lx lb jo ly lz lf ma bi translated">具有LSTM模型的宇宙魔方—较大的训练集</h2><p id="5a08" class="pw-post-body-paragraph iv iw hy ix b iy lh ja jb jc li je jf jg lj ji jj jk lk jm jn jo ll jq jr js hb bi translated">然后，我们用一个LSTM模型(名为<strong class="ix hz"> <em class="lm"> storysquad </em> </strong>)来评估Tesseract，该模型用一个更大的可用数据子集来训练。这个模型是由一位队友创建的，她在<a class="ae jt" rel="noopener" href="/@saraewestds/how-to-read-messy-handwriting-computer-edition-8546c7678c01"> this </a> blog post中记录了她的工作。</p><p id="d2fd" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们期望比较小训练集的性能更好，这就是我们得到的，错误值为3567。</p><h2 id="0f24" class="ln kk hy bd kl lo lp lq kp lr ls lt kt jg lu lv kx jk lw lx lb jo ly lz lf ma bi translated">比较它们</h2><p id="54b8" class="pw-post-body-paragraph iv iw hy ix b iy lh ja jb jc li je jf jg lj ji jj jk lk jm jn jo ll jq jr js hb bi translated">从上面描述的各种运行中加载并绘制每幅图像的误差值给了我们这个有趣的图表。</p><figure class="hs ht hu hv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ml"><img src="../Images/659e2fe171f9f7387b567572cfaa6f79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HJSVsDu8m5XM2-g_ruvesw.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx">Error-values per image for the Google, Tesseract/ssq, and Tesseract/storysquad OCR engines</figcaption></figure><h1 id="65d7" class="kj kk hy bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">前进的道路</h1><p id="e77e" class="pw-post-body-paragraph iv iw hy ix b iy lh ja jb jc li je jf jg lj ji jj jk lk jm jn jo ll jq jr js hb bi translated">在这一点上，我们已经验证了工具和假设，即宇宙魔方引擎的性能将随着更多的训练数据而提高。</p><p id="cfe0" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，整个重点转移到重复以下步骤，直到我们能够匹配/击败谷歌的CloudVision性能:</p><ul class=""><li id="ddae" class="jv jw hy ix b iy iz jc jd jg jx jk jy jo jz js ka kb kc kd bi translated">用更多的训练数据收集和更新宇宙魔方的LSTM模型</li><li id="50ce" class="jv jw hy ix b iy ke jc kf jg kg jk kh jo ki js ka kb kc kd bi translated">用更新的模型生成新的<em class="lm">假设</em>转录本</li><li id="3cf3" class="jv jw hy ix b iy ke jc kf jg kg jk kh jo ki js ka kb kc kd bi translated">用新的抄本计算错误/性能数字</li></ul><p id="3724" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦达到该性能水平，就有必要划分出一个不用于训练LSTM模型的验证集，然后重新配置工具，只使用验证集来比较Google和Tesseract OCR引擎。</p><h1 id="1173" class="kj kk hy bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">感谢</h1><p id="36d2" class="pw-post-body-paragraph iv iw hy ix b iy lh ja jb jc li je jf jg lj ji jj jk lk jm jn jo ll jq jr js hb bi translated">非常感谢<a class="mm mn ge" href="https://medium.com/u/d6121049f7b2?source=post_page-----39865975a954--------------------------------" rel="noopener" target="_blank"> Sara E West </a>和<a class="mm mn ge" href="https://medium.com/u/37efc3d2d7a4?source=post_page-----39865975a954--------------------------------" rel="noopener" target="_blank"> Temsy Chen </a>帮助提高这篇文章的可读性。</p></div></div>    
</body>
</html>
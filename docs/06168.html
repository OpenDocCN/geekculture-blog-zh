<html>
<head>
<title>Deep Learning: Meaning, Motivation, and NN Basic Structure</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习:意义、动机和神经网络基本结构</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/deep-learning-meaning-motivation-and-nn-basic-structure-44b57b481e4c?source=collection_archive---------7-----------------------#2021-08-09">https://medium.com/geekculture/deep-learning-meaning-motivation-and-nn-basic-structure-44b57b481e4c?source=collection_archive---------7-----------------------#2021-08-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/bd406d995d8b2a756393465e4524a78f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-cprxnyeLVN1oD_GU7EfgQ.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="5fd0" class="pw-subtitle-paragraph iq hs ht bd b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh dx translated">什么是深度学习，为什么需要深度学习，通过神经网络最基本的结构了解神经网络</h2></div><p id="4da9" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">人工智能、机器学习、神经网络:如果你正在阅读这篇文章，这些都是你每天都会听到的时髦词汇。这是有原因的。</p><p id="b299" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">人工智能已经成为当今计算机科学中最热门的话题之一，该领域的进步丝毫没有减缓。见鬼，即使你不是计算机科学家，你一生中也至少有一次想过人工智能系统如何变得比人类更聪明，最终接管整个社会。</p><p id="cb85" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">如今一个更热门的话题是深度学习(DL)，它是人工智能的直接衍生物，据称将带给我们最接近全计算机智能的东西。</p><blockquote class="ke kf kg"><p id="ea73" class="ji jj kh jk b jl jm iu jn jo jp ix jq ki js jt ju kj jw jx jy kk ka kb kc kd hb bi translated"><strong class="jk hu">深度学习</strong>是机器学习的一个子领域，涉及受大脑结构和功能启发的算法，称为<strong class="jk hu">人工神经网络</strong>。<em class="ht">【1】</em></p></blockquote><p id="bff3" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">在这个定义中有很多东西需要解开，这也是我们在这个新系列文章中要做的事情。</p><p id="4fe3" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">在第一篇文章中，我们将首先描述传统机器学习算法的一些局限性，以及神经网络(NN)如何被用作这些局限性的答案。从那里，我们将通过解释一个神经网络的基本结构，以及如何进一步开发这个基本神经网络来创建高度复杂的DL算法，为以后的DL文章打下基础。</p><p id="ed38" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我写这篇文章的前提是假设您已经了解基本的机器学习概念，例如:</p><ul class=""><li id="5cad" class="kl km ht jk b jl jm jo jp jr kn jv ko jz kp kd kq kr ks kt bi translated">什么是机器学习算法</li><li id="ef2c" class="kl km ht jk b jl ku jo kv jr kw jv kx jz ky kd kq kr ks kt bi translated">监督与非监督学习</li><li id="7f10" class="kl km ht jk b jl ku jo kv jr kw jv kx jz ky kd kq kr ks kt bi translated">回归与分类</li><li id="88fb" class="kl km ht jk b jl ku jo kv jr kw jv kx jz ky kd kq kr ks kt bi translated">如何训练、验证和测试ML算法</li><li id="f3c3" class="kl km ht jk b jl ku jo kv jr kw jv kx jz ky kd kq kr ks kt bi translated">基本的最大似然算法，如线性回归、逻辑回归等。</li></ul><p id="97a5" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">如果没有，我会让你参考我写的关于基本机器学习算法和概念的系列文章:</p><div class="hh hi ez fb hj kz"><a href="https://ali-h-khanafer.medium.com/list/introductory-machine-learning-concepts-44fd6466f471" rel="noopener follow" target="_blank"><div class="la ab dw"><div class="lb ab lc cl cj ld"><h2 class="bd hu fi z dy le ea eb lf ed ef hs bi translated">中等</h2><div class="lg l"><h3 class="bd b fi z dy le ea eb lf ed ef dx translated">基本机器学习算法和概念列表</h3></div><div class="lh l"><p class="bd b fp z dy le ea eb lf ed ef dx translated">ali-h-khanafer.medium.com</p></div></div></div></a></div><p id="d497" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">让我们开始吧。</p><h1 id="9cec" class="li lj ht bd lk ll lm ln lo lp lq lr ls iz lt ja lu jc lv jd lw jf lx jg ly lz bi translated">传统最大似然算法的局限性</h1><p id="4c94" class="pw-post-body-paragraph ji jj ht jk b jl ma iu jn jo mb ix jq jr mc jt ju jv md jx jy jz me kb kc kd hb bi translated">一方面，当你处理简单的数据集时，传统的机器学习算法非常好。如果你研究过线性回归、逻辑回归或KNN之类的算法，那么你就有了第一手经验，知道这些算法可以如何很好地解决复杂的问题，例如预测房价或向用户推荐商品。</p><p id="8704" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">另一方面，当处理更复杂的数据集，即要素和输出之间的关系不容易理解的数据集时，这些算法可能会变得非常低效。让我们看一个例子。</p><p id="a3a0" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">假设您要使用某种分类算法来确定肿瘤是癌性的还是恶性的。为此，您决定使用肿瘤的两个特征:大小(<code class="du mf mg mh mi b">x_1</code>)和硬度(<code class="du mf mg mh mi b">x_2</code>)。下面是一些训练数据的绘制示例，其中<code class="du mf mg mh mi b">O</code>表示标记为恶性的肿瘤，<code class="du mf mg mh mi b">X</code>表示标记为癌性的肿瘤:</p><figure class="mk ml mm mn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mj"><img src="../Images/2ce02b9eaf6be4e434f6c710557844e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6HaFTNgDO9kjLKTixUhIVA.jpeg"/></div></div><figcaption class="mo mp et er es mq mr bd b be z dx"><strong class="bd lk">Figure 1: </strong>Example Tumor Training Data</figcaption></figure><p id="35f5" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我们可以用什么样的假设来拟合这个训练数据？以下是几个选项，以及它们的大致决策界限:</p><figure class="mk ml mm mn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ms"><img src="../Images/b8b39d96ec050c7e570f4f4ed1662481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I1H52jkv8p6PesPfTyd6DQ.png"/></div></div></figure><figure class="mk ml mm mn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mt"><img src="../Images/cad7e71eb1062e168d049dc3c7c767c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KgIno8ghOmkczExdOAX4xg.png"/></div></div></figure><figure class="mk ml mm mn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mu"><img src="../Images/58a540ca05d30f3bf7ed8e77429c47b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Hm6vrb26_ETAyG2aTGS6A.png"/></div></div><figcaption class="mo mp et er es mq mr bd b be z dx"><strong class="bd lk">Figure 2: </strong>Different Hypothesis Possibilities</figcaption></figure><p id="e323" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">这些都没有真正成功地对我们的训练集进行正确分类，迫使我们提出一个更复杂的假设，一个更高阶的假设，可能还有更多术语。反过来，这将导致更复杂的决策边界，这可能是我们区分恶性肿瘤和癌性肿瘤所需要的。例如:</p><figure class="mk ml mm mn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mv"><img src="../Images/f491b0e22c45e8f3e24c7885ae693e33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E5y0YThLHMSdcbjr7sV-Cg.png"/></div></div><figcaption class="mo mp et er es mq mr bd b be z dx"><strong class="bd lk">Figure 3: </strong>Example of a More Complex Hypothesis</figcaption></figure><p id="b4dc" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">好点了，但没那么有说服力。我们能提出一个更详细的假设吗？一个更高的层次，更多的术语？大概吧。我们应该吗？我们愿意走多远？如果一个问题只有两个特征，我们已经有了一个复杂的假设，有很多术语。想象一下，现在，我们要用100个特征来解决我们的问题:肿瘤的大小、硬度、患者的性别、年龄等等。最坏的情况是，我们最终不得不采用所有可能的二阶项来形成我们的假设，留给我们一个10000项的假设。事实上，对于任何考虑了所有二阶项的假设，我们都会得到<code class="du mf mg mh mi b">O(n^2)</code>项，其中<code class="du mf mg mh mi b">n</code>是特征的数量:</p><figure class="mk ml mm mn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mw"><img src="../Images/6552baa0d835ac11033bdd94fc7ab39e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2YXMsvI5qUlyN4esMJ9I3g.jpeg"/></div></div><figcaption class="mo mp et er es mq mr bd b be z dx"><strong class="bd lk">Figure 4: </strong>Example of All the Possible Second-Order Terms</figcaption></figure><p id="82a5" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">如果我们考虑所有三阶项，我们最终会得到<code class="du mf mg mh mi b">O(n^3)</code>项。那是许多术语。</p><p id="0fdf" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">这是不是很牵强？我们会需要那么多特征或者那么复杂的假设吗？让我们举最后一个例子来说明这一点。</p><p id="46d9" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">神经网络的一个非常常见的用途是对象识别。例如，给定一个任意的图像，我们希望我们的算法能够识别图像中是否包含一个足球。</p><figure class="mk ml mm mn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mx"><img src="../Images/198c52dcc355cc82ec1b9710acafe4d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QM1vNn3_68K1gugV4VhRrw.jpeg"/></div></div></figure><p id="6ab8" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">处理图像意味着我们的算法将把像素颜色值作为输入。假设我们的图像是灰度图像，图像大小为800 x 801(这是上面图像的实际大小)，那么我们的特征空间中就有640800个特征…在最坏的情况下，如果我们考虑所有的二阶项，这将超过4000亿项。想象一下，例如，在自动驾驶汽车中，这将如何表现。</p><p id="7059" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">那么解决的办法是什么。现在你可能已经猜到了:神经网络。</p><h1 id="874d" class="li lj ht bd lk ll lm ln lo lp lq lr ls iz lt ja lu jc lv jd lw jf lx jg ly lz bi translated">神经网络</h1><p id="c094" class="pw-post-body-paragraph ji jj ht jk b jl ma iu jn jo mb ix jq jr mc jt ju jv md jx jy jz me kb kc kd hb bi translated">虽然神经网络是在最近十年左右普及的，但它已经存在了一段时间。这个课题是由沃伦·麦卡洛克和沃尔特·皮茨在20世纪40年代开创的，此后许多其他人对这个领域做出了贡献。1958年，弗兰克·罗森布拉特创造了<strong class="jk hu">感知器</strong>，一种监督学习分类算法【3】。虽然感知器不再像过去那样被广泛使用，但它是通向今天使用的更现代网络的一个很好的途径，所以我们将从解释它的工作方式开始。</p><h2 id="0a3c" class="my lj ht bd lk mz na nb lo nc nd ne ls jr nf ng lu jv nh ni lw jz nj nk ly nl bi translated">感知器</h2><p id="3b50" class="pw-post-body-paragraph ji jj ht jk b jl ma iu jn jo mb ix jq jr mc jt ju jv md jx jy jz me kb kc kd hb bi translated">感知器是一种二进制分类算法，它将几个二进制值作为输入，并输出一个二进制值:</p><figure class="mk ml mm mn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nm"><img src="../Images/3b9ae96f8ed3208262701446e16fe8c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qlig_eR7vBjsLiVlASnC5Q.jpeg"/></div></div><figcaption class="mo mp et er es mq mr bd b be z dx"><strong class="bd lk">Figure 5: </strong>Basic Perceptron</figcaption></figure><p id="4450" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><strong class="jk hu">图5 </strong>是一个最基本的感知器的例子。我们来解释一下它的不同部分。</p><p id="70ef" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><code class="du mf mg mh mi b">x_1,x_2,...,x_m</code>是<code class="du mf mg mh mi b">m</code>二进制输入，它们组成了<strong class="jk hu">输入层</strong>。<code class="du mf mg mh mi b">w_1,w_2,...,w_n</code>就是罗森布拉特所说的<strong class="jk hu">砝码</strong>。这些权重用于描述一个输入对总输出的影响。权重较高的输入比权重较低的输入对整体结果的影响更大。所有有箭头进入<strong class="jk hu">和离开</strong>的圆圈代表通过某个函数<code class="du mf mg mh mi b">z</code>运行输入的想法，这又会产生一些输出。这些被称为<strong class="jk hu">神经元</strong>。请注意，<code class="du mf mg mh mi b">x_1,x_2,...,x_m</code>的圆圈没有箭头进入，只有箭头出来，因此它们不运行任何功能，它们仅仅代表输入。在感知器中，神经元中使用的函数是输入和权重之间的加权和:</p><figure class="mk ml mm mn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nn"><img src="../Images/604bec78cdd5b15140654b6776b62b2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sr-imvKIPB3jqkSiu4Vasw.jpeg"/></div></div><figcaption class="mo mp et er es mq mr bd b be z dx"><strong class="bd lk">Equation 1: </strong>Perceptron Neuron Function</figcaption></figure><p id="d9c7" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">输出基于以下规则:</p><figure class="mk ml mm mn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es no"><img src="../Images/9eee9897d89215ee1a75c381f0066604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W43ap18LOs1NSo9bzypkQw.jpeg"/></div></div><figcaption class="mo mp et er es mq mr bd b be z dx"><strong class="bd lk">Equation 2: </strong>Perceptron Output Rule</figcaption></figure><p id="a70b" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><em class="kh">阈值</em>是我们选择的预定值。更大的阈值意味着我们对感知机何时输出1没有那么严格。更小的门槛意味着我们更严格。</p><p id="e147" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我们对感知机的代数描述可能会有点混乱。我们试着简化一下。我们可以做的一件事是创建我们的输入和权重的向量，并使用两者之间的点积，给我们留下与我们在<strong class="jk hu">等式1 </strong>中使用求和符号时相同的精确结果。我们还可以将阈值移到不等式的另一边，并为其分配变量<code class="du mf mg mh mi b">b</code>，使得<code class="du mf mg mh mi b">b = -threshold</code>:</p><figure class="mk ml mm mn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es np"><img src="../Images/d87a3a0521884026cc0cef5a202fc86c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dZU-Ff9jjnzVTyTpkeZEFg.jpeg"/></div></div><figcaption class="mo mp et er es mq mr bd b be z dx"><strong class="bd lk">Equation 3: </strong>Vectorized Output Rule</figcaption></figure><p id="2cb7" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">这是你通常会发现的感知器的描述，也是我们在学习更现代的神经网络时会用到的。</p><p id="45a3" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">感知器不一定只包含一个神经元。我们可以创建一个感知器网络，因此命名为<strong class="jk hu">神经网络</strong>:</p><figure class="mk ml mm mn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nq"><img src="../Images/182ed60b8078c7e2891aa0f7a27fc905.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7_k-KXmWnqtAbz70w-nuqQ.jpeg"/></div></div><figcaption class="mo mp et er es mq mr bd b be z dx"><strong class="bd lk">Figure 6: </strong>Multilayer Perceptron</figcaption></figure><p id="549f" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我们将带有输出神经元的层称为<strong class="jk hu">输出层</strong>。输入层和输出层之间的任何层都被称为<strong class="jk hu">隐藏层</strong>。</p><p id="f39a" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">感知器对于线性可分离数据非常有用，即可以使用线、平面或超平面分离的数据[5]。这意味着它没有解决我们之前的问题，我们需要一种方法来创建高度复杂的决策边界，而不需要非常大的假设。</p><p id="bf4f" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">输出仅限于零和一也带来了一些限制。假设我们想使用图6 中的网络来确定写在图像上的数字是否为零。如果图像包含零，我们希望输出1，否则输出0。出于某种未知的原因，我们的网络不断地将数字1的图像标记为数字0。我们能做什么？我们可以尝试稍微改变一些重量。但是权重的微小变化会在<strong class="jk hu"> </strong>输出节点上产生更显著的变化。它的影响相当于将输出节点的结果从1变为0，反之亦然。因此，即使我们开始正确地识别1不是0，网络的其他部分在处理1和0以外的数字时，比如说9，也可能出错。</p><p id="af2b" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">感知器所有这些局限性的解决方案是<strong class="jk hu">乙状神经元</strong>、<strong class="jk hu"> </strong>当今更现代、更广泛使用的神经网络类型。</p><h2 id="cbf5" class="my lj ht bd lk mz na nb lo nc nd ne ls jr nf ng lu jv nh ni lw jz nj nk ly nl bi translated">乙状结肠神经元</h2><p id="ab8e" class="pw-post-body-paragraph ji jj ht jk b jl ma iu jn jo mb ix jq jr mc jt ju jv md jx jy jz me kb kc kd hb bi translated">使用sigmoid神经元的神经网络的结构与使用感知器的神经网络完全相同:输入和权重被馈送到隐藏层中的神经元，这些神经元又产生一个输出，该输出被馈送到输出层中的输出节点，这些节点最终吐出一个输出。</p><p id="66fa" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">然而，乙状结肠神经元仍然在一些方面不同于感知器。第一，输入和输出不限于值1或0。它们可以是介于两者之间的任何东西。这是第二个差异的直接影响，即现在使用的函数是<strong class="jk hu"> sigmoid函数(σ) </strong>，而不是我们在<strong class="jk hu">等式1 </strong>中使用的函数:</p><figure class="mk ml mm mn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nr"><img src="../Images/b67a432af48bc1b953623de9bc130523.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_UxAY-YM5uUjmbYMl6Cg-g.jpeg"/></div></div><figcaption class="mo mp et er es mq mr bd b be z dx"><strong class="bd lk">Equation 4: </strong>Sigmoid Function</figcaption></figure><p id="e4a1" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">在我们的例子中，<code class="du mf mg mh mi b">x</code>永远是<strong class="jk hu">等式1 </strong>。您可以将sigmoid函数视为将任何连续值缩小为0到1之间的值的函数。由于<strong class="jk hu">等式1 </strong>的输出确实是一个连续值，所以这个思路成立。</p><p id="2811" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">乙状结肠神经元也可以具有一个以上的输出，并且不限于二进制分类问题:</p><figure class="mk ml mm mn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ns"><img src="../Images/70d3ea177fbb65ab399ee72289255da2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Curor083radPxxArH98LoA.jpeg"/></div></div><figcaption class="mo mp et er es mq mr bd b be z dx"><strong class="bd lk">Figure 6: </strong>Multilayer Neural Network Using Sigmoid Neurons</figcaption></figure><p id="958d" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">理解sigmoid神经网络如何工作的最佳方式是通过sigmoid曲线:</p><figure class="mk ml mm mn fd hk er es paragraph-image"><div class="er es nt"><img src="../Images/771c8736a21bcc3e5fde8f9c1f5ca8aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*5Y8vGWF7mN6wEh7w4H0raA.png"/></div><figcaption class="mo mp et er es mq mr bd b be z dx"><strong class="bd lk">Figure 7: </strong>Sigmoid Curve [4]</figcaption></figure><p id="f832" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">注意在<code class="du mf mg mh mi b">y = 1</code>和<code class="du mf mg mh mi b">y = 0</code>处有渐近线，满足我们得到0和1之间的值的要求。但这是否解决了我们提到的问题，即重量的微小变化会导致输出的巨大变化？为了回答这个问题，我们可以将<strong class="jk hu">图7 </strong>与阶跃函数的曲线进行比较:</p><figure class="mk ml mm mn fd hk er es paragraph-image"><div class="er es nu"><img src="../Images/7f01b7dc41644700d9a3711d86cf3928.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*DmanLdlSpItCq3oKiajG_A.png"/></div><figcaption class="mo mp et er es mq mr bd b be z dx"><strong class="bd lk">Figure 8: </strong>Step Function [4]</figcaption></figure><p id="0867" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">对于感知器，我们正在处理阶跃函数，其中<code class="du mf mg mh mi b">x</code>(权重)的微小变化可能导致<code class="du mf mg mh mi b">y</code>(输出)的完全变化。sigmoid曲线只不过是阶跃函数的平滑版本。这种新的、更平滑的曲线允许通过权重的变化量来更好地反映输出的变化。</p><h1 id="ed01" class="li lj ht bd lk ll lm ln lo lp lq lr ls iz lt ja lu jc lv jd lw jf lx jg ly lz bi translated">深度学习</h1><p id="e2e8" class="pw-post-body-paragraph ji jj ht jk b jl ma iu jn jo mb ix jq jr mc jt ju jv md jx jy jz me kb kc kd hb bi translated">到目前为止，您应该对深度学习的上述定义有了更好的理解:</p><blockquote class="ke kf kg"><p id="4137" class="ji jj kh jk b jl jm iu jn jo jp ix jq ki js jt ju kj jw jx jy kk ka kb kc kd hb bi translated"><strong class="jk hu">深度学习</strong>是机器学习的一个子领域，涉及受大脑结构和功能启发的算法，称为<strong class="jk hu">人工神经网络</strong>。<em class="ht">【1】</em></p></blockquote><p id="4073" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">你知道什么是神经网络，以及它们是如何工作的。但为什么是“深”，又为什么是“学”？</p><p id="67e3" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">“学习”来自于这样一个事实，即权重的选择和更新是由我们的神经网络自主学习的，使用的是我们所谓的<strong class="jk hu">学习算法</strong>。我们将在下一篇文章中讨论学习算法。</p><p id="57e9" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">“深度”描述了我们神经网络的特殊结构。我们今天看到的神经网络非常简单和新颖，只有不超过两个隐藏层和几个输出。在现实中，深度学习算法通过将极其复杂的问题分解为更小的子问题来解决这些问题。这种分解包括通过首先构建较小的神经网络并将它们放置在彼此之上来构建大型神经网络，共同努力解决更大、更复杂的问题。</p><figure class="mk ml mm mn fd hk er es paragraph-image"><div class="er es nv"><img src="../Images/dfd3368587e67cc82915ba99f4d4ac74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*tUveXEfbW9fSmwbBtJLnRw.png"/></div><figcaption class="mo mp et er es mq mr bd b be z dx"><strong class="bd lk">Figure 9: </strong>Deep Neural Network Example [4]</figcaption></figure><h1 id="62a6" class="li lj ht bd lk ll lm ln lo lp lq lr ls iz lt ja lu jc lv jd lw jf lx jg ly lz bi translated">结论</h1><p id="dcce" class="pw-post-body-paragraph ji jj ht jk b jl ma iu jn jo mb ix jq jr mc jt ju jv md jx jy jz me kb kc kd hb bi translated">这篇文章为我们将来将要讨论的一切奠定了基础。</p><p id="1187" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我们从研究传统机器学习算法的局限性开始，试图理解我们为什么需要神经网络。从那里，我们看了一个最早开发的神经网络，并研究了如何用它来解决线性可分的分类问题。虽然很有用，但感知器缺乏区分不可线性分离的类的能力，因此需要更复杂的方法。</p><p id="82dd" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">通过简单地改变每个神经元使用的功能，我们能够提出一个可以处理更复杂数据集的神经网络。虽然感知器和乙状结肠神经元的结构相似，但权重变化对输出的影响的差异使它们的能力显著不同。</p><p id="ed02" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">在下一篇文章中，我们将讨论神经网络如何使用一种称为梯度下降的学习算法进行自我学习。在此之前，我留给你以下几点思考:</p><ul class=""><li id="8b90" class="kl km ht jk b jl jm jo jp jr kn jv ko jz kp kd kq kr ks kt bi translated">sigmoid函数是我们神经元中唯一可以使用的非线性函数吗？其他神经元可以用来解决复杂问题吗？</li><li id="9d98" class="kl km ht jk b jl ku jo kv jr kw jv kx jz ky kd kq kr ks kt bi translated">遵循迈克尔·尼尔森关于深度学习的在线书籍第一章<a class="ae nw" href="http://neuralnetworksanddeeplearning.com/chap1.html" rel="noopener ugc nofollow" target="_blank">中关于如何对写在图像上的数字进行分类的教程。然后用一些传统的最大似然算法(例如SVM)来解决同样的问题。比较不同的指标，比如算法速度、准确率、CPU使用率等。</a></li><li id="6a58" class="kl km ht jk b jl ku jo kv jr kw jv kx jz ky kd kq kr ks kt bi translated">研究梯度下降算法，作为我们将在下一篇文章中讨论的内容的介绍。</li></ul><h1 id="ee64" class="li lj ht bd lk ll lm ln lo lp lq lr ls iz lt ja lu jc lv jd lw jf lx jg ly lz bi translated">参考</h1><ol class=""><li id="1549" class="kl km ht jk b jl ma jo mb jr nx jv ny jz nz kd oa kr ks kt bi translated">杰森·布朗利，“什么是深度学习”，机器学习大师，2019年</li><li id="9495" class="kl km ht jk b jl ku jo kv jr kw jv kx jz ky kd oa kr ks kt bi translated">维基百科，“人工神经网络”，维基百科，2021年</li><li id="7d5f" class="kl km ht jk b jl ku jo kv jr kw jv kx jz ky kd oa kr ks kt bi translated">维基百科，“感知机”，维基百科，2021年</li><li id="de70" class="kl km ht jk b jl ku jo kv jr kw jv kx jz ky kd oa kr ks kt bi translated">迈克尔·a·尼尔森，《神经网络和深度学习》，决心出版社，2015年</li><li id="2a34" class="kl km ht jk b jl ku jo kv jr kw jv kx jz ky kd oa kr ks kt bi translated">Gaurav Tendolkar，“线性分类器”，机器学习笔记本，2016年</li></ol></div></div>    
</body>
</html>
<html>
<head>
<title>Submitting spark job in Azure HDInsight through Apache Livy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过Apache Livy在Azure HDInsight中提交spark作业</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/submitting-spark-job-in-azure-hdinsight-through-apache-livy-93a6129e8489?source=collection_archive---------23-----------------------#2021-06-04">https://medium.com/geekculture/submitting-spark-job-in-azure-hdinsight-through-apache-livy-93a6129e8489?source=collection_archive---------23-----------------------#2021-06-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="fb3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Apache Livy被设计成可以通过REST接口与任何运行spark的远程集群进行同步/异步交互，而无需对集群进行太多控制。访问spark不需要边缘节点。另外，livy服务器本身管理长期运行的spark上下文，而不是在spark服务器中运行它们。这篇博客是关于如何从本地机器访问spark运行HDInsight集群的。</p><p id="f838" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">李维有各种类型—</p><p id="2f1d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">▹ <strong class="ih hj">交互模式</strong>——由spark-shell、pySpark和sparkr repls提供。</p><p id="1d31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">▹ <strong class="ih hj">批处理模式</strong>——使用spark-submit向集群提交应用程序，在运行时中间没有交互</p><h2 id="ca69" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">正在创建HDInsight集群</h2><p id="d7a6" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">按照<a class="ae kd" href="https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-use-data-lake-storage-gen2-portal" rel="noopener ugc nofollow" target="_blank">官方文档</a>创建一个HDInsight集群和一个ADLS gen2存储。除了作为集群集成存储，它还可以存储所有支持spark代码的包/jar。spark job会使用哪个。默认情况下，HDInsight集群将在端口8998运行livy。</p><p id="8689" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要在集群中触发作业，需要几个参数——</p><ol class=""><li id="31a0" class="ke kf hi ih b ii ij im in iq kg iu kh iy ki jc kj kk kl km bi translated">群集的公共端点(<em class="kn">https://{ my cluster name } . azurehdinsight . net</em>)</li><li id="28bc" class="ke kf hi ih b ii ko im kp iq kq iu kr iy ks jc kj kk kl km bi translated">集群凭据</li></ol><p id="dae7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">只需通过Ambari dashboard确认livy服务器是否启动并运行。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es kt"><img src="../Images/b71d8e5e0cd0083a7822c22432e28d98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zFJzalr73m5TsbW4mPbLTA.jpeg"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx">Ambari dashboard showing livy server running</figcaption></figure><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lj"><img src="../Images/5dea3ad426615d429112a5397f48ed5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x_DtCvnDVTcSupgzuknW0w.png"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx">livy configuration setting</figcaption></figure><h2 id="76ae" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">livy-提交作业</h2><p id="ded8" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">首先，它需要pip安装一个开源包‘Livy-Submit’，该包使用Livy的交互模式来连接集群。</p><pre class="ku kv kw kx fd lk ll lm ln aw lo bi"><span id="a4b5" class="jd je hi ll b fi lp lq l lr ls">pip install Livy-Submit</span></pre><p id="c3a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要提交作业，请使用所需的参数进行调用。</p><pre class="ku kv kw kx fd lk ll lm ln aw lo bi"><span id="527d" class="jd je hi ll b fi lp lq l lr ls">livy_submit <strong class="ll hj">--livy-url</strong> https://{clustername}.azurehdinsight.net/livy <strong class="ll hj">-u</strong> &lt;cluster_username&gt; \<br/><strong class="ll hj">-p</strong> &lt;cluster_password&gt; \<br/><strong class="ll hj">-s</strong> &lt;local path of python file&gt;</span></pre><p id="e57f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个库将REST API请求传递给集群，传递所提供的参数。</p><h2 id="cd9c" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">提交具有程序包相关性的作业</h2><p id="6f76" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">执行作业的理想场景是将多个python文件作为包依赖项。在spark-submit中，这可以通过将归档文件作为jar传递来实现。在这里也可以遵循相同的归档过程，当在YARN模式下运行时，它工作得非常好。另一种方法是压缩文件，并从与集群集成的ADLS存储中访问它们。</p><p id="a86c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">压缩包装，使其在ADLS第二代集装箱中可用。要导入它，请按以下格式将参数添加到"- -py-files "中。</p><pre class="ku kv kw kx fd lk ll lm ln aw lo bi"><span id="4591" class="jd je hi ll b fi lp lq l lr ls">abfs://&lt;BlobStorageContainerName&gt;@&lt;StorageAccountName&gt;.blob.core.windows.net/&lt;filepath&gt;/&lt;file&gt;.zip</span></pre><blockquote class="lt lu lv"><p id="becf" class="if ig kn ih b ii ij ik il im in io ip lw ir is it lx iv iw ix ly iz ja jb jc hb bi translated">对于除ADLS Gen2以外的存储类型，使用“wasb”而不是“abfs”。</p></blockquote><pre class="ku kv kw kx fd lk ll lm ln aw lo bi"><span id="ece3" class="jd je hi ll b fi lp lq l lr ls">livy_submit <strong class="ll hj">--livy-url</strong> https://{clustername}.azurehdinsight.net/livy <strong class="ll hj">-u</strong> &lt;cluster_username&gt; \<br/><strong class="ll hj">-p</strong> &lt;cluster_password&gt; \<br/><strong class="ll hj">--py-files</strong> abfs://&lt;BlobStorageContainerName&gt;@&lt;StorageAccountName&gt;.blob.core.windows.net/scripts.zip \<br/><strong class="ll hj">-s</strong> &lt;local path of python file&gt;</span></pre><h2 id="f7c8" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">示例作业</h2><p id="36cb" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">假设结构如下，依赖关系在data_processing.py之上，只需在scripts文件夹级别压缩即可。</p><pre class="ku kv kw kx fd lk ll lm ln aw lo bi"><span id="fe11" class="jd je hi ll b fi lp lq l lr ls">| — scripts<br/>| | — project_f<br/>| | | — __init__.py<br/>| | | — data_processing.py<br/>| | | — main.py</span></pre><p id="20f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> main.py </strong></p><pre class="ku kv kw kx fd lk ll lm ln aw lo bi"><span id="771c" class="jd je hi ll b fi lp lq l lr ls">from scripts.project_f import data_processing</span><span id="326c" class="jd je hi ll b fi lz lq l lr ls">def <strong class="ll hj">main</strong>():<br/> print(‘Into main’)<br/> data_processing.processing()</span><span id="c494" class="jd je hi ll b fi lz lq l lr ls"><strong class="ll hj">main</strong>()</span></pre><p id="47f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> data_processing.py </strong></p><pre class="ku kv kw kx fd lk ll lm ln aw lo bi"><span id="edb8" class="jd je hi ll b fi lp lq l lr ls">def <strong class="ll hj">processing</strong>():<br/> print(‘Processing block’)</span></pre><p id="2863" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">执行—</p><pre class="ku kv kw kx fd lk ll lm ln aw lo bi"><span id="f4d6" class="jd je hi ll b fi lp lq l lr ls">livy_submit <strong class="ll hj">--livy-url</strong> https://livytest.azurehdinsight.net/livy <strong class="ll hj">-u</strong> &lt;cluster_username&gt; \<br/><strong class="ll hj">-p</strong> &lt;cluster_password&gt; \<br/><strong class="ll hj">--py-files</strong> abfs://testcontainer@testlivystorage.blob.core.windows.net/livytest/scripts.zip \<br/><strong class="ll hj">-s</strong> D:\scripts\project_f\main.py</span></pre><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es ma"><img src="../Images/5c011e48e8da70177eb44d65d6ddb6d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*FScpNKXUGsk3fxQDRDoARg.jpeg"/></div><figcaption class="lf lg et er es lh li bd b be z dx">Status in console</figcaption></figure><p id="5003" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">可选参数— </strong> livy提交包提供了一些用于管理资源分配的可选参数。提到的是那些—</p><pre class="ku kv kw kx fd lk ll lm ln aw lo bi"><span id="6c27" class="jd je hi ll b fi lp lq l lr ls"><strong class="ll hj">--driver-cores </strong>-<strong class="ll hj"> </strong>to specify the number of cores for the driver (default 4)<strong class="ll hj"><br/>--driver-memory </strong>- to specify the memory for the driver (default 32g)<strong class="ll hj"><br/>--num-executors </strong>- to specify the number of executors you want to use in non dynamic setting (default 10)<strong class="ll hj"><br/>--executor-cores </strong>-<strong class="ll hj"> </strong>to specify the number of cores you want to use for each executor<strong class="ll hj"> </strong>(default 4)<strong class="ll hj"><br/>--executor-memory </strong>- to specify the memory for each executor<strong class="ll hj"> </strong>(default 32g)<strong class="ll hj"><br/>--dynamic-max-executors </strong>- to specify the maximum number of dynamic executors(default 50)<strong class="ll hj"><br/>--spark-yarn-executor-memoryoverhead - </strong>to specify the <em class="kn">spark.yarn.executor.memoryOverhead</em> value in megabytes (default 10% for executor memory)</span></pre><p id="fc6e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">运行livy之前需要了解的事实</strong> —</p><ol class=""><li id="538d" class="ke kf hi ih b ii ij im in iq kg iu kh iy ki jc kj kk kl km bi translated">作业的资源将仅根据命令行参数进行分配。代码中的Spark-session配置设置将不被考虑，因为资源甚至在代码被读取之前就被分配了。</li><li id="eb32" class="ke kf hi ih b ii ko im kp iq kq iu kr iy ks jc kj kk kl km bi translated">外部参数可以通过实参"<em class="kn"> - exe-env" </em>传递，作为键值对，可以通过spark的executor变量访问。</li></ol><pre class="ku kv kw kx fd lk ll lm ln aw lo bi"><span id="9653" class="jd je hi ll b fi lp lq l lr ls">spark.sparkContext.getConf().get(“spark.executorEnv.&lt;variable_name&gt;”)</span></pre><p id="9265" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.如果livy作业被异常中止，那么提供的ADLS文件存储路径可能不正确。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mb"><img src="../Images/d0c18c3ea8676ecd621e57565b7504cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*uPaosBn0BZ_Y3P_T1OGWGA.jpeg"/></div></figure><h2 id="fcb1" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">结论</h2><p id="bb2e" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">Livy有多种用途，因为它管理大部分资源工作负载。事实上，运行在HDInsight中的Jupyter在幕后使用了livy cluster，他们称之为‘Spark magic’，这是一套通过livy与远程Spark clusters交互工作的工具。这篇博客解释了如何利用livy直接从本地环境提交作业，而无需处理/控制spark相关的依赖项。</p><h2 id="f96b" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">参考资料:</h2><div class="mc md ez fb me mf"><a href="https://docs.microsoft.com/en-us/rest/api/hdinsightspark/hdinsight-spark-interactive-session" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hj fi z dy mk ea eb ml ed ef hh bi translated">会话- REST API (Azure HDInsight)</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">以下信息适用于您可能要执行的与群集相关的所有任务:将{cluster-endpoint}替换为…</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">docs.microsoft.com</p></div></div></div></a></div><div class="mc md ez fb me mf"><a href="https://databricks.com/session/livy-a-rest-web-service-for-apache-spark" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hj fi z dy mk ea eb ml ed ef hh bi translated">livy:Apache Spark-data bricks的REST Web服务</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">Livy是一个开源的Apache许可的REST web服务，用于管理长期运行的Spark上下文和提交Spark…</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">databricks.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt ld mf"/></div></div></a></div></div></div>    
</body>
</html>
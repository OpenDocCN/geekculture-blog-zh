<html>
<head>
<title>Neural Networks Part 2: Backpropagation and Gradient Checking</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络第2部分:反向传播和梯度检验</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/neural-networks-part-2-backpropagation-and-gradient-checking-4f8d1350fb0b?source=collection_archive---------33-----------------------#2021-06-01">https://medium.com/geekculture/neural-networks-part-2-backpropagation-and-gradient-checking-4f8d1350fb0b?source=collection_archive---------33-----------------------#2021-06-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/30a93f1dbdb76ef0f8e111b9b1163860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TBdGxMcx6lQGP2a-"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx">Photo by <a class="ae hv" href="https://unsplash.com/@universaleye?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Munro Studio</a> on <a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="975d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是我正在从事的系列的第五部分，其中我们将讨论和定义介绍性的机器学习算法和概念。在这篇文章的最后，你会找到这个系列的所有前几篇文章。我建议你按顺序读这些。原因很简单，因为我在以前的文章中介绍了一些对理解神经网络至关重要的概念，我将在许多场合引用它们。</p><p id="a077" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上一篇文章中，我们介绍了神经网络的基础知识。我们看了理论，以及一些基本术语。</p><p id="f272" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我们继续相同的主题，除了这一次，我们更多地研究如何使用梯度下降和反向传播算法来找到正确的<code class="du jt ju jv jw b">Theta</code>向量。</p><p id="b1bc" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们开始吧。</p><h1 id="43b8" class="jx jy hy bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">反向传播</h1><p id="96a3" class="pw-post-body-paragraph iv iw hy ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">在我们之前看到的所有算法中，梯度下降被用来寻找最小化我们的成本函数的<code class="du jt ju jv jw b">Theta</code>向量。为此，我们遵循以下算法:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es la"><img src="../Images/c9915a094020bc617649c2846aae8dff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/0*AhAk6xFa5n-VGOo5.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jz">Figure 1: </strong>Gradient Descent Algorithm</figcaption></figure><p id="7b7f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">算法的主要部分在于寻找成本函数<code class="du jt ju jv jw b">J</code>的导数。这个任务的难度取决于我们的成本函数有多复杂。我们看到，对于线性和逻辑回归，成本函数的导数只需要基本的求导法则。当谈到我们将使用的神经网络的成本函数时，部分微分的任务变得更加困难。这就是反向传播算法发挥作用的地方。该算法用于区分神经网络的成本函数。</p><p id="d366" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在给出成本函数之前，让我们定义几个变量:</p><ol class=""><li id="5b7e" class="lf lg hy ix b iy iz jc jd jg lh jk li jo lj js lk ll lm ln bi translated"><code class="du jt ju jv jw b">L</code>是网络的总层数</li><li id="3b58" class="lf lg hy ix b iy lo jc lp jg lq jk lr jo ls js lk ll lm ln bi translated"><code class="du jt ju jv jw b">s_l</code>是<code class="du jt ju jv jw b">l</code>层的激活单元数。这不包括偏置单元</li><li id="55f0" class="lf lg hy ix b iy lo jc lp jg lq jk lr jo ls js lk ll lm ln bi translated"><code class="du jt ju jv jw b">K</code>是输出单位的数量</li></ol><p id="48ef" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们现在可以给出成本函数:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lt"><img src="../Images/cb69db848b6ceca1c77bf96f2b925b8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4OQMafc2hkTiVhTfaNBZDg.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jz">Equation 1: </strong>Neural Network Cost Function</figcaption></figure><p id="c26a" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我知道看起来很吓人。但是，实际上，这并不多。将此与逻辑回归模型的成本函数进行比较，您会注意到相似之处:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lu"><img src="../Images/ba1a6e1b59e02c480ace055be4e3d2f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sjWEp2Z8hxdGSAXX.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jz">Equation 2:</strong> Logistic Regression Cost Function</figcaption></figure><p id="cbc1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于一个输出，两个成本函数是等价的。只有当我们的神经网络中有一个以上的输出时，我们才开始看到功能上的差异。从1到<code class="du jt ju jv jw b">K</code>的总和被相加，以说明多个输出节点。因此，<code class="du jt ju jv jw b">h_theta(x^(i))_k</code>代表在第k个输出节点使用的<code class="du jt ju jv jw b">h</code>函数。在我们的例子中，使用了逻辑函数。成本函数中的第二项(以ƛ/2m开始的项)被称为<a class="ae hv" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)" rel="noopener ugc nofollow" target="_blank">正则化项</a>，用于成本函数中以防止过拟合。我以前没有——也不会在本文中——谈到正则化术语。在我们的例子中需要注意的是，这个术语适用于所有的<code class="du jt ju jv jw b">Theta</code>。也就是说，在我们的神经网络的不同层定义的<code class="du jt ju jv jw b">Theta</code>向量中的所有值将被平方、求和，并且最终乘以ƛ/2m.因子</p><p id="ad0d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">既然已经解决了这个问题，让我们来看看反向传播算法。</p><h2 id="c77c" class="lv jy hy bd jz lw lx ly kd lz ma mb kh jg mc md kl jk me mf kp jo mg mh kt mi bi translated">直觉</h2><p id="012a" class="pw-post-body-paragraph iv iw hy ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">算法本身并不难理解。然而，理解<em class="mj">如何</em>和<em class="mj">为什么</em>能给出我们正在寻找的偏导数，是事情变得稍微复杂的地方。我们的目的是给出一个基本的直觉。我们不会看到任何复杂的数学和证明。但是，如果你对算法所做的一切背后的深入解释感兴趣，我会让你参考迈克尔·尼尔森关于神经网络和深度学习的书的第二章。此外，请记住，你很可能不会从一开始就理解它的全部要旨。不要让那打击你的士气。它需要大量的时间和算法的实际操作，才能开始得到它为什么以这种方式工作的最细微的想法。</p><p id="a80c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">考虑具有三个输入、一个隐藏层和一个输出的神经网络:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mk"><img src="../Images/f21acac82f9fb7b929cfb2de533a9d46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*36zNdi5Y3pYwIXEzdzYjHg.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jz">Figure 2: </strong>Neural Network with Three Inputs and One Output</figcaption></figure><p id="a427" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">想象一下，我们有三个不同的神经网络，每个都具有与图2 所示网络完全相同的属性。假设我们正在检查的数据集对于所有网络都是相同的。我们如何比较一个神经网络和另外两个神经网络的性能？我们可以计算一个简单的值，如<em class="mj">期望值</em>和<em class="mj">输出值</em>(<strong class="ix hz">误差</strong>)之间的差值。为了减少网络中的误差，我们可以改变哪些值？肯定不是输入，因为那些是提供给我们的。我们也不能<strong class="ix hz">直接</strong>改变由我们的激活节点计算的值，但是我们可以通过改变我们的<code class="du jt ju jv jw b">Thetas</code>间接地<strong class="ix hz">这样做。考虑图2 </strong>中<strong class="ix hz">的红色箭头。对该箭头的权重(该箭头的θ)的任何改变被传播到<code class="du jt ju jv jw b">a_11</code>、输出节点<strong class="ix hz">以及错误</strong>。想象一下，如果我们有一个以上的隐藏层。对红色箭头的权重所做的更改也会传播到新层中的激活节点。这意味着所有使用的砝码都会影响我们的误差。如果我们可以找到每个θ对误差的影响，那么我们真正要做的就是找到这些θ操纵成本函数的方向和影响。如果你读了这个系列的第二部分<strong class="ix hz"> </strong>，<a class="ae hv" href="https://ali-h-khanafer.medium.com/linear-regression-using-gradient-descent-intuition-and-implementation-522d43453fc3" rel="noopener">使用梯度下降的线性回归:直觉和实现</a>，那么你就会知道这是对我们的成本函数进行微分的定义。为了找到由我们的<code class="du jt ju jv jw b">Thetas</code>引起的影响，我们需要从我们的输出节点向后移动，在计算它和我们的误差之后。这就是“反向传播”这个术语的由来。同样，这可能很难让你理解。重读这一节，回顾你关于偏导数的笔记，对这个算法有一个好的感觉。</strong></p><p id="cf41" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看看完整算法的伪代码:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ml"><img src="../Images/f014a10bf6492db5e6cea30dc3456c6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jOJpiIgQ0GpeQRkQAqiIGA.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jz">Figure 3: </strong>Back Propagation Algorithm</figcaption></figure><p id="8a32" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在最后计算的是最终结果，即我们的成本函数的导数。<code class="du jt ju jv jw b">delta^(l)_ij</code>(第二行的三角形)是我们初始化为0的矩阵。这将被更新，并将在最后用于计算我们的成本函数的偏导数。<code class="du jt ju jv jw b">a^(1)</code>被设置为<code class="du jt ju jv jw b">x^(i)</code>，因为它是我们的输入层。<strong class="ix hz">前向传播</strong>是使用我们的输入变量实际计算我们的激活和输出节点的值的行为。执行正向传播后，我们使用第6行中描述的公式计算输出端的误差。其余层(第7行)的所有误差使用以下公式计算:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es mm"><img src="../Images/e2fdfdfac33a08d23db358dbc877122e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*SVpJXe-04QFk4vMyLMoYaQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jz">Figure 4: </strong>Calculating Delta for l = L-1, L-2,…, 2</figcaption></figure><p id="5970" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中<code class="du jt ju jv jw b">Theta^(l)</code>是层<code class="du jt ju jv jw b">l</code>处的<code class="du jt ju jv jw b">Theta</code>向量，<code class="du jt ju jv jw b">Delta^(l+1)</code>(第1行使用大写delta代替小写)是前一层的误差，<code class="du jt ju jv jw b">.*</code>是逐元素的向量乘法，<code class="du jt ju jv jw b">a^(l)</code>是层<code class="du jt ju jv jw b">l</code>的激活值的向量。从那里，我们可以像在第8行那样更新我们的<code class="du jt ju jv jw b">delta^(j)_ij</code>矩阵，最后，在第9行和第10行计算我们的导数。</p><h2 id="6649" class="lv jy hy bd jz lw lx ly kd lz ma mb kh jg mc md kl jk me mf kp jo mg mh kt mi bi translated">梯度检查</h2><p id="e8d6" class="pw-post-body-paragraph iv iw hy ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">正如您在上一节中可能注意到的那样，反向传播算法有很多工作要做。由于它的复杂性，有许多可能出错的情况。有时候，看起来你的梯度下降工作得很好，你的成本随着每次迭代而降低，但实际上并不是这样。因此，我们需要一种方法来检查我们在运行带有反向传播的梯度下降后得到的成本是正确的。本节介绍的方法称为<strong class="ix hz">梯度检查</strong>。</p><p id="d972" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">考虑<code class="du jt ju jv jw b">J(theta)</code>的下图:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mn"><img src="../Images/119348e029e847f7165afe394ba07872.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SsEpM63gtg4Z9Um73yzwlw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jz">Figure 5: </strong>Random Cost Function Graph</figcaption></figure><p id="cdf2" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，假设我希望估计随机点θ处的偏导数，其中θ是一个实数，即我希望计算以下切线的斜率:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mn"><img src="../Images/168c4eef17a5f080b835ce2c0ef2d63a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UcVH59b2V1ZV1Nw0r4wJtQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jz">Figure 6: </strong>Random Cost Function Graph With Random Tangent Line</figcaption></figure><p id="29c9" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我怎样才能做出这个近似值呢？这里有一个建议:我们可以挑两个点，<code class="du jt ju jv jw b">theta + epsilon</code>和<code class="du jt ju jv jw b">theta — epsilon</code>，用一条线把这两个点连接起来，假设这条连接线是θ点切线的一个很好的近似。ε是我们选择的随机阈值:</p><p id="8002" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">来计算</p><figure class="lb lc ld le fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mn"><img src="../Images/b897c8bc89db0f9eff0d678346ad18a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QZ7BMgF48VDsVCv6lIHAbg.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jz">Figure 7: </strong>Random Cost Function Graph With Line Tangent Approximation</figcaption></figure><p id="53e3" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了计算两点之间的直线(绿色斜边),我们简单地将垂直距离除以水平距离:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mo"><img src="../Images/e5a9153c9de69ac97c32113ad74c3765.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rvQOtuzCLDj6RDLHAxZXjg.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jz">Equation 3: </strong>Partial Derivative Approximation When Theta is A Real Number</figcaption></figure><p id="96b7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">等式3 </strong>是当θ为实数时。到目前为止，在大多数情况下，<code class="du jt ju jv jw b">Theta</code>是一个向量。这个方程更一般的情况变成:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mp"><img src="../Images/c0fbea87d53a30d8410d480cf21de66a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8lg6tfsECmyMfHiJNzv-7g.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jz">Equation 4: </strong>Generalized Partial Derivative Approximation When Theta is A Vector of Thetas</figcaption></figure><p id="c2b6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里需要注意的是，这种逼近<code class="du jt ju jv jw b">Theta</code>导数的数值方法用于检查反向传播是否正确计算了导数。所以你可能会问自己<em class="mj">为什么我不用这个简单的公式来代替令人头疼的反向传播算法呢？答案很简单:这种用数字来计算事物的方式是非常低效的。因此，反向传播已经成为使用神经网络时最小化成本函数的主要方式。</em></p><h1 id="112e" class="jx jy hy bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">结论</h1><p id="158f" class="pw-post-body-paragraph iv iw hy ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">在本文中，我们研究了一种用于最小化神经网络成本函数的重要算法:<strong class="ix hz">反向传播</strong>。我们还寻找了一种方法来测试我们的算法是否正确地计算导数，称为梯度检查。</p><p id="4b40" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你不理解我们谈论的大部分内容，没关系。重要的是你要继续使用这个算法，这样你才能更好地理解它在做什么。在我们谈论的所有事情中，有一个关键点你<strong class="ix hz">必须</strong>带走:反向传播<strong class="ix hz">不是</strong>对梯度下降的替代。这是一种用来计算我们的成本函数相对于<code class="du jt ju jv jw b">Theta</code>的偏导数的方法。算法输出的值随后被用于梯度下降中的<strong class="ix hz">以最小化我们的成本函数。</strong></p><h1 id="6180" class="jx jy hy bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">过去的文章</h1><ol class=""><li id="3ff0" class="lf lg hy ix b iy kv jc kw jg mq jk mr jo ms js lk ll lm ln bi translated"><strong class="ix hz">第一部分:</strong> <a class="ae hv" href="https://ali-h-khanafer.medium.com/data-pre-processing-ee81bbe5cc77" rel="noopener">数据预处理</a></li><li id="d8e6" class="lf lg hy ix b iy lo jc lp jg lq jk lr jo ls js lk ll lm ln bi translated"><strong class="ix hz">第二部分:</strong> <a class="ae hv" href="https://ali-h-khanafer.medium.com/linear-regression-using-gradient-descent-intuition-and-implementation-522d43453fc3" rel="noopener">使用梯度下降的线性回归:直觉和实现</a></li><li id="6d69" class="lf lg hy ix b iy lo jc lp jg lq jk lr jo ls js lk ll lm ln bi translated"><strong class="ix hz">第三部分:</strong> <a class="ae hv" rel="noopener" href="/geekculture/logistic-regression-using-gradient-descent-intuition-and-implementation-36a8498afdcb">使用梯度下降的逻辑回归:直觉和实现</a></li><li id="47a4" class="lf lg hy ix b iy lo jc lp jg lq jk lr jo ls js lk ll lm ln bi translated"><strong class="ix hz">第四部分— 1: </strong> <a class="ae hv" rel="noopener" href="/geekculture/neural-networks-part-1-terminology-motivation-and-intuition-73675fc43947">神经网络第一部分:术语、动机和直觉</a></li></ol><h1 id="9b2d" class="jx jy hy bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">无耻的插头</h1><ul class=""><li id="b50d" class="lf lg hy ix b iy kv jc kw jg mq jk mr jo ms js mt ll lm ln bi translated"><strong class="ix hz">推特:</strong>T33】twitter.com/ali_khanafer2</li><li id="c3d1" class="lf lg hy ix b iy lo jc lp jg lq jk lr jo ls js mt ll lm ln bi translated"><strong class="ix hz">LinkedIn</strong>:【www.linkedin.com/in/ali-khanafer-319382152/ T2】</li></ul><h1 id="6a4b" class="jx jy hy bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">参考</h1><ol class=""><li id="4297" class="lf lg hy ix b iy kv jc kw jg mq jk mr jo ms js lk ll lm ln bi translated"><a class="ae hv" href="https://www.coursera.org/learn/machine-learning?page=1" rel="noopener ugc nofollow" target="_blank">吴恩达的机器学习Coursera课程</a></li><li id="882a" class="lf lg hy ix b iy lo jc lp jg lq jk lr jo ls js lk ll lm ln bi translated"><a class="ae hv" href="http://neuralnetworksanddeeplearning.com/chap2.html" rel="noopener ugc nofollow" target="_blank">神经网络和深度学习第二章:反向传播算法如何工作</a></li></ol></div></div>    
</body>
</html>
<html>
<head>
<title>What Are Word Embeddings?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是单词嵌入？</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/what-are-word-embeddings-6f6f677b13ce?source=collection_archive---------26-----------------------#2021-05-17">https://medium.com/geekculture/what-are-word-embeddings-6f6f677b13ce?source=collection_archive---------26-----------------------#2021-05-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="4650" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">增强语言理解能力的人工智能简介</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/b7c23ade053306fd36bc74ac3870e3df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hvnyTT_BOEg2M_lt"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Photo by <a class="ae jn" href="https://unsplash.com/@robertlukeman?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Robert Lukeman</a> on <a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="f0c3" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">单词嵌入几乎用于每一个涉及人工智能和人类语言的商业应用。一些<a class="ae jn" href="https://github.com/SocialQu/Borges/blob/v.0.0.5-UX/whitepaper/1.%20Applications.md" rel="noopener ugc nofollow" target="_blank">示例应用</a>包括搜索引擎、社交媒体推荐算法、语言翻译、语音识别、市场研究、自动交易和语言生成。</p><p id="d3c6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">单词嵌入是单词含义的数字表示<strong class="jq hj">。它们是基于意义是语境的假设而形成的。也就是说，一个词的意义取决于它的邻居:</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es kk"><img src="../Images/86bc9a48d8084e742fb0757ed62f4fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1JilkAo3wHgL7Y-S.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">A sliding window to find the word’s neighbors. [1]</figcaption></figure><blockquote class="kl km kn"><p id="281b" class="jo jp ko jq b jr js ij jt ju jv im jw kp jy jz ka kq kc kd ke kr kg kh ki kj hb bi translated">例如，如果单词“冰”通常出现在“水”的旁边，人们可以推断这两个单词有相似的意思。</p></blockquote><p id="7820" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">单词嵌入被表示为<strong class="jq hj">数学向量</strong>。这种表示法能够用文字进行标准的数学运算，如加法和减法。</p><p id="5c14" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这些操作在语言中有着有趣的应用，比如查找同义词、对文档进行分类或推荐内容。此外，可以绘制二维向量来产生对文档或人的语言的视觉理解。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ks"><img src="../Images/652ee1f7ac94b6b1c94965d44b468d99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QcFWhMO-HSNHiLG_.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Sample word embeddings and their respective graphical representation.</figcaption></figure><h1 id="9bc3" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">应用程序:查找同义词</h1><p id="e569" class="pw-post-body-paragraph jo jp hi jq b jr ll ij jt ju lm im jw jx ln jz ka kb lo kd ke kf lp kh ki kj hb bi translated">查找同义词是单词嵌入最简单的应用之一。同义词是指与另一个单词或短语意思完全相同或几乎相同的单词或短语。因为单词嵌入是单词意义的数字表示。为了找到同义词，我们只需要<strong class="jq hj">找到最接近单词的向量。</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lq"><img src="../Images/97ae9e3304dae650e8f2b3601c0b9dd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/0*YDBKJ72o2ZA61pQo.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Clusters of words based on their similarity.</figcaption></figure><p id="e7f0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">寻找同义词的第一步是<strong class="jq hj">选择一个距离度量</strong>来比较两个向量之间的接近度或相似度。最常见的度量之一是欧几里德距离，它是每个向量维度的平方差之和:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lr"><img src="../Images/bd1200c13e62ced20d2f31500741ee3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/0*cScgGJflwZvMf0uj.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Euclidean Distance Calculation</figcaption></figure><p id="5a90" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">另一个常用的距离度量是<strong class="jq hj">绝对值</strong>。在TypeScript中，这是度量绝对值距离的方法:</p><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="84f5" class="lx ku hi lt b fi ly lz l ma mb">/* Similarity: compute the absolute distance for two vectors */<br/>const similarity = (a:number[], b: number[]) =&gt; {<br/>  // Only compute the distance if the vectors have the same length.<br/>  if(a.length !== b.length) return Infinity</span><span id="0f47" class="lx ku hi lt b fi mc lz l ma mb">  // Sum the absolute value difference across every dimension. <br/>  const delta = a.reduce((d, i, idx) =&gt; d + Math.abs(i — b[idx]), 0)</span><span id="4d23" class="lx ku hi lt b fi mc lz l ma mb">  // Return the distance as a proxy of a vector's similarity.<br/>  return delta<br/>}</span><span id="c57e" class="lx ku hi lt b fi mc lz l ma mb">similarity([3,4], [1,2]) // Returns 4<br/>// Math.abs(3–1) + Math.abs(4–2) = 2+ 2 = 4</span><span id="15f2" class="lx ku hi lt b fi mc lz l ma mb">similarity([3,4], [1,6]) // Also returns 4<br/>// Math.abs(3–1) + Math.abs(4–6) = 2 + 2 = 4</span><span id="64d3" class="lx ku hi lt b fi mc lz l ma mb">// 3D Vectors Distance<br/>similarity([3,4,5], [4,6,8]) // Returns 6<br/>// Math.abs(3–4) + Math.abs(4–6) + Math.abs(5–8) = 1 + 2 + 3 = 6</span></pre><p id="4484" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">有趣的是，使用单词嵌入也可以找到反义词。唯一的区别是找到最大化单词距离的向量。</p><h1 id="5f4f" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">应用:主题分类</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es md"><img src="../Images/9ae299c1d6ed9743d702e552d4736b74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/0*4OJT1c3E0Y-Oj7uX.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Topics can be labeled using word clusters.</figcaption></figure><p id="809c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">单词嵌入和距离度量对于按主题标记文档也是有用的。该过程从按主题分类的标记文档数据集开始。然后，<strong class="jq hj">将文档内容转换成单词嵌入</strong>并平均每个向量的位置:</p><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="9c99" class="lx ku hi lt b fi ly lz l ma mb">/* <br/> * getCenter: find the average the position of a matrix of <br/> * word embeddings to find the "center" of a document. <br/> */<br/>export const getCenter = (vectors: number[][]) =&gt; {<br/>  const dimensions = vectors[0].length <br/>  const dimensionArr = [...Array(dimensions)]</span><span id="0639" class="lx ku hi lt b fi mc lz l ma mb">  // Iterate through each dimension.<br/>  const center = dimensionArr.map((_ , idx) =&gt; {</span><span id="77e3" class="lx ku hi lt b fi mc lz l ma mb">    // Sum the value of the dimension (idx) for each vector.<br/>    const dimensionSum = vectors.reduce((d,i) =&gt; d + i[idx], 0)<br/>    const dimensionAvg = dimensionSum/vectors.length</span><span id="9d6d" class="lx ku hi lt b fi mc lz l ma mb">    // Return the average for each dimension.<br/>    return dimensionAvg<br/>  })</span><span id="6f0c" class="lx ku hi lt b fi mc lz l ma mb">  // Return a vector with the same shape, and averaging values.<br/>  return center</span><span id="55f2" class="lx ku hi lt b fi mc lz l ma mb">}</span><span id="a758" class="lx ku hi lt b fi mc lz l ma mb">getCenter([[1,2], [3,4]]) // Returns [2,3]<br/>// [(1+3)/2, (2+4)/2] = [4/2, 6/2] = [2,3]</span><span id="e3dc" class="lx ku hi lt b fi mc lz l ma mb">getCenter([[2,3,3], [4,4,-1], [0,2,4]]) // Returns [2,3,1]<br/>// [(2+4+0)/3, (3+4+2)/3, (3-1+4)/3] = [6/3, 9/3, 6/3] = [2,3,1]</span></pre><blockquote class="kl km kn"><p id="1c05" class="jo jp ko jq b jr js ij jt ju jv im jw kp jy jz ka kq kc kd ke kr kg kh ki kj hb bi translated">我们可以把文档的中心看作文档的嵌入:它是内容的数字表示。</p></blockquote><p id="780b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">下一步是导出主题的中心。在类似的过程中，我们为每个主题找到其文档的平均位置。最后，当我们想要对一个未标记的文档进行分类时，我们可以将其内容转换为向量表示，并使用距离度量来找到最近的主题。</p><p id="3477" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">反思:</strong> <em class="ko">你如何专门使用未标记的数据对文档进行分类？那就是无监督学习。</em></p><h1 id="4862" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">如何构建单词嵌入？</h1><p id="f65b" class="pw-post-body-paragraph jo jp hi jq b jr ll ij jt ju lm im jw jx ln jz ka kb lo kd ke kf lp kh ki kj hb bi translated">如前所述，单词嵌入背后的思想是单词的含义与其上下文相关。因此，单词嵌入是<strong class="jq hj">映射频繁接近的单词</strong>的结果。该过程包括3个步骤:</p><ol class=""><li id="7ae2" class="me mf hi jq b jr js ju jv jx mg kb mh kf mi kj mj mk ml mm bi translated"><strong class="jq hj">标记化:</strong>在一个语料库上进行拆分、分类、寻找唯一词。</li><li id="b2ed" class="me mf hi jq b jr mn ju mo jx mp kb mq kf mr kj mj mk ml mm bi translated"><strong class="jq hj">共现矩阵:</strong>映射彼此接近的单词。</li><li id="e871" class="me mf hi jq b jr mn ju mo jx mp kb mq kf mr kj mj mk ml mm bi translated"><strong class="jq hj">降维:</strong>压缩共生矩阵的大小。</li></ol><h2 id="5077" class="lx ku hi bd kv ms mt mu kz mv mw mx ld jx my mz lf kb na nb lh kf nc nd lj ne bi translated">1.标记化</h2><p id="dbdb" class="pw-post-body-paragraph jo jp hi jq b jr ll ij jt ju lm im jw jx ln jz ka kb lo kd ke kf lp kh ki kj hb bi translated">在自然语言处理(NLP)中有一个长期的传统，即<strong class="jq hj">分离单词</strong>，包括词干化和词汇化。为了举例说明我们在拆分文本时需要考虑的一些困难，请考虑以下内容[1]:</p><ul class=""><li id="7ce1" class="me mf hi jq b jr js ju jv jx mg kb mh kf mi kj nf mk ml mm bi translated">具有相同含义的单词，包括复数和动词变化。</li><li id="c7b9" class="me mf hi jq b jr mn ju mo jx mp kb mq kf mr kj nf mk ml mm bi translated">代词、介词和冠词经常出现，但几乎没有额外的意义。</li><li id="60ec" class="me mf hi jq b jr mn ju mo jx mp kb mq kf mr kj nf mk ml mm bi translated"><strong class="jq hj">缩写</strong>和复合词如N.Y.C .或New York。</li><li id="7726" class="me mf hi jq b jr mn ju mo jx mp kb mq kf mr kj nf mk ml mm bi translated">带有内部连字符或撇号的单词。</li><li id="c3e6" class="me mf hi jq b jr mn ju mo jx mp kb mq kf mr kj nf mk ml mm bi translated">数字、符号和标点符号，如括号或省略号。</li><li id="5175" class="me mf hi jq b jr mn ju mo jx mp kb mq kf mr kj nf mk ml mm bi translated">拼写错误。</li></ul><p id="7d2d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了简单起见，我们将使用一个简单的正则表达式:</p><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="f0f4" class="lx ku hi lt b fi ly lz l ma mb">/* @function tokenize: split the words in a text or document. */<br/>const tokenize (text:string) =&gt; text.match(/(\b[^ $]+\b)/g)</span></pre><p id="7e79" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">随着深度学习的出现，标记化已经部分失去了相关性。因为从理论上来说，<strong class="jq hj">最好的人工智能模型可以自己处理不规则性</strong>，而构建记号化器是一个缓慢的手工过程。但是，使用标记化器或其他输入转换会大大降低训练速度并提高准确性。</p><h2 id="b31b" class="lx ku hi bd kv ms mt mu kz mv mw mx ld jx my mz lf kb na nb lh kf nc nd lj ne bi translated">2.Co- <strong class="ak">产状</strong>矩阵</h2><p id="d70f" class="pw-post-body-paragraph jo jp hi jq b jr ll ij jt ju lm im jw jx ln jz ka kb lo kd ke kf lp kh ki kj hb bi translated">共现矩阵包含两个单词相邻出现的频率。构建矩阵有四个步骤:</p><ol class=""><li id="b254" class="me mf hi jq b jr js ju jv jx mg kb mh kf mi kj mj mk ml mm bi translated">用训练数据集中的所有<strong class="jq hj">唯一单词</strong>定义一个集合。</li><li id="e32e" class="me mf hi jq b jr mn ju mo jx mp kb mq kf mr kj mj mk ml mm bi translated">创建一个<strong class="jq hj">方阵</strong>，其中每行和每列代表一个单词。</li><li id="ca1b" class="me mf hi jq b jr mn ju mo jx mp kb mq kf mr kj mj mk ml mm bi translated">基于<strong class="jq hj"> N字窗口</strong>对相邻字的出现次数进行计数。</li><li id="68df" class="me mf hi jq b jr mn ju mo jx mp kb mq kf mr kj mj mk ml mm bi translated"><strong class="jq hj">将计数</strong>插入矩阵中相应的单元。</li></ol><p id="79be" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">以下三个句子的共现矩阵如下所示:</p><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="ff0f" class="lx ku hi lt b fi ly lz l ma mb">1. I like deep learning.</span><span id="d7d8" class="lx ku hi lt b fi mc lz l ma mb">2. I enjoy flying.</span><span id="83f3" class="lx ku hi lt b fi mc lz l ma mb">3. I like NLP.</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ng"><img src="../Images/8da2f0c70d59ab36b2b4b0de6f14987c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-V9HY1Io75A9U_pa.png"/></div></div></figure><p id="f2b4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">上面的例子使用了一个只有一个单词的窗口。为了说明<strong class="jq hj">基于窗口大小的不同</strong>，考虑下面的句子:</p><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="582e" class="lx ku hi lt b fi ly lz l ma mb">// For an 3-word window:<br/>const text = ‘I enjoy learning about Natural Language Proccesing.’</span><span id="9050" class="lx ku hi lt b fi mc lz l ma mb">// enjoy &amp; learning are adyacent.<br/>// enjoy &amp; about are adyacent.<br/>// enjoy &amp; Natural are adyacent.<br/>// enjoy &amp; Language are NOT adyacent.</span></pre><p id="dc0c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">关于如何构建共生矩阵，存在微小的变化。这些可能包括基于紧密程度的不同考虑、不对称窗口以及使用标点符号来确定动态大小的窗口。你可以查看<strong class="jq hj">完整代码，在<a class="ae jn" href="https://github.com/SocialQu/Borges/blob/main/borges/src/views/Home.tsx" rel="noopener ugc nofollow" target="_blank">故事的资源库</a>中构建一个共生矩阵</strong>。</p><h2 id="2b40" class="lx ku hi bd kv ms mt mu kz mv mw mx ld jx my mz lf kb na nb lh kf nc nd lj ne bi translated">3.降维</h2><p id="e817" class="pw-post-body-paragraph jo jp hi jq b jr ll ij jt ju lm im jw jx ln jz ka kb lo kd ke kf lp kh ki kj hb bi translated">理论上，我们已经完成了。我们可以使用共现矩阵的行作为单词嵌入。但是再看看矩阵，注意它有多稀疏。与此相关的有几个问题:</p><ul class=""><li id="799c" class="me mf hi jq b jr js ju jv jx mg kb mh kf mi kj nf mk ml mm bi translated">向量会占用太多的存储空间。</li><li id="f9b9" class="me mf hi jq b jr mn ju mo jx mp kb mq kf mr kj nf mk ml mm bi translated">在这些向量之上的训练模型会<strong class="jq hj">慢</strong>。</li><li id="4ec3" class="me mf hi jq b jr mn ju mo jx mp kb mq kf mr kj nf mk ml mm bi translated">单词之间的关系很难被注意到。</li></ul><p id="1233" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在这种意义上，构建单词嵌入的最后一步是<strong class="jq hj">降低共现矩阵的维度</strong>。一个小的文本语料库可以有数万个独特的单词，但单词嵌入往往小于1000个维度。例如，TensorflowJS中的通用句子编码器向量具有512维的大小。</p><p id="2cd6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">深度学习中有复杂的方法来压缩矩阵。但是为了保持简单，我们将使用<strong class="jq hj">主成分分析</strong> (PCA) <strong class="jq hj"> </strong>方法:</p><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="80cf" class="lx ku hi lt b fi ly lz l ma mb">import { PCA } from 'ml-pca'</span><span id="721e" class="lx ku hi lt b fi mc lz l ma mb">const reduceDimensionality = (dimensions:number) =&gt; {<br/>  const pca = new PCA(embeddings)</span><span id="b948" class="lx ku hi lt b fi mc lz l ma mb">  const newSize = {nComponents:dimensions}  <br/>  const reducedVectors = pca.predict(embeddings, newSize)</span><span id="774c" class="lx ku hi lt b fi mc lz l ma mb">  return reducedVectors<br/>}</span></pre><p id="1635" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如果我们将单词嵌入维数减少到只有2，我们可以绘制它们，并且<strong class="jq hj">获得不同单词之间关系的视觉理解</strong>。此图表显示了60K单词语料库中选定的单词嵌入:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nh"><img src="../Images/3b3749793a92e96ef27d0923dfd3aba9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1icURrXYp0JebDmE.jpg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Sample word-embeddings map.</figcaption></figure><h1 id="fb3f" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">应用:解决类比</h1><p id="e019" class="pw-post-body-paragraph jo jp hi jq b jr ll ij jt ju lm im jw jx ln jz ka kb lo kd ke kf lp kh ki kj hb bi translated">加减法单词嵌入有一个有趣且令人惊讶的应用:解决类比。传统上，类比用于衡量学生的推理和语言技能。今天，他们还评估单词嵌入的准确性。考虑以下类比:</p><p id="df1c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">国王之于人，犹如王后之于______。</p><p id="af38" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">解决这个问题的思路是找到这样一个词，即<strong class="jq hj">到</strong>到<em class="ko">皇后</em>的距离与“人”到“王”的距离相同。这是向量的样子:</p><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="2360" class="lx ku hi lt b fi ly lz l ma mb">// Measure the distance between 2 vectors.<br/>const distance = (a:number[], b:number[]) =&gt;<br/>    a.map((i, idx) =&gt; i - b[idx]<br/>)</span><span id="7e78" class="lx ku hi lt b fi mc lz l ma mb">// Get the word embedding vectors for king, man and queen.<br/>const king = wordEmbeddings['king']<br/>const man = wordEmbeddings['man']<br/>const queen = wordEmbeddings['queen']</span><span id="ab48" class="lx ku hi lt b fi mc lz l ma mb">// Get the distance between king &amp; man:<br/>const delta = distance(king, man)</span><span id="c125" class="lx ku hi lt b fi mc lz l ma mb">// The solution is located at the same distance starting from queen.<br/>const solutionLocation = distance(queen, delta)</span><span id="921c" class="lx ku hi lt b fi mc lz l ma mb">// Find the words embeddings closest to the solution's location.<br/>const analogy = findClosest(solutionLocation)</span></pre><p id="1c61" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">单词嵌入词典可以位于数据库中，也可以从包中加载。第二个选项在Python中很常见；第三种选择是使用TensorflowJS从浏览器实时计算单词嵌入。为了计算距离，您可以使用我们在同义词一节中得到的相似性函数。而如果你想学习如何寻找最近的向量，你可能会有兴趣阅读教程:<strong class="jq hj"/><a class="ae jn" rel="noopener" href="/geekculture/how-to-develop-a-text-recommendation-engine-99d3b46effdb?sk=508c65e2139bfdfbd8043b2090ee1bd7"><strong class="jq hj">如何构建一个文本推荐引擎。</strong></a><strong class="jq hj"/></p><p id="2769" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">视觉上，这是解决类比的方式:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ni"><img src="../Images/e2deea8fe2cce34250724f15a3fece75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/0*7VOYz7_AEu3ISGuV.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Graphically, the distance between queen and woman is similar to the distance from king to man.</figcaption></figure><h1 id="01ec" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">应用:检测偏差</h1><p id="a9ee" class="pw-post-body-paragraph jo jp hi jq b jr ll ij jt ju lm im jw jx ln jz ka kb lo kd ke kf lp kh ki kj hb bi translated">不幸的是，我们倾向于根据不相关或不公平的属性做出价值判断。人工智能可以帮助自动分析、测量和报告这些偏差。下面的单词云显示了<strong class="jq hj">基于性别的工作偏见</strong>。医疗保健行业的工作更有可能与女性联系在一起，而工程行业的情况则相反:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nj"><img src="../Images/172a48f28d21ba034c9d2b5937c809e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uvYRTox8bCgfVzGg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Word cloud of biases in jobs based on gender.</figcaption></figure><p id="73f1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">检测偏差的过程也很简单:找到你要比较的两个概念或两个组的单词嵌入。然后，选择<strong class="jq hj">测量潜在偏差的术语。</strong>最后，计算每个术语的相对距离:两个术语之间的距离越大，偏差越大。</p><pre class="iy iz ja jb fd ls lt lu lv aw lw bi"><span id="bf09" class="lx ku hi lt b fi ly lz l ma mb">// Arbitrary threshold to determine if there is a bias.<br/>const biasThreshold = 2 </span><span id="1a24" class="lx ku hi lt b fi mc lz l ma mb">// Evaluate if there is a gener bias for a particular job.<br/>const jobBiasDetection = (job:string) =&gt; {</span><span id="de27" class="lx ku hi lt b fi mc lz l ma mb">  // Find the word embeddings of woman, man and the input.<br/>  const woman = wordEmbeddings['woman']<br/>  const man = wordEmbeddings['man']<br/>  const jobEmbedding = wordEmbeddings[job]</span><span id="5196" class="lx ku hi lt b fi mc lz l ma mb">  // Measure the distance of the job to both concepts.<br/>  const distanceToWoman = distance(woman, jobEmbedding)<br/>  const distanceToMan = distance(man, jobEmbedding)</span><span id="33e8" class="lx ku hi lt b fi mc lz l ma mb">  // Determine if the job is usually associated to men.<br/>  if(distanceToMan/biasThreshold &gt; distanceToWoman) return true</span><span id="a50b" class="lx ku hi lt b fi mc lz l ma mb">  // Determine if the job is usually associated to women.<br/>  if(distanceToWoman/biasThreshold &gt; distanceToMan) return true</span><span id="e459" class="lx ku hi lt b fi mc lz l ma mb">  // There is no bias for the specified job and threshold.<br/>  return false<br/>}</span></pre><p id="3b2d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因为人工智能模型包含了我们作为一个社会持有的偏见，所以它仍然是一个未解决的问题，<strong class="jq hj">如何在没有偏见的情况下训练单词嵌入。</strong>诚邀您在评论中反思和分享:对于如何训练不偏不倚的单词嵌入，您有哪些新颖的想法？正如康德所说:</p><blockquote class="nk"><p id="7b85" class="nl nm hi bd nn no np nq nr ns nt kj dx translated">"真理是整体判断的谓词，而不是部分表述的谓词."</p></blockquote><h1 id="af88" class="kt ku hi bd kv kw kx ky kz la lb lc ld io nu ip lf ir nv is lh iu nw iv lj lk bi translated">高级主题:深度学习</h1><p id="289b" class="pw-post-body-paragraph jo jp hi jq b jr ll ij jt ju lm im jw jx ln jz ka kb lo kd ke kf lp kh ki kj hb bi translated">构建单词嵌入的过程是基于20世纪80年代的研究，称为潜在语义分析(LSA)。但是在过去的十年中，它已经发展到包含神经网络。在计算机视觉中成功实现后，<strong class="jq hj">神经网络很快被自然语言处理学术界采用</strong>。</p><p id="a237" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">四篇研究论文塑造了当前单词嵌入的构建方式:</p><ol class=""><li id="1fc0" class="me mf hi jq b jr js ju jv jx mg kb mh kf mi kj mj mk ml mm bi translated"><strong class="jq hj"> Word2Vec: </strong>向量空间中单词表示的高效估计。</li><li id="7220" class="me mf hi jq b jr mn ju mo jx mp kb mq kf mr kj mj mk ml mm bi translated"><strong class="jq hj"> GloVe: </strong>单词表示的全局向量。</li><li id="7d28" class="me mf hi jq b jr mn ju mo jx mp kb mq kf mr kj mj mk ml mm bi translated"><strong class="jq hj"> ELMO: </strong>深层语境化的话语表述</li><li id="3de5" class="me mf hi jq b jr mn ju mo jx mp kb mq kf mr kj mj mk ml mm bi translated"><strong class="jq hj"> BERT: </strong>用于语言理解的深度双向转换器的预训练</li></ol><p id="bc5f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在这篇介绍中，我将只关注前两个。</p><h2 id="3c8c" class="lx ku hi bd kv ms mt mu kz mv mw mx ld jx my mz lf kb na nb lh kf nc nd lj ne bi translated"><strong class="ak"> Word2Vec:根据预测训练的单词嵌入</strong></h2><p id="0d32" class="pw-post-body-paragraph jo jp hi jq b jr ll ij jt ju lm im jw jx ln jz ka kb lo kd ke kf lp kh ki kj hb bi translated">谷歌科学家的论文Word2Vec基于这样一个前提，即如果被训练来预测一个单词的出现，单词嵌入会更加准确。该论文介绍了两种互补的神经网络架构来进行预测并随后导出单词嵌入。</p><p id="ba32" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">使用相同的N单词窗口概念，Word2Vec提出了<strong class="jq hj">连续单词包(CBOW) </strong>架构，根据其邻居预测一个单词。类似地，<strong class="jq hj">跳格结构</strong>试图根据特定单词预测邻居。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nx"><img src="../Images/7444c1f5936c1e8bfb60032cb019ffee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gSq3D7TviAceOb7b.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">CBOW &amp; Skip-Gram: Novel Neural Network Architectures</figcaption></figure><p id="9f84" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这篇论文的创新之处还在于用于训练神经网络的数据集的规模。它还引入了第二个数据集，成为衡量新模型准确性的标准。它主要由类比构成，包括两个部分:句法部分和语义部分。</p><h2 id="be60" class="lx ku hi bd kv ms mt mu kz mv mw mx ld jx my mz lf kb na nb lh kf nc nd lj ne bi translated"><strong class="ak">手套:寻找单词间隐藏的关系</strong></h2><p id="bd7b" class="pw-post-body-paragraph jo jp hi jq b jr ll ij jt ju lm im jw jx ln jz ka kb lo kd ke kf lp kh ki kj hb bi translated">仅一年后(2014年)，由斯坦福研究人员开发。GloVe融合了两个世界的精华:通过潜在语义分析发现的<strong class="jq hj">微妙的语义关系</strong>和Word2Vec预测的语法准确性。</p><p id="2498" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">GloVe基于一种强大的直觉，即一个单词的真正含义来源于两个相邻单词出现的预期概率的差异。手套减少了由经常出现的单词产生的噪音。这意味着，如果两个单词相对不常见，但经常相邻出现，则这种特定关系的影响更有助于确定它们的单词嵌入值。</p><p id="fe30" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">GloVe设计了一个聪明的方法来识别、转换和映射那些“高价值”的关系。因此，先前稀疏的<strong class="jq hj">同现矩阵被转换成密集的矩阵。</strong>在一个漂亮的数学推导中，新矩阵的单元是原始矩阵的行和列之间的点积的结果。</p><p id="a397" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">通过保持共生矩阵较小，有可能保持训练时间较短。更重要的是，GloVe被证明是当时唯一一个受益于训练数据集规模增长的模型<strong class="jq hj">(从60亿到420亿个令牌)。</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ny"><img src="../Images/2a8e371c398b27554d190e91f7693fe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*-XQCspjyX1A42wiWxi96LQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">GloVe’s state of the art results.</figcaption></figure><h1 id="df3d" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">结论:单词嵌入的未来</h1><p id="6b96" class="pw-post-body-paragraph jo jp hi jq b jr ll ij jt ju lm im jw jx ln jz ka kb lo kd ke kf lp kh ki kj hb bi translated">我在这篇文章的开头提到，单词嵌入渗透到每一个涉及人工智能和人类语言的应用程序中。但未来甚至更光明:我看到单词嵌入的地方与10年前移动开发的地方相似。对于企业家和软件开发人员来说，这项技术在未来十年有无限的机会。我将其归因于4个主要原因:</p><ol class=""><li id="f2d4" class="me mf hi jq b jr js ju jv jx mg kb mh kf mi kj mj mk ml mm bi translated"><strong class="jq hj">开发的容易程度</strong>:与常识相反，将人工智能，尤其是单词嵌入，集成到现有的应用程序中是很容易的。这个领域已经足够<strong class="jq hj">成熟</strong>，可以在不理解其背后的高级数学的情况下使用这项技术。</li><li id="378d" class="me mf hi jq b jr mn ju mo jx mp kb mq kf mr kj mj mk ml mm bi translated"><strong class="jq hj">快速采用</strong>:计算机视觉在过去十年中备受瞩目。但其产品和应用需要专门的硬件，如摄像头和处理器，这对用户的隐私和采用速度提出了挑战。TensorflowJS 使用标准手机或台式机无缝集成AI-NLP软件。</li><li id="0c40" class="me mf hi jq b jr mn ju mo jx mp kb mq kf mr kj mj mk ml mm bi translated">本地化:多语言世界创造了有趣的进入壁垒。多个参与者可以利用相同的技术应用服务于不同的本地市场。</li><li id="f7ee" class="me mf hi jq b jr mn ju mo jx mp kb mq kf mr kj mj mk ml mm bi translated"><strong class="jq hj">成长</strong>:这个领域正在经历重大变革，吸引了全世界最聪明的头脑的关注。就在2019年，最先进的模型和突破性创新不断重塑该技术的极限和应用。这方面的例子包括<strong class="jq hj"> BERT的</strong>研究论文，它极大地促进了迁移学习和OpenAI的文本生成<strong class="jq hj"> GPT-3 API。</strong></li></ol><p id="9ce1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我邀请你思考以下几点:人类语言是一种压缩机制。它使复杂的想法能够快速有效地传递。但是它也有其局限性:当说话时，只传输39位信息。相比之下，通过USB 2.0传输的信息为480Mbps。那是1000多倍！</p><blockquote class="nk"><p id="9dfc" class="nl nm hi bd nn no np nq nr ns nt kj dx translated">最终，单词嵌入将通过增加我们可以处理的信息量来提高生产率。</p></blockquote><p id="4e7a" class="pw-post-body-paragraph jo jp hi jq b jr nz ij jt ju oa im jw jx ob jz ka kb oc kd ke kf od kh ki kj hb bi translated">如果你对这个内容的<strong class="jq hj">说教版本感兴趣，我邀请你访问<a class="ae jn" href="https://borgez.ml/" rel="noopener ugc nofollow" target="_blank"> borgez.ml </a>。这是一门在线互动课程，你可以使用这些概念来查找同义词，并使用TensorflowJS对文档进行分类。您将训练您的单词嵌入，通过测验验证您的知识，并找到具有有趣见解的图表。</strong></p></div><div class="ab cl oe of gp og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="hb hc hd he hf"><p id="733c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="ko">感谢您的阅读！我计划在接下来的几周里写一些关于情感分析、注意力和变形金刚的内容。如果你有兴趣，请考虑给我一个关注和分享这个故事。我祝你有美好的一天，你的掌声将会非常感谢。🙏</em></p><p id="7e29" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">真诚地，<br/>圣地亚哥·米</p></div></div>    
</body>
</html>
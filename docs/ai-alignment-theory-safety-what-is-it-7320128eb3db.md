# 人工智能校准理论与安全。这是什么？

> 原文：<https://medium.com/geekculture/ai-alignment-theory-safety-what-is-it-7320128eb3db?source=collection_archive---------5----------------------->

![](img/536244e3da058f4087cafcb11369c9d8.png)

Photo by [Maxim Hopman](https://unsplash.com/@nampoh?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

如果您想观看本文的视频版本，请点击下面的链接。

我们看过电影，读过书。一个人工智能去流氓和实现世界统治。但这都是虚构的，对吗？在这篇文章中，我们将讨论一个旨在防止这种情况发生的理论。这就是科学家所说的排列理论。简而言之，让人工智能系统的目标与人类价值观保持一致，并确保超级智能人工智能系统不会做任何对人类有害的事情。人工智能比对是人工智能安全的一个子领域。人工智能安全涵盖其他领域，如监控、鲁棒性和能力控制。这些都是构建安全的 AI 系统所需要的。在我们进一步讨论之前，如果你不太了解人工通用智能，那么请点击下面的链接寻找解释者，并回到这篇文章来更好地理解。

人工智能对齐理论的流行可以归因于越来越多的高级人工智能系统，这些系统开始在某些领域匹配人类的能力，甚至表现得更好。虽然与人类价值观一致听起来像是打开和关闭了几个旋钮，但事实远非如此。人类原则的复杂性使得这个理论非常困难。让我们来看看一些可能的挑战，以确保超级智能系统根据人类价值观以安全的方式运行。

**对准问题**

对齐的部分问题是试图指定开发人工智能系统时要注意的所有漏洞和不必要的后果。这些系统是为实现一个目标功能而构建的，这个目标功能可以是分类图像、生成文本或驾驶汽车。人工智能程序找出了达到这个目标的最佳方法。将人类价值观和不希望的场景硬编码到系统中是不可持续的，因为人类行为是复杂的，每个程序员都知道在你的程序部署后总会有不希望的结果。例如，2016 年 OpenAI 的研究人员发现，一个人工智能赛艇游戏找到了一种实现最高成绩的方法，只需绕圈并击中目标奖励，而不是完成比赛并沿着赛道收集奖励。这是一个有缺陷的奖励系统，因为人工智能只是利用了最大化它的目标，例如在计算好的时间间隔重复击中目标。你可以看到这如何扩展到一个场景，我们要求一个超级人工智能找到长寿的治疗方法，这样它就找到了冷冻保存人类的新方法。这是一个可怕的想法。我们换个话题吧，快点！人工智能的商业吸引力及其巨大的投资回报进一步强调了发展某种联盟的重要性，因为越来越多的公司在没有充分测试或考虑其社会影响的情况下迅速发布人工智能产品。

到目前为止，我们已经讨论了人工智能试图通过忽略重要的人类价值来达到其目标的场景，只是因为它根本没有意识到这些价值。一个更高的风险情况是，如果人工智能比人类聪明得多，并且选择忽略人类的伦理和原则。这是一个注定的案例，超级人工智能可能会寻求权力；积累财政资源和计算能力，最终成为地球上最具支配地位的实体。显然，这听起来像是从科幻剧本中撕下来的。一些科学家认为这些是可能的结果，而其他人对此表示严重怀疑。怀疑者认为，人们对这种不太可能的场景给予了太多的关注，因此，分散了一些非常聪明的人的注意力，使他们无法专注于人工智能中更重要的领域。Francois Chollet 指出，人工智能对齐这一术语具有误导性，因为它假设人工智能系统具有意向性、价值和自主性，尽管这样的系统并不存在。在 AGI 和结盟的辩论中，你站在哪一边？请在评论区留下你的答案，我非常期待阅读它们。在被称为诚实人工智能的真实人工智能任务的研究领域，人们已经看到了对准的努力。像文本生成这样强大的机器学习工具的出现已经引起了潜在的危险的错误信息。例如，看看这项研究，它展示了人工智能文本生成如何创造或模仿虚假。目前有如此多的在线社交档案由人工智能机器人控制，有时它们与人类难以区分。这些机器人可以很容易地传播阴谋论、不正确的医疗建议或谎言，影响世界各地的政治和其他重要部门。一些研究实验室正在通过实施人工智能程序来寻找解决方案，这些程序可以引用它们的来源，并在回答问题时提供理由，以进一步提高可验证性和透明度。

**人工智能目标**

对齐理论旨在对齐人工智能系统的 3 个主要部分。第一部分是预期目标，这是我们希望人工智能理想完成的目标。这通常很难定义。第二个是指定的目标或外部目标，这是人类通过目标函数为模型实际定义的。第三部分是紧急目标或内部规范，这是人工智能系统经过一段时间的训练并从其环境中学习后产生的。预期目标和指定目标之间的不一致被称为外部不一致，这是大多数人工智能系统目前试图解决的问题。例如，在识别子组的面部时表现不佳的面部识别系统具有外部未对准。内在错位是预期目标和紧急目标之间的差异。这通常很难解决，因为在它出现之前，你不知道紧急目标是什么。生物进化经常被用作紧急目标的类比。史前人类有纯粹生存的包容性遗传适应性的直接目标。然后，为了更高的能量储备，作为一个紧急目标，对含糖食物的偏好普遍存在，从而增加了在恶劣条件下生存的机会。现代人仍然吃糖，但不是为了生存，因为时代确实变了。你认为我们在进化的道路上还发展出了什么其他的紧急目标？

**对准计划**

多年来，研究人员试图创造能够长期规划以实现目标的人工智能系统。然而，一些人认为，一个相当先进的系统，能够进行复杂的长期规划，但可能无法控制其环境。已经提出了几种可能控制一个强大的人工系统的方法。这里我们只提几个。一些人已经提议设置一个关闭开关来完全关闭这样的系统。这种修复的可能性很低，因为一个能够提前计划许多步骤的超级智能系统最有可能避免被关闭，除非它以某种方式激励它。另一个被称为“AI box”的拟议控制解决方案涉及在一个独立的计算机系统上运行 AI，该系统具有严重受限的输入和输出，如纯文本通道和无互联网连接。它减少了人工智能不良行为的能力，同时也降低了它的有用性。这导致了另一个被称为神谕的控制提议。这是人工智能系统仅用于询问和回答问题的地方，不能直接影响其环境之外的任何东西。然而，这方面的警告是，系统可能倾向于基于突发动机来欺骗人类，以实现其最终目标。虽然这都是理论，但各国和国际监管机构正在提出道德准则，以确保未来超级强大的人工智能系统的一致性。联合国秘书长安东尼奥·古特雷斯建议对人工智能进行监管，以确保它“符合共同的全球价值观”。在我们结束关于这个主题的章节之前，我将留给你科幻作家艾萨克·阿西莫夫在 1942 年提出的机器人三定律。这些法律已经成为人类和机器人之间未来伦理体系的一种指导原则。

第一定律: *机器人不得伤害人类，也不得坐视人类受到伤害。*

**第二定律:** *机器人必须服从人类给它的命令，除非这些命令与第一定律相冲突。*

**第三定律:** *机器人必须保护自己的存在，只要这种保护与第一或第二定律不冲突。*

一些人认为这些法律需要更新，因为现代系统存在一些模糊和漏洞。我们将在另一篇文章中深入探讨阿西莫夫定律。你认为这些法律足以保护人类免受强大的人工系统的伤害吗？

你对人工智能的整体一致性有什么看法，你认为随着这些人工智能系统变得越来越智能，我们如何才能确保它们达到更高的安全标准？期待大家的想法。

感谢阅读！

**相关主题:**
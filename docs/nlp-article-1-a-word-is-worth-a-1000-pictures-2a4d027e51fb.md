# NLP 文章#1:一个单词抵得上 1000 张图片

> 原文：<https://medium.com/geekculture/nlp-article-1-a-word-is-worth-a-1000-pictures-2a4d027e51fb?source=collection_archive---------17----------------------->

![](img/95edc5c359515a6562592eb9165044bc.png)

Image created by Author

自然语言处理，简称 NLP，很简单，就是研究和使用机器来智能地使用和创造自然语言。我有意暂时省略“理解”这个词，因为在使用概率模型时，这是一个有点棘手的话题。

但是难题如下:就比特率而言，口头交流是糟糕的。一个演讲者可能每分钟说 300 个单词，这是每秒 70 字节的微不足道的速度。但是如果换成双向对话，速度会进一步下降到 100 字/分钟，或者绝对是 25 字节/秒。然而，其有效性掩盖了这一事实。会议是有效的。面对面的会议更是如此。这可能与非语言暗示有很大关系。即使在今天，一些人仍然使用/s 后缀来表示讽刺，因为没有线索的基于文本的交流充满了困难。但它也可能与被接受的、共享的参照和共同的记忆(有时称为长程模型)有很大关系。就像一个事实上穷尽的散列表。其中对单个令牌的引用——比如说“WW2”——意味着不止 2 个字节。两个人之间的一个更私人的例子，可能指的是“那个东西”，这里讨论的“那个东西”是一个复杂的理解。

所以难题是，它显然不仅仅是比特率。要实现真正的意义需要付出很多——这就是为什么 NLP 一直是一个难题。

NLP 研究世界-非常明确地定义为 2013 年前和 2013 年后。这是高效的深度神经网络(贝尔实验室的 LaCun 已经在计算机视觉领域高效使用了几年，以惊人的准确度识别手写数字)在 NLP 领域取得了第一次重大的突破性进展，实现了一个名为 Word2Vec (Mikolov)的应用。这使得新(重新)发现的深度学习统计和优化方法能够在 NLP 空间中使用。在许多任务中，它真的，真的起作用了。翻译，句子完成，内容生成是最重要的。情感分析也非常重要。

但是，由于这些是统计和概率方法，任何“理解”的说法充其量是虚假的。并受到当前激烈辩论的影响。

出于这个原因，我认为即使是最热心的人工神经网络(ANN)实践者也会同意，在人类的意义上没有理解或“意义”。事实上，几乎所有神经网络研究人员都煞费苦心地明确声明，他们的想法和模型不是基于复制人脑。该领域的一些原创研究人员，如美国的巨人罗森布拉特和日本的福岛，肯定会证明他们的原始灵感来自于观察当达到某个阈值时神经元的放电——特别是在他们的视觉皮层中。但是这两个领域很快就分道扬镳了，因为计算机科学家从根本上说不是认知科学家，因此是由基于经验成功的指标驱动的，而不是为生物学理解而努力。

所以我们在这里。在特定领域取得惊人的成功，但实际了解很少。

在列举我们的确切位置之前——再多一点历史，和 NLP 的几个里程碑。

(NLP 最著名的早期概念可能来自英国数学家艾伦·图灵，他在 20 世纪 40 年代提出了图灵测试。这是一个思想实验，表明计算机将通过人类理解的某个门槛，当计算机(输出文本)可以欺骗不知情的主体，使其认为它正在与另一个人而不是计算机交谈。(还没有成功通过的测试，但可能很快就会通过……)

(2)在 20 世纪 60 年代，控制论领域诞生了，由 Weitzenbaum 领导的麻省理工学院的一个 NLP 小组提出了一个名为 Eliza 的程序，它使用大量手工编码的规则来尝试和交谈——第一个聊天机器人。它使用了一个名为 Doctor 的相当简单的脚本，该脚本寻找要回流的关键词，然后参与一个开放式的“罗杰式”治疗会话。在没有关键词的情况下，它回复为开放的回答，比如“你对它感觉如何？或者‘你怎么看*？“当它遇到错误时，它表现得好像它令人困惑的反应是一个笑话。尽管它很简单，但可以愚弄很多人。*

(3)在 70 年代、80 年代甚至 90 年代的大部分时间里，NLP 被称为计算语言学，并且通常在语言学系和计算机科学系中都能找到。继续取得小的进展，但这些进展继续朝着普遍理解的方向发展。他们专注于语法结构，因此他们对手工编码规则和洞察力的依赖阻碍了进度。

概率方法继续取得小进展，使用“单词袋”方法，其中单词频率和短语(n-gram)频率产生了一些好的结果。递归神经网络架构引入了时间序列组件。洞察力表明一些邻近的单词比其他的更重要。但是 n 维中不可能大的词向量空间，其中 n 是字典中的词的数量(比如英语中的 100，000)，只考虑相邻的词，并且当相邻词的窗口扩大时，该词向量空间呈指数增长。

(4)然后在 2013 年 9 月，谷歌的 Miklos 等人发表了 Word2Vec。

有了足够的未标记数据，它可以在一个单词和相邻的 n 个单词(n 是预定义的超参数)之间创建可管理的概率分布，然后可以分配哪个单词将是下一个单词的概率。这立即产生了翻译和句子完成方面的最新成果。一旦你有了句子完成和下一个词的生成，它只需要引入生成性对抗网络(GANS)的额外步骤，就可以开始产生散文和副本。

(5)然后在 2017 年，《变形金刚》带来了额外的严格规范，即哪些邻近的词增加了最大的价值，以及如何将注意力集中在相关的地方。随着计算能力的增长，具有越来越大的语料库和越来越多的特征(参数)的变压器模型被训练——花费巨大。其中一个商业模式是，像 GPT 2 这样的语言模式，可以被许可、转移训练和优化，以达到特定的更狭窄的目的。例如，建立一个关于特定医疗问题的聊天机器人。

事实证明，这些方法非常有效。有一个巨大的警告。

由于他们是在巨大的语料库上接受训练的，这些语料库甚至是从网络最黑暗的角落收集来的，所以他们不能轻易地区分事实和虚构。因此，当他们接触到糟糕的数据时——比如大量未经证实的阴谋论，他们很容易做出错误的假设，并得到大量错误断言的支持。

正因为如此，第一个公开发行的 GPT-2 版本被证明有能力很快变成种族主义者。所以未来的版本有一些自我监督机制和明确的警告。

这是对自然语言处理的历史和现状的概述。下一篇文章 NLP #2 将着眼于目前市场上存在的特定参与者和产品。
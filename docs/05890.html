<html>
<head>
<title>Stochastic Gradient Descent using PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PyTorch的随机梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/stochastic-gradient-descent-using-pytotch-bdd3ba5a3ae3?source=collection_archive---------17-----------------------#2021-08-02">https://medium.com/geekculture/stochastic-gradient-descent-using-pytotch-bdd3ba5a3ae3?source=collection_archive---------17-----------------------#2021-08-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="bba3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">神经网络如何自我学习？</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/ab370585b191a71966220873cb0e970d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RCmwNqVEuNRQzsLMFKXsXA.png"/></div></div></figure><p id="e9ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> **Pytorch让深度学习变得自动化和强大** </strong></p><p id="f4f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">什么是<strong class="ih hj">梯度下降</strong>？</p><p id="9b0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一种自动机制，它使我们的模型变得越来越好，这基本上意味着它可以自我学习。</p><p id="6453" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简而言之，梯度(我们函数的斜率)衡量的是每个重量，改变重量会如何改变损失。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jp"><img src="../Images/2ce7084e9e99f2d9afc71646f1640631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*-k6RuQd4usjeA8HHpt2ahw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx"><strong class="bd ju">what would happen if we increased or decreased our parameter by a little bit — the <em class="jv">adjustment</em>. This is simply the slope at a particular point.</strong></figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jw"><img src="../Images/91da30311beecc7ab2fb1133844cf8a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*8foUd2agiiplcKynpXBVCw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx"><strong class="bd ju">In other words, the gradients will tell us how much we have to change each weight to make our model better.</strong></figcaption></figure><p id="0136" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可能还记得高中微积分课上讲的，一个函数的<em class="jx">导数</em>告诉你它的参数的变化会在多大程度上改变它的结果。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jy"><img src="../Images/dd1dbb48d07f8212780e6d9ad0ebb238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zoywHDM6_AdN2LmQbro9Iw.png"/></div></div></figure><p id="1934" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">好吧，但它是如何工作的呢？</strong></p><p id="c9b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的问题是，我们如何使这个学习过程自动化，以便它最终给出最好的结果。让我们举一个例子，我们试图测量过山车越过驼峰顶部时的速度，因此基本上建立了速度随时间变化的模型。</p><p id="e719" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">手动测量，它看起来会像下面这样—</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jz"><img src="../Images/ad39fe02112061703490781856811b8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SBAX07Wybi79SsJaLidHsQ.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Speed measured manually for 20 secs</figcaption></figure><p id="326d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用<strong class="ih hj"> SGD，</strong>我们可以尝试找到一个与我们的观察相匹配的函数。在这种情况下，我们假设它是一个形式为<strong class="ih hj"> a*(t**2) + (b*t) + c </strong>的二次函数</p><p id="9d03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中t是以秒为单位的时间，a、b、c是参数。</p><p id="1e81" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们想要清楚地区分函数的输入(我们测量过山车速度的时间)和它的参数(定义我们正在尝试的<em class="jx">二次方的值)。因此，让我们将参数收集在一个参数中，从而将函数签名中的输入<code class="du ka kb kc kd b">t</code>和参数<code class="du ka kb kc kd b">params</code>分开:</em></p><pre class="je jf jg jh fd ke kd kf kg aw kh bi"><span id="416d" class="ki kj hi kd b fi kk kl l km kn">def f(t, params):<br/>     a,b,c = params<br/>     return a*(t**2) + (b*t) + c</span></pre><p id="6e50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">换句话说，我们已经将寻找符合数据的最佳想象函数的问题限制为寻找最佳的二次函数。</p><p id="3fea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">我们所说的“最好”是什么意思？."</strong></p><p id="38cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们通过选择一个<strong class="ih hj"> <em class="jx">损失函数</em> </strong>来定义它，它将返回一个基于预测和目标的值，其中该函数的较低值对应于“<strong class="ih hj">更好的</strong>”预测。对于连续数据，常用<strong class="ih hj"> <em class="jx">均方误差</em> : </strong></p><pre class="je jf jg jh fd ke kd kf kg aw kh bi"><span id="58eb" class="ki kj hi kd b fi kk kl l km kn">def mse(preds, targets): <br/>     return ((preds-targets)**2).mean().sqrt()</span></pre><p id="dc71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">好吧，那进展如何？</p><p id="e190" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面解释了七个步骤—</p><ol class=""><li id="d61c" class="ko kp hi ih b ii ij im in iq kq iu kr iy ks jc kt ku kv kw bi translated"><strong class="ih hj">初始化参数— </strong></li></ol><p id="a50e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们将参数初始化为随机值，并使用<code class="du ka kb kc kd b">requires_grad_</code>告诉PyTorch我们想要跟踪它们的梯度</p><p id="53f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">特殊方法<code class="du ka kb kc kd b">requires_grad</code>是我们用来告诉PyTorch我们想要计算那个变量在那个值上的梯度的神奇咒语。它本质上是标记变量，所以PyTorch会记住如何计算另一个变量的梯度，直接根据你的要求进行计算。</p><pre class="je jf jg jh fd ke kd kf kg aw kh bi"><span id="3e27" class="ki kj hi kd b fi kk kl l km kn"><strong class="kd hj">params = torch.randn(3).requires_grad_()</strong></span></pre><p id="481d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2 </strong>。<strong class="ih hj">计算预测值</strong> —</p><pre class="je jf jg jh fd ke kd kf kg aw kh bi"><span id="f331" class="ki kj hi kd b fi kk kl l km kn">preds = f(time, params)</span></pre><p id="2df0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们创建一个小函数来看看我们的预测与我们的目标有多接近，看看:</p><pre class="je jf jg jh fd ke kd kf kg aw kh bi"><span id="0b56" class="ki kj hi kd b fi kk kl l km kn">def show_preds(preds, ax=None):<br/>    if ax is None: ax=plt.subplots()[1]<br/>    ax.scatter(time, speed)<br/>    ax.scatter(time, to_np(preds), color='red')<br/>    ax.set_ylim(-300,100)<br/>    show_preds(preds)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kx"><img src="../Images/6e912fad718c2a5559a1001d771774cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l8keSVCB3snh6KutLQBseQ.png"/></div></div></figure><p id="570f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这看起来不太接近——我们的随机参数表明过山车最终会向后倒，因为我们的速度是负的！</p><p id="bac1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3 </strong>。<strong class="ih hj">计算损失</strong> —</p><pre class="je jf jg jh fd ke kd kf kg aw kh bi"><span id="4ace" class="ki kj hi kd b fi kk kl l km kn">loss = mse(preds, speed)<br/>loss</span><span id="5a4d" class="ki kj hi kd b fi ky kl l km kn">tensor(25823.8086, grad_fn=&lt;MeanBackward0&gt;)</span></pre><p id="2f2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在的目标是改善这种情况。为此，我们需要知道梯度。</p><h2 id="8827" class="ki kj hi bd ju kz la lb lc ld le lf lg iq lh li lj iu lk ll lm iy ln lo lp lq bi translated">4.计算梯度—</h2><p id="bd4f" class="pw-post-body-paragraph if ig hi ih b ii lr ik il im ls io ip iq lt is it iu lu iw ix iy lv ja jb jc hb bi translated">下一步是计算梯度。换句话说，计算参数需要如何改变的近似值:</p><pre class="je jf jg jh fd ke kd kf kg aw kh bi"><span id="ca99" class="ki kj hi kd b fi kk kl l km kn">loss.backward()<br/>params.grad</span><span id="65ca" class="ki kj hi kd b fi ky kl l km kn">tensor([-53195.8594,  -3419.7146,   -253.8908])</span><span id="8ca0" class="ki kj hi kd b fi ky kl l km kn">params.grad * 1e-5</span><span id="9611" class="ki kj hi kd b fi ky kl l km kn">tensor([-0.5320, -0.0342, -0.0025])</span></pre><p id="7c67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以使用这些梯度来改进我们的参数。我们需要选择一个<strong class="ih hj">学习率</strong>，现在我们只使用1e-5，或0.00001):</p><p id="9b97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参数</strong></p><pre class="je jf jg jh fd ke kd kf kg aw kh bi"><span id="5bec" class="ki kj hi kd b fi kk kl l km kn">tensor([-0.7658, -0.7506,  1.3525], requires_grad=True)</span></pre><h2 id="a0df" class="ki kj hi bd ju kz la lb lc ld le lf lg iq lh li lj iu lk ll lm iy ln lo lp lq bi translated">5.增加重量——</h2><pre class="je jf jg jh fd ke kd kf kg aw kh bi"><span id="f656" class="ki kj hi kd b fi kk kl l km kn">lr = 1e-5<br/>params.data -= lr * params.grad.data<br/>params.grad = None</span></pre><p id="b4e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">理解这一点取决于记住最近的历史。为了计算梯度，我们在<code class="du ka kb kc kd b">loss</code>上称之为<code class="du ka kb kc kd b">backward</code>。但是这个<code class="du ka kb kc kd b">loss</code>本身是由<code class="du ka kb kc kd b">mse</code>计算的，它又将<code class="du ka kb kc kd b">preds</code>作为输入，这是使用<code class="du ka kb kc kd b">f</code>作为输入<code class="du ka kb kc kd b">params</code>计算的，T7是我们最初调用<code class="du ka kb kc kd b">required_grads_</code>的对象——这是最初的调用，现在允许我们在<code class="du ka kb kc kd b">loss</code>上调用<code class="du ka kb kc kd b">backward</code>。这个函数调用链代表了函数的数学组成，这使得PyTorch可以使用微积分的链规则来计算这些梯度。</p><p id="6d48" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6。重复该过程</p><p id="47b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们迭代。通过循环和执行许多改进，让我们希望我们得到一个好的结果—</p><pre class="je jf jg jh fd ke kd kf kg aw kh bi"><span id="33bf" class="ki kj hi kd b fi kk kl l km kn">for i in range(10): apply_step(params)</span><span id="a903" class="ki kj hi kd b fi ky kl l km kn">5435.53662109375</span><span id="2edc" class="ki kj hi kd b fi ky kl l km kn">1577.4495849609375</span><span id="afc2" class="ki kj hi kd b fi ky kl l km kn">847.3780517578125</span><span id="71b4" class="ki kj hi kd b fi ky kl l km kn">709.22265625</span><span id="0f6e" class="ki kj hi kd b fi ky kl l km kn">683.0757446289062</span><span id="a417" class="ki kj hi kd b fi ky kl l km kn">678.12451171875</span><span id="17b7" class="ki kj hi kd b fi ky kl l km kn">677.1839599609375</span><span id="e839" class="ki kj hi kd b fi ky kl l km kn">677.0025024414062</span><span id="96d6" class="ki kj hi kd b fi ky kl l km kn">676.96435546875</span><span id="96cd" class="ki kj hi kd b fi ky kl l km kn">676.9537353515625</span></pre><p id="a86c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们所希望的那样，损失正在减少。</p><p id="e322" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们可以通过可视化看到形状是如何接近我们数据的最佳可能二次函数的—</p><pre class="je jf jg jh fd ke kd kf kg aw kh bi"><span id="9b0e" class="ki kj hi kd b fi kk kl l km kn">_,axs = plt.subplots(1,4,figsize=(12,3))</span><span id="5b0a" class="ki kj hi kd b fi ky kl l km kn">for ax in axs: show_preds(apply_step(params, False), ax)</span><span id="80e7" class="ki kj hi kd b fi ky kl l km kn">plt.tight_layout()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/fb14154ff44a6d3230a5f62182406181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0MtMLSsfFhtT5Snw35mISg.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx"><strong class="bd ju">Equation getting better with each Epoch(Iteration)</strong></figcaption></figure><p id="e6a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">7。停止</p><p id="42e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们只是武断地决定在10个纪元后停止。在实践中，我们会观察训练和验证的损失以及我们的度量来决定何时停止。</p><h1 id="3197" class="lx kj hi bd ju ly lz ma lc mb mc md lg me mf mg lj mh mi mj lm mk ml mm lp mn bi translated"><strong class="ak">结论</strong></h1><p id="e450" class="pw-post-body-paragraph if ig hi ih b ii lr ik il im ls io ip iq lt is it iu lu iw ix iy lv ja jb jc hb bi translated">让我们总结一下，在开始时，我们的模型的权重可以是随机的(从零开始训练<em class="jx"/>)或者来自预训练的模型(<em class="jx">迁移学习</em>)。在第一种情况下，我们将从输入中获得的输出与我们想要的没有任何关系，即使在第二种情况下，预训练模型也很可能不太擅长我们所针对的特定任务。所以模特需要<em class="jx">学习</em>更好的权重。</p><p id="8373" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们首先使用<em class="jx">损失函数</em>将模型给出的输出与我们的目标(我们已经标记了数据，因此我们知道模型应该给出什么结果)进行比较，该函数返回一个数字，我们希望通过提高权重使该数字尽可能低。为此，我们从训练集中提取一些数据项(比如图像),并将它们提供给我们的模型。我们使用我们的损失函数来比较相应的目标，我们得到的分数告诉我们我们的预测有多错误。然后我们稍微改变一下权重，让它稍微好一点。</p><p id="8dac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了找到如何改变权重以使损失稍微好一点，我们使用微积分来计算<em class="jx">梯度</em>。(其实我们是让PyTorch替我们做的！).</p><h1 id="6171" class="lx kj hi bd ju ly lz ma lc mb mc md lg me mf mg lj mh mi mj lm mk ml mm lp mn bi translated">一个解释学习率的类比—</h1><p id="41a0" class="pw-post-body-paragraph if ig hi ih b ii lr ik il im ls io ip iq lt is it iu lu iw ix iy lv ja jb jc hb bi translated">想象你在山里迷路了，你的车停在最低点。为了找到回去的路，你可能会朝一个随机的方向走，但那可能不会有多大帮助。因为你知道你的车在最低点，你最好下山。总是朝着最陡的下坡方向迈出一步，你最终会到达目的地。我们用梯度的大小(即斜率的陡度)来告诉我们该走多大的一步；具体来说，我们将梯度乘以一个我们选择的称为<em class="jx">学习率</em>的数来决定步长。然后我们<em class="jx">迭代</em>直到我们到达最低点，这将是我们的停车场，然后我们可以<em class="jx">停止</em>。</p><h1 id="13d4" class="lx kj hi bd ju ly lz ma lc mb mc md lg me mf mg lj mh mi mj lm mk ml mm lp mn bi translated">以学习的速度前进</h1><p id="5bd3" class="pw-post-body-paragraph if ig hi ih b ii lr ik il im ls io ip iq lt is it iu lu iw ix iy lv ja jb jc hb bi translated">决定如何基于梯度值改变我们的参数是深度学习过程的重要部分。几乎所有的方法都是从将梯度乘以某个小数字开始的，这个小数字叫做<em class="jx">学习率</em> (LR)。学习率通常是0.001到0.1之间的一个数字，尽管它可以是任何值。通常，人们只是通过尝试一些来选择学习率，并在训练后找到最佳模型(我们将在本书的后面向您展示一种更好的方法，称为<em class="jx">学习率查找器</em>)。一旦你选择了一个学习率，你可以使用这个简单的函数来调整你的参数:</p><pre class="je jf jg jh fd ke kd kf kg aw kh bi"><span id="41ff" class="ki kj hi kd b fi kk kl l km kn">w -= gradient(w) * lr</span></pre><p id="827e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是所谓的<em class="jx">步进</em>你的参数，使用<em class="jx">优化器步进</em>。</p><p id="ce69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你选择了一个太低的学习率，这可能意味着你需要做很多步骤。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mo"><img src="../Images/f2510199069dc139e7a047d6d9838847.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*5w8sbFJyY-fp1rHNPB5Qvw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx"><strong class="bd ju">Very low Learning Rate</strong></figcaption></figure><p id="b716" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">选择一个过高的学习率更糟糕——它实际上会导致损失变得更严重。T3】</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mp"><img src="../Images/d8d7118d849a844e411209baf07fc594.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*FXhHtWg6-QFV9-1Yf7GTQg.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx"><strong class="bd ju">Very High LR</strong></figcaption></figure><p id="a65d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">如果学习率过高，也可能“弹”向四周，而不是实际发散；展示了采取许多步骤来成功训练的结果。</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mq"><img src="../Images/93a40e2dc0941f60c6a1784ea456791a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*RXZaqvO0Vt3q2ZIKkB-O5Q.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx"><strong class="bd ju">Bouncy LR</strong></figcaption></figure><p id="e391" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">好吧！！所以基本上，我试图让SGD神经网络中的一个非常重要的概念——在这个故事中变得更容易解释和理解。</p><p id="8c0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请鼓掌或分享您的评论。</p><p id="db08" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">谢谢，</p><p id="3e5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">阿希什</strong></p></div></div>    
</body>
</html>
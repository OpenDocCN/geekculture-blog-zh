<html>
<head>
<title>How My LeNet Achieves 99% Accuracy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我的网络如何达到99%的准确率</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/how-my-lenet-achieves-99-accuracy-acb8a1c737f0?source=collection_archive---------16-----------------------#2021-09-23">https://medium.com/geekculture/how-my-lenet-achieves-99-accuracy-acb8a1c737f0?source=collection_archive---------16-----------------------#2021-09-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/8742fc395a2db3bb83db5183330f514c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hwXu3jpgoa9qDbCv"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Photo by <a class="ae iu" href="https://unsplash.com/@ianstauffer?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ian Stauffer</a> on <a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="51a1" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">介绍</h1><p id="b781" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">微调在模型训练中的作用很大，认识到每个超参数的意义让你成功。</p><p id="29ee" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在这篇文章中，我将向您展示我如何通过微调三个超参数，在MNIST手写数字识别上达到99%的最高准确率。我还尝试从头开始实现一个经典的CNN模型LeNet-5，让我熟悉CNN模型的结构和基本组件。更重要的是，我将在<a class="ae iu" href="https://fluxml.ai/" rel="noopener ugc nofollow" target="_blank"> Flux.jl </a>中构建我的LeNet-5模型，以展示一个Julia神经网络框架的例子。</p><h1 id="ddfb" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">基础模型</h1><blockquote class="kw kx ky"><p id="c721" class="jt ju kz jv b jw kr jy jz ka ks kc kd la kt kg kh lb ku kk kl lc kv ko kp kq hb bi translated"><em class="hi">你可以从</em> <a class="ae iu" href="https://github.com/Cuda-Chen/flux-lenet" rel="noopener ugc nofollow" target="_blank"> <em class="hi">我的回购</em> </a> <em class="hi">获取代码。</em></p></blockquote><p id="bbde" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">基础模型是大家熟知的5层LeNet [ ]，实现采用自Flux.jl模型zoo [ ]。因此，原始LeNet和Flux.jl模型zoo [ ]中的实现之间存在一些差异:</p><ol class=""><li id="0100" class="ld le hi jv b jw kr ka ks ke lf ki lg km lh kq li lj lk ll bi translated">LeNet中卷积层的激活函数使用<strong class="jv hj"> sigmoid </strong>，而在Flux.jl model zoo中使用<strong class="jv hj"> ReLU </strong>。</li><li id="d65c" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">LeNet中的池层使用<strong class="jv hj">平均</strong>池，而在Flux.jl模式中zoo使用<strong class="jv hj">最大</strong>池。</li><li id="d0cb" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">LeNet中的pooling层激活函数使用<strong class="jv hj">比例双曲正切</strong>，而Flux.jl model zoo中的使用<strong class="jv hj">恒等式</strong>(线性)。</li><li id="40c6" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">LeNet原始论文中使用的多类分类使用<strong class="jv hj">欧氏径向基(RBF)函数</strong>。[3]但是，在Flux.jl的实现中使用了<strong class="jv hj"> softmax </strong>。</li></ol><p id="b220" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">为了方便起见，我列出了我的实现的结构:</p><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="lv lw l"/></div><figcaption class="iq ir et er es is it bd b be z dx"><em class="lx">The structure of base model. You can click “view raw” to show the image in new tab. Sorry for your inconvenience because NN-SVG (http://alexlenail.me/NN-SVG/LeNet.html) does not have any options to resize the image.</em></figcaption></figure><h1 id="ed0d" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">让我们微调一下！</h1><p id="b03d" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">因此，hypermeter调整在机器学习模型开发中起着至关重要的作用。虽然Flux.jl的LeNet实现可以达到98%的top-1准确率，但我还是想试试能否突破极限。更重要的是，通过实验微调，我可以获得哪些参数在某些任务中起主要作用的知识。</p><h1 id="3c78" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">基线及其性能</h1><blockquote class="kw kx ky"><p id="57b1" class="jt ju kz jv b jw kr jy jz ka ks kc kd la kt kg kh lb ku kk kl lc kv ko kp kq hb bi translated"><em class="hi">基线模型可以在这里找到:</em><a class="ae iu" href="https://github.com/FluxML/model-zoo/blob/master/vision/conv_mnist/conv_mnist.jl" rel="noopener ugc nofollow" target="_blank"><em class="hi">https://github . com/flux ml/model-zoo/blob/master/vision/conv _ mnist/conv _ mnist . JL</em></a></p></blockquote><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><h1 id="d653" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">批量</h1><p id="238b" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">批量意味着在一次迭代中使用多少训练样本。此外，它表示您在摄取一定数量的训练样本后，更新或正式计算损失，然后反向传播模型的参数。因此，假设以下场景:</p><ol class=""><li id="a211" class="ld le hi jv b jw kr ka ks ke lf ki lg km lh kq li lj lk ll bi translated">如果您在摄取<strong class="jv hj">整个数据</strong>后更新参数。你可能得到一个快速的参数更新时间，但是模型在实际情况下表现不佳，因为模型陷入局部极小值的陷阱。此外，它需要大量的内存来加载数据。</li><li id="5de7" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">如果你在摄取<strong class="jv hj">后更新了参数的每个数据</strong>(每次迭代中只有一个数据)。您可能会得到一个具有奇妙结果的模型，但是它需要花费大量的时间来训练，因为它会在每次迭代中更新参数。</li></ol><p id="9157" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">因此，选择正确的批量数量可以:</p><ul class=""><li id="4557" class="ld le hi jv b jw kr ka ks ke lf ki lg km lh kq ly lj lk ll bi translated">减少训练时间和记忆</li><li id="9dad" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq ly lj lk ll bi translated">覆盖更好的性能</li></ul><p id="344b" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在这篇文章中，我尝试了不同数量的批量，我的培训平台的最佳批量是<strong class="jv hj"> 32 </strong>。</p><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="2a62" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">以下段落是不同批量的训练日志:</p><h2 id="7d2e" class="lz iw hi bd ix ma mb mc jb md me mf jf ke mg mh jj ki mi mj jn km mk ml jr mm bi">32</h2><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><h2 id="6bd8" class="lz iw hi bd ix ma mb mc jb md me mf jf ke mg mh jj ki mi mj jn km mk ml jr mm bi">64</h2><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><h2 id="56ae" class="lz iw hi bd ix ma mb mc jb md me mf jf ke mg mh jj ki mi mj jn km mk ml jr mm bi">256</h2><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><h2 id="3ec6" class="lz iw hi bd ix ma mb mc jb md me mf jf ke mg mh jj ki mi mj jn km mk ml jr mm bi">512</h2><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><h2 id="5d55" class="lz iw hi bd ix ma mb mc jb md me mf jf ke mg mh jj ki mi mj jn km mk ml jr mm bi translated">正则参数</h2><p id="3339" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">正则化是添加惩罚，以便模型降低变得过拟合的概率。通常，我们可以使用L1和L2正则，我选择L2正则为我的LeNet-5模型。</p><p id="a65e" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在本实验中，最佳L2正则化参数为<strong class="jv hj"> 1e-6 </strong>。</p><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="1cca" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">像往常一样，我把训练日志与不同的正则化参数:</p><h2 id="b7d5" class="lz iw hi bd ix ma mb mc jb md me mf jf ke mg mh jj ki mi mj jn km mk ml jr mm bi translated">1e-2</h2><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><h2 id="f48e" class="lz iw hi bd ix ma mb mc jb md me mf jf ke mg mh jj ki mi mj jn km mk ml jr mm bi translated">1e-4</h2><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><h2 id="0954" class="lz iw hi bd ix ma mb mc jb md me mf jf ke mg mh jj ki mi mj jn km mk ml jr mm bi translated">1e-6</h2><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><h1 id="be7d" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">【计算机】优化程序</h1><p id="e742" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">机器学习中的优化器是根据预先指定的参数来改变学习率，从而改变模型的学习率，使模型更容易推广。在这篇文章中，我在常见的ADAM中选择了三个优化器:ADAMW、NADAM和AdaBelief。关于这些优化器的描述，可以访问<a class="ae iu" href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Optimisers" rel="noopener ugc nofollow" target="_blank">flux . JL</a>的优化器文档。</p><p id="842d" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在这篇文章中，最好的优化器是ADAMW。</p><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="7bfb" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">这里是不同优化器的训练日志:</p><h2 id="329e" class="lz iw hi bd ix ma mb mc jb md me mf jf ke mg mh jj ki mi mj jn km mk ml jr mm bi translated">阿达姆</h2><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><h2 id="4018" class="lz iw hi bd ix ma mb mc jb md me mf jf ke mg mh jj ki mi mj jn km mk ml jr mm bi translated">那达慕</h2><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><h2 id="f32f" class="lz iw hi bd ix ma mb mc jb md me mf jf ke mg mh jj ki mi mj jn km mk ml jr mm bi translated">阿达信仰</h2><figure class="lr ls lt lu fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><h1 id="04f1" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">结论</h1><p id="ad69" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在这篇文章中，我构建了经典的LeNet-5模型，不仅练习了我的机器学习技能，还让我熟悉了新兴的Flux.jl框架。我还展示了三个可能的标准——批量大小、正则化和优化——用于超参数调优或微调的过程。最后，我给大家展示了我的LeNet-5模型在MNIST数据集上可以达到99%的top-1准确率。</p><h1 id="32c1" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">显示培训环境的列表</h1><ul class=""><li id="99ce" class="ld le hi jv b jw jx ka kb ke mn ki mo km mp kq ly lj lk ll bi translated">CPU:英特尔酷睿i7–9700 CPU，3.00GHz</li><li id="d922" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq ly lj lk ll bi translated">内存:16磅</li><li id="08b8" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq ly lj lk ll bi translated">操作系统:Fedora 33 (Linux内核5.13.12)</li><li id="bd89" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq ly lj lk ll bi translated">朱莉娅版本:1.6.2</li><li id="8b62" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq ly lj lk ll bi translated">Flux.jl版本:v0.12.4</li></ul><h1 id="2397" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">参考</h1><p id="84ff" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">【<a class="ae iu" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf" rel="noopener ugc nofollow" target="_blank">http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf</a></p><p id="c4d0" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">[]<a class="ae iu" href="https://github.com/FluxML/model-zoo/blob/33f5c472c321a50fc2105358a00eb7b3ec0ffa5e/vision/conv_mnist/conv_mnist.jl#L21" rel="noopener ugc nofollow" target="_blank">https://github . com/flux ml/model-zoo/blob/33 F5 C5 c 472 c 321 a 50 fc 2105358 a 00 EB 7 B3 EC 0 FFA 5 e/vision/conv _ mni ST/conv _ mni ST . JL # L21</a></p><p id="0bb1" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">https://pabloinsente.github.io/the-convolutional-network<a class="ae iu" href="https://pabloinsente.github.io/the-convolutional-network" rel="noopener ugc nofollow" target="_blank"/></p></div><div class="ab cl mq mr gp ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="hb hc hd he hf"><p id="b6fa" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><em class="kz">原载于2021年9月23日</em><a class="ae iu" href="https://cuda-chen.github.io/deep%20learning/2021/09/23/lenet-99.html" rel="noopener ugc nofollow" target="_blank"><em class="kz">https://cuda-Chen . github . io</em></a><em class="kz">。</em></p></div></div>    
</body>
</html>
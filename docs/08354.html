<html>
<head>
<title>How to stop Overfitting your ML and Deep Learning models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何停止过度适应你的ML和深度学习模型</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/how-to-stop-overfitting-your-ml-and-deep-learning-models-bb8324ace80b?source=collection_archive---------18-----------------------#2021-10-26">https://medium.com/geekculture/how-to-stop-overfitting-your-ml-and-deep-learning-models-bb8324ace80b?source=collection_archive---------18-----------------------#2021-10-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="966b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">机器学习和深度学习中的过拟合是一个普遍问题。这是模型没有概括数据的结果，因此具有很高的方差。在本文中，我们将通过一些常见的方法来减轻过度拟合。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/b4585fc3c41eb2e6ea211092092d1ab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*62E0-mu43N_yfrO6mgdCnw.jpeg"/></div><figcaption class="jl jm et er es jn jo bd b be z dx">Creator: © Mark Evans | Credit: Mark Evans</figcaption></figure><p id="2c33" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将通过sci-kit learn和Tensorflow代码示例使用一些最常见的过拟合解决方案</p><h1 id="0e8f" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">过度拟合的高级解决方案</h1><ul class=""><li id="1f5b" class="kn ko hi ih b ii kp im kq iq kr iu ks iy kt jc ku kv kw kx bi translated">降低模型复杂性</li><li id="76df" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">正则化(调整权重)</li><li id="b82e" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">集合模型</li><li id="7765" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">提前停止</li><li id="a5d8" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">更多数据</li><li id="6b53" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">交互效度分析</li></ul><h1 id="7e7d" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">降低模型复杂性</h1><p id="4a44" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">可以通过将以下内容分别应用于机器和深度学习模型来降低模型复杂度:</p><ul class=""><li id="80c1" class="kn ko hi ih b ii ij im in iq lg iu lh iy li jc ku kv kw kx bi translated"><strong class="ih hj">降特征/</strong>降维(使用PCA)</li><li id="54af" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">丢弃神经元(使用丢弃层)</li></ul><p id="dacd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">停止过度拟合模型的最有争议的方法是降低其复杂性。有几种方法可以做到这一点，但最简单的方法是简单地<strong class="ih hj">删除一些功能！</strong>！</p><p id="a4ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在你告诉我我疯了！信息越多不是越好吗？</p><p id="3095" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一般来说，你是对的；与目标相关并向特征空间提供<strong class="ih hj">附加</strong>信息的特征将有助于模型的准确性或减少误差。然而，一些特征可能具有存在于另一个特征中的信息，或者它们根本不影响目标。因此，我们可以<strong class="ih hj">移除冗余特征</strong>或那些与目标<strong class="ih hj">低(或零)相关性</strong>的特征。</p><p id="f644" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这可以通过反复试验，通过丢弃具有相同信息的特征或通过分析它们的相关性来手动完成，但是这需要对数据有深入的了解。</p><p id="e7c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用监督机器学习算法<strong class="ih hj">主成分分析(PCA)可以完成减少维度/特征的“自动化”过程。</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lj"><img src="../Images/8c8cf233b9f48ae0682895f5eb4c2812.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*tGoDYawFm8s5nAIlYkCK3w.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx">The number of features are reduced to 6 (specified by n_components in PCA)</figcaption></figure><p id="778a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于深度学习，我们除了上述选项之外还有其他选项，通过使用<strong class="ih hj">Dropout</strong><strong class="ih hj">Layers</strong>。顾名思义，在每次运行(epoch)中，随机丢弃%的神经元。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lk"><img src="../Images/93e718e8b5f0eaf2f7a6502f5819431b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*zlU2BgngV_zv_x6Ly3sr2g.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx">At layer 1 and layer 2 we drop 50% (0.5) of the neurons being used</figcaption></figure><h1 id="363d" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">正规化</h1><p id="223c" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">正则化是操纵输入的系数或<strong class="ih hj">权重</strong>的实践。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es ll"><img src="../Images/bbc615653a40f1032962e8de33818e6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VGXsHsm-BqU7cJ44"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx">Photo by <a class="ae lq" href="https://unsplash.com/@genejeter?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Gene Jeter</a> on <a class="ae lq" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="045f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有许多方法可以做到这一点，一些模型默认包含正则化，如回归机器学习模型；<strong class="ih hj">套索</strong> (L1正则化)和<strong class="ih hj">山脊</strong> (L2正则化)。</p><ul class=""><li id="6bd9" class="kn ko hi ih b ii ij im in iq lg iu lh iy li jc ku kv kw kx bi translated">L1正则化-尝试将一些权重设置为零，这将从预测过程中消除这些要素。</li><li id="91f4" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">L2正则化-尝试将要素的权重/系数限制为零(但不完全为零)。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/45d6e06586c6b008990fa910874560ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*89zIrR8StdMY5cAyR2U7dg.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx">Alpha determines the amount of restriction applied to the feature coefficients</figcaption></figure><p id="52c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用于正则化的附加深度学习过程使用在拟合模型时设置的<strong class="ih hj"> class_weight </strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/19d3f9593a38b1df90abc2739faf79e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*B0X1Ev8xCUh6CCdp2Za52w.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx">Example of setting class weights for binary classification</figcaption></figure><h1 id="4d43" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">全体</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es lt"><img src="../Images/da0852c6ac923a7c2835db1d779d2e1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ou1yXm7SOw62wgLD"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx">Photo by <a class="ae lq" href="https://unsplash.com/@chesnutt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Eran Menashri</a> on <a class="ae lq" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="7cb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">克服过度拟合的一个有趣的方法是使用集成模型，它采用“弱学习者”模型，并将它们组合起来创建一个“超级”模型。这可以通过三种方式实现:</p><ul class=""><li id="dad7" class="kn ko hi ih b ii ij im in iq lg iu lh iy li jc ku kv kw kx bi translated">装袋—均质模型在<strong class="ih hj">平行</strong>运行</li><li id="1028" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">增压—均质模型在<strong class="ih hj">系列</strong>中运行</li><li id="6848" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">堆叠— <strong class="ih hj">异质</strong>机型组合</li></ul><p id="423e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">装袋</strong>是用多个<strong class="ih hj">相似的</strong>模型并行预测目标变量，并对各个预测值取平均值，形成最终预测值的过程。这方面的一个例子是随机森林模型，其中并行运行多个“较弱”的决策树，并对结果输出进行平均以形成预测。</p><p id="8db6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">增压</strong>类似于装袋，但是模型是线性运行的(串联)，由此串联中的下一个模型“学习”先前的模型。流行的型号，如XGboost和ADAboost都采用这种工艺。</p><p id="16bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">堆叠</strong>使用<strong class="ih hj">不同的</strong>模型类型来预测结果。这可以通过sci-kit learn中的<strong class="ih hj">投票回归器</strong>或<strong class="ih hj">投票分类器</strong>实际看到。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/e0a8116453ea4395fe1ce108fc819889.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*8F-qPeAMhTKFEFKD7qSK5A.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx">Multiple models are fitted to the data with the “hard” voting</figcaption></figure><p id="def3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">投票分类器的一个重要超参数是“硬投票”，它使用所有模型的<strong class="ih hj">最高</strong>概率预测，或“软投票”，它取<strong class="ih hj">平均值</strong>。</p><p id="1da8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不幸的是，我们在Tensorflow中没有定义的包来做同样的事情，但是我们可以通过运行多个深度学习模型并平均结果来手动复制这一点。一个深入的例子可以在<a class="ae lq" href="https://machinelearningmastery.com/train-neural-networks-with-noise-to-reduce-overfitting/" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p><h1 id="63be" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">提前停止</h1><p id="f694" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">早期停止通常在深度学习模型中被注意到，其中当针对测试/验证数据的性能开始下降时，时期(模型的迭代)被停止。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es lv"><img src="../Images/18a57efc50f483cfdeed4a26840b04cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6_azj_x6BUBSGZnL"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx">Photo by <a class="ae lq" href="https://unsplash.com/@possessedphotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Possessed Photography</a> on <a class="ae lq" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="83ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到一个在Tensorflow中使用<strong class="ih hj"> EarlyStopping </strong>模块的例子，其中我们将历元的数量设置得非常高</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es lw"><img src="../Images/2980a1ecbe6a206fd4b23d720cc91642.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rY_v0838tOaK_ZCab8LUUQ.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx">Large number of epochs and callback parameter set to EarlyStopping</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es lx"><img src="../Images/343ec51f36e704862ae776fd71d35147.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PX9XDxyLGZCUTSOS5KnU2g.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx">Left graph without early stopping. Right graph includes early stopping</figcaption></figure><p id="b221" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以在机器学习中通过在某些sci-kit学习模型中设置参数来做到这一点，如<strong class="ih hj">随机森林</strong>中的<strong class="ih hj"> n_estimators </strong>或<strong class="ih hj">线性回归</strong>中的<strong class="ih hj"> n_jobs </strong></p><h1 id="0db9" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">更多数据</h1><p id="4947" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">这总是一个很好的选择，但是我们需要小心不要包含那些信息已经存在于其他中的特征(见上面的降低模型复杂性)</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es ly"><img src="../Images/7c45dbaaf5a6d327a950c1a26824642c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nb2kFAZaKPjgvxV8"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx">Photo by <a class="ae lq" href="https://unsplash.com/@swimstaralex?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Alexander Sinn</a> on <a class="ae lq" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="c97c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一种方法是获取更多的数据样本，然而这并不总是可能的。</p><p id="42e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一种方式是通过<strong class="ih hj">特征工程</strong>，我们从其他人那里获得额外的特征。这可以在我们的<a class="ae lq" href="https://www.kaggle.com/slythe/infamous-titanic-80-accuracy" rel="noopener ugc nofollow" target="_blank"> Kaggle Titanic </a>提交中得到最好的展示，在这里我们提取了额外的特征，例如乘客姓名的<strong class="ih hj">长度</strong>、乘客的<strong class="ih hj">头衔</strong>(先生、小姐等)。)、特征的分组/宁滨等。</p><h1 id="5f06" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">交互效度分析</h1><p id="cb3b" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">交叉验证是一个重采样过程，其中数据集被分成k个组，每个组的某些子集用于<strong class="ih hj">训练</strong>模型，其他子集用于验证或<strong class="ih hj">测试</strong>模型。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lz"><img src="../Images/5002be9ed9461f0a7d4bcc3bb21a0670.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*5Y3rPieJ67qWQkAYP9XnZg.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx">K-fold cross validation where k=3</figcaption></figure><p id="58be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后评估模型在<strong class="ih hj">测试</strong>数据上的表现，也称为“未知数据”,因为该数据没有用于拟合/训练模型。</p><p id="1fe9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">交叉验证的方法有很多，如<strong class="ih hj"> K折</strong>和<strong class="ih hj">留一法</strong>等。然而，最简单也是最广为人知的方法是<strong class="ih hj">保持法；</strong>其中一部分数据留作以后测试。这最常用于sci-kit学习模块<strong class="ih hj"> train_test_split: </strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es ma"><img src="../Images/dcfe56f334b4981f03cf0a9de19fb10e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4dMxHJzhD4GYAsNzVcPEMA.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx">Example of using hold-out cross validation with train_test_split</figcaption></figure><h1 id="7118" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">结论</h1><p id="098a" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">这是一些快速和肮脏的例子，以减轻过度拟合，而不是深入钻研每一步。我建议，如果您使用上述任何步骤，您应该在实现之前全面调查该过程，因为其中一些步骤可能会妨碍模型预测。</p><p id="2442" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于使用早期停止和脱落层减轻过拟合的示例，请查看我们的<a class="ae lq" href="https://www.kaggle.com/slythe/combat-overfitting-with-early-stopping-dropout" rel="noopener ugc nofollow" target="_blank"> Kaggle笔记本</a> : <strong class="ih hj">使用早期停止&amp;脱落</strong>对抗过拟合</p></div></div>    
</body>
</html>
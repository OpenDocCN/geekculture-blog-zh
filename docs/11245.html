<html>
<head>
<title>Ensemble Techniques Part 1-Bagging &amp; Pasting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">整体技术第1部分-装袋和粘贴</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/ensemble-techniques-part-1-bagging-pasting-b8cc7fd69edf?source=collection_archive---------11-----------------------#2022-03-12">https://medium.com/geekculture/ensemble-techniques-part-1-bagging-pasting-b8cc7fd69edf?source=collection_archive---------11-----------------------#2022-03-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="40a3" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">装袋、粘贴、随机子空间、随机补丁、随机森林、额外树、出袋、特征重要性</h2><div class=""/><div class=""><h2 id="542d" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">在scikit learn中实现集成技术背后的理论直觉</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/13b9d07af03e62268bb05db3d842976d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2VuXTApY6Vb3lc8y3gKoMA.jpeg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Photo by <a class="ae jw" href="https://unsplash.com/@charlesdeluvio?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">charlesdeluvio</a> on <a class="ae jw" href="https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="2fbe" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><a class="ae jw" href="https://scikit-learn.org/stable/modules/ensemble.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jz hs"> <em class="kt">“集成方法的目标是结合用给定学习算法构建的几个基本估计量的预测，以提高单个估计量的可推广性/稳健性。</em></strong></a><strong class="jz hs"><em class="kt"/></strong></p><p id="bcc7" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">我们有数据，我们正在训练一个<strong class="jz hs">单个</strong> ML模型进行预测，这可能经常导致准确性较低。但是如果我们把一组模型(回归器或者分类器)的预测聚合起来呢？答案是，我们通常会得到比最佳个体预测者更好预测。如果有足够数量的弱学习者并且它们足够多样化(相关性较低)，那么对具有低准确度的单个预测者(弱学习者)进行分组会导致<strong class="jz hs">集合</strong>(强学习者)实现高准确度，同时降低方差。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ku"><img src="../Images/c0066f9280f8b81748f8153cf8c793c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WLfYK7UUFgJEbNGMAwcRaQ.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Source: Oreilly ‘s Hands-On machine learning with Scikit-learn, Keras &amp; Tensor flow</figcaption></figure><p id="0eed" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">最流行的集成方法有:</p><ol class=""><li id="7ad8" class="kv kw hi jz b ka kb kd ke kg kx kk ky ko kz ks la lb lc ld bi translated">引导聚集</li><li id="118e" class="kv kw hi jz b ka le kd lf kg lg kk lh ko li ks la lb lc ld bi translated">助推技术</li><li id="0e2e" class="kv kw hi jz b ka le kd lf kg lg kk lh ko li ks la lb lc ld bi translated">堆垛</li></ol><p id="540c" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">在本文中，我们将只研究装袋粘贴法及其随机森林算法的特例&amp;额外树法，但在此之前，让我们看看来自单个预测者的预测将如何聚合。对于回归问题，它将主要是单个独立估计量的所有预测的平均值，而对于分类问题，它可以有硬投票或软投票方法。</p><p id="ae67" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">硬投票分类器:</strong> <em class="kt">预测类别标签的多数投票分类器</em>。这背后的想法是，每个输出的预测将被汇总，得到最多投票的类将是最终的预测输出。对于下图，大多数分类器预测类别1，一个分类器预测类别2，因此根据多数投票，分类器类别1将是最终的预测输出。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es lj"><img src="../Images/7987d91c278929ed4a7ff05fba27bb1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zy5JLJUSHG2vxGTxfCDd4g.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Source: Oreilly ‘s Hands-On machine learning with Scikit-learn, Keras &amp; Tensor flow</figcaption></figure><p id="aebb" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">软投票分类器:</strong>测量平均预测概率(软投票)。它将预测具有最高分类概率的输出分类，该分类概率在所有单个分类器上平均。对于下面的例子，单个模型预测1类和2类的概率，大多数模型具有1类的最高概率，因此1类所有最高概率的平均值为0.65。有时它通过<code class="du lk ll lm ln b">weights</code>参数给每个分类器分配权重。权重将与各个分类器的准确度成正比。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es lo"><img src="../Images/a83da4c320acd15885b1e44f301a2b1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8F-Bz2vQQIAv6ZdNFh3-tA.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Credit: Author</figcaption></figure><p id="41d1" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">软投票比硬投票具有更高的性能，因为它给予高度信任的投票更大的权重。</p><blockquote class="lp lq lr"><p id="a834" class="jx jy kt jz b ka kb is kc kd ke iv kf ls kh ki kj lt kl km kn lu kp kq kr ks hb bi translated"><strong class="jz hs"> <em class="hi">注意:当所有单个模型尽可能相互独立时，集成技术工作得最好，产生不相关的误差，如果它们在相同的数据上进行训练，则不可能产生相同类型的误差，从而降低集成精度。获得不同分类器的最好方法是使用不同的算法来训练它们，这会导致不同类型的错误，从而提高集成精度。</em>T9】</strong></p></blockquote><p id="2a02" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">现在继续，让我们看看装袋技术，随机森林和额外树木的方法。</p></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><p id="30c6" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi mc translated"><span class="l md me mf bm mg mh mi mj mk di"> 1。</span> <strong class="jz hs">装袋:</strong>在这种方法中，相同的训练算法被用作集成中的预测器，但是所有这些预测器都在训练数据集的不同随机子集上被训练。装袋和粘贴的细微区别在于，在装袋<strong class="jz hs"><em class="kt">(bootstrap = True)</em></strong>中使用替换进行训练数据集的采样，而在粘贴<strong class="jz hs"><em class="kt">(bootstrap = False)</em></strong>中不使用替换。因此，只有bagging允许对训练实例进行多次采样以获得预测值。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ml"><img src="../Images/5568bf63de2bc0771a6eed8482ceaf7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iskng0M2Qv9GF0CADcl0Ww.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Source: Oreilly ‘s Hands-On machine learning with Scikit-learn, Keras &amp; Tensor flow</figcaption></figure><p id="ab86" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">一般来说，每个单独的预测值都有较高的偏差，但是与在相同训练数据集上训练的单个预测值相比，聚合将最终产生具有相似偏差但方差较低的集合。BaggingClassifier也自动执行软投票。所有预测器都是并行训练的，并通过不同的CPU内核执行预测的并行计算(scikit learn中的n_jobs参数表示用于训练和预测的CPU内核数量，-1表示使用所有可用的内核)。</p></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><p id="70cf" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs"> <em class="kt">像采样实例一样，我们也可以对特征进行采样</em> </strong>，这样每个预测器都可以在输入特征的随机子集上进行训练。BaggingClassifier通过参数<code class="du lk ll lm ln b">bootstrap_features</code> &amp; <code class="du lk ll lm ln b">max_features. </code>提供这两种技术:</p><p id="968a" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">随机子空间:</strong>保留<em class="kt">所有训练实例</em> ( <code class="du lk ll lm ln b">bootstrap=False</code> ) &amp; ( <code class="du lk ll lm ln b">max_samples=1</code>)但采样特征(<code class="du lk ll lm ln b">bootstrap_features=True</code>)和<code class="du lk ll lm ln b">max_features</code>为小于1的值称为随机子空间法。</p><p id="a66d" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">随机补丁:</strong>对训练实例和特征进行采样的技术称为随机补丁。</p></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><p id="7245" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">开箱评估(OOB): </strong>在bagging中，我们已经看到培训是通过替换培训实例来完成的。因此，有些实例可能被多次采样，而有些实例根本没有被采样。在数学上，可以看到，对于每个预测器，63%的训练实例被平均采样，而37%的训练实例根本没有被采样，并被称为<strong class="jz hs"> <em class="kt">外袋(oob)实例</em> </strong>。由于预测器从未见过这些实例，因此可以使用这些实例评估预测器性能，而无需使用验证集。为了检查评估分数，将使用以下代码，该代码具有500个决策树预测器，100个训练实例，使用所有核心进行并行处理。</p><pre class="jh ji jj jk fd mm ln mn mo aw mp bi"><span id="f4a8" class="mq mr hi ln b fi ms mt l mu mv">from sklearn.ensemble import BaggingClassifier<br/>from sklearn.tree import DecisionTreeClassifier</span><span id="00a3" class="mq mr hi ln b fi mw mt l mu mv">bagging_clf=BaggingClassifier(DecisionTreeClassifier(),n_estimators=500,max_samples=100, bootstrap=True, n_jobs=-1,oob_score=True)</span><span id="5b6b" class="mq mr hi ln b fi mw mt l mu mv">bagging_clf.fit(X_train, y_train)<br/>y_pred = bagging_clf.predict(X_test)</span><span id="ee13" class="mq mr hi ln b fi mw mt l mu mv">bagging_clf.oob_score_</span></pre></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><h1 id="e2eb" class="mx mr hi bd my mz na nb nc nd ne nf ng ix nh iy ni ja nj jb nk jd nl je nm nn bi translated"><strong class="ak"> <em class="no">随机森林算法</em> </strong></h1><p id="ba8f" class="pw-post-body-paragraph jx jy hi jz b ka np is kc kd nq iv kf kg nr ki kj kk ns km kn ko nt kq kr ks hb bi translated"><strong class="jz hs"> <em class="kt">是通过bagging技术</em> </strong>训练的决策树的集合，其中<code class="du lk ll lm ln b">max_sample</code>设置为训练集的大小。森林估计量的主要目标是减少方差，因为单棵树往往会过度拟合并表现出较高的方差。它在生长树中引入了额外的随机性，因为当在节点处分裂时，它从选择的特征的随机子集中搜索最佳特征，而不是从所有特征中搜索最佳特征。这将导致更大的树多样性，有时会导致低方差的偏差增加，从而导致整体更好的模型。</p><p id="ed16" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">我们可以直接使用sklearn.ensemble中的RandomForestClassifier，而不是构建BaggingClassifier，然后将其传递给DecisionTreeClassifier。</p><p id="6cd2" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><code class="du lk ll lm ln b"><strong class="jz hs">Note:</strong> <em class="kt">Random forest helps in performing feature selection by measuring feature importance on looking at how much the tree nodes that use that feature reduces impurity on average.</em></code></p><h1 id="87b4" class="mx mr hi bd my mz nu nb nc nd nv nf ng ix nw iy ni ja nx jb nk jd ny je nm nn bi translated">极度随机化的树(额外树)</h1><p id="22db" class="pw-post-body-paragraph jx jy hi jz b ka np is kc kd nq iv kf kg nr ki kj kk ns km kn ko nt kq kr ks hb bi translated">在这种情况下，计算分裂的方式增加了额外的随机性。它对每个特征使用随机阈值，而不是搜索可能的最佳阈值(像常规决策树一样)，然后选择这些随机生成的阈值中的最佳阈值作为分割规则。这种技术允许以略微增加偏差为代价来减少方差。</p></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><p id="a5de" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">这让我们了解了装袋技术，在接下来的几篇文章中，我们将深入探讨不同的增压技术及其不同类型。</p><p id="12e3" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">如果你发现什么错误，请评论。此外，DM是开放的。快乐学习。</p><p id="9e1d" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">参考资料:</p><ol class=""><li id="7bc2" class="kv kw hi jz b ka kb kd ke kg kx kk ky ko kz ks la lb lc ld bi translated"><a class="ae jw" href="https://scikit-learn.org/stable/modules/ensemble.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/ensemble.html</a></li><li id="15a1" class="kv kw hi jz b ka le kd lf kg lg kk lh ko li ks la lb lc ld bi translated">O'Reilly Media，Inc .使用Scikit-Learn、Keras和TensorFlow进行机器学习。</li><li id="7d79" class="kv kw hi jz b ka le kd lf kg lg kk lh ko li ks la lb lc ld bi translated">克里斯纳伊克机器学习播放列表。</li></ol></div></div>    
</body>
</html>
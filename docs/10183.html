<html>
<head>
<title>The Perceptron Algorithm: How it Works and Why it Works</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">感知器算法:如何工作和为什么工作</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/the-perceptron-algorithm-how-it-works-and-why-it-works-3668a80f8797?source=collection_archive---------7-----------------------#2022-01-17">https://medium.com/geekculture/the-perceptron-algorithm-how-it-works-and-why-it-works-3668a80f8797?source=collection_archive---------7-----------------------#2022-01-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="a14e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感知器算法是最简单的机器学习算法，也是神经网络和支持向量机等更复杂模型的基本构建模块。理解它是如何工作的以及为什么会工作是监督学习问题的基础。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/ea55f71499c45af5d44b78d3ccc341f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pqHt4gtedC15oR7I.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Image by <a class="ae jt" href="https://www.datamation.com/author/shelbyhiter/" rel="noopener ugc nofollow" target="_blank"><strong class="bd ju">Shelby Hiter</strong></a><strong class="bd ju"> </strong>on Datamation</figcaption></figure><p id="6e08" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jv">注:NumPy是唯一用于实现该算法的库，本帖中的所有代码都可以在下面的colab笔记本中找到并运行。此外，所有的图像、代码和数字都是由作者生成的。</em></p><p id="0110" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jt" href="https://colab.research.google.com/drive/1XpC16z4gE0Q9REGfs-ExwziY_cVlbDdi?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/drive/1 XP C16 z 4g E0 q 9 regfs-EXW ziy _ cVlbDdi？usp =共享</a></p><h1 id="682e" class="jw jx hi bd ju jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">感知器算法:它是如何工作的</h1><p id="28ac" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">感知器算法是一种用于监督学习的模型。所有监督学习模型都在一组特征向量<em class="jv"> X </em>和一组相应的标签<em class="jv"> Y、</em>上训练，标签可以是1 <em class="jv">。</em>感知器算法的目标是在特征空间中找到判定边界，使得属于给定类别的每个特征向量落在边界的同一侧，并且边界将两个类别分开。</p><p id="fff7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数学上，特征向量<em class="jv"> X </em>是特征空间中的向量，我们可以把特征空间想象成一个向量空间，所以决策边界必须把向量空间分开。在3D空间中，2D平面分隔了整个空间，而在2D，1D线分隔了整个空间。对于一个<em class="jv"> n- </em>维空间，一个<em class="jv"> (n-1)- </em>维空间称为超平面，感知器算法的目标是找到一个划分该空间的超平面。虽然决策边界不需要是线性的，但感知器算法的最简单版本可以处理线性决策边界。</p><p id="fedd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将我们自己限制在通过特征空间的原点的所有超平面的集合，这是我们稍后将放松的假设，超平面和决策边界可以由向量<em class="jv"> θ </em>来定义，该向量与超平面正交或垂直。使用特征向量<em class="jv"> X、</em>标签<em class="jv"> Y </em>和<em class="jv"> θ </em>来表示决策边界，感知器算法的工作用下面的伪代码来描述:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ky"><img src="../Images/2ab02fff68bf776bc3e44e85187b2878.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M5fC9P_v1-85-wyecJeYZA.png"/></div></div></figure><p id="61b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于所有线性可分的数据，这个算法总是会找到一个判定边界。在python中，这段伪代码是这样写的:</p><pre class="je jf jg jh fd kz la lb lc aw ld bi"><span id="4178" class="le jx hi la b fi lf lg l lh li">def train_perceptron(X, y):<br/>    theta = np.zeros(len(X[0]))<br/>    misclassified = True<br/>    while misclassified:<br/>        misclassified = False<br/>        for i in range(len(X)):<br/>            if y[i] * np.dot(theta, X[i]) &lt;= 0:<br/>                theta += y[i] * X[i]<br/>                misclassified = True<br/>return theta</span></pre><p id="213c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">算法的第一步是将<em class="jv"> θ </em>初始化为特征空间中的零向量。这是一个重要的步骤，因为感知器使用<em class="jv"> θ </em>和一个特征向量的点积来确定适当的标签，所以训练感知器的重点是学习<em class="jv"> θ </em>的最佳值。</p><p id="c3b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有两个主循环，从伪代码的第2行和第3行开始。在第2行，执行这个循环，直到所有数据点都被正确分类，或者插入另一个终止条件，我们将在后面看到。在第3行，这个循环迭代训练数据中的每个特征向量，根据需要更新<em class="jv"> θ </em>。</p><h1 id="b617" class="jw jx hi bd ju jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">工作原理:感知机的更新</h1><p id="9f76" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">感知器更新步骤是算法的关键部分，出现在伪代码的第4行和第5行。第4行的if语句确定<em class="jv"> θ </em>是否正确分类了特征向量<em class="jv"> xᵢ.</em>由于<em class="jv"> θ </em>是决策边界的法向量，我们可以使用点生成来确定特征向量在超平面的哪一侧。回想一下，平行单位向量的点积为+1，反平行(方向完全相反的向量)单位向量的点积为-1。基于此，如果<em class="jv"> θ </em>和<em class="jv"> xᵢ </em>指向同一方向，则点积为&gt; 0，如果指向相反方向，则点积为&lt; 0。</p><p id="0be8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果<em class="jv"> yᵢ </em>是+1，点积是&gt; 0，那么<em class="jv">yᵢ(θ</em><strong class="ih hj">⋅</strong><em class="jv">xᵢ)</em>&gt;0。同理，如果<em class="jv"> yᵢ </em>为-1，点积为&lt; 0，那么<em class="jv">yᵢ(θ</em>t24】⋅t26】xᵢ)&gt;0。这些条件表明<em class="jv"> θ </em>对<em class="jv"> xᵢ </em>的分类是正确的。但是，如果<em class="jv"> yᵢ </em>和<em class="jv">θ</em><strong class="ih hj">⋅</strong><em class="jv">xᵢ</em>没有相同的符号，那么<em class="jv">yᵢ(θ</em><strong class="ih hj">⋅</strong><em class="jv">xᵢ)</em>将小于或等于0。这是在第4行的if语句中检查的条件。</p><p id="ddb7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当决策边界正确地对特征向量进行分类时，没有任何更新。然而，当错误分类发生时，在“正确的方向”上更新决策边界，以防止错误分类再次发生。这次更新采取的形式是将(<em class="jv">yᵢ</em><strong class="ih hj">⋅</strong><em class="jv">xᵢ</em>)增加到<em class="jv"> θ </em>。这一增加的原因是因为新的<em class="jv"> θ </em>更好地分类了<em class="jv"> xᵢ.要了解这一点，请考虑以下几点:</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lj"><img src="../Images/7f37237450ca7555afd92f41085973ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*DgVcxMzvDPTKgBeNraw01A.png"/></div></figure><p id="e6b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这显示了在递增<em class="jv"> θ </em>之后，错误分类的特征向量和<em class="jv"> θ </em>之间的点积更大的标签倍。由于这个量对于正确分类的特征向量来说是正的，并且更新使得这个值更大，因此它沿着正确的方向移动决策边界。</p><h1 id="2ad1" class="jw jx hi bd ju jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">行动中的感知机</h1><p id="dd2a" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">一旦训练了一个感知器，通过寻找<em class="jv"> θ </em>学习了决策边界，就可以用它来对数据进行分类。colab笔记本(链接如上)包含生成数据、训练感知器和绘制决策边界的代码。本节将展示这种算法的一些优点和缺点。</p><p id="c1eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">生成了两个2D特征向量簇，如下图所示。正如上面所写的，感知器算法被训练来寻找由<em class="jv"> θ </em>定义的决策边界，并被视为下图中的绿线。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lk"><img src="../Images/00a99dd4f7b82b0efde4bef698228dd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*ciDTi2gz0G_6P7FBSitK7g.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx">This data is linearly separable with a decision boundary through the origin.</figcaption></figure><p id="b46e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感知器算法在寻找适合这个数据集的决策边界方面做得很好。但是，这里有两点需要注意:</p><ol class=""><li id="3933" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated">不是所有的数据都可以用一条穿过原点的直线(或超平面)分开。</li><li id="fcbd" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">该数据集中没有噪声。紫色和黄色的星团没有重叠，这使得它们很容易分离。</li></ol><p id="c98a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然感知器算法在这些条件为真时工作，但如果这些条件不满足，它就会悲惨地失败。要解决这个问题，可以对算法进行一些修改。</p><h1 id="99d1" class="jw jx hi bd ju jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">改进的感知器</h1><h2 id="2816" class="le jx hi bd ju lz ma mb kb mc md me kf iq mf mg kj iu mh mi kn iy mj mk kr ml bi translated">带偏移的感知器</h2><p id="f2e6" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">首先，放松判定边界必须通过特征空间的原点的假设，可以通过包括偏移项<em class="jv">b</em>来修改算法。先前，通过原点的判定边界，正确分类的特征向量满足条件<em class="jv">yᵢ(θ</em>t8】⋅T10】xᵢ)T50】0。利用偏移项<em class="jv"> b，</em>正确分类的特征向量满足条件<em class="jv">yᵢ(θ</em><strong class="ih hj">⋅</strong><em class="jv">xᵢ+b)</em>&gt;0。这被称为带偏移的感知器。</p><p id="dc98" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与上面原始感知器算法中的<em class="jv"> θ </em>类似，<em class="jv"> b </em>只在一个特征向量被错误分类时更新。当一个点被错误分类时，<em class="jv"> θ </em>更新为<em class="jv">θ=θ</em>+y<em class="jv">ᵢ</em><strong class="ih hj">⋅</strong><em class="jv">xᵢ、</em>和<em class="jv"> b </em>更新为<em class="jv"> b = b + </em> y <em class="jv"> ᵢ.</em></p><p id="0c15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以将偏移项合并到原始感知器算法中，而不添加偏移项<em class="jv"> b </em>，而是如下扩展<em class="jv"> θ </em>和<em class="jv"> xᵢ </em>:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mm"><img src="../Images/92c4eddf1fb071f83af1da1375eb2f34.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*s4KN_Th7VhvA6ViLT3Qzzg.png"/></div></figure><p id="6199" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这种情况下，原感知器算法相当于偏移量为<em class="jv"> b </em>的感知器。在colab笔记本中，算法明确使用了偏移<em class="jv"> b. </em>感知器通过偏移学习的决策边界示例如下所示。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mn"><img src="../Images/87c2349edf87fd281137ea2256e9d32a.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*ymn7r6BXCvyTG4TgA3D35Q.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx">A decision boundary that does NOT pass through the origin</figcaption></figure><h2 id="119e" class="le jx hi bd ju lz ma mb kb mc md me kf iq mf mg kj iu mh mi kn iy mj mk kr ml bi translated">不可分割的数据</h2><p id="6f87" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">从伪代码的第2行开始的循环一直执行，直到感知器算法找到分隔两类数据的决策边界。然而，这并不总是可能的！这是有问题的，因为当感知器算法应用于无法完美分离的数据时，它将执行无限循环。</p><p id="ffae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一般来说，有两种策略可以解决这个问题。最简单的策略是限制外部循环的执行次数。外部循环的每次执行都是一个时期，因此我们可以添加一个条件，设置要执行的时期的最大数量。</p><p id="900d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二种策略是当在每个时期之间对<em class="jv"> θ </em>的更新变得足够小时，退出第二循环。这种方法实现起来稍微复杂一些，并且由于在感知器算法中没有正则化项，所以<em class="jv"> θ </em>变得无界，这使得该方法更加复杂。</p><p id="1e5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是感知器算法的一个例子，该算法根据10个时期内不可分离的数据进行偏移训练。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mo"><img src="../Images/8967c5eb583a2d8de337fc4bb3eb257c.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*OwlQuZSW5mB4zMwn10maQg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx">This example applies the Perceptron Algorithm with Offset to inseparable data. The code for this plot is available towards the bottom of the colab notebook.</figcaption></figure><p id="f9cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然决策边界确实在某种程度上分隔了两个集群，但它并没有做好工作。这突出了感知器算法在处理可分离数据时是有用的，但在其他情况下不是很有用。</p><h1 id="bfaf" class="jw jx hi bd ju jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">感知器的扩展</h1><p id="90ed" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">我们已经看到了感知器算法的工作方式和原因，以及它的局限性。虽然算法本身目前并不是一个流行的选择，因为它只对线性可分数据有效，但它背后的思想形成了当今许多流行的机器学习概念的主干。</p><p id="65d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策边界、多个时期的训练以及感知器如何处理特征向量等概念对于理解神经网络等更复杂的模型来说都是至关重要的。应用</p><ol class=""><li id="f254" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated">核</li><li id="99a5" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">激活功能</li><li id="c9aa" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">正规化</li><li id="caf0" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">多重感知器</li></ol><p id="4861" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">和其他技术是将感知器算法扩展到支持向量机和神经网络的关键。这些是我将在以后的文章中涉及的概念。</p><p id="11c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢您阅读我在Medium上的第一个故事！</p><p id="7c90" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">—约书亚·皮卡德</p></div></div>    
</body>
</html>
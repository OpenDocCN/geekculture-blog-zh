<html>
<head>
<title>A Look Under the Hood of Pytorch’s Recurrent Neural Network Module</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pytorch递归神经网络模块的内幕</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/a-look-under-the-hood-of-pytorchs-recurrent-neural-network-module-47c34e61a02d?source=collection_archive---------10-----------------------#2022-06-19">https://medium.com/geekculture/a-look-under-the-hood-of-pytorchs-recurrent-neural-network-module-47c34e61a02d?source=collection_archive---------10-----------------------#2022-06-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="a069" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">py torch RNN模块中的权重初始化和矩阵乘法指南。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/a96685b0cf9542013ca6471ad6db590b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QlD4utNjjEOUQYiWhe9hkw.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Photo by <a class="ae jn" href="https://unsplash.com/@alfonsmc10?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Alfons Morales</a> on <a class="ae jn" href="https://unsplash.com/s/photos/books?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="6cd5" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">神经网络在模仿人脑识别物体、分割图像甚至与人互动方面一直做得非常出色。说到交互，几十年的研究已经进入了自然语言处理领域(人工智能的一个子领域，涉及计算机和人类之间的交互)，以使与计算机的交互过程成为一种更无缝的体验。目前用于自然语言处理的最先进的系统主要基于一种神经网络架构，即递归神经网络(RNN)。</p><p id="22e4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在本文中，我们将浏览一个普通递归神经网络的内部架构，并深入探讨两个内部机制:RNN中的权重初始化和矩阵乘法</p><h2 id="ea10" class="kk kl hi bd km kn ko kp kq kr ks kt ku jx kv kw kx kb ky kz la kf lb lc ld le bi translated">是什么让RNNs如此特别？</h2><p id="2ff0" class="pw-post-body-paragraph jo jp hi jq b jr lf ij jt ju lg im jw jx lh jz ka kb li kd ke kf lj kh ki kj hb bi translated">在RNNs之前，前馈网络用于解决NLP任务。该网络被输入编码成整数的文本数据，并通过反向传播对权重和偏差进行传统的调整来进行训练。尽管他们产生了可观的结果，但是他们缺乏理解输入数据的上下文含义的能力。</p><p id="7dca" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">另一方面，rnn具有记忆输入数据的<strong class="jq hj">顺序含义</strong>的固有能力。因此，当他们进行预测时(例如，在序列预测问题中)，预测是基于输入到模型中的整个序列，而不仅仅是输入序列的最后一个字符或单词。</p><h2 id="9cf6" class="kk kl hi bd km kn ko kp kq kr ks kt ku jx kv kw kx kb ky kz la kf lb lc ld le bi translated">深入了解内部架构:</h2><p id="0331" class="pw-post-body-paragraph jo jp hi jq b jr lf ij jt ju lg im jw jx lh jz ka kb li kd ke kf lj kh ki kj hb bi translated">递归神经网络的基本架构如下所示:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lk"><img src="../Images/85e563eb762601e6aed282e2a560f7cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9NaDSfXNkvOGrBC7N16u4g.jpeg"/></div></div></figure><p id="f7dd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这是一个RNN的模糊表现。如果递归部分像下图这样展开，整个过程将更容易理解。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ll"><img src="../Images/f0802fc176de916ea977489bac7d58e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mLsQ9w2zB4fw5L2E75aX8g.jpeg"/></div></div></figure><p id="ee24" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">当一个输入被送入RNN时，一个隐藏状态被计算出来。这种隐藏的状态被反馈到RNN细胞，它同时接收下一个输入。现在，这个隐藏状态连同新的输入被用来计算新的隐藏状态，该隐藏状态被再次反馈到RNN单元中。这个循环持续指定的次数，直到它给出所需的输出。在RNN中产生隐藏状态的每个递归步骤被称为<strong class="jq hj">时间步骤</strong>。</p><p id="8059" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">例如，考虑这些句子:“艾丽西娅喜欢宠物。她收养了一只名叫奥利弗的狗。无论她去哪里，她总是随身带着一个单肩包。艾丽西娅去度假时，她无法带着她的宠物____。在这里，神经网络应该能够记住第二句话中的信息，以填补第四句话中的空白。因此，当这些句子被一个字一个字地输入RNN时，它会借助隐藏状态记住每个时间步获得的顺序信息。</p><p id="d87e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">rnn有不同的变体，每个变体用于不同的用例。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lm"><img src="../Images/1fb14cd18b1df8b0b9d08173871b0c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kje9TpsSvMGeBF6vIv6k5Q.jpeg"/></div></div></figure><p id="5d23" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如上图所示，自动RNN的输入和输出数量可以根据具体需求而变化。Andrej Karpathy在他的博客<a class="ae jn" href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj">中给出了递归神经网络</strong> </a>不合理的有效性的例子。从左至右:<strong class="jq hj"> (1) </strong>没有RNN的香草模式处理，从固定大小的输入到固定大小的输出(例如图像分类)。<strong class="jq hj"> (2) </strong>顺序输出(如图像字幕拍摄图像，输出一句话)。<strong class="jq hj"> (3) </strong>序列输入(例如，情感分析，其中给定句子被分类为表达正面或负面情感)。<strong class="jq hj"> (4) </strong>顺序输入和顺序输出(如机器翻译:一个RNN读一句英语，然后输出一句法语)。<strong class="jq hj"> (5) </strong>同步序列输入和输出(例如，视频分类，其中我们希望标记视频的每一帧)。</p><h2 id="f99f" class="kk kl hi bd km kn ko kp kq kr ks kt ku jx kv kw kx kb ky kz la kf lb lc ld le bi translated">数学</h2><p id="4f16" class="pw-post-body-paragraph jo jp hi jq b jr lf ij jt ju lg im jw jx lh jz ka kb li kd ke kf lj kh ki kj hb bi translated">使用RNN的序列预测可以用这五个步骤来说明。</p><ul class=""><li id="c1cd" class="ln lo hi jq b jr js ju jv jx lp kb lq kf lr kj ls lt lu lv bi translated">一个输入被送入RNN单元(第一时间步)</li><li id="ffde" class="ln lo hi jq b jr lw ju lx jx ly kb lz kf ma kj ls lt lu lv bi translated">RNN单元计算该时间步长的隐藏状态。</li><li id="4f2e" class="ln lo hi jq b jr lw ju lx jx ly kb lz kf ma kj ls lt lu lv bi translated">这种隐藏状态被反馈到RNN单元(第二时间步)，同时RNN接受下一个顺序输入。</li><li id="a4e9" class="ln lo hi jq b jr lw ju lx jx ly kb lz kf ma kj ls lt lu lv bi translated">第一时间步的隐藏状态和新输入乘以各自的权重矩阵并求和。</li><li id="33e9" class="ln lo hi jq b jr lw ju lx jx ly kb lz kf ma kj ls lt lu lv bi translated">将激活函数应用于所获得的总和，以产生该时间步长的隐藏状态。这种隐藏状态被反馈到RNN单元，并重复循环，直到产生所需的输出。</li></ul><p id="ac3a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">每个时间步长的隐藏状态由下面的公式给出</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mb"><img src="../Images/c4cac7c3b12893d4d4b19fa0379e5b5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C2l3Gva4XOu2p70c7i5WxA.jpeg"/></div></div></figure><p id="6ac5" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在每个时间步长，输出隐藏状态基本上是RNN前一个时间步长及其新的顺序输入的隐藏状态之和，乘以各自的权重矩阵并通过激活函数。</p><p id="b1ae" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">通过将这些隐藏状态乘以输出权重矩阵，还可以使用这些隐藏状态来获得每个时间步长的RNN输出。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mc"><img src="../Images/27a34330f443d79659424accbd75dda3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xYjR2L2KzVQAQNHXuOg-rQ.jpeg"/></div></div></figure><h2 id="4451" class="kk kl hi bd km kn ko kp kq kr ks kt ku jx kv kw kx kb ky kz la kf lb lc ld le bi translated">权重初始化和矩阵乘法</h2><p id="fe12" class="pw-post-body-paragraph jo jp hi jq b jr lf ij jt ju lg im jw jx lh jz ka kb li kd ke kf lj kh ki kj hb bi translated">到目前为止，我们所看到的只是理论。但是，如果不接触代码，就几乎不可能彻底了解一个网络。这个奇妙的知识库有一个完整的代码演练，解释了如何使用Pytorch从头构建一个RNN。你也可以跟随<a class="ae jn" href="https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/" rel="noopener ugc nofollow" target="_blank">这篇</a>博文来更好地了解代码。</p><p id="c14a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这篇文章不包含代码演示，因为上面提到的博客已经很好地解释了代码，在这篇文章中重做会使它变得多余。</p><p id="92a2" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">但是Pytorch中有一些内部初始化设置和机制需要注意。在进入细节之前，应该定义六个变量。</p><p id="4262" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">输入-输入特性<br/> w_ih-输入的权重矩阵<br/> b_ih-输入的偏置<br/> h0-初始隐藏状态<br/> w_hh-隐藏状态的权重矩阵<br/> b_hh-隐藏状态的偏置</p><p id="4034" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">重量初始化</strong></p><p id="8241" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">考虑下面的例子。为RNN模型类初始化rnn对象。输入和初始隐藏状态被定义并传递到RNN。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="md me l"/></div></figure><p id="23d0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">RNN类的构造函数参数为(输入要素、隐藏维度、RNN层数)。我们将使用一个单一的RNN层在我们的例子。</p><p id="8b15" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">输入的形状应该是以下格式(批量大小、输入序列的长度、输入要素的数量)。批处理大小应该是第一维，因为我们在初始化rnn对象时已经设置了参数<em class="mf"> batch_first </em> = True。</p><p id="09ce" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">初始隐藏状态的形状应该是以下格式(RNN层数、批量大小、隐藏尺寸)。</p><p id="f212" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在根据我们的RNN公式，在每一个时间步，隐藏状态是这样计算的，</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mb"><img src="../Images/c4cac7c3b12893d4d4b19fa0379e5b5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C2l3Gva4XOu2p70c7i5WxA.jpeg"/></div></div></figure><p id="5db7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们将输入和初始隐藏状态传递到我们的RNN模型中。它们中每一个的权重被初始化并被插入到上面的公式中，以计算下一个隐藏状态。这些计算是在Pytorch的nn模块中完成的。让我们看看Pytorch如何初始化输入和隐藏状态的权重矩阵。</p><p id="ea53" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这是Pytorch官方github库的<a class="ae jn" href="https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/rnn.py" rel="noopener ugc nofollow" target="_blank"> RNN.py </a>(第84到94行)的截图</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mg"><img src="../Images/b546fbbdcf27a7f9542b776997e755e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LaADxFz1fBuWegYanqZKCA.jpeg"/></div></div></figure><p id="fe6c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这是定义砝码形状的部分。在第三和第四行中，定义了两个变量，即<em class="mf">real _ hidden _ size</em><strong class="jq hj"/>和<em class="mf"> layer_input_size </em>。</p><p id="35dc" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因为我们正在构建一个普通的RNN，所以我们没有任何投影尺寸。所以<em class="mf">真实隐藏尺寸</em>将等于<em class="mf">隐藏尺寸</em>(隐藏尺寸的数量)，在我们的例子中等于20。</p><p id="0145" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们的模型中有一个RNN层。正如我们在上面的代码截图中看到的，权重形状是通过循环模型中的层数来初始化的。所以在第一次迭代中，<em class="mf">层</em>将等于0。因此<em class="mf"> layer_input_size </em>将等于输入大小，在我们的示例中是10</p><p id="a05e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在为了初始化输入权重的形状，我们可以看到一个空的Pytorch张量被定义为shape <em class="mf"> (gate_size，layer_input_size) </em>。门尺寸在<a class="ae jn" href="https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/rnn.py" rel="noopener ugc nofollow" target="_blank"> RNN.py </a>的第71–80行初始化。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mh"><img src="../Images/c50c9a15218c4d8c0ba8632ca1c2747d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VVlOXlrUFwG0OZyIaY3JsA.jpeg"/></div></div></figure><p id="2a76" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">由于我们使用tanh作为外部激活函数，<em class="mf"> gate_size </em>将等于<em class="mf"> hidden_size </em>，在我们的示例中为20。</p><p id="0f89" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">最后，我们的输入和隐藏状态的权重和偏差的形状将是:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mi"><img src="../Images/be73d9ffa890e90cfaac9f9419aa9361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8PsCJxQnbYusoqnWvIDIFg.jpeg"/></div></div></figure><p id="f3cf" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在我们已经初始化了形状，是时候给这些矩阵赋值了。这些RNN参数的权重从范围从-k到k的均匀分布初始化，其中k = 1/sqrt( <em class="mf"> hidden_size) </em>。这在<a class="ae jn" href="https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/rnn.py" rel="noopener ugc nofollow" target="_blank"> RNN.py </a>的<em class="mf"> reset_parameters </em>函数中定义(第193-196行)</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mj"><img src="../Images/b0ec93935257adb50d88208d3eecd3ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xz0IiR1zppJirF_iGJOohg.jpeg"/></div></div></figure><p id="05f7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">好的，看起来我们已经达到了一个里程碑。但是如果我们也理解Pytorch下发生的矩阵乘法就更好了。</p><p id="8003" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">计算结果</strong></p><p id="7494" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们的输入是(3，5，10)的形式。3是批量大小——我们的例子有3批数据。因此，当我们传入第一批时，输入形状将是(1，5，20)，也可以认为是(5，20)。输入<em class="mf"> (w_ih) </em>的权重矩阵的形状为(20，10)。对于两个可相乘的矩阵，矩阵a中的列数(dim 1)应该等于矩阵b中的行数(dim 0)。为此，我们转置权重矩阵<em class="mf"> (w_ih) </em>。所以现在<em class="mf"> w_ih </em>的形状将是(10，20)并且我们可以将矩阵相乘。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mk"><img src="../Images/8af5b73bf2cf044d97b63291545ce8f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-3SFdcwUsx2fAOJ00Ci_Ag.jpeg"/></div></div></figure><p id="9a27" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">加权隐藏状态的计算也遵循相同的事件序列。隐藏状态的形状是(1，3，20)，其中3是<em class="mf"> batch_size。</em>因此，对于每个批次，隐藏状态的形状为(1，1，20)，也可以认为是(1，20)。隐藏状态的权重矩阵的形状为(20，20)。这里，不需要转置，因为矩阵满足乘法规则。但是我们需要一致地转置所有的矩阵，如果我们已经转置了一个的话。所以<em class="mf"> w_hh </em>在这里也被调换了。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ml"><img src="../Images/b15f1617854b4d8102f12323b180bb0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*25CHk_e8oSM6h4hlsdiPWg.jpeg"/></div></div></figure><p id="e0de" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在，可以将加权输入和加权隐藏状态相加，并对其应用tanh激活，以获得新的隐藏状态。但是等一下..加权输入和加权隐藏状态具有不同的形状。对于要相加的两个矩阵，它们应该是相同的形状！</p><p id="cef3" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了添加这些矩阵，Pytorch使用了一种称为广播的技术。我们的加权隐藏状态有20个元素，我们的加权输入有5X20 = 100个元素，每列有20个元素(dim 1)。对于广播，加权隐藏状态矩阵与加权输入矩阵的每一列相加。你可以在这里了解更多关于广播的信息<a class="ae jn" href="https://numpy.org/doc/stable/user/basics.broadcasting.html" rel="noopener ugc nofollow" target="_blank"/>。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mh"><img src="../Images/226e28e22309140c3dfad7f14e8b79df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m4N0eOd3LJZx6Hu7R0_YFw.jpeg"/></div></div></figure><p id="5c61" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这样我们得到了新的隐藏状态，它将作为RNN下一时间步的隐藏状态反馈给RNN。</p><p id="df6e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了简单起见，我们没有讨论计算中的偏差。它们是一维矩阵，形状=隐藏维数。使用广播在每个时间步将它们与加权输入和加权隐藏状态相加。</p><h2 id="af5c" class="kk kl hi bd km kn ko kp kq kr ks kt ku jx kv kw kx kb ky kz la kf lb lc ld le bi translated">谢谢你</h2><p id="918e" class="pw-post-body-paragraph jo jp hi jq b jr lf ij jt ju lg im jw jx lh jz ka kb li kd ke kf lj kh ki kj hb bi translated">就这样结束了！这就是Pytorch如何初始化权重，并在内核中为RNN乘上矩阵。</p></div></div>    
</body>
</html>
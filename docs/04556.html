<html>
<head>
<title>Dimensionality Reduction and Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维与主成分分析</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/dimensionality-reduction-and-principal-component-analysis-6e4d8ec6a312?source=collection_archive---------27-----------------------#2021-06-29">https://medium.com/geekculture/dimensionality-reduction-and-principal-component-analysis-6e4d8ec6a312?source=collection_archive---------27-----------------------#2021-06-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/20253f1b66e085314d07e1a9d700b5c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*N3wx9ivQmPHqhK8C"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx">Photo by <a class="ae hv" href="https://unsplash.com/@moreno303?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Misael Moreno</a> on <a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="a39c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是我正在从事的系列文章的第九部分，其中我们将讨论和定义介绍性的机器学习算法和概念。在这篇文章的最后，你会找到这个系列的所有前几篇文章。我建议你按顺序读这些。原因很简单，因为我在那里介绍了一些概念，这些概念对于理解本文中讨论的概念至关重要，我将在许多场合引用它们。</p><p id="75b0" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">今天我们来看看降维的重要性，以及最广泛使用的降维算法——主成分分析(PCA)算法。</p><p id="56fe" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们开始吧。</p><h1 id="7f51" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">降维</h1><p id="865b" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">降维是指从一个复杂的多维数据集到一个更简单的更少维度的数据集。例如，从三维数据集到二维数据集。重要的是，我们这样做是为了保持原始数据的有意义的属性。</p><p id="9a1e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这可能是有用的，原因有二。</p><h2 id="e6d7" class="kw ju hy bd jv kx ky kz jz la lb lc kd jg ld le kh jk lf lg kl jo lh li kp lj bi translated">原因一:数据压缩</h2><p id="a1ca" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">第一个原因是数据压缩。很多时候，我们将要处理的数据有许多冗余特征。例如，如果我们希望使用线性回归来预测波士顿房子的价格，我们可能会使用以两种不同的度量单位(比如米和英尺)来表示房子大小的数据。我们真的需要所有这些信息吗？房子的项目经理可能会，但你的机器学习模型肯定会很好。我们可以通过将这些信息压缩到一个单元而不是两个单元来减小数据集的大小。这不仅会产生更小的数据集，还能更快地从算法中学习。考虑下面的二维图形，其中<code class="du lk ll lm ln b">x^(i)</code>表示由特征<code class="du lk ll lm ln b">x_1</code>和<code class="du lk ll lm ln b">x_2</code>定义的训练点:</p><figure class="lp lq lr ls fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lo"><img src="../Images/09c0af663fa4bc39a35ed3ef0c370d64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1FzZq_1g4Jp_NSs2FVzz6Q.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 1: </strong>Random Training Points on a Graph</figcaption></figure><p id="0bf4" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们希望把它简化成一维图。为此，我们将通过我们的点拟合一条线:</p><figure class="lp lq lr ls fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lt"><img src="../Images/d714be32e5d2cb9c83212c980fb5c6aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oEPjVyzi82kfjygPXow6eg.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 2: </strong>Line Fitted Through Random Training Points</figcaption></figure><p id="f3c1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后计算从我们的点到拟合线的<strong class="ix hz">正交距离</strong>:</p><figure class="lp lq lr ls fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lu"><img src="../Images/ed079d85d4cc79b143e5a3816b76b867.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6JyOeAkJC6S85Ssf-xwY3Q.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 3: </strong>Orthogonal Distance from Training Points to Fitted Line</figcaption></figure><p id="e731" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你可能会认为上图与我们使用普通最小二乘法进行线性回归时看到的一样(参见<a class="ae hv" href="https://ali-h-khanafer.medium.com/linear-regression-using-gradient-descent-intuition-and-implementation-522d43453fc3" rel="noopener">第二部分</a>)，但是我们稍后会解释它们之间是如何没有关系的。</p><p id="2d88" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过计算正交距离，我们可以将我们的点投影到拟合线上。训练点在直线上的值就是它在一维平面上的值。这就是它的样子:</p><figure class="lp lq lr ls fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lv"><img src="../Images/e451c74c76914a56de2f25f69e2d1cee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NuU6K4Aucgse3ANocyWTbw.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 3: </strong>Training Points After Dimensionality Reduction</figcaption></figure><p id="60ec" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中<code class="du lk ll lm ln b">{z^(1),z^(2),z^(3),z^(4)}</code>是将<code class="du lk ll lm ln b">{x^(1),x^(2),x^(3),x^(4)}</code>投影到我们拟合的线上后的新值，<code class="du lk ll lm ln b">z_1</code>是描述我们训练点的新特征。</p><h2 id="829c" class="kw ju hy bd jv kx ky kz jz la lb lc kd jg ld le kh jk lf lg kl jo lh li kp lj bi translated">正交与平方距离</h2><p id="f23a" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在<strong class="ix hz">图3 </strong>中，我们看到了一个图表，该图表可能与我们在线性回归中计算训练点和拟合线之间的距离时看到的典型图表相混淆。</p><p id="835f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">虽然它们在视觉上很相似，但两者传达的是完全不同的信息。让我们并排比较一下:</p><figure class="lp lq lr ls fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lw"><img src="../Images/07bba1c8c1dba573f38e5d4da3cc74e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VG0IsHoZAn97vDJdfux4eQ.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 4: </strong>Squared vs Orthogonal Distance</figcaption></figure><p id="6b3b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们看到正交距离垂直于拟合线。这具有假设因变量(x轴)和自变量(y轴)中有误差的影响。我们可以通过训练点和直线之间的距离同时具有<code class="du lk ll lm ln b">x</code>和<code class="du lk ll lm ln b">y</code>分量这一事实看出这一点:</p><figure class="lp lq lr ls fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lx"><img src="../Images/3001d80ca1fdffcee40a30055ecbfc62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ofd1S-CIcXwUgMuWQJ804g.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 5: </strong>X and Y Components in Orthogonal Distances</figcaption></figure><p id="65aa" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在为线性回归计算的平方距离中，假设因变量没有错误。因此，我们只计算从训练点到拟合线的<code class="du lk ll lm ln b">y</code>距离。</p><h2 id="76d5" class="kw ju hy bd jv kx ky kz jz la lb lc kd jg ld le kh jk lf lg kl jo lh li kp lj bi translated">原因之二:可视化</h2><p id="bbf6" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">当我们的数据集是二维或三维的时候，它更容易被可视化。当处理具有数千个特征的数据集时，降维可以通过缩小数据所在的空间来帮助我们更容易地可视化数据。</p><h1 id="eccf" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">线性代数评论</h1><p id="71c8" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在描述PCA算法之前，我们需要回顾一些线性代数和统计概念。</p><h2 id="dd9c" class="kw ju hy bd jv kx ky kz jz la lb lc kd jg ld le kh jk lf lg kl jo lh li kp lj bi translated">方差、协方差和协方差矩阵</h2><ol class=""><li id="8849" class="ly lz hy ix b iy kr jc ks jg ma jk mb jo mc js md me mf mg bi translated"><strong class="ix hz">方差:</strong>描述数据集分散程度的度量。它描述了数据点和数据集均值之间的分布:</li></ol><figure class="lp lq lr ls fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mh"><img src="../Images/6c794cc26ac20229cf303385634877d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ktffa3DeS5C-cbftoeeQnA.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 1: </strong>Variance of a Population</figcaption></figure><p id="b890" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.<strong class="ix hz">协方差:</strong>描述两个随机变量如何对彼此的变化做出反应的度量。你可能会认为这个定义等于两个变量之间相关性的定义。你没有完全错。两者的区别在于相关性是标准化的，而协方差不是。也就是说，两个变量之间的相关性是一个介于-1和+1之间的值。<strong class="ix hz">变量与其自身的协方差等于该变量的方差</strong>。</p><figure class="lp lq lr ls fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mi"><img src="../Images/27dcb6ba29c77846295a9bfb1dd1bd29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ye03C1IF-6NvIBamx_n-8g.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 2: </strong>Covariance of a Population</figcaption></figure><p id="d5c1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.<strong class="ix hz">协方差矩阵:</strong>描述所有特征组合对之间协方差的方形对称矩阵。考虑描述两个特征<code class="du lk ll lm ln b">A</code>和<code class="du lk ll lm ln b">B</code>之间的协方差的简单2x2矩阵。协方差矩阵看起来像这样:</p><figure class="lp lq lr ls fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mj"><img src="../Images/5097c620339eba02b4019c92b94df3c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kqU1uexORgQnDDWRdSPQIQ.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 7: </strong>Covariance Matrix Example</figcaption></figure><p id="382d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请注意，对角线由特征集的所有方差组成。还要注意<code class="du lk ll lm ln b">Cov(A,B) = Cov(B,A)</code>，使我们的矩阵对称。</p><h1 id="6496" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">主成分分析</h1><p id="3844" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">主成分分析(PCA)是处理降维时最广泛使用的算法。</p><p id="84ce" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们将训练集的维度从二维问题降低到一维问题的示例中，我们的目标是找到一条最小化投影误差的线。也就是说，我们希望找到一条直线，在这条直线上，训练点和直线之间的平均正交距离最小。更正式地说，我们希望找到一个二维单位向量<code class="du lk ll lm ln b">u^(1)</code>(或<code class="du lk ll lm ln b">-u^(1)</code>)，我们可以将训练数据投影到这个向量上，以便最小化投影误差:</p><figure class="lp lq lr ls fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mk"><img src="../Images/977061f3310c9d95fcaed2013f15fb20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Heq9eS8HGj4TMC8CmZTxuA.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 8: </strong>Unit Vector That Minimizes Projection Error</figcaption></figure><p id="3e6c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是很明显，我们的数据并不总是二维的。因此，陈述我们的问题的一种更一般化的方式是:找到将数据投影到的<code class="du lk ll lm ln b">k</code>向量<code class="du lk ll lm ln b">{u^(1),u^(2),...,u^(k)}</code>，以便最小化投影误差。</p><p id="6701" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看看PCA是如何找到这些向量的。</p><h2 id="935a" class="kw ju hy bd jv kx ky kz jz la lb lc kd jg ld le kh jk lf lg kl jo lh li kp lj bi translated">直觉</h2><p id="8f1d" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">我们要做的第一件事是使用<strong class="ix hz">均值归一化</strong>来归一化我们的数据。如果您不知道这是什么意思，请参考本系列的第一部分，<strong class="ix hz"> </strong> <a class="ae hv" href="https://ali-h-khanafer.medium.com/data-pre-processing-ee81bbe5cc77" rel="noopener">数据预处理</a>。要记住的最重要的事情是，标准化将改变我们的数据，使其平均值为零。</p><p id="3bff" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第二步是要素缩放，但前提是您的数据需要。同样，如果你不知道那是什么意思，我建议你参考第一部分。</p><p id="11a6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦我们完成了所有的数据预处理，我们就剩下一个均值为零的数据集，并准备好用于我们的PCA算法。首先，我们计算训练数据的协方差矩阵。你可能想知道<em class="ml">为什么</em>我们要计算协方差矩阵。请这样想:我们说过协方差类似于相关性，因为它计算任何两个变量之间变化的相似性。那么，如果PCA的目标是通过找到冗余特征来减少我们的训练数据的维度，那么确定相似特征的更好方法是确定它们是否以相似的方式变化。下面是我们构建协方差矩阵的方法:</p><figure class="lp lq lr ls fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mm"><img src="../Images/cb0448c6f4b86b68ad2eaaf4aee9b461.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X-lX2vSPmZsqDEGDxUYSIg.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Equation 3: </strong>Covariance Matrix</figcaption></figure><p id="9354" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个等式可能会带来一些混乱:</p><ol class=""><li id="ab6a" class="ly lz hy ix b iy iz jc jd jg mn jk mo jo mp js md me mf mg bi translated">不要将协方差符号sigma误认为求和符号。相同的符号，不同的意义。</li><li id="8dbd" class="ly lz hy ix b iy mq jc mr jg ms jk mt jo mu js md me mf mg bi translated">如果您将此与我们在<strong class="ix hz">等式2 </strong>中看到的协方差等式进行比较，您会注意到我们没有从训练示例中减去平均值。为什么？因为我们对数据进行了标准化，所以数据的平均值为零，所以我们省略了等式的这一部分。</li><li id="d00f" class="ly lz hy ix b iy mq jc mr jg ms jk mt jo mu js md me mf mg bi translated">我建议你运行一个例子，看看如何成功地计算协方差矩阵。给自己两个<code class="du lk ll lm ln b">3x1</code>训练矩阵，然后应用协方差矩阵方程。</li></ol><p id="05da" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，我们对协方差矩阵执行<strong class="ix hz">奇异值分解(SVD) </strong>。SVD背后的理论远远超出了本文的范围，所以我们将快速描述一下<em class="ml">它如何给我们带来好处。SVD是一种矩阵分解算法。给定一个矩阵<code class="du lk ll lm ln b">M</code>，SVD会把它分解成三个独立的矩阵<code class="du lk ll lm ln b">U</code>、<code class="du lk ll lm ln b">S</code>和<code class="du lk ll lm ln b">V</code>。这三个矩阵都有重要的意义。在我们的例子中，我们最关心的矩阵是<code class="du lk ll lm ln b">U</code>矩阵。这些矩阵的列描述了我们的单位向量<code class="du lk ll lm ln b">{u^(1),u^(2),...,u^(n)}</code>。注意这个向量是一个<code class="du lk ll lm ln b">n x n</code>矩阵。我们需要做的就是选择第一个<code class="du lk ll lm ln b">k</code>向量。例如，如果我们希望将数据集从三维减少到二维，<code class="du lk ll lm ln b">U</code>将是一个<code class="du lk ll lm ln b">3 x 3</code>矩阵，但是我们将选择前两列。</em></p><figure class="lp lq lr ls fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mv"><img src="../Images/24356001e30c8d104e16e1e4436d9b66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ILfksM0IwvMnK_oczl30ZA.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 9: </strong>Unit Vector Matrix</figcaption></figure><p id="1625" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，剩下要做的就是将我们的训练示例乘以<code class="du lk ll lm ln b">U</code>单位向量，我们最终得到所有<code class="du lk ll lm ln b">x^(i)</code>训练示例的投影值:</p><figure class="lp lq lr ls fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mw"><img src="../Images/d38852c75cc92c46eee7bb2cee8a3ee4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Afd1-tY2brqfPX7TVz0HKw.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx"><strong class="bd jv">Figure 10: </strong>Projected Training Data</figcaption></figure><h1 id="f55b" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结论</h1><p id="ceab" class="pw-post-body-paragraph iv iw hy ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在本文中，我们研究了降维的概念和最广泛使用的降维算法，主成分分析(PSA)。我们还快速复习了重要的线性代数和统计概念。</p><p id="81dd" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在下一篇文章中，我们将研究异常检测。在此之前，我们让您思考以下几点:</p><ul class=""><li id="bc0a" class="ly lz hy ix b iy iz jc jd jg mn jk mo jo mp js mx me mf mg bi translated">所有的数据维度都是可约的吗？如果不是，在什么情况下不是？</li><li id="8f56" class="ly lz hy ix b iy mq jc mr jg ms jk mt jo mu js mx me mf mg bi translated">扩展我们在将数据集从二维缩减为一维时所做的工作，但这一次，将三维数据集扩展为二维数据集。</li></ul><h1 id="8ff2" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">过去的文章</h1><ol class=""><li id="0cde" class="ly lz hy ix b iy kr jc ks jg ma jk mb jo mc js md me mf mg bi translated"><strong class="ix hz">第一部分:</strong>T21】数据预处理</li><li id="2e0f" class="ly lz hy ix b iy mq jc mr jg ms jk mt jo mu js md me mf mg bi translated"><strong class="ix hz">第二部分:</strong> <a class="ae hv" href="https://ali-h-khanafer.medium.com/linear-regression-using-gradient-descent-intuition-and-implementation-522d43453fc3" rel="noopener">使用梯度下降的线性回归:直觉和实现</a></li><li id="f392" class="ly lz hy ix b iy mq jc mr jg ms jk mt jo mu js md me mf mg bi translated"><strong class="ix hz">第三部分:</strong> <a class="ae hv" rel="noopener" href="/geekculture/logistic-regression-using-gradient-descent-intuition-and-implementation-36a8498afdcb">使用梯度下降的逻辑回归:直觉和实现</a></li><li id="28f8" class="ly lz hy ix b iy mq jc mr jg ms jk mt jo mu js md me mf mg bi translated"><strong class="ix hz">第四部分— 1: </strong> <a class="ae hv" rel="noopener" href="/geekculture/neural-networks-part-1-terminology-motivation-and-intuition-73675fc43947">神经网络第一部分:术语、动机和直觉</a></li><li id="2e48" class="ly lz hy ix b iy mq jc mr jg ms jk mt jo mu js md me mf mg bi translated"><strong class="ix hz">第四部分— 2: </strong> <a class="ae hv" rel="noopener" href="/geekculture/neural-networks-part-2-backpropagation-and-gradient-checking-4f8d1350fb0b">神经网络第二部分:反向传播和梯度检测</a></li><li id="5641" class="ly lz hy ix b iy mq jc mr jg ms jk mt jo mu js md me mf mg bi translated"><strong class="ix hz">第六部分:</strong> <a class="ae hv" rel="noopener" href="/geekculture/evaluating-your-hypothesis-and-understanding-bias-vs-variance-86512cce4253">评估你的假设，理解偏差与方差</a></li><li id="9ad4" class="ly lz hy ix b iy mq jc mr jg ms jk mt jo mu js md me mf mg bi translated"><strong class="ix hz">第七部分:</strong> <a class="ae hv" rel="noopener" href="/geekculture/support-vector-machines-and-kernels-8b064ee53fc3">支持向量机和核</a></li><li id="7a2e" class="ly lz hy ix b iy mq jc mr jg ms jk mt jo mu js md me mf mg bi translated"><strong class="ix hz">第八部分:</strong> <a class="ae hv" rel="noopener" href="/geekculture/unsupervised-learning-and-the-intuition-behind-k-means-clustering-9805ed89fa0">无监督学习和K-Means聚类背后的直觉</a></li></ol><h1 id="ad69" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">无耻的插头</h1><ul class=""><li id="d0a9" class="ly lz hy ix b iy kr jc ks jg ma jk mb jo mc js mx me mf mg bi translated"><strong class="ix hz">推特:</strong>T26】twitter.com/ali_khanafer2</li></ul><h1 id="c93d" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">参考</h1><ol class=""><li id="fe4b" class="ly lz hy ix b iy kr jc ks jg ma jk mb jo mc js md me mf mg bi translated"><a class="ae hv" href="https://www.coursera.org/learn/machine-learning?page=1" rel="noopener ugc nofollow" target="_blank">吴恩达的机器学习Coursera课程</a></li><li id="0b8d" class="ly lz hy ix b iy mq jc mr jg ms jk mt jo mu js md me mf mg bi translated"><a class="ae hv" href="https://docs.scipy.org/doc/scipy/reference/odr.html" rel="noopener ugc nofollow" target="_blank"> Scipy正交距离回归</a></li><li id="cc00" class="ly lz hy ix b iy mq jc mr jg ms jk mt jo mu js md me mf mg bi translated"><a class="ae hv" href="https://www.youtube.com/watch?v=dvoHB9djouc&amp;ab_channel=KhanAcademy" rel="noopener ugc nofollow" target="_blank">人口方差|描述统计|概率统计|可汗学院</a></li><li id="b340" class="ly lz hy ix b iy mq jc mr jg ms jk mt jo mu js md me mf mg bi translated"><a class="ae hv" href="https://www.investopedia.com/ask/answers/041515/what-difference-between-variance-and-covariance.asp" rel="noopener ugc nofollow" target="_blank"> Investopedia的理解方差vs协方差:有什么区别？</a></li><li id="03e7" class="ly lz hy ix b iy mq jc mr jg ms jk mt jo mu js md me mf mg bi translated"><a class="ae hv" href="https://en.wikipedia.org/wiki/Covariance_matrix" rel="noopener ugc nofollow" target="_blank">协方差矩阵</a></li></ol></div></div>    
</body>
</html>
<html>
<head>
<title>Ensemble Techniques Part 2- AdaBoost Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集成技术第二部分- AdaBoost算法</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/ensemble-techniques-part-2-adaboost-algorithm-8dd927cacba4?source=collection_archive---------18-----------------------#2022-03-16">https://medium.com/geekculture/ensemble-techniques-part-2-adaboost-algorithm-8dd927cacba4?source=collection_archive---------18-----------------------#2022-03-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="7780" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">集成技术，增强，AdaBoost，机器学习</h2><div class=""/><div class=""><h2 id="080e" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">AdaBoost技术背后的理论和数学直觉</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/a397d673c11bca6cad9e574589225303.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cDjF657dRt-tqpsTQN6YLw.jpeg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Photo by <a class="ae jw" href="https://unsplash.com/@bradyn?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Bradyn Trollip</a> on <a class="ae jw" href="https://unsplash.com/s/photos/sequence?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="97fd" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><a class="ae jw" rel="noopener" href="/geekculture/ensemble-techniques-part-1-bagging-pasting-b8cc7fd69edf">本文是上一篇文章(综合技术第一部分-装袋&amp;粘贴)</a>的延续。它将主要集中在下一个非常重要的提升集成方法，即AdaBoost。正如我们在以前的文章中看到的，集成方法背后的主要思想是将许多弱学习器组合成一个强估计器。因此，Boosting主要通过顺序学习依次训练预测器来工作。这是一个迭代过程，在这个过程中，每个估计器都试图纠正其前任的错误预测。升压方法的主要类型有:</p><ol class=""><li id="68cd" class="kt ku hi jz b ka kb kd ke kg kv kk kw ko kx ks ky kz la lb bi translated">自适应增强(AdaBoost)</li><li id="05c7" class="kt ku hi jz b ka lc kd ld kg le kk lf ko lg ks ky kz la lb bi translated">梯度推进</li><li id="4ef8" class="kt ku hi jz b ka lc kd ld kg le kk lf ko lg ks ky kz la lb bi translated">极端梯度增强(Xgboost)</li></ol><p id="6b54" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs"> <em class="lh">这些顺序学习技术的问题在于它不能像打包和粘贴那样并行化。</em> </strong></p></div><div class="ab cl li lj gp lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="hb hc hd he hf"><p id="4ef7" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi lp translated"><span class="l lq lr ls bm lt lu lv lw lx di"> 1。</span> <strong class="jz hs"> AdaBoost: </strong>该算法背后的核心思想是，新的预测器通过给予前任预测错误分类或欠拟合的训练实例更多的关注来校正其前任。这是通过更新权重，通过增加欠拟合或错误分类的训练实例的相对权重来进行下一次预测，然后重复该过程直到满足所需的条件。AdaBoost将这些顺序估计器添加到集成中，逐渐使它变得更好。当一个测试数据到来时，它依次通过所有的基本学习器，并根据分配给每个预测器的权重进行最终预测。它可用于回归和分类问题。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ly"><img src="../Images/21faf5189fb4f2ee174082c486558969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dhrecywXzYS5QvuRq79xYw.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Source: Oreilly ‘s Hands-On machine learning with Scikit-learn, Keras &amp; Tensor flow</figcaption></figure><p id="352d" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">一旦所有的估计器被训练，基于它们在加权训练实例上的准确度，不同的权重将被分配给这些估计器，以进行总体的最终预测。</p><p id="3377" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">现在一步一步地用数学方法研究它的算法，</p><ol class=""><li id="ce18" class="kt ku hi jz b ka kb kd ke kg kv kk kw ko kx ks ky kz la lb bi translated">最初，每个实例被分配相等的权重w( <em class="lh"> i </em> )=( <em class="lh"> 1/m </em>)，其中<em class="lh"> m </em>是总实例数。</li><li id="c580" class="kt ku hi jz b ka lc kd ld kg le kk lf ko lg ks ky kz la lb bi translated">将使用AdaBoost中的决策树树桩创建第一个基础学习者。决策树桩基本上就是带有max的决策树。深度为1，由一个决策节点和两个叶节点组成。</li></ol><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lz"><img src="../Images/868808d7609e43158104db6e9f3ad29a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*VUzaV6gUHc7XqNsrrDfMuA.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">Decision stump with max_depth 1</figcaption></figure><p id="fedd" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">3.从众多决策树桩中选择最佳决策树桩将基于基尼系数或熵标准来完成。<a class="ae jw" rel="noopener" href="/geekculture/criterion-used-in-constructing-decision-tree-c89b7339600f"> <em class="lh">同样可以参考这篇文章。</em>T19】</a></p><p id="f00e" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">4.在训练第一基础学习者之后，将根据第一基础学习者的预测(<code class="du ma mb mc md b">y(observer) != y(predicted)</code>)为错误分类或不适合的类计算加权误差r( <em class="lh"> j </em>)。错误率的公式如下。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es me"><img src="../Images/a234fea3bea50f6c0305583eef073737.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m6yubsRF4VXmmdITUj8Zqg.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Source: Oreilly ‘s Hands-On machine learning with Scikit-learn, Keras &amp; Tensor flow</figcaption></figure><p id="14d7" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">5.为了观察决策树树桩的性能，预测值的权重将计算如下:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mf"><img src="../Images/0e1a8303a2a40cc5222b6500d92ed735.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*xeDk_qzwHa3NjU7D17LQdQ.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx">Source: Oreilly ‘s Hands-On machine learning with Scikit-learn, Keras &amp; Tensor flow</figcaption></figure><p id="cf5f" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">其中<em class="lh"> η </em>是学习率超参数(默认为1)。它在集成的最终预测中贡献弱学习者的权重。预测值越精确，分配给它的权重就越高。</p><p id="a9f7" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">6.在数据修改中向前移动，对于不正确预测的事件，权重将被更新和指数增加，而对于正确预测的实例，权重将减少。数学上，</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mg"><img src="../Images/3b93fd72fddc9ab939ca34279606fe1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l339fNwW1hZ67Oi5LHxuSg.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Source: Oreilly ‘s Hands-On machine learning with Scikit-learn, Keras &amp; Tensor flow</figcaption></figure><p id="1129" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">7.对于接下来的连续迭代，基础学习器将再次用重新加权的数据进行训练。该算法将这样工作，即它将主要挑选错误分类的实例用于训练，并且整个过程将被重复。当训练了期望数量的弱学习器或找到完美的预测器时，算法停止。</p><p id="1328" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">8.为了进行最终预测，AdaBoost将使用预测器权重α来计算和加权所有基本学习器的预测，最终预测的类将是获得大多数加权投票的类。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mh"><img src="../Images/17e128eb7432f351932948563074b169.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Awqn9ZFwISGd4PjTadwe4w.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Source: Oreilly ‘s Hands-On machine learning with Scikit-learn, Keras &amp; Tensor flow</figcaption></figure></div><div class="ab cl li lj gp lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="hb hc hd he hf"><p id="b601" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">这就把我们带到了Boosting技术中AdaBoost算法的结尾。它在每次迭代中的权重分配技术使它成为最好的算法之一。在下一篇文章中，我们将按照顺序探索梯度推进算法。</p><p id="d036" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">如果你发现什么错误，请评论。此外，DM是开放的。快乐学习。关注我，了解更多关于机器学习的文章和概念:</p><p id="2092" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">推特句柄:<a class="ae jw" href="https://twitter.com/sdeeksha07" rel="noopener ugc nofollow" target="_blank">https://twitter.com/sdeeksha07</a></p><p id="35f5" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">中等:<a class="ae jw" href="https://t.co/pWF2eKMqY6" rel="noopener ugc nofollow" target="_blank">sdeeksha07.medium.com</a></p><p id="d1b2" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">参考资料:</p><ol class=""><li id="90ec" class="kt ku hi jz b ka kb kd ke kg kv kk kw ko kx ks ky kz la lb bi translated"><a class="ae jw" href="https://scikit-learn.org/stable/modules/ensemble.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/ensemble.html</a></li><li id="f574" class="kt ku hi jz b ka lc kd ld kg le kk lf ko lg ks ky kz la lb bi translated">O'Reilly Media，Inc .使用Scikit-Learn、Keras和TensorFlow进行机器学习。</li><li id="3260" class="kt ku hi jz b ka lc kd ld kg le kk lf ko lg ks ky kz la lb bi translated">优酷网上的克里斯·纳伊克机器学习播放列表:<a class="ae jw" href="https://www.youtube.com/watch?v=NLRO1-jp5F8" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=NLRO1-jp5F8</a></li><li id="a7f4" class="kt ku hi jz b ka lc kd ld kg le kk lf ko lg ks ky kz la lb bi translated">机器学习大师Youtube:<a class="ae jw" href="https://www.youtube.com/watch?v=kzcetBfltx0" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=kzcetBfltx0</a></li><li id="b578" class="kt ku hi jz b ka lc kd ld kg le kk lf ko lg ks ky kz la lb bi translated">机器学习大师Youtube:<a class="ae jw" href="https://www.youtube.com/watch?v=peh2l4dePBc&amp;t=3s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=peh2l4dePBc&amp;t = 3s</a></li></ol></div></div>    
</body>
</html>
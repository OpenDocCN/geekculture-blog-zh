<html>
<head>
<title>Machine Learning-based Navigation System for the Visually Impaired</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于机器学习的视障导航系统</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/machine-learning-based-navigation-system-for-the-visually-impaired-26a8c3d0a3e7?source=collection_archive---------14-----------------------#2021-04-30">https://medium.com/geekculture/machine-learning-based-navigation-system-for-the-visually-impaired-26a8c3d0a3e7?source=collection_archive---------14-----------------------#2021-04-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/6571ac8bf9827443c4379cdc8ff18433.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5VwUpUe37ls7ov9HuE8i1w.png"/></div></div></figure><p id="dabd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在过去的几十年里，机器学习领域有了很大的发展。由于计算性能的巨大飞跃，即使是移动设备也可以以30fps的速度从摄像机直播流中执行对象检测。可能从中受益最大的人群之一实际上是盲人或视力受损者。视觉智能可以用来弥补他们失去的视力，并为他们识别周围的环境。</p><p id="dca9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这篇文章中，你将了解我们(作为一家铁路公司)如何使用机器学习和其他尖端技术来帮助视力障碍者的旅程。</p><h1 id="7f84" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">用例</h1><p id="a18a" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">2020年12月，我们发布了我们的应用程序“SBB全纳”(<a class="ae kr" href="https://apps.apple.com/ch/app/sbb-inclusive/id1495023290" rel="noopener ugc nofollow" target="_blank"> iOS </a>，<a class="ae kr" href="https://play.google.com/store/apps/details?id=ch.sbb.inclusive&amp;hl=en&amp;gl=US" rel="noopener ugc nofollow" target="_blank"> Android </a>)，专门针对视力不好的人。该应用程序可以检测到你坐在哪辆火车上，或者你在哪个火车站，并为你提供所有与你的情况相关的信息(显然该应用程序在可访问性方面进行了大量的<a class="ae kr" href="https://appbakery.medium.com/designing-mobile-apps-for-accessibility-fe4ef7ef4e7f" rel="noopener">优化</a>)。</p><p id="16aa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">视觉受损的人在使用公共交通工具时的一个痛点是找到进入火车或公共汽车的门(或门按钮，如果门是关闭的)。平台上有触觉引导线，这样他们就能感觉到平台的尽头在哪里。但是没有办法知道火车门在哪里。他们经常被迫用手来“扫描”火车上的车门按钮。虽然我们会说我们的火车很干净，但是它们仍然在外面运行。然而，安全问题要重要得多:想象一下，当一个盲人用手扫描火车时，如果火车突然启动会发生什么。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ks"><img src="../Images/5ac85e21003c0e52787ae6cce6fc021c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dUxSj2GoasuegV32dTVcDQ.jpeg"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx">A blind person using tactile lines for navigation.</figcaption></figure><p id="2f87" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于我们的团队在图像分类和物体检测方面非常有经验，我们决定利用我们的知识来帮助视觉障碍者。我们最初的想法是:</p><ul class=""><li id="9136" class="lb lc hi is b it iu ix iy jb ld jf le jj lf jn lg lh li lj bi translated">在火车站收集许多火车的照片，并给车门、车门按钮等贴上标签。然后训练可以识别这些对象的对象检测模型。</li><li id="3ec8" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated">使用用户的手机摄像头实时拍摄模型。</li><li id="a7d5" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated">使用VoiceOver和振动来引导用户找到门或门按钮(类似于雪崩信号灯的工作方式)。</li></ul><p id="331d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于大多数视障人士都是iOS用户，我们决定先从iOS的概念验证开始，如果我们的想法成功的话，我们将在以后跟进Android。</p><h1 id="ea07" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">收集图片和创建模型</h1><p id="323c" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">虽然Covid的情况真的让整个世界屏息以待，但我们实际上能够利用它。许多员工无法继续他们的日常工作，所以我们伸出援手，请他们支持我们，在火车站拍照，并在家里贴上标签。我们使用微软的CustomVision平台进行标记过程(感谢微软让我们在这个用例中免费使用他们的平台)。我们还使用CustomVision来训练一个模型，然后我们可以在我们的应用程序中使用它。CustomVision平台被证明是一个不错的选择，因为它非常容易用于我们所有的贴标机。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lp"><img src="../Images/8487531f7919cfbf267e83ee6c6d4cf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8_oGO-7nIId3tRwSo2GU7g.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx">Labelling experience with CustomVision.</figcaption></figure><h1 id="d4e1" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">实时应用内物体检测</h1><p id="fdd0" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">虽然CustomVision平台对于创建模型非常方便，但结果表明生成的CoreML模型并不是我们所期望的。CustomVision会生成一个CoreML v1模型(v3是当前的标准)，并强制您使用Microsoft CustomVisionMobile库。我们还注意到，微软的开发人员也是人:如果你配置错了，库就会崩溃，而不会给你任何出错的提示(致命错误)。几个小时后，我们仍然设法让它运行起来，并能够第一次在我们的应用程序中测试我们的模型。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/d5b8ca973280ed55f5407ef9fb354770.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*1WrQVSwT-NEvWN7oHi4plQ.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx">First results after integrating the model in the app.</figcaption></figure><p id="3551" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">虽然我们能够在现代iPhone上每秒处理30张图像，但我们注意到它消耗了大量电池。所以我们决定将物体检测速率降低到每秒1个周期左右。这样做的缺点是——正如你可能想象的那样——当移动相机时，边界框不能平滑地更新。</p><p id="894d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">幸运的是，我们找到了这个问题的解决方案，它被称为对象跟踪。对象跟踪是一种随着时间跟踪已经检测到的对象的帧的技术。在引擎盖下，对象跟踪也依赖于机器学习，但比对象检测消耗的能量少得多。</p><p id="ee88" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们还必须在这里寻找最佳设置(特别是对象检测率)，所以我们允许测试用户动态地更改它。目前，甜蜜点似乎以每秒1帧的速率存在。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/f570c448df3f1ff1cb17db2a91b0c4dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*zkopucqJUM2LEZckFmR38A.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx">Playing with the settings to find the optimal configuration.</figcaption></figure><h1 id="b325" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">额外收获:测量距离</h1><p id="1ca5" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">到目前为止，我们已经实现了第一个目标:知道车门按钮在哪里。然而我们仍然不知道，它有多远。为了测量它的距离，我们有两种不同的选择:</p><ol class=""><li id="a96a" class="lb lc hi is b it iu ix iy jb ld jf le jj lf jn lr lh li lj bi translated"><strong class="is hj">使用激光雷达传感器</strong> <br/> ✅非常精确<br/> ❌只有最新最贵的iPhone 12 Pro配备了它。它只测量传感器垂直前方单点的距离。</li><li id="a58e" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lr lh li lj bi translated"><strong class="is hj">使用深度数据</strong> <br/> ✅可以在所有新款iphones上运行(至少有两个摄像头)。你得到了一个整个相机内容的“地形”图。<br/> ❌不太精确(特别是对于非常远的点)。</li></ol><p id="6451" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因为在我们的例子中，知道一扇门是2.53米还是2.54米并不重要，所以我们决定用DepthData进行实验。如果你在问自己，如何使用两个不同的相机计算距离，秘密在于两个相机使用的不同镜头(角度)。如果你知道一个物体在两幅不同图像中的位置，你就可以计算出它的距离。我们在这里不会涉及太多细节，但是如果你感兴趣的话，<a class="ae kr" href="https://developer.apple.com/videos/play/wwdc2017/507/" rel="noopener ugc nofollow" target="_blank">这段视频</a>会给你所有的答案。</p><p id="c312" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，DepthData将为包含测量距离的每一帧返回一个二维矩阵。由于使用这些矩阵的文本表示进行测试不太方便，我们决定直接在我们的相机流中可视化测量的距离。距离更远的像素会更暗，而靠近相机的像素会有更浅的灰色调。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ls"><img src="../Images/2b3898bebb25bb2bc11f4b348787d864.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XcsbElD-9q5gRwj0SKjUQw.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx">Visualizing captured depth Data using graytones.</figcaption></figure><p id="d848" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你仔细观察上面的图像，你会注意到测量的距离并不是超精确的。例如，中心的参考点距离右上角的参考点不超过2米。此外，深度数据似乎与一些特殊的模式，如触觉线斗争。然而，如果你不需要最好的精度，它工作得很好。好消息是:物体越近，深度数据就越有用。老实说，在开发过程中，探索深度数据是我们个人的亮点之一。</p><p id="1931" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">到现在为止，我们手头已经有了所有相关的信息:我们知道门(按钮)在哪里，并且我们可以计算它的距离。尽管如此，仍然缺少一些相关的东西。我们仍然需要引导视障用户到门口。</p><h1 id="5c13" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">为盲人导航</h1><p id="1bb6" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">到目前为止，视障人士可以通过VoiceOver和/或使用大尺寸内容来使用SBB全纳应用程序。然而，我们觉得这还不足以导航它们。幸运的是，来自其他领域的现有产品提供了大量的灵感。行人导航的一个例子是雪崩信标，一旦你靠近，它们就会发出更强更快的蜂鸣声来为用户导航。最终，我们决定尝试一种同时使用VoiceOver和振动强度/速度的解决方案(在iOS世界中也称为CoreHaptics):</p><ul class=""><li id="e187" class="lb lc hi is b it iu ix iy jb ld jf le jj lf jn lg lh li lj bi translated">检测到的对象按优先顺序排列:开门&gt;门按钮&gt;关门。</li><li id="36a3" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated">初始检测时，VoiceOver会读出优先级最高的对象。</li><li id="b799" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated">到物体的距离在VoiceOver中以3种不同类别读出(大于2米、大于0.5米、小于0.5米)。</li><li id="fed9" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated">物体的方向(左/右，理论上也是上/下)是通过振动模式“感觉”的。</li></ul><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/8175abd93d78641e74d82981da65c8b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ScezWN-5DhTatbV9tOlQlw.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx">Navigation of blind persons using a combination of VoiceOver and vibration patterns.</figcaption></figure><h1 id="1e9c" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">第一印象</h1><p id="f59a" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">提出的解决方案会结束视力障碍者在试图登上火车或公共汽车时的痛苦吗？我们还不能给一个明确的答复。目前，我们的测试用户正在为我们测试这个新功能，并开始给出反馈。我们真的很想听听他们要说些什么。</p><p id="92a1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一方面，我们非常有信心，因为探测物体和测量距离的技术部分似乎工作得很好，所以将它们导航到门口是可能的。另一方面，我们不太确定，如果我们的导航模式原封不动地投入生产。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/0cf9ba33d5ec47f9fd8a90400ddf3e69.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/1*ezlc2amFAHyzkjzQp3WyrA.gif"/></div><figcaption class="kx ky et er es kz la bd b be z dx">Coach door / coach button detector demo.</figcaption></figure><p id="b552" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">除了导航模式之外，在这个新特性发布之前，还有一些东西需要改进:</p><ul class=""><li id="0e42" class="lb lc hi is b it iu ix iy jb ld jf le jj lf jn lg lh li lj bi translated">门按钮识别还不能从远处工作(我们希望训练我们自己的基于YOLO的模型将改善这一点。<a class="ae kr" href="https://devpost.com/software/blinddetector" rel="noopener ugc nofollow" target="_blank">EPFL洛桑学生在<a class="ae kr" href="https://lauzhack.com/challenges/sbb.html" rel="noopener ugc nofollow" target="_blank">我们在劳扎克的挑战</a>期间获得的第一批结果</a>似乎支持了这一论点。</li><li id="2be5" class="lb lc hi is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated">对于某些火车类型，可以改进车门识别(这真的很简单:我们需要这些火车类型的更多图像)。</li></ul></div></div>    
</body>
</html>
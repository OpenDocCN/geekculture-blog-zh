<html>
<head>
<title>Neural Machine Translation Using Sequence to Sequence Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用序列到序列模型的神经机器翻译</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/neural-machine-translation-using-sequence-to-sequence-model-164a5905bcd7?source=collection_archive---------16-----------------------#2021-06-07">https://medium.com/geekculture/neural-machine-translation-using-sequence-to-sequence-model-164a5905bcd7?source=collection_archive---------16-----------------------#2021-06-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="971e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用编码器解码器LSTM模型的单词级英语到马拉地语翻译。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/530d9de143825dabf24e805476ef65f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*R4Qho2dWtzUYYAGAWsAi6Q.gif"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">1) Encoder-Decoder (Animation Source: Author)</figcaption></figure></div><div class="ab cl jn jo gp jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="hb hc hd he hf"><h2 id="d91d" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">简介:-</h2><p id="fa9e" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi ll translated">由于人类语言的流动性，机器翻译是计算机最早也是最具挑战性的任务之一。它只是文本从一种语言到另一种语言的自动翻译。</p><p id="b75e" class="pw-post-body-paragraph ks kt hi ku b kv lu ij kx ky lv im la kf lw lc ld kj lx lf lg kn ly li lj lk hb bi translated">在这篇文章中，我们将讨论一点编码器-解码器。然后我们将演练神经机器翻译的代码。这将是一次有趣的旅程。</p><h2 id="1bbf" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">先决条件:-</h2><p id="7263" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">在我们继续之前，你应该知道如何跟随-</p><ul class=""><li id="e570" class="lz ma hi ku b kv lu ky lv kf mb kj mc kn md lk me mf mg mh bi translated">递归神经网络(RNN)，长短期记忆(<a class="ae mi" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"> LSTM </a>)。</li><li id="0839" class="lz ma hi ku b kv mj ky mk kf ml kj mm kn mn lk me mf mg mh bi translated">序列到序列架构<a class="ae mi" href="https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf" rel="noopener ugc nofollow" target="_blank">(编码器解码器)</a>。</li></ul><h2 id="b474" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">简而言之，编码器解码器</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/1892dc285c14c186e1382e90279b9a2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*R-Ul_DUk74cj79bPr5UalQ.gif"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">2) Working of encoder-decoder (Animation source: Author) (<strong class="bd jw"><em class="mo">Note:</em></strong><em class="mo">- </em><strong class="bd jw"><em class="mo">SOS</em></strong><em class="mo"> →Start Of String, </em><strong class="bd jw"><em class="mo">EOS</em></strong><em class="mo"> →End Of String)</em></figcaption></figure><ul class=""><li id="9b46" class="lz ma hi ku b kv lu ky lv kf mb kj mc kn md lk me mf mg mh bi translated">顺序到顺序模型的主要部分是编码器和解码器。</li></ul><blockquote class="mp mq mr"><p id="f337" class="ks kt ms ku b kv lu ij kx ky lv im la mt lw lc ld mu lx lf lg mv ly li lj lk hb bi translated"><strong class="ku hj"> <em class="hi">编码器:- </em> </strong></p><p id="711b" class="ks kt ms ku b kv lu ij kx ky lv im la mt lw lc ld mu lx lf lg mv ly li lj lk hb bi translated">编码器可以是任何网络，如递归神经网络、LSTM、GRU或卷积神经网络，但我们使用seq2seq模型进行语言翻译，因此编码器和解码器都应该是可以处理顺序输入的模型。我们将使用LSTM。</p><p id="cedd" class="ks kt ms ku b kv lu ij kx ky lv im la mt lw lc ld mu lx lf lg mv ly li lj lk hb bi translated">编码器在输入中学习模式，所以我们只需要学习它的隐藏状态[h，c](上面动画中的棕色矩形)，并将其作为解码器的初始状态。我们不从每个时间步中获取输出。</p><p id="b0bd" class="ks kt ms ku b kv lu ij kx ky lv im la mt lw lc ld mu lx lf lg mv ly li lj lk hb bi translated"><strong class="ku hj"> <em class="hi">解码器:- </em> </strong></p><p id="7c4f" class="ks kt ms ku b kv lu ij kx ky lv im la mt lw lc ld mu lx lf lg mv ly li lj lk hb bi translated">解码器将编码器的状态作为其初始状态，一次预测一个字。这里需要理解的重要部分是，与编码器不同，解码器在训练和测试期间的工作方式不同。</p><p id="5a08" class="ks kt ms ku b kv lu ij kx ky lv im la mt lw lc ld mu lx lf lg mv ly li lj lk hb bi translated"><em class="hi">在培训期间</em> :-我们使用一种叫做教师强制的技术，这有助于更快更有效的培训。现在什么是老师逼？</p><p id="197d" class="ks kt ms ku b kv lu ij kx ky lv im la mt lw lc ld mu lx lf lg mv ly li lj lk hb bi translated"><a class="ae mi" href="https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/" rel="noopener ugc nofollow" target="_blank"> <em class="hi">老师强行</em> </a> <em class="hi"> :-是一种训练神经网络的策略，用实际输出(地面真实)作为解码器每个时间步的输入，而不是用前一个时间步的输出作为输入。</em></p><p id="5bd6" class="ks kt ms ku b kv lu ij kx ky lv im la mt lw lc ld mu lx lf lg mv ly li lj lk hb bi translated">在解码器端仔细观察动画(2 ),想象教师强迫观看。在每个时间步，我们都将实际输出作为输入，它们(预测的和实际的)在动画中看起来是一样的，因为我们的模型非常好<em class="hi">😆如果模型仍然预测了错误的单词，我们将传递正确的单词作为输入。</em></p><p id="32ee" class="ks kt ms ku b kv lu ij kx ky lv im la mt lw lc ld mu lx lf lg mv ly li lj lk hb bi translated">在测试期间:-每个时间步长的输入是解码器的前一个时间步长的预测输出。</p><p id="cd94" class="ks kt ms ku b kv lu ij kx ky lv im la mt lw lc ld mu lx lf lg mv ly li lj lk hb bi translated">解码器需要一些特殊的记号来知道句子是开始还是结束。因此，我们必须在目标语言(即我们的例子中的马拉地语)的每个句子的开头和结尾添加SOS(字符串开头)和EOS(字符串结尾)标记。因此解码器可以比编码器处理不同长度句子。</p></blockquote><h2 id="224a" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">数据集:-</h2><p id="ef7c" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">我们有41028个英语句子和它们各自的马拉地语翻译。我从manythings.org得到了这个数据。您可以获得翻译更多语言的数据集。</p><h2 id="a53c" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">预处理:-</h2><p id="a871" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">首先，我们需要了解翻译需要什么。</p><ul class=""><li id="2118" class="lz ma hi ku b kv lu ky lv kf mb kj mc kn md lk me mf mg mh bi translated">我们需要一种语言的句子和另一种语言的句子。</li><li id="43d5" class="lz ma hi ku b kv mj ky mk kf ml kj mm kn mn lk me mf mg mh bi translated">它们可能是一些英文单词的缩写，这可能会使我们的模型混淆。它会以不同的方式看到像“不能”和“不能”这样的单词，因此我们将扩展所有的缩写。</li><li id="68b5" class="lz ma hi ku b kv mj ky mk kf ml kj mm kn mn lk me mf mg mh bi translated">像coma和dot这样的字符在翻译中没有用，所以我们将把它们都去掉。</li><li id="459a" class="lz ma hi ku b kv mj ky mk kf ml kj mm kn mn lk me mf mg mh bi translated">也不需要数字。</li></ul><p id="6655" class="pw-post-body-paragraph ks kt hi ku b kv lu ij kx ky lv im la kf lw lc ld kj lx lf lg kn ly li lj lk hb bi translated">现在，在下面的代码中，我们将完成所有的清理过程并保存数据。</p><p id="6342" class="pw-post-body-paragraph ks kt hi ku b kv lu ij kx ky lv im la kf lw lc ld kj lx lf lg kn ly li lj lk hb bi translated"><em class="ms">要运行收缩功能，你需要收缩-扩展字典，你可以从</em> <a class="ae mi" href="https://github.com/AdiShirsath/Neural-Machine-Translation/tree/master/Data" rel="noopener ugc nofollow" target="_blank"> <em class="ms">这里</em> </a> <em class="ms">下载这个文件包含125个以上的收缩。</em></p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mw mx l"/></div></figure><h2 id="b04f" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">为编码器解码器准备数据:-</h2><p id="101e" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">下面是我们如何为编码器/解码器模型准备数据</p><ul class=""><li id="d4c9" class="lz ma hi ku b kv lu ky lv kf mb kj mc kn md lk me mf mg mh bi translated">在马拉地语句子中添加SOS和EOS tokes。</li><li id="a6f0" class="lz ma hi ku b kv mj ky mk kf ml kj mm kn mn lk me mf mg mh bi translated">获取英语和马拉地语的所有独特的单词，并创建各自的词汇和排序。</li><li id="780a" class="lz ma hi ku b kv mj ky mk kf ml kj mm kn mn lk me mf mg mh bi translated">使用我们的排序单词表，我们可以给每个单词编号，并形成字典，这是非常有助于将单词转换成数字。我们必须将单词转换成数字，因为神经网络不接受文本作为输入。</li><li id="1eca" class="lz ma hi ku b kv mj ky mk kf ml kj mm kn mn lk me mf mg mh bi translated">最后将数据分为训练和测试。</li></ul><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mw mx l"/></div></figure><h2 id="519f" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">数据生成器:-</h2><p id="f7fc" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">这很重要，为什么我们需要数据生成器？</p><p id="8450" class="pw-post-body-paragraph ks kt hi ku b kv lu ij kx ky lv im la kf lw lc ld kj lx lf lg kn ly li lj lk hb bi translated">正如keras <a class="ae mi" href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" rel="noopener ugc nofollow" target="_blank">教程</a>中给出的，我们必须将数据转换成形状的3D张量=[批量大小，时间步长，特征]。现在，在我们的例子中，批量大小、时间步长和特征分别是句子的数量、最大长度句子和唯一单词的数量。</p><p id="b693" class="pw-post-body-paragraph ks kt hi ku b kv lu ij kx ky lv im la kf lw lc ld kj lx lf lg kn ly li lj lk hb bi translated">使用这个，对于马拉地语输入张量数组变成[41028，37，13720]这将消耗大量的内存。</p><p id="bde1" class="pw-post-body-paragraph ks kt hi ku b kv lu ij kx ky lv im la kf lw lc ld kj lx lf lg kn ly li lj lk hb bi translated">为了提高工作效率，我们将以批处理的形式发送数据。所以我们将创建数据批处理生成器。该数据生成器由<a class="ae mi" href="https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly" rel="noopener ugc nofollow" target="_blank"> keras团队</a>开发。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mw mx l"/></div></figure><h2 id="73c3" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">构建编码器-解码器LSTM :-</h2><p id="99fa" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">在我们进入编码部分之前，我们必须了解一些事情</p><blockquote class="mp mq mr"><p id="46e9" class="ks kt ms ku b kv lu ij kx ky lv im la mt lw lc ld mu lx lf lg mv ly li lj lk hb bi translated"><strong class="ku hj"> <em class="hi">嵌入:- </em> </strong></p><p id="ce63" class="ks kt ms ku b kv lu ij kx ky lv im la mt lw lc ld mu lx lf lg mv ly li lj lk hb bi translated">我们将首先在嵌入层传递编码器和解码器的输入，然后在LSTM层传递。</p><p id="cd68" class="ks kt ms ku b kv lu ij kx ky lv im la mt lw lc ld mu lx lf lg mv ly li lj lk hb bi translated">这一层把数字作为输入，并把它们转换成给定的维数，但是为什么我们需要这样做呢？→答案是，利用这一点，我们可以保留单词的语义信息，这意味着相似的单词会彼此更接近。</p><p id="7474" class="ks kt ms ku b kv lu ij kx ky lv im la mt lw lc ld mu lx lf lg mv ly li lj lk hb bi translated">例如:单词“man”将更接近“women”，“dog”将更接近“cat”。这仅仅意味着男性词汇的向量在女性中的数量与狗和猫相似。</p><p id="e42a" class="ks kt ms ku b kv lu ij kx ky lv im la mt lw lc ld mu lx lf lg mv ly li lj lk hb bi translated">要了解更多细节<a class="ae mi" href="https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/" rel="noopener ugc nofollow" target="_blank">，请查看</a>。</p></blockquote><p id="d941" class="pw-post-body-paragraph ks kt hi ku b kv lu ij kx ky lv im la kf lw lc ld kj lx lf lg kn ly li lj lk hb bi translated">我实验了一些不同的嵌入和LSTM单位的值，对我来说，跟随是最好的，但你可以尝试不同的，机器学习就是要实验。</p><p id="e8b4" class="pw-post-body-paragraph ks kt hi ku b kv lu ij kx ky lv im la kf lw lc ld kj lx lf lg kn ly li lj lk hb bi translated"><strong class="ku hj">编码器:- </strong></p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mw mx l"/></div></figure><ul class=""><li id="a17a" class="lz ma hi ku b kv lu ky lv kf mb kj mc kn md lk me mf mg mh bi translated">在编码器中，传递编码器输入数据，并将编码器最后一个时间步长的隐藏状态作为上下文向量[h，c]。</li><li id="4737" class="lz ma hi ku b kv mj ky mk kf ml kj mm kn mn lk me mf mg mh bi translated">为什么在嵌入中设置掩码zero = True当我们在生成器中生成输入数组时，我们用零填充它们，使它们具有最大长度。这个屏蔽零将告诉模型屏蔽掉0。</li></ul><p id="1e3e" class="pw-post-body-paragraph ks kt hi ku b kv lu ij kx ky lv im la kf lw lc ld kj lx lf lg kn ly li lj lk hb bi translated"><strong class="ku hj">解码器:- </strong></p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mw mx l"/></div></figure><ul class=""><li id="3a27" class="lz ma hi ku b kv lu ky lv kf mb kj mc kn md lk me mf mg mh bi translated">现在，解码器输入数据将被传递到解码器嵌入中。</li><li id="07d4" class="lz ma hi ku b kv mj ky mk kf ml kj mm kn mn lk me mf mg mh bi translated">LSTM层的初始状态是编码器的最终状态。</li><li id="c5e7" class="lz ma hi ku b kv mj ky mk kf ml kj mm kn mn lk me mf mg mh bi translated"><a class="ae mi" href="https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/" rel="noopener ugc nofollow" target="_blank">老师强制</a> :-这里每一个时间步的输入都是解码器前一步的实际输出。</li><li id="cbde" class="lz ma hi ku b kv mj ky mk kf ml kj mm kn mn lk me mf mg mh bi translated">通过应用SoftMax获得输出，soft max将的数字转换为概率。</li></ul><h2 id="d9b3" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">培训:-</h2><p id="fc34" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">用30个时代的一些回调来训练我们的模型。不要忘记保存模型的重量。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mw mx l"/></div></figure><h2 id="0dbe" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">推理模型:-</h2><p id="6103" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">我们使用该模型通过使用预训练模型的权重来预测输出序列。</p><p id="bcc1" class="pw-post-body-paragraph ks kt hi ku b kv lu ij kx ky lv im la kf lw lc ld kj lx lf lg kn ly li lj lk hb bi translated">这里，我们不能像其他ML和DL模型一样应用<em class="ms"> model.predict() </em>，因为在我们的情况下，编码器模型学习输入句子中的特征，而解码器只是获取编码器状态，并使用解码器输入来逐字预测。因此，对于预测，我们必须做同样的过程。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mw mx l"/></div></figure><h2 id="28d7" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">预测:-</h2><p id="102d" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">最后要讨论的一件事，模型将预测数字和一个单词的向量，所以我们必须创建函数来从预测的数字创建句子。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mw mx l"/></div></figure><h2 id="5d2f" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">结果:-</h2><p id="068c" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">现在我们可以通过简单的python代码得到结果:-</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mw mx l"/></div></figure><p id="6f9e" class="pw-post-body-paragraph ks kt hi ku b kv lu ij kx ky lv im la kf lw lc ld kj lx lf lg kn ly li lj lk hb bi translated">以下是我的一些结果</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es my"><img src="../Images/3d7037ace38b76b07208fe5bfcf37156.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tE3FhiEL9CEOPZw8AFdB1A.jpeg"/></div></div></figure><p id="0b70" class="pw-post-body-paragraph ks kt hi ku b kv lu ij kx ky lv im la kf lw lc ld kj lx lf lg kn ly li lj lk hb bi translated">万岁！！我们得到了一些惊人的结果。</p><h2 id="3e3e" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">结束注释:-</h2><p id="fb38" class="pw-post-body-paragraph ks kt hi ku b kv kw ij kx ky kz im la kf lb lc ld kj le lf lg kn lh li lj lk hb bi translated">现在，编码器-解码器LSTM的准确性随着句子长度的增加而降低(参见我的结果中的最后一个预测)，因为这里我们仅使用来自编码器的最后一个LSTM单元的状态(上下文向量)。这就像记住整本书并翻译，所以很明显准确性会降低。</p><p id="8eed" class="pw-post-body-paragraph ks kt hi ku b kv lu ij kx ky lv im la kf lw lc ld kj lx lf lg kn ly li lj lk hb bi translated">因此，在下一篇文章中，我们将尝试使用注意力模型。</p><h2 id="395f" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">参考资料:- </strong></h2><div class="mz na ez fb nb nc"><a href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" rel="noopener  ugc nofollow" target="_blank"><div class="nd ab dw"><div class="ne ab nf cl cj ng"><h2 class="bd hj fi z dy nh ea eb ni ed ef hh bi translated">Keras中序列对序列学习的十分钟介绍</h2><div class="nj l"><h3 class="bd b fi z dy nh ea eb ni ed ef dx translated">我经常看到这个问题——如何在Keras中实现RNN序列对序列学习？这里有一个简短的介绍…</h3></div><div class="nk l"><p class="bd b fp z dy nh ea eb ni ed ef dx translated">blog.keras.io</p></div></div></div></a></div><div class="mz na ez fb nb nc"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/machine-translation-encoder-decoder-model-7e4867377161"><div class="nd ab dw"><div class="ne ab nf cl cj ng"><h2 class="bd hj fi z dy nh ea eb ni ed ef hh bi translated">机器翻译(编码器-解码器模型)！！</h2><div class="nj l"><h3 class="bd b fi z dy nh ea eb ni ed ef dx translated">理解和建立一个简单的模型，将英语翻译成印地语的指南。</h3></div><div class="nk l"><p class="bd b fp z dy nh ea eb ni ed ef dx translated">medium.com</p></div></div><div class="nl l"><div class="nm l nn no np nl nq jh nc"/></div></div></a></div><div class="mz na ez fb nb nc"><a href="https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/" rel="noopener  ugc nofollow" target="_blank"><div class="nd ab dw"><div class="ne ab nf cl cj ng"><h2 class="bd hj fi z dy nh ea eb ni ed ef hh bi translated">如何用Keras使用单词嵌入层进行深度学习——机器学习掌握</h2><div class="nj l"><h3 class="bd b fi z dy nh ea eb ni ed ef dx translated">单词嵌入提供了单词及其相关含义的密集表示。它们比…有所改进</h3></div><div class="nk l"><p class="bd b fp z dy nh ea eb ni ed ef dx translated">machinelearningmastery.com</p></div></div><div class="nl l"><div class="nr l nn no np nl nq jh nc"/></div></div></a></div></div></div>    
</body>
</html>
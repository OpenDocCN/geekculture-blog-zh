<html>
<head>
<title>Understanding Basic architecture of LSTM, GRU diagrammatically</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图解式理解GRU LSTM的基本建筑</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/understanding-basic-architecture-of-lstm-gru-diagrammatically-6365befc64d?source=collection_archive---------14-----------------------#2021-09-19">https://medium.com/geekculture/understanding-basic-architecture-of-lstm-gru-diagrammatically-6365befc64d?source=collection_archive---------14-----------------------#2021-09-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="802e" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">NLP: LSTM和GRU</h2><div class=""/><div class=""><h2 id="8fb1" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">初学者的LSTM和GRU</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/e14928519ad0503a26fcbd72b89c8961.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ie0SsisJfchHrQgJ"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Photo by <a class="ae jw" href="https://unsplash.com/@dslr_newb?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Anita Jankovic</a> on <a class="ae jw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e4e2" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">深度学习有不同的模块实现不同的功能。深度学习的专业知识包括设计架构来完成特定的任务。它将复杂的功能简化为功能模块图(可能是动态的)，这些模块的功能是通过学习最终确定的。递归神经网络(RNN)是一种我们可以用来处理数据序列的架构。我们了解到，信号可以是1D信号、2D信号或3D信号，具体取决于信号的域。该域由映射源和映射目标来定义。除了数据处理之外，从这种数据序列中提取上下文信息对于实现所需功能至关重要。这种类型的RNN的希望是，他们可能能够将以前的信息与当前的任务联系起来。但不幸的是，随着序列的增长，RNN变得无法学会连接信息，这被称为RNN的“长期依赖”问题。</p><p id="7697" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">“长短期记忆网络”(LSTM) </strong>是一种特殊的RNN，能够学习长期依赖关系。它是由<a class="ae jw" href="http://www.bioinf.jku.at/publications/older/2604.pdf" rel="noopener ugc nofollow" target="_blank">hoch Reiter&amp;schmid Huber(1997)</a>提出的，它对各种各样的问题都非常有效。长时间记住信息实际上是它的默认行为。LSTMs也具有类似rnn的链式结构，但是中继模块具有改进的架构。不是只有一层，而是有四层以一种非常特殊的方式相互作用。<a class="ae jw" href="https://arxiv.org/pdf/1412.3555.pdf" rel="noopener ugc nofollow" target="_blank"> Junyoung Chung、卡格拉尔·古尔切雷、KyungHyun Cho、Yoshua Bengio (2014) </a>提出了更简单方法的类似功能，即<strong class="jz hs">“门控循环单元”(GRU) </strong>。</p><p id="18c2" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">对这种特殊建筑的研究总是涉及到对向量及其形状的数学运算的深入理解。对于初学者来说，有时会令人望而生畏，难以理解。在这篇文章中，作者试图展示这两个架构，并敦促所有读者使用作为现成的参考。</p><h1 id="d6df" class="kt ku hi bd kv kw kx ky kz la lb lc ld ix le iy lf ja lg jb lh jd li je lj lk bi translated">先决条件:</h1><p id="e71a" class="pw-post-body-paragraph jx jy hi jz b ka ll is kc kd lm iv kf kg ln ki kj kk lo km kn ko lp kq kr ks hb bi translated">以下基本定义是LSTM和GRU的先决条件，</p><ol class=""><li id="bd23" class="lq lr hi jz b ka kb kd ke kg ls kk lt ko lu ks lv lw lx ly bi translated"><strong class="jz hs">密集层</strong> —涉及向量与相关“权重”和“偏差”线性组合。这在数学上用矩阵乘法来表示，并且输入和输出的维数是兼容的。</li><li id="36fd" class="lq lr hi jz b ka lz kd ma kg mb kk mc ko md ks lv lw lx ly bi translated"><strong class="jz hs">激活功能</strong> —将非线性引入输出，并相应地激活学习过程。</li><li id="c116" class="lq lr hi jz b ka lz kd ma kg mb kk mc ko md ks lv lw lx ly bi translated"><strong class="jz hs">点态操作</strong> —涉及元素态操作；因此，所有操作数必须具有完全相同的形状，因此输出也包含相同的维度。</li><li id="ebe0" class="lq lr hi jz b ka lz kd ma kg mb kk mc ko md ks lv lw lx ly bi translated"><strong class="jz hs">电池状态</strong> —单个电池/单元的输出称为电池状态；有时它只有一个输出(GRU)，有时有两个输出(LSTM有长期和短期记忆)。通常表示为c[t]或h[t]。</li><li id="e153" class="lq lr hi jz b ka lz kd ma kg mb kk mc ko md ks lv lw lx ly bi translated"><strong class="jz hs">输入向量</strong> —将要馈入一个单元/小区的序列数据。通常表示为x[t]。</li><li id="0e1b" class="lq lr hi jz b ka lz kd ma kg mb kk mc ko md ks lv lw lx ly bi translated"><strong class="jz hs">门模块</strong> —不同密集层的输出通过一些逐点操作进行组合，以完成某些任务(如记住长期信息、更新短期信息等)。).LSTM或GRU的单个细胞/单位包括很少的这种特定的门块。</li></ol><h1 id="a923" class="kt ku hi bd kv kw kx ky kz la lb lc ld ix le iy lf ja lg jb lh jd li je lj lk bi translated">演示的注意事项:</h1><p id="2a8f" class="pw-post-body-paragraph jx jy hi jz b ka ll is kc kd lm iv kf kg ln ki kj kk lo km kn ko lp kq kr ks hb bi translated">该演示将基于以下考虑因素:</p><p id="6883" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">a)为简单起见，我们考虑形状为<strong class="jz hs"> 3X1 </strong>的输入向量</p><p id="2083" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">b)细胞状态维度，即细胞/单位中神经元的数量，通常是超参数。对于这里的演示，它被认为是<strong class="jz hs"> 2。</strong></p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es me"><img src="../Images/895b6814ceb864150908bb147d8c8917.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rpJomQSzfpYo5G6d"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Photo by <a class="ae jw" href="https://unsplash.com/@dmey503?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Dan Meyers</a> on <a class="ae jw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="063e" class="kt ku hi bd kv kw kx ky kz la lb lc ld ix le iy lf ja lg jb lh jd li je lj lk bi translated">门控循环单元(GRU):</h1><p id="c222" class="pw-post-body-paragraph jx jy hi jz b ka ll is kc kd lm iv kf kg ln ki kj kk lo km kn ko lp kq kr ks hb bi translated">GRU细胞/单位的典型结构连同数学方程如下所示，</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mf"><img src="../Images/fbdaac8ddddaebe8959754c73a68698a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AFJ3hTpKZtJRMD4T9iP25g.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Source: <a class="ae jw" href="https://atcold.github.io/pytorch-Deep-Learning/images/week06/06-2/GRU.png" rel="noopener ugc nofollow" target="_blank">atcold</a></figcaption></figure><p id="940f" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">复位门</strong> r[t]用于决定忘记多少过去的信息，z[t]是<strong class="jz hs">更新门向量</strong>,它决定应该将多少过去的信息传递给未来。</p><p id="b489" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">上述表达式的<strong class="jz hs">示意图</strong>如下:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mg"><img src="../Images/469483d5fabd875e553344e905b295b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3pqOEf7AsZJNm1bAVSCqLQ.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Source : Author</figcaption></figure><p id="4dd0" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><strong class="jz hs">逐点操作</strong>(点划线)涉及元素到元素的操作。因此，单元输出h[t]的矢量形状与前一单元h[t-1]的矢量形状<strong class="jz hs">相同</strong>。但是在<strong class="jz hs">密集层</strong>(连续细线)的情况下，h[t-1]和输入x[t]的线性组合通过<strong class="jz hs">六(6) </strong>权重矩阵<strong class="jz hs">(x[t]的[Wi]和h[t-1]的[Ui]各3个)</strong>和<strong class="jz hs">每单位/单元三(3) </strong>偏置矩阵来表征。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mh"><img src="../Images/c043945524db96f7ee3f2fc426db9e01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TISVoKIpRiXD7xKN"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Photo by <a class="ae jw" href="https://unsplash.com/@mattmoloney?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Matt Moloney</a> on <a class="ae jw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="2ef7" class="kt ku hi bd kv kw kx ky kz la lb lc ld ix le iy lf ja lg jb lh jd li je lj lk bi translated">长期短期记忆网络(LSTM):</h1><p id="9336" class="pw-post-body-paragraph jx jy hi jz b ka ll is kc kd lm iv kf kg ln ki kj kk lo km kn ko lp kq kr ks hb bi translated">LSTM细胞/单位的典型结构连同数学方程如下所示，</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mi"><img src="../Images/2a9ade2299a51ad28b9f62bdda708e1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VlJPhj3-KLsAm31LzwUtOA.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Source: <a class="ae jw" href="https://atcold.github.io/pytorch-Deep-Learning/images/week06/06-2/LSTM.png" rel="noopener ugc nofollow" target="_blank">atcold</a></figcaption></figure><p id="218e" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">lstm处理长期记忆(LTM / c[t-1])和短期记忆(STM / h[t-1])，为了使计算简单有效，它使用了门的概念。</p><ol class=""><li id="7a10" class="lq lr hi jz b ka kb kd ke kg ls kk lt ko lu ks lv lw lx ly bi translated"><strong class="jz hs">遗忘之门:</strong> LTM去了遗忘之门，它忘记了无用的信息。</li><li id="5ae5" class="lq lr hi jz b ka lz kd ma kg mb kk mc ko md ks lv lw lx ly bi translated"><strong class="jz hs">Learn Gate:</strong>Event(sequence input/x[t])和STM组合在一起，以便我们最近从STM学到的必要信息可以应用到当前输入。</li><li id="cc91" class="lq lr hi jz b ka lz kd ma kg mb kk mc ko md ks lv lw lx ly bi translated"><strong class="jz hs">记忆之门:</strong>我们没有忘记的LTM信息和STM、Event结合在一起，成为更新的LTM。</li><li id="55c0" class="lq lr hi jz b ka lz kd ma kg mb kk mc ko md ks lv lw lx ly bi translated"><strong class="jz hs">使用Gate: </strong>这个Gate也使用LTM、STM和Event来预测当前事件的输出，作为更新的STM。</li></ol><p id="7c07" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">上述表达式的<strong class="jz hs">图示</strong>如下:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mj"><img src="../Images/7947c0247e82b1a876918338c16d02ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O46ThcTUY9I9xSqdcWNG8g.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Source : Author</figcaption></figure><p id="9e07" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">就长期依赖性而言，与GRU或简单的RNN相比，这些架构被认为是最高效的网络。这些通过每个单元/小区的<strong class="jz hs">八(8) </strong>权重矩阵<strong class="jz hs">(x[t]的【Wi】和h[t-1]的【Ui】各4个)</strong>和<strong class="jz hs">四(4) </strong>偏置矩阵来表征。</p><h1 id="7605" class="kt ku hi bd kv kw kx ky kz la lb lc ld ix le iy lf ja lg jb lh jd li je lj lk bi translated">主要途径:</h1><p id="ea46" class="pw-post-body-paragraph jx jy hi jz b ka ll is kc kd lm iv kf kg ln ki kj kk lo km kn ko lp kq kr ks hb bi translated">图形表示简化了用于架构的数学符号和方程。可供参考主要途径有，</p><ol class=""><li id="2eaa" class="lq lr hi jz b ka kb kd ke kg ls kk lt ko lu ks lv lw lx ly bi translated">任何细胞/单位的神经元数目(n)( <strong class="jz hs">此处为2 </strong>)都是一个超参数。</li><li id="2cd2" class="lq lr hi jz b ka lz kd ma kg mb kk mc ko md ks lv lw lx ly bi translated">向量c[t]，h[t-1]和h[t]的形状等于神经元数目(n)。</li><li id="1741" class="lq lr hi jz b ka lz kd ma kg mb kk mc ko md ks lv lw lx ly bi translated">输入向量维数(i) ( <strong class="jz hs">这里是3 </strong>)独立于c[t]或h[t-1]的维数。</li><li id="9944" class="lq lr hi jz b ka lz kd ma kg mb kk mc ko md ks lv lw lx ly bi translated">每个单位/单元的可学习参数矩阵在上面的相应章节中定义。</li><li id="7f74" class="lq lr hi jz b ka lz kd ma kg mb kk mc ko md ks lv lw lx ly bi translated">每个<strong class="jz hs">【权重】</strong>矩阵包含{输入向量形状(i) X神经元数量(n)}个可学习参数。</li><li id="ad72" class="lq lr hi jz b ka lz kd ma kg mb kk mc ko md ks lv lw lx ly bi translated">每个<strong class="jz hs">“偏差”</strong>矩阵包含{Number of neuron (n)}个可学习参数。</li><li id="7e8a" class="lq lr hi jz b ka lz kd ma kg mb kk mc ko md ks lv lw lx ly bi translated"><strong class="jz hs">点态操作</strong>不涉及任何可学习的参数。</li></ol></div><div class="ab cl mk ml gp mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="hb hc hd he hf"><p id="ce11" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><em class="mr">如果你觉得这篇文章有用，请点赞并分享，也欢迎发表评论。也可以在</em><a class="ae jw" href="http://www.linkedin.com/in/pathakchiranjit" rel="noopener ugc nofollow" target="_blank"><em class="mr">LinkedIn</em></a><em class="mr">上找我。有兴趣了解工程领域的数据分析、数据科学和机器学习应用的更多信息吗？通过访问我的</em> <a class="ae jw" rel="noopener" href="/@pathakc"> <em class="mr">中简介</em> </a> <em class="mr">来探索我以前的文章。感谢阅读。</em></p><p id="e7b5" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><em class="mr"> - Chiranjit </em></p></div></div>    
</body>
</html>
<html>
<head>
<title>ML Series7: Bernoulli Naive Bayes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML系列7:伯努利朴素贝叶斯</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/ml-series7-naive-bayes-e740731a658a?source=collection_archive---------37-----------------------#2021-08-18">https://medium.com/geekculture/ml-series7-naive-bayes-e740731a658a?source=collection_archive---------37-----------------------#2021-08-18</a></blockquote><div><div class="dt gx gy gz ha hb"/><div class="hc hd he hf hg"><div class=""/><p id="5eea" class="pw-post-body-paragraph ig ih hj ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hc bi translated">ML的概率方法&amp;朴素贝叶斯不是贝叶斯</p><p id="6a15" class="pw-post-body-paragraph ig ih hj ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hc bi translated">朴素贝叶斯是一种简单有效的算法，用于解决各种分类问题。它易于构建，对于大型数据集尤其有用。更重要的是，该模型引入了一种概率方法来理解机器学习。</p><figure class="jf jg jh ji fe jj es et paragraph-image"><div class="es et je"><img src="../Images/530e4ef6e3a5690834b6fb9f693049b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*h0N1aPldJMYGs_37ULnbww.png"/></div><figcaption class="jm jn eu es et jo jp bd b be z dy">Father of Bayes’ Theorem</figcaption></figure><p id="76a0" class="pw-post-body-paragraph ig ih hj ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hc bi translated">按照概率方法，假设我们有n个类别标签y = {c1，c2，…cn}，ƛ是错误分类cj到ci的损失，x是我们的样本。我们有条件风险</p><figure class="jf jg jh ji fe jj es et paragraph-image"><div class="es et jq"><img src="../Images/110c2ae3a523cd20be69b1e4117f5221.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*8dp0307eIHqjTpETQtvIZw.png"/></div><figcaption class="jm jn eu es et jo jp bd b be z dy">Risk is expected loss from classifying x to c.</figcaption></figure><figure class="jf jg jh ji fe jj es et paragraph-image"><div class="es et jr"><img src="../Images/5f2fd0c419f89269080c68d2a8b1e92c.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*mnUTxBXDF5mxv4EMFEsRTA.png"/></div></figure><figure class="jf jg jh ji fe jj es et paragraph-image"><div class="es et jr"><img src="../Images/2c4871f3a04920391e9589e6c0c59c1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*JwFzGubW_dZWnVUMHR_fDA.png"/></div></figure><p id="c82c" class="pw-post-body-paragraph ig ih hj ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hc bi translated">并试图找到最佳的h: X -&gt; Y，使整体风险最小化。h是我们的分类器。</p><figure class="jf jg jh ji fe jj es et paragraph-image"><div class="es et js"><img src="../Images/cc8b7b77d447c4f7fef08c59f3b4c0e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*CLGzNjQP8Wo0FweSBjALPw.png"/></div></figure><p id="b384" class="pw-post-body-paragraph ig ih hj ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hc bi translated">从上面的定义，我们可以看到关键是找到<strong class="ii hk"> P(c|x) </strong>。求P(c|x)有两种策略:<strong class="ii hk">判别模型</strong>和<strong class="ii hk">生成模型</strong>。给定x，判别模型直接预测c。(示例包括逻辑回归、决策树和SVM)另一方面，生成模型使用贝叶斯定理转换<strong class="ii hk"> P(c|x】</strong></p><figure class="jf jg jh ji fe jj es et paragraph-image"><div class="es et jt"><img src="../Images/97e459411d4b34422d5f8dd03a5bb6f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*5l-9nxcTEa0oRYBfxySLdw.png"/></div><figcaption class="jm jn eu es et jo jp bd b be z dy">P(c) is prior probability. P(x|c) is likelihood.</figcaption></figure><h1 id="3b17" class="ju jv hj bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">从最大似然估计导出朴素贝叶斯</h1><p id="3f1b" class="pw-post-body-paragraph ig ih hj ii b ij ks il im in kt ip iq ir ku it iu iv kv ix iy iz kw jb jc jd hc bi translated">因为P(x)与分类无关，而只是一个规格化器，所以我们现在忽略它。因此，我们的目标是找到最有可能使P(c)P(x|c)最大化的c ∈ {1…k}。用d =属性的数量，并且在<strong class="ii hk">属性条件独立性假设下，</strong>似然函数为</p><figure class="jf jg jh ji fe jj es et paragraph-image"><div class="es et kx"><img src="../Images/46cb8412ad224904ed3fbefc8844ad53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*3LfxS7mxSnl07lUIF_OJvQ.png"/></div><figcaption class="jm jn eu es et jo jp bd b be z dy"><strong class="bd jw">In order to differentiate, I use q as probability sign for the right side. And use y in replacement of c.</strong></figcaption></figure><p id="f5ac" class="pw-post-body-paragraph ig ih hj ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hc bi translated">为了估计两个q_s的值，朴素贝叶斯使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank"> <strong class="ii hk"> MLE(最大似然估计)</strong> </a> <strong class="ii hk">，</strong>，它试图在参数空间中找到使似然函数最大化的点。</p><figure class="jf jg jh ji fe jj es et paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="es et kz"><img src="../Images/5831b2d7a014995216ef2d005032f483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bZEte7Qldy04WDWhFUQgNA.png"/></div></div><figcaption class="jm jn eu es et jo jp bd b be z dy">Maximize this function L to get optimal parameters</figcaption></figure><p id="9bf7" class="pw-post-body-paragraph ig ih hj ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hc bi translated">知道如何使用MLE是很重要的。现在评估分成两部分。由于它们互不依赖，我们可以简单地逐个最大化它们。</p><figure class="jf jg jh ji fe jj es et paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="es et le"><img src="../Images/26e5913ef2539a78ddfe800a10657fc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rwvUKI3ljZiasNo0y_veJg.png"/></div></div><figcaption class="jm jn eu es et jo jp bd b be z dy">1st Part, *important note we define [[y^(i) = y]] to be 1 if they are equal, y^(i) is ith <strong class="bd jw">class</strong> of y.</figcaption></figure><figure class="jf jg jh ji fe jj es et paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="es et lf"><img src="../Images/eb89226d10ce2dc498acb57319cb7f50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RlFYpUeB5LH95WcrMwpXhA.png"/></div></div><figcaption class="jm jn eu es et jo jp bd b be z dy">2nd Part</figcaption></figure><p id="7623" class="pw-post-body-paragraph ig ih hj ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hc bi translated">得到上述最终估计量的证明。两部分的最后一步。</p><figure class="jf jg jh ji fe jj es et paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="es et lg"><img src="../Images/86046d8188637712100ba5987c8afd3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PqKCLhADJMhE8fWkp42bgQ.png"/></div></div></figure><p id="e3b3" class="pw-post-body-paragraph ig ih hj ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hc bi translated">最终估算值为</p><figure class="jf jg jh ji fe jj es et paragraph-image"><div class="es et lh"><img src="../Images/39844518baf6e9a1d5c486c266688eb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*D8WbUZZ3b2PMs4C4801K8Q.png"/></div><figcaption class="jm jn eu es et jo jp bd b be z dy">Prior Probability. D_c is number of class c in dataset D</figcaption></figure><figure class="jf jg jh ji fe jj es et paragraph-image"><div class="es et li"><img src="../Images/15bdbb35890a1cacd4057d56d1ce358a.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*jJhw8Nb3CxC3W7EQ1u9t-w.png"/></div><figcaption class="jm jn eu es et jo jp bd b be z dy">Posterior Probability. D_c,x is number of attribute x in D_c</figcaption></figure><p id="4b13" class="pw-post-body-paragraph ig ih hj ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hc bi translated">统计推断中有两个学派:<strong class="ii hk">频率主义者</strong>和<strong class="ii hk">贝叶斯。</strong>你可能认为既然我们使用贝叶斯定理，朴素贝叶斯就是一种贝叶斯推断。<strong class="ii hk">不是这样的</strong>，因为它不做任何先验分布的假设，它使用最大似然法来推导它的参数。所以，这仍然是一个频繁主义者的推论。</p><p id="392f" class="pw-post-body-paragraph ig ih hj ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hc bi translated">正如您所看到的，最终的参数很容易计算，也很简单。因为算法的强假设<strong class="ii hk">“属性条件独立性”和简单性</strong>。它被命名为天真。以下是一些我觉得有用的资源:</p><ol class=""><li id="e1df" class="lj lk hj ii b ij ik in io ir ll iv lm iz ln jd lo lp lq lr bi translated"><a class="ae ky" href="https://towardsdatascience.com/all-about-naive-bayes-8e13cef044cf" rel="noopener" target="_blank">朴素贝叶斯的使用</a></li><li id="5e1a" class="lj lk hj ii b ij ls in lt ir lu iv lv iz lw jd lo lp lq lr bi translated"><a class="ae ky" href="https://www.behind-the-enemy-lines.com/2008/01/are-you-bayesian-or-frequentist-or.html" rel="noopener ugc nofollow" target="_blank">贝叶斯和频率主义者的区别</a>举例</li></ol><h1 id="9d81" class="ju jv hj bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">面试问题</h1><ol class=""><li id="6dc5" class="lj lk hj ii b ij ks in kt ir lx iv ly iz lz jd lo lp lq lr bi translated">如何利用拉格朗日乘数进行优化？</li><li id="366d" class="lj lk hj ii b ij ls in lt ir lu iv lv iz lw jd lo lp lq lr bi translated">MLE怎么用？</li><li id="d36b" class="lj lk hj ii b ij ls in lt ir lu iv lv iz lw jd lo lp lq lr bi translated">贝叶斯条件概率定理怎么用？</li><li id="5a36" class="lj lk hj ii b ij ls in lt ir lu iv lv iz lw jd lo lp lq lr bi translated">频繁主义者和贝叶斯的区别，是什么造就了朴素贝叶斯频繁主义者？</li></ol><p id="44c4" class="pw-post-body-paragraph ig ih hj ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hc bi translated"><strong class="ii hk">感谢您阅读这篇文章！希望这对你有帮助。如果你需要更多的信息，请告诉我。</strong></p></div></div>    
</body>
</html>
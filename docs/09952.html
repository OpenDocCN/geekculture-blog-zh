<html>
<head>
<title>Getting started with Apache Spark III</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark III入门</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/getting-started-with-apache-spark-iii-1758581d87f3?source=collection_archive---------18-----------------------#2022-01-06">https://medium.com/geekculture/getting-started-with-apache-spark-iii-1758581d87f3?source=collection_archive---------18-----------------------#2022-01-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="0b08" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Apache Spark之旅的第三部分！</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/2a3bed2ac1a457b01e9095132def7ff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yKCRYzrnFkLivh_f8FtLfA.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Photo by <a class="ae jt" href="https://unsplash.com/@altumcode?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">AltumCode</a> on <a class="ae jt" href="https://unsplash.com/s/photos/code?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="a690" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将讨论Spark中的数据结构和其他一些更深入的主题。Spark中有三种类型的数据结构可以保存数据:</p><ol class=""><li id="a255" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">RDD(弹性分布式数据集)</li><li id="715e" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">资料组</li><li id="bcca" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">数据帧</li></ol><p id="fba8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">弹性分布式数据集(RDD)</strong><br/>spark的基础(遗留)数据结构可以描述为:<br/> -数据元素的不可变分布式集合<br/> -容错<br/> -跨集群节点分区，可以并行运行</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ki"><img src="../Images/b46adfbc83419f4d8c4de7772597b12f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*hk2-8S3o2ieUtFJ8dBB61A.jpeg"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Source : <a class="ae jt" href="http://image.slidesharecdn.com/youtubespk-141216130447-conversion-gate02/95/apache-spark-rdd-101-3-638.jpg" rel="noopener ugc nofollow" target="_blank">Slideshare</a></figcaption></figure><p id="af82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">创建rdd:</p><ul class=""><li id="a472" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc kj ka kb kc bi translated">并行化pyspark导入SparkContext的集合<br/><br/>sc = spark context . getor create()<br/>data =[1，2，3，4]<br/>dist data = sc . parallelise(data)</li><li id="f895" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc kj ka kb kc bi translated">外部数据集<br/>distFile = sc . textfile(" data . txt ")</li><li id="bdc1" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc kj ka kb kc bi translated">来自另一个RDD</li></ul><p id="d9eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">DataSet<br/></strong>Spark SQL中的强类型数据结构，它是dataframe API的扩展</p><p id="30fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Dataframe <br/> </strong>它是一个分布式的数据集合，这些数据被分组到指定的列中，Dataframe相当于一个数据库表。</p><p id="5a1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">创建数据框架:<br/>NewDF = spark . read . parquet(" file _ path ")</p><p id="390b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">并行性和分区</strong><br/>Spark中控制并行性的两个主要因素是<br/> 1。分区数量<br/> 2。遗嘱执行人人数</p><h2 id="6ce0" class="kk kl hi bd km kn ko kp kq kr ks kt ku iq kv kw kx iu ky kz la iy lb lc ld le bi translated">ETL/ELT流程</h2><p id="719b" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">ETL(提取、转换和加载):<br/>提取:从多个来源提取数据的过程<br/>转换:对提取的数据应用业务逻辑并丢弃不需要的数据<br/>加载:将数据加载到最终的表中，或者作为文件</p><p id="f889" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ELT(提取、加载和转换):<br/>当我们出于分析和数据科学目的需要原始数据时，加载和转换部分会被切换<br/>我们保留原始数据，并让团队根据需要转换数据。</p><h2 id="398f" class="kk kl hi bd km kn ko kp kq kr ks kt ku iq kv kw kx iu ky kz la iy lb lc ld le bi translated">数据仓库和数据湖</h2><p id="3fa1" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated"><strong class="ih hj">数据仓库:<br/> </strong>专为OLAP而建<br/>专注于一个业务领域<br/>通常以更结构化的形式包含数据<br/>最终用途是为下游应用程序和web界面生成报告<br/>最流行的数据仓库数据库有— Oracle、Hive、GBQ、雪花</p><p id="3916" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据湖:<br/> </strong>重点是创建一个集中的数据可用性，供多个职能团队使用<br/>数据保持原样，无需从源进行太多更改<br/>最终用户可以是多个用户，如数据科学、数据工程、数据分析师等。<br/>可以进行数据编目来标记数据，以便于使用</p><h2 id="2c05" class="kk kl hi bd km kn ko kp kq kr ks kt ku iq kv kw kx iu ky kz la iy lb lc ld le bi translated">对数据结构的操作</h2><p id="d177" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">我们可以对rdd、数据集和数据帧进行以下操作<br/>转换<br/>操作</p><p id="d734" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，我们希望获得工资高于10万美元的员工，我们可以在收集数据时执行这些转换<br/>操作，或者将数据保存在文件或数据库中。所以它把我们在执行器中的所有数据带给我们的司机。一旦我们执行了操作，就会在执行器中创建作业。</p><p id="c680" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不同的转换我们有<br/>过滤数据<br/>改变数据类型<br/>对字段应用计算<br/>连接<br/>合并<br/>透视<br/>选择等等！</p><h2 id="7bdc" class="kk kl hi bd km kn ko kp kq kr ks kt ku iq kv kw kx iu ky kz la iy lb lc ld le bi translated">转换的类型</h2><p id="098f" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">窄变换<br/> -在这种情况下，每个输入分区将只贡献给一个输出分区<br/> -不需要混洗<br/> -可以流水线化到单个阶段<br/> -映射、过滤、联合等。</p><p id="1969" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">宽转换<br/> -在这种情况下，输入分区将贡献给多个输出分区<br/> -它定义了一个新的阶段<br/> - Reduce by、group by、join等。</p><h2 id="b169" class="kk kl hi bd km kn ko kp kq kr ks kt ku iq kv kw kx iu ky kz la iy lb lc ld le bi translated">行动</h2><p id="5b8f" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">一个动作指示Spark计算一系列转换的结果<br/>动作将数据从执行器发送到驱动程序，一旦完成<br/>它们可以返回原始值，或者我们可以将它们写入文件和表格</p><p id="717a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以执行的一些典型转换有:<br/> -收集<br/> -减少<br/> -计数<br/> -取<br/> -顶部<br/> -聚集</p><h2 id="907e" class="kk kl hi bd km kn ko kp kq kr ks kt ku iq kv kw kx iu ky kz la iy lb lc ld le bi translated">Spark中的代码执行</h2><p id="7f62" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">代码执行是在集群中完成的，一旦我们创建了数据框、数据集或SQL代码，Spark就会为其执行创建一个逻辑计划。所有这些都是通过在后台运行的优化器来完成的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lk"><img src="../Images/03a84cfb1e6d2181f9438bd67d9067b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*tATRsRSonpNfLITNAh1AMg.jpeg"/></div><figcaption class="jp jq et er es jr js bd b be z dx">Source : <a class="ae jt" href="https://www.learntospark.com/2020/02/spark-sql-catalyst-optimizer.html" rel="noopener ugc nofollow" target="_blank">Learn to Spark</a></figcaption></figure><p id="cca7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑计划:<br/>一系列的数字或代数结构，没有定义计算将如何进行。<br/>物理计划:<br/>这是一个物理计划，它也定义了如何进行计算。</p><h2 id="5d18" class="kk kl hi bd km kn ko kp kq kr ks kt ku iq kv kw kx iu ky kz la iy lb lc ld le bi translated">SQL Catalyst优化器</h2><p id="9a1f" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">catalyst优化器将逻辑计划转换为物理计划。优化只不过是对长期运行的应用程序或系统进行微调并做出一些改变的方法，以使应用程序有效地管理资源并有效地减少处理时间。</p><p id="18f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">众所周知，直接使用RDD完成的转换的性能不会那么高效，Spark SQL API数据帧以及数据集的性能都优于RDD。Spark SQL与一个名为Catalyst optimizer的优化引擎一起运行，该引擎可以帮助开发人员通过对源代码进行任何更改来优化基于数据帧和数据集构建的查询。</p><p id="6f2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Catalyst是Spark SQL模块化库之一，受基于规则的优化和基于成本的优化支持。基于规则的优化建议使用一组规则来查找执行查询的方式，而基于成本的优化则是计算所有基于规则的查询的成本，并帮助引擎找到执行SQL查询的最佳方法。</p><p id="c7d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SQL Catalyst优化器的主要重点是最小化混洗操作，最小化执行器之间的数据传输</p><p id="5bdd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上图所示有4个主要组件:<br/> -分析器<br/> -优化器<br/> -物理规划<br/> -代码生成器</p><h2 id="2b16" class="kk kl hi bd km kn ko kp kq kr ks kt ku iq kv kw kx iu ky kz la iy lb lc ld le bi translated">火花应用</h2><p id="09ac" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">spark应用的组件主要包括:-</p><ol class=""><li id="c709" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">创建spark会话并定义应用程序</li><li id="d761" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">与数据源连接</li><li id="d017" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">以rdd和数据帧的形式获取输入数据</li><li id="43b0" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">应用转换和业务逻辑</li><li id="1723" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">应用操作</li><li id="8342" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">定义运行应用程序的模式</li><li id="23e8" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">提交申请</li></ol><p id="43d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将在下一篇文章中讨论一些代码！:)</p><p id="a30a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其他可能感兴趣的文章:<br/> - <strong class="ih hj">本系列的一部分</strong>:</p><p id="d801" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jt" rel="noopener" href="/geekculture/getting-started-with-apache-spark-i-5fbbe7b47667">Apache Spark-I入门|作者Sam |极客文化| 2022年1月| Medium</a><br/><a class="ae jt" rel="noopener" href="/geekculture/getting-started-with-apache-spark-ii-fffeab9f5df7">Apache Spark II入门|作者Sam |极客文化| 2022年1月| Medium </a></p><p id="6c17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">杂项</strong> : <a class="ae jt" rel="noopener" href="/geekculture/streamlit-and-palmer-penguins-92a09004ed45">细流企鹅和帕尔默企鹅。上周在网飞的Binged非典型… |作者Sam | Geek Culture | Medium</a><br/>-<a class="ae jt" rel="noopener" href="/geekculture/getting-started-with-streamlit-ed81eafcb298">Streamlit入门。使用Streamlit解释你的EDA和… | by Sam | Geek Culture | Medium </a></p><p id="cec9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">干杯，请关注更多此类内容！:)</p><p id="8dce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你喜欢它的内容，你现在也可以给我买一杯咖啡！<br/> <a class="ae jt" href="https://www.buymeacoffee.com/samunderscore12" rel="noopener ugc nofollow" target="_blank"> samunderscore12正在创作数据科学内容！(buymeacoffee.com)</a></p></div></div>    
</body>
</html>
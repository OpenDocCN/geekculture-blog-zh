<html>
<head>
<title>Want Responsible AI: Start Early in the Model Development Cycle</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">想要负责任的人工智能:在模型开发周期的早期开始</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/want-responsible-ai-start-early-in-the-model-development-cycle-1a74585af675?source=collection_archive---------27-----------------------#2021-11-15">https://medium.com/geekculture/want-responsible-ai-start-early-in-the-model-development-cycle-1a74585af675?source=collection_archive---------27-----------------------#2021-11-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/ee78c276ec3e0706b44fff9ec2f77220.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_d6bJWTHAoB1Tcgr"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Photo by <a class="ae iu" href="https://unsplash.com/@jankolar?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jan Antonin Kolar</a> on <a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="0efb" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">为什么是负责任的人工智能</h1><h2 id="d015" class="jt iw hi bd ix ju jv jw jb jx jy jz jf ka kb kc jj kd ke kf jn kg kh ki jr kj bi translated">政策含义:围绕人工智能的国家战略</h2><p id="3994" class="pw-post-body-paragraph kk kl hi km b kn ko kp kq kr ks kt ku ka kv kw kx kd ky kz la kg lb lc ld le hb bi translated">加拿大是2017年第一个发布国家人工智能战略的国家，此后大多数国家都发布了自己的人工智能战略。您可以在以下位置阅读这些策略:</p><p id="1a92" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">印度政府第一部分:<a class="ae iu" href="https://www.niti.gov.in/sites/default/files/2021-02/Responsible-AI-22022021.pdf" rel="noopener ugc nofollow" target="_blank">负责任人工智能的原则</a></p><p id="52d3" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">印度政府第二部分:<a class="ae iu" href="https://www.niti.gov.in/sites/default/files/2021-08/Part2-Responsible-AI-12082021.pdf" rel="noopener ugc nofollow" target="_blank">实施负责任人工智能的原则</a></p><p id="24aa" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">美国:<a class="ae iu" href="https://www.nitrd.gov/nitrdgroups/images/c/c1/American-AI-Initiative-One-Year-Annual-Report.pdf" rel="noopener ugc nofollow" target="_blank">美国人工智能报告</a></p><p id="a451" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">美国FDA: <a class="ae iu" href="https://www.fda.gov/media/145022/download" rel="noopener ugc nofollow" target="_blank">软件作为医疗设备(SaMD)行动计划</a></p><p id="0982" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">欧洲:<a class="ae iu" href="https://www.bmjv.de/SharedDocs/Downloads/DE/Themen/Fokusthemen/Gutachten_DEK_EN.pdf?__blob=publicationFile&amp;v=2" rel="noopener ugc nofollow" target="_blank">数据伦理委员会的意见</a></p><p id="af22" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">欧洲:<a class="ae iu" href="https://ec.europa.eu/info/sites/default/files/commission-white-paper-artificial-intelligence-feb2020_en.pdf" rel="noopener ugc nofollow" target="_blank">人工智能/欧洲追求卓越和信任的方法</a></p><p id="38d5" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">澳大利亚:<a class="ae iu" href="https://www.industry.gov.au/sites/default/files/June%202021/document/australias-ai-action-plan.pdf" rel="noopener ugc nofollow" target="_blank">人工智能行动计划</a></p><p id="acbc" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">英国:<a class="ae iu" href="https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1020402/National_AI_Strategy_-_PDF_version.pdf" rel="noopener ugc nofollow" target="_blank">国家人工智能战略</a></p><p id="be14" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">英国:<a class="ae iu" href="https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/868284/Web_Version_AI_and_Public_Standards.PDF" rel="noopener ugc nofollow" target="_blank">人工智能和公共标准</a></p><p id="57d6" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">现在，如果你仔细阅读上述报告，你会观察到一个共同的模式，他们强调拥有负责任和道德的人工智能系统。这使得将负责任的人工智能实践纳入所有组织变得至关重要。</p><p id="dea7" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">最近在HBR发表的一篇文章强调了负责任的人工智能治理的必要性</p><div class="lk ll ez fb lm ln"><a href="https://hbr.org/2021/09/ai-regulation-is-coming" rel="noopener  ugc nofollow" target="_blank"><div class="lo ab dw"><div class="lp ab lq cl cj lr"><h2 class="bd hj fi z dy ls ea eb lt ed ef hh bi translated">AI监管来了</h2><div class="lu l"><h3 class="bd b fi z dy ls ea eb lt ed ef dx translated">随着公司越来越多地将人工智能嵌入到他们的产品、流程和产品中，我们面临着挑战</h3></div><div class="lv l"><p class="bd b fp z dy ls ea eb lt ed ef dx translated">hbr.org</p></div></div><div class="lw l"><div class="lx l ly lz ma lw mb io ln"/></div></div></a></div><h2 id="dafb" class="jt iw hi bd ix ju jv jw jb jx jy jz jf ka kb kc jj kd ke kf jn kg kh ki jr kj bi translated">所涉费用:</h2><p id="3a07" class="pw-post-body-paragraph kk kl hi km b kn ko kp kq kr ks kt ku ka kv kw kx kd ky kz la kg lb lc ld le hb bi translated">我们已经听到很多关于基于人工智能的解决方案如何为商业用户提供动力的说法。他们可以提供高精度的未来预测，这将极大地有助于带来商业价值。现在，所有这些解决方案都是为了寻找历史数据中的隐藏模式而构建的。如果业务过于关注获得非常高精度的模型，我们可能最终会整合各种可用的功能、新的最先进的库，只是为了获得90%以上的模型精度，而没有意识到它的下游效应。</p><p id="5edf" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">人工智能模型开发是一个高度迭代的过程，这反过来对任何组织来说都是一件昂贵的事情。如果我们在各种迭代之后开发了一个模型，只是后来意识到它不符合隐私、责任和安全指南。这将对组织产生巨大的成本影响，包括模型开发成本、模型收益损失成本、机会成本等等。</p><p id="86ef" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">对于组织来说，在模型开发阶段之前包含负责任的人工智能的最佳实践是非常有用的。</p><h1 id="6624" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">负责任的人工智能的支柱:为什么要早点开始</h1><figure class="md me mf mg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/90cc1c7aa417db3834955ac89e3840f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BkLuQBuGvayKGxbt"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Photo by <a class="ae iu" href="https://unsplash.com/@santesson89?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Andrea De Santis</a> on <a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4e57" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">以下是任何人工智能系统需要负责的重要方面。</p><h2 id="131e" class="jt iw hi bd ix ju jv jw jb jx jy jz jf ka kb kc jj kd ke kf jn kg kh ki jr kj bi translated">隐私</h2><p id="3c1c" class="pw-post-body-paragraph kk kl hi km b kn ko kp kq kr ks kt ku ka kv kw kx kd ky kz la kg lb lc ld le hb bi translated">各种基于人工智能的系统是如此之好，因为它们提供个性化的推荐，对最终用户有直接价值。对于这些系统，我们从各种数据源获取数据，其中也可能包含私人信息。有时候信息不能直接私密，但是多个特征结合起来有时候可以泄露私密信息。</p><p id="ebec" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">现在，我们需要在建立任何人工智能模型之前，考虑我们在人工智能模型中使用的数据和特征。在此基础上，我们需要看看有哪些潜在的特征，可以直接或间接揭示PII信息。在机器学习模型中有多种方式来保护隐私，即通过使用差分隐私、生成合成数据、通过数据压缩等等。</p><p id="6dbe" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">现在，当我们使用上述技术处理数据时，我们可能无法获得与使用原始数据一样高的准确性。在模型开发阶段的早期，决定数据、隐私问题以及处理数据的正确方法就变得非常重要。你可以阅读这篇<a class="ae iu" rel="noopener" href="/georgian-impact-blog/a-brief-introduction-to-differential-privacy-eacf8722283b">文章</a>了解更多关于差分隐私的内容。对于更多热心的读者，我推荐曼宁出版社出版的《保护隐私的机器学习》一书。</p><h2 id="9931" class="jt iw hi bd ix ju jv jw jb jx jy jz jf ka kb kc jj kd ke kf jn kg kh ki jr kj bi translated">安全性</h2><p id="cc5b" class="pw-post-body-paragraph kk kl hi km b kn ko kp kq kr ks kt ku ka kv kw kx kd ky kz la kg lb lc ld le hb bi translated">所有IT应用程序都应该有适用于它们的多层防御(数据、应用程序、计算、网络、边界、身份和访问、物理安全)，那么是什么让AI系统更容易受到攻击。它是我们建立人工智能模型的独特方式，我们使用的开源库，有时是那些人工智能模型背后的数学。攻击者想出各种独特的方法来欺骗系统。他们可以通过向实时人工智能系统提供虚假数据来扭曲训练数据，他们可以将对立的例子引入生产系统来欺骗它，等等。有时，数据科学家可能最终使用开源python库，其中存在潜在的漏洞。</p><p id="8203" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">现在，人工智能系统可能引发的许多其他网络安全风险将取决于模型训练频率/数据模式/使用的算法/使用的库等。我们在模型开发的早期阶段做出的所有这些选择。你可以在这里阅读更多关于python系统<a class="ae iu" href="https://www.anaconda.com/blog/why-understanding-cves-is-critical-for-data-scientists" rel="noopener ugc nofollow" target="_blank">中的常见漏洞，以及为什么理解这一点对于数据科学家来说非常重要。</a></p><h2 id="2972" class="jt iw hi bd ix ju jv jw jb jx jy jz jf ka kb kc jj kd ke kf jn kg kh ki jr kj bi translated">公平</h2><p id="7a4e" class="pw-post-body-paragraph kk kl hi km b kn ko kp kq kr ks kt ku ka kv kw kx kd ky kz la kg lb lc ld le hb bi translated">任何ML模型都不应该有任何预设的偏见。然而，当我们使用真实世界的数据训练模型时，数据会包含偏差。由于样本偏斜、人为偏差、样本大小等原因，数据可能会有偏差。这可能导致ML模型预测有偏差的结果，简单的例子，由于训练数据中的偏差，学生的负载被拒绝。</p><p id="3cc0" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">现在，由于数据是罪魁祸首，对于数据科学家来说，寻找可能在模型中引入偏差的特征变得至关重要。这应该发生在开发机器学习模型之前。有多个基于python的库可用于处理ML模型中的公平性，即Fair-learn、Themis-ml等，请参考此处的<a class="ae iu" href="https://techairesearch.com/most-essential-python-fairness-libraries-every-data-scientist-should-know/" rel="noopener ugc nofollow" target="_blank">了解更多细节。如果你有兴趣了解这些算法背后的数学，我推荐你阅读下面的</a><a class="ae iu" href="https://towardsdatascience.com/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb" rel="noopener" target="_blank">文章</a>。</p><h2 id="c5a3" class="jt iw hi bd ix ju jv jw jb jx jy jz jf ka kb kc jj kd ke kf jn kg kh ki jr kj bi translated">透明度</h2><p id="af29" class="pw-post-body-paragraph kk kl hi km b kn ko kp kq kr ks kt ku ka kv kw kx kd ky kz la kg lb lc ld le hb bi translated">这些人工智能系统中的许多都是通过使用复杂的算法来构建的，即深度学习、梯度增强等。这些算法不是非常直观和可解释的，因此解释某些预测背后的推理变得非常困难。然而，很多时候我们需要知道为什么人工智能会做出特定的决定，因此人工智能模型的透明性非常重要。现在，这是数据科学家需要了解互操作性、可扩展性以及实现这一目标的各种方法的地方。</p><p id="ab49" class="pw-post-body-paragraph kk kl hi km b kn lf kp kq kr lg kt ku ka lh kw kx kd li kz la kg lj lc ld le hb bi translated">这又是由业务问题驱动的，因此it必须在模型开发的开始就考虑周全。</p><h1 id="0400" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">结论</h1><p id="b99e" class="pw-post-body-paragraph kk kl hi km b kn ko kp kq kr ks kt ku ka kv kw kx kd ky kz la kg lb lc ld le hb bi translated">人工智能模型的可靠性和责任性是负责任的人工智能系统的另外两个重要方面。我将在下一篇文章中详细介绍这些内容。你也可以参考这些<a class="ae iu" href="https://pair.withgoogle.com/guidebook/patterns" rel="noopener ugc nofollow" target="_blank">模式</a>来构建负责任的AI系统。</p></div></div>    
</body>
</html>
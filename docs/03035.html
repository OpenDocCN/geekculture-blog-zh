<html>
<head>
<title>Understanding the paper Generating Sequences with RNNs (by Alex Graves)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用RNNs理解纸张生成序列(Alex Graves)</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/understanding-the-paper-generating-sequences-with-rnns-by-alex-graves-18635cdd32be?source=collection_archive---------20-----------------------#2021-05-30">https://medium.com/geekculture/understanding-the-paper-generating-sequences-with-rnns-by-alex-graves-18635cdd32be?source=collection_archive---------20-----------------------#2021-05-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6ad5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我写下了我对这篇论文的理解，这将有助于你对它有一个概念，之后你自己阅读这篇论文将会理清思路。最后，分享了我使用该论文合成网络生成的手写文本样本。</p><p id="7a8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文共分五个部分，具体如下:</p><ul class=""><li id="350b" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">介绍</li><li id="2f5a" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">预测网络</li><li id="d864" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">文本预测</li><li id="59e4" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">手写预测</li><li id="28b4" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">手写合成</li></ul><h2 id="7591" class="jr js hi bd jt ju jv jw jx jy jz ka kb iq kc kd ke iu kf kg kh iy ki kj kk kl bi translated"><strong class="ak">简介</strong></h2><p id="6ceb" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">递归神经网络(RNNs)是一种神经网络，其中来自前一步骤的输出作为输入被馈送到当前步骤。然而，在传统的神经网络中，所有的输入和输出都是相互独立的。</p><p id="8fe1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为什么我们需要rnn来生成序列？</p><p id="89fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，序列意味着任何语言语句、歌曲、诗歌等。每当你读一首押韵的诗，你会发现最后一个词听起来和上一行的最后一个词很相似。</p><blockquote class="kr"><p id="b4e0" class="ks kt hi bd ku kv kw kx ky kz la jc dx translated">他们将输掉比赛<br/>没有人会受到责备</p></blockquote><p id="3236" class="pw-post-body-paragraph if ig hi ih b ii lb ik il im lc io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">你可以看到上面的两行，最后一个单词听起来很相似。所以重点是当你写第二行时，你需要记住前一行，那是我们传统网络失败的地方。同样，在任何序列中，我们都需要记住过去。并且我们的传统神经网络独立于先前的输出。</p><p id="1632" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">即使标准的rnn也不能长时间存储过去的信息，这种“健忘症”使它们易于产生不稳定的序列。</p><p id="02e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">长短期记忆(LSTMs)是一种特殊的RNN结构，它被设计成比标准的rnn更好地存储和访问来自过去的信息。</p><p id="ca94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇论文的主要目的是证明LSTM可以利用它的记忆产生复杂的、真实的包含长程结构的序列。</p></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h2 id="4011" class="jr js hi bd jt ju jv jw jx jy jz ka kb iq kc kd ke iu kf kg kh iy ki kj kk kl bi translated">预测网络</h2><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es ln"><img src="../Images/90b7ffaccf349feb5f4e9ea1bc8f7cca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hc2IazDoQm94gWIVNICVyA.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx"><strong class="bd jt">Deep recurrent neural network prediction architecture. </strong>The circle represents the cells of the network layer, solid lines represents weighted connections (each lines have some weight W) and the dashed lines represent the predictions</figcaption></figure><p id="8dde" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面我们可以看到RNNs架构，其中x (x1，x2，…xT)输入单元格也直接传递到第二和第三隐藏层。正如我们所知，RNN将之前的输出作为下一个单元格的输入，这可以在上图(虚线)中以概率的形式看到(给定之前的输出，下一个输入的概率)。</p><p id="4575" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看其中一个隐藏层单元格的方程<br/>这是第一个隐藏层的第二个单元格的方程，从上图可以验证。我们可以看到，指向单元格的箭头数量是2，这就是为什么它有两个加权项。</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es md"><img src="../Images/d268ff756a04b7eb4110d8286dbf0d7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h-F7L1Fuyejg5zx3dSPAGw.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx">2nd cell of 1st hidden layer</figcaption></figure><p id="271c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中H表示隐藏层激活函数，W表示连接(线)的权重(例如W(i，h1):将输入连接到第一隐藏层的线的权重矩阵)，X_t是输入向量的第t项，b表示偏置向量。</p><p id="d5d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">类似地，其他方程都可以写成。通常</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es me"><img src="../Images/829b119c2817cc7fb267d6868541873f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dCuxWFHDtB6loBZxdJcK2A.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx">General equation for hidden layer cells</figcaption></figure><p id="ba76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出向量Y用于确定下一个输入的预测分布(Pr(X_t|Y_t-1))。</p><div class="lo lp lq lr fd ab cb"><figure class="mf ls mg mh mi mj mk paragraph-image"><img src="../Images/6c08c0333d41447a22b87ed1bf077351.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*9l5Z0SeoDhhXYAggttpRAw.png"/></figure><figure class="mf ls ml mh mi mj mk paragraph-image"><img src="../Images/a44ac74667da32e976e03b1079800530.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*Fm-9gC_thv0AcS_ltes8Kg.png"/></figure></div><p id="60fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面我们可以看到损耗(L(x))和输出向量(Y_t)等式，其中，γ是输出层函数。在下一节中，我们将看到预测分布的方程(Pr(X_t|Y_t-1))。</p><p id="57a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这一节中，作者还简要介绍了LSTM单元的体系结构，它更善于发现和利用数据中的长程相关性(序列中的前几个词)。</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es mm"><img src="../Images/fd2dc316ddcdb97b358ae1b08347ddff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HhImrChpJHmu-Lgh5tl_sg.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx">Long Short-Term Memory Cell Architecture</figcaption></figure><p id="1939" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是LSTM蜂窝的架构，而LSTM网络是这些蜂窝的集合。这些方程式可以在建筑中看到</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div class="er es mn"><img src="../Images/15909f83616bc3a225a0fdf8710652f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*S44ByMevH-Ju2HD9KDhDYw.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx">Equations for the above LSTM cell</figcaption></figure><p id="3217" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中，σ是sigmoid函数，I、f、o和c分别是输入门、遗忘门、输出门和单元输入激活向量。</p></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h2 id="9993" class="jr js hi bd jt ju jv jw jx jy jz ka kb iq kc kd ke iu kf kg kh iy ki kj kk kl bi translated">文本预测</h2><p id="82d5" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">文本数据是离散的，并且通常在使用“一键编码”方法之后呈现给神经网络。Pr(X_t+1 | Y_t)为</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div class="er es mo"><img src="../Images/63b9c5c22a790b7cfedc0585c436c8c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*jsk4TBOyGToQhD8M0FvH3A.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx">Predictive Probability</figcaption></figure><p id="842b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中k =用于独热编码的K向量文本类的元素。</p><p id="a4fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，对于文本预测，我们需要在字符级建模(K:字符的表征)或单词级建模(K:单词的表征)之间选择建模技术的类型。作者选择了字符级模型，尽管它的性能比等价的单词级模型稍差。<br/>在真实世界的数据中，不同单词的数量经常超过100，000，单词级模型可能会有问题。我们还需要增加网络的参数，这使得训练计算变得困难，我们还需要能够覆盖k个类别的总数的训练数据</p><p id="056c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总的来说，本文中的实验旨在预测数据中发现的最佳质量，以便最大化网络的生成灵活性。</p><p id="ca39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作者进行了两个实验<code class="du mp mq mr ms b">Penn Treebank Experiment</code>和<code class="du mp mq mr ms b">Wikipedia Experiment</code>。<br/> Penn Treebank是第一个文本预测实验，Penn Treebank数据被广泛用作语言建模基准。主要目的是估计网络的预测能力，而不是生成序列。此外，实验比较了单词和字符级别的LSTM预测器在Penn语料库上的性能。在这两种情况下，网络架构是一个具有1000个LSTM单元的单一隐藏层。</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es mt"><img src="../Images/31d642cdbb96274852f4e39353ef1f93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uuJbpSXnEf2asK5O7hqWqA.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx"><strong class="bd jt">Penn Treebank Test Set Results.</strong> <strong class="bd jt">‘BPC’ </strong>is bits-per-character. ‘Error’ is next-step classification error rate, for either characters or words.</figcaption></figure><p id="8ad5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他们发现，使用迭代增加的正则化进行再训练比使用正则化从随机权重进行训练更快。发现自适应权重噪声正则化对于字级网络非常慢，因此它仅用固定方差权重噪声正则化。表中显示，单词级RNN比字符级网络表现得更好(复杂度越低，模型越好)，但是当使用正则化时，差距似乎缩小了。</p><p id="9f5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从序列生成的角度来看，Wikipedia数据很有趣，因为它不仅包含大量的字典单词，还包含许多不会包含在Penn Treebank文本数据中的字符序列。例如外语、元数据的XML标签、网站地址和用于格式化页面的标记语言。由于大量的数据，使用了比Penn data更大的网络，具有700个LSTM单元的7个隐藏层。并且维基百科中几乎有四页的样本是从论文中可以看到的模型生成的(第5-8页)。样本显示，网络已经从数据中学习了很多结构。然而，模型生成的所有文本都没有意义(没有意义)。</p></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h2 id="5a8e" class="jr js hi bd jt ju jv jw jx jy jz ka kb iq kc kd ke iu kf kg kh iy ki kj kk kl bi translated">手写预测</h2><p id="b911" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">本节演示了如何通过使用混合密度输出层将预测网络应用于真实世界的数据，并提供了在<a class="ae mu" href="https://fki.tic.heia-fr.ch/databases/iam-on-line-handwriting-database" rel="noopener ugc nofollow" target="_blank"> IAM在线手写</a>数据库上的实验结果。</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es mv"><img src="../Images/4b34653840b7a02d5298fca90acadb86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9TjOk91S-PdNl1QnJC8O_g.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx"><strong class="bd jt">Training samples from the IAM online handwriting database.</strong> Notice the wide range of writing styles, the variation in line angle and character sizes, and the writing errors, such as the scribbled out letters in the first line and the repeated word in the final line.</figcaption></figure><p id="f9df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在线手写输入数据包含每个字符的x、y笔坐标以及当笔离开白板时序列中的点(如果字符已完成，则为1，否则为0)。我们的模型将一次预测一个点的笔轨迹，这给了网络最大的灵活性来发明新的笔迹，这需要大量的内存，其中平均一个字母占用超过25个时间步，平均一行占用大约700个时间步。</p><p id="60fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据作者，将预测网络应用于在线手写数据的主要挑战是确定适合于实值输入的预测分布。<br/>为了确定这里的预测分布，作者使用了<a class="ae mu" href="https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca" rel="noopener" target="_blank">混合密度网络</a>。网络输出的子集用于定义混合权重(与每个混合成分相关的概率)，而剩余的输出用于确定单独的混合成分(单独的输出分布)。</p><p id="c5b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文将有助于进一步了解混合密度网络</p><p id="2385" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae mu" href="https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca" rel="noopener" target="_blank">混合密度网络搭便车指南Oliver Borchers博士|走向数据科学</a></p><p id="c3a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于本文中的手写实验，基本的RNN体系结构和更新方程与第2部分(预测网络)保持不变。每个输入向量X_t由一个实数值对x1、x2和一个二进制值x3组成，其中x1、x2定义笔相对于前一个输入的偏移量，如果向量结束一个笔画(即，如果笔在下一个向量被记录之前被从板上提起)，x3的值为1，否则为0。</p><p id="e5d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面你可以看到预测概率和序列丢失的等式</p><div class="lo lp lq lr fd ab cb"><figure class="mf ls mw mh mi mj mk paragraph-image"><img src="../Images/38428abe667ece4b32a330e20df00233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*XtLA6uJMZ3VRLQ6fY3dt_Q.png"/></figure><figure class="mf ls mx mh mi mj mk paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><img src="../Images/f788a30465debc3c949adb85a86e205b.png" data-original-src="https://miro.medium.com/v2/resize:fit:206/format:webp/1*EBfssdCKb9dWZ2pvCis0Zg.png"/></div></figure></div><p id="c7d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，每个输出向量Y_t由冲程结束概率e、一组均值j、标准偏差σ j、相关性ρ j和M个混合成分的混合权重π j(输出的单独分布)组成。</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es my"><img src="../Images/4b9fc1fd2a5de318459d5d942aefc2d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IV09ZF2Nn2bzrA-WzdUkpg.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx">Probability density for the next input layer</figcaption></figure><p id="3b22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">给定输出向量Y t，下一个输入X t+1的概率密度Pr(X t+1 | Y t)。</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es mz"><img src="../Images/25c5ba695f59de55429aef2609bdbf1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*24UC4IEFDNAK_ZMbrs-pgQ.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx">Sequence loss</figcaption></figure><p id="66a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="na">实验:<br/> </em>数据序列中的每个点都由三个数字组成:相对于前一个点的x和y偏移，以及二进制笔画结束特征。对于隐藏层，比较了两种网络架构:一种具有三个隐藏层，每个包含400个LSTM单元，另一种具有900个LSTM单元的单个隐藏层。两个网络的权重都在340万左右。用自适应加权噪声重新训练三层网络，所有标准偏差初始化为0.075。使用固定方差权重噪声进行训练被证明是无效的，这可能是因为它阻止了混合密度层使用精确指定的权重。</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es nb"><img src="../Images/9a887efc5abf885e9f44a10e43e56d07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8lOePb5xHbMDAzf1CArE4w.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx"><strong class="bd jt">Handwriting Prediction Results</strong>. ‘Log-Loss’ is the mean value of L(x). ‘SSE’ is the mean sum-squared-error per data point.</figcaption></figure><p id="5827" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">手写生成的样本</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es mt"><img src="../Images/c0e0e962b465866ec5800519f24215be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LTVdMwvtPr-xyXW39S-efQ.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx"><strong class="bd jt">Online handwriting samples generated by the prediction network.</strong> All sample lines are 700 timesteps long.</figcaption></figure><p id="ec06" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以看到从预测网络生成的序列，难以阅读和理解句子。手写生成在预测笔画结束时面临高变化的问题，因为当笔离开纸张时没有记录笔的位置，因此在一个笔画的结束和下一个笔画的开始之间可能有很大的距离(因书写者而异)。</p></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><h2 id="3cb6" class="jr js hi bd jt ju jv jw jx jy jz ka kb iq kc kd ke iu kf kg kh iy ki kj kk kl bi translated">手写合成(从给定的计算机文本生成手写文本)</h2><p id="1d69" class="pw-post-body-paragraph if ig hi ih b ii km ik il im kn io ip iq ko is it iu kp iw ix iy kq ja jb jc hb bi translated">这是论文的最后一部分。在以前的预测网络中，没有办法限制网络写哪个字母。在新的网络中引入了“窗口层”,其一个输入直接是字符序列(一个热编码向量),输出W_t向量在时间t被传递到第二和第三隐藏层，并且在时间t+1传递到第一隐藏层(以避免在处理图中产生循环)。可以在下面的架构中看到。</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es nc"><img src="../Images/319d38ce3694b126688a81011ba55500.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wrUn0-9qr7BfjB2PQCI5ew.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx"><strong class="bd jt">Synthesis Network Architecture.</strong> Circles represent layers, solid lines represent connections and dashed lines represent predictions. The topology is similar to the prediction network, except that extra input from the character sequence c, is presented to the hidden layers via the window layer (with a delay in the connection to the first hidden layer to avoid a cycle in the graph).</figcaption></figure><p id="e5c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">给定长度为U(令牌数)的字符序列c和长度为T(字符的时间步长)的数据序列x，在时间步长t (1 ≤ t ≤ T)时进入c的软窗口W_t定义为</p><div class="lo lp lq lr fd ab cb"><figure class="mf ls nd mh mi mj mk paragraph-image"><img src="../Images/deb2a4f65b671cadb46ecfeaf138f1ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*v7yMeQhQCy_WQv9rfdkRcw.png"/></figure><figure class="mf ls ne mh mi mj mk paragraph-image"><img src="../Images/524fdefeb200036690a7b7479fe13a69.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*pGiq9lDrII__YK0BJMm9Ew.png"/></figure></div><p id="6411" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中φ(t，u)是c_u在时间步长t的窗口权重。直观地说，<strong class="ih hj"> κt </strong>参数控制窗口的位置，<strong class="ih hj"> βt </strong>参数控制窗口的宽度，<strong class="ih hj"> αt </strong>参数控制窗口在混合中的重要性。软窗口向量的大小与字符向量c_u的大小相同(假设是一键编码，这将是字母表中的字符数)。</p><p id="0a81" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">隐藏层的方程以与先前网络相同的形式书写，输出层方程保持不变。</p><p id="a156" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="na">实验:<br/> </em>将合成网络应用于与手写预测网络(IAM Online DB)相同的输入数据。使用字符级建模，数据包含80个不同的字符(大写字母、小写字母、数字和标点)。然而，作者只使用了57个字符的子集，所有的数字和大多数标点符号都被替换成了通用的“非字母”标签。</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div class="er es nf"><img src="../Images/a64bc31fdf27da7a4ff3ab39fadbf8bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*qCW2e-ERrqf3yJOLqJouMA.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx"><strong class="bd jt">Handwriting Synthesis Results</strong></figcaption></figure><p id="9b4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">网络结构类似于最佳预测网络:三个隐藏层，每个隐藏层有400个LSTM单元。我们可以看到，预测网络的平方误差减少了44%。</p><div class="lo lp lq lr fd ab cb"><figure class="mf ls ng mh mi mj mk paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><img src="../Images/a161340d6872298c9d3a999cf535b2f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*St-c38h2omvmkSgIxIuq1Q.png"/></div></figure><figure class="mf ls nh mh mi mj mk paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><img src="../Images/b73f343e1a96037199c6d854e7371771.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*oG4uojR-Je3Wu_UV_6hxTg.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx ni di nj nk"><strong class="bd jt">Real and generated handwriting.</strong> The top line in each block is real, the rest are <strong class="bd jt">unbiased samples</strong> from the synthesis network. These two texts are from the validation set and were not seen during the training.</figcaption></figure></div><p id="4771" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上述无偏样本中的一个问题是，它们往往难以阅读(部分是因为真实笔迹难以阅读，部分是因为网络是一个不完美的模型)。凭直觉，我们会期望网络给予好的笔迹更高的概率，因为它往往比糟糕的笔迹更平滑、更可预测。因为每个输出的概率取决于所有先前的输出。作者将概率偏差b定义为大于或等于零的实数。</p><p id="ae9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在从Pr(xt+1|yt)中抽取样本之前，高斯混合中的每个标准差和混合权重被重新计算为</p><div class="lo lp lq lr fd ab cb"><figure class="mf ls nl mh mi mj mk paragraph-image"><img src="../Images/caab22d27ab4b4b6e7c686b6dc5af3a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*MT5Oh9yswTmCIfOeSpigzg.png"/></figure><figure class="mf ls nm mh mi mj mk paragraph-image"><img src="../Images/8680ed9b334ca448ec429192a144cdf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*LO29oHOhxAq81w6uukU5_A.png"/><figcaption class="lz ma et er es mb mc bd b be z dx nn di no nk">Biased standard deviation and mixture weights</figcaption></figure></div><p id="f94e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当b = 0时，恢复无偏采样，并且当b → ∞时，采样中的方差消失，网络总是输出混合物中最可能组分的模式。</p><div class="lo lp lq lr fd ab cb"><figure class="mf ls np mh mi mj mk paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><img src="../Images/6da064c41763b9b4c65203be42583c72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*LoLSgutZrlq8kW7L3q84hg.png"/></div></figure><figure class="mf ls nq mh mi mj mk paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><img src="../Images/a5f60aa50b84cc543b18b6a8ff152449.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*MLHMFAJ48OMGwdWApdfVFg.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx nr di ns nk"><strong class="bd jt">A slight bias samples.</strong> The top line in each block is real. The rest are samples from the synthesis network with a probability <strong class="bd jt">bias of 0.15</strong>, which seems to give a good balance between diversity and legibility.</figcaption></figure></div><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es nt"><img src="../Images/8e43d45d70df83b3c3dcc2510d41895b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hUtPwIle9H1MTbCN0s4RDw.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx"><strong class="bd jt">Samples biased towards higher probability.</strong> The probability biases b are shown at the left. As the bias increases the diversity decreases and the samples tend towards a kind of ‘average handwriting’ which is extremely regular and easy to read (easier, in fact, than most of the real handwriting in the training set). Note that even when the variance disappears, the same letter is not written the same way at different points in a sequence (for examples the ‘e’s in “exactly the same”, the ‘l’s in “until they all look”), because the predictions are still influenced by the previous outputs. If you look closely you can see that the last three lines are not quite exactly the same.</figcaption></figure><p id="d8d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="na"> Primed Sampling: <br/> </em>为了生成特定书写者风格的手写文本，一种方法是仅在该书写者文本上重新训练模型。无需重新训练，通过用真实序列“启动”网络，然后用仍然在网络存储器中的真实序列生成扩展，就可以模仿特定的风格。这可以通过将字符序列设置为<strong class="ih hj"> c` = c + s </strong>并将数据输入箝位到<strong class="ih hj"> x </strong>达第一个T时间步，然后照常采样直到序列结束，来为实数<strong class="ih hj"> x </strong>、<strong class="ih hj"> c </strong>(一个热码编码向量)和合成字符串<strong class="ih hj"> s </strong>实现。启动工作证明，网络能够记住序列中较早识别的文体特征。与网络从未见过的序列相比，这种技术似乎对训练数据中的序列更有效。</p><div class="lo lp lq lr fd ab cb"><figure class="mf ls nu mh mi mj mk paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><img src="../Images/c455bad3d9d92b6a08d91bc01bff45e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*yPT78Huqr5E9wjgzZrClVA.png"/></div></figure><figure class="mf ls nv mh mi mj mk paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><img src="../Images/13907aefc532dcee21f60f6857ecdf30.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*CNfZ31tcGCddsxPN__PaZQ.png"/></div><figcaption class="lz ma et er es mb mc bd b be z dx nw di nx nk"><strong class="bd jt">Samples primed with real sequences and biased towards higher probability.</strong> The priming sequences are at the top of the blocks. The <strong class="bd jt">probability bias was 1</strong>. None of the lines in the sampled text exist in the training set.</figcaption></figure></div><p id="df03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="na">(所有图片均取自官方文件，其链接在参考资料部分给出)</em></p></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><p id="9320" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我用偏差值为10的合成网络生成了下面的样本图像，其中包含随机生成的文本。</p><div class="lo lp lq lr fd ab cb"><figure class="mf ls ny mh mi mj mk paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><img src="../Images/835982e9a627b7cab1371e5c74dc915c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*1SGAQZTaQOlJXvgUN4Xutw.jpeg"/></div></figure><figure class="mf ls nz mh mi mj mk paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><img src="../Images/4f8490a3c20376d824f9b10d9d88fab0.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*h408V4CtZxEwmLeqPptcaw.jpeg"/></div><figcaption class="lz ma et er es mb mc bd b be z dx oa di ob nk"><strong class="bd jt">Left Side: Computer Text, Right Side: Handwritten Generated Text. </strong>Both have the same random generated text.</figcaption></figure></div><h2 id="6147" class="jr js hi bd jt ju jv jw jx jy jz ka kb iq kc kd ke iu kf kg kh iy ki kj kk kl bi translated">参考资料:</h2><ul class=""><li id="2bdb" class="jd je hi ih b ii km im kn iq oc iu od iy oe jc ji jj jk jl bi translated"><a class="ae mu" href="https://arxiv.org/abs/1308.0850" rel="noopener ugc nofollow" target="_blank">【1308.0850】用递归神经网络生成序列(arxiv.org)</a></li><li id="efa3" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">grz ego/手写生成:在tensorflow中使用递归神经网络实现手写生成。基于亚历克斯·格雷夫斯的论文(github.com)</li><li id="48ef" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><a class="ae mu" href="https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca" rel="noopener" target="_blank">混合密度网络搭便车指南Oliver Borchers博士|走向数据科学</a></li></ul></div></div>    
</body>
</html>
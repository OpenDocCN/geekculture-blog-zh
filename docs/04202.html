<html>
<head>
<title>An AI’s Guide To Utilitarianism</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">艾的功利主义指南</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/an-ais-guide-to-utilitarianism-485572f91a1?source=collection_archive---------57-----------------------#2021-06-22">https://medium.com/geekculture/an-ais-guide-to-utilitarianism-485572f91a1?source=collection_archive---------57-----------------------#2021-06-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="885b" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">造福人类:超级智能人工智能指南</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/47244b06acec43fc8c0fef91a8583ec3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FMWjDTd4PqXxReKW"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Photo by <a class="ae jn" href="https://unsplash.com/@askkell?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Andy Kelly</a> on <a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="6c07" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在他开创性的<a class="ae jn" href="https://people.eecs.berkeley.edu/~russell/papers/mi19book-hcai.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>、<em class="kk">与人类兼容的人工智能</em>中，Stuart Russell将“超智能人工智能”视为人类的生存风险。罗素解释说，其中一个原因是很难为一台比我们更聪明的机器定义一系列目标，从而产生“对人类有益的结果”。</p><p id="936d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">1960年，美国数学家兼哲学家诺伯特·维纳<a class="ae jn" href="https://science.sciencemag.org/content/131/3410/1355" rel="noopener ugc nofollow" target="_blank">写下了</a>，</p><blockquote class="kl km kn"><p id="bbba" class="jo jp kk jq b jr js ij jt ju jv im jw ko jy jz ka kp kc kd ke kq kg kh ki kj hb bi translated">“如果我们使用一个我们无法有效干预其运作的机械机构来达到我们的目的。。。我们最好能确信输入机器的目的就是我们真正想要的目的。”</p></blockquote><p id="3189" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在道格拉斯·亚当的<em class="kk">银河系漫游指南中，</em>超级计算机Deep think工作了750万年，计算出42是生命、宇宙和一切终极问题的答案。在这种情况下，糟糕的教学只会导致巨大的时间浪费。但是想象一下，如果有足够的自主权，一个被误导的超级智能人工智能会造成什么样的损害。</p><p id="643e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">挑战是，我们如何知道比我们更聪明的东西要做什么？它将如何按照我们赋予它的目的行动？</p><p id="b5a1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了负责任地创造这样的人工智能，我们不仅要考虑它要做什么是正确的(或道德的)事情，还要确保人工智能造福人类。我们还必须考虑，我们应该如何对这种本质上是算法的东西进行编程或教授，但最重要的是，它将拥有超越人类的能力。</p><h1 id="c4ea" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">定义功利主义</h1><p id="bb9f" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">罗素和维纳在上述引文中分别使用的有益结果和预期目的之间的区别非常重要。</p><p id="a069" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在任何关于道德伦理的介绍中，你通常会学到两类关键的伦理思想:</p><ul class=""><li id="c417" class="lo lp hi jq b jr js ju jv jx lq kb lr kf ls kj lt lu lv lw bi translated">结果主义者——一个行为是否会产生预期的结果</li><li id="b3f2" class="lo lp hi jq b jr lx ju ly jx lz kb ma kf mb kj lt lu lv lw bi translated">义务论——一个行为本身是否道德</li></ul><p id="9026" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因此，确保有益的结果可以被认为是一种功利主义的方法。如果目的是为了产生有益的结果，那么实现某些期望的目的也确实是功利的。但是，如果目的是履行道德行为，它同样可以是义务论的。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mc"><img src="../Images/0a9d8dfcbce522ff7c2f7d98691fc24d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eB36r8TukN30wMnZ"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Photo by <a class="ae jn" href="https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Tingey Injury Law Firm</a> on <a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4c19" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在第一种情况下，义务论伦理似乎是一个更简单的人工智能解决方案。与功利主义伦理学不同，它不要求对结果不同的每一种情况都做出独特的决定。规则可以简单地为每个行为进行硬编码。</p><p id="70e8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">事实上，许多哲学家认为义务论框架，如康德伦理学，为人工智能提供了“与实现道德行为相关并适用的实用哲学”。然而，当我们考虑到这涉及到明确考虑人工智能可能执行的每一个可能行为的道德性时，它就变得不那么实际了。</p><p id="ac90" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">另一种选择是允许人工智能自己决定行为是否道德。然而，即使是积极讨论道德人工智能的义务论框架的作者也<a class="ae jn" href="https://static1.squarespace.com/static/592ee286d482e908d35b8494/t/5c49c2ca0e2e728dff00ab2b/1548337867931/p130-hooker.pdf" rel="noopener ugc nofollow" target="_blank">说</a></p><blockquote class="kl km kn"><p id="b2b7" class="jo jp kk jq b jr js ij jt ju jv im jw ko jy jz ka kp kc kd ke kq kg kh ki kj hb bi translated"><em class="hi">在目前的研究阶段，伦理原则的应用仍然是人类计划</em>的任务。</p></blockquote><p id="63f2" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">既然如此，也许结果主义是一个更实际的方法。人工智能不需要定义一套严格的规则，只需要对可能的结果进行一些测量。这类似于一个标准的机器学习框架，其中一个算法被给予一个目标变量和度量，通过它来测量它在预测这一点上的性能。</p><p id="14cd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">然而，像义务论一样，这提出了许多实际问题。第一个问题是，什么被认为是积极的结果？在这篇文章中，我们将考虑功利主义，最流行的结果主义理论，经常被描述为“最大多数人的最大幸福”。</p><h1 id="1c52" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">最大的幸福…</h1><p id="028f" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">最常见的是，18世纪的哲学家杰里米·边沁被认为是功利主义之父。在他的<a class="ae jn" href="https://en.wikipedia.org/wiki/An_Introduction_to_the_Principles_of_Morals_and_Legislation" rel="noopener ugc nofollow" target="_blank">书</a>，<em class="kk">道德和立法原则介绍</em>中，他讨论了效用最大化，他将其定义为，</p><blockquote class="kl km kn"><p id="b480" class="jo jp kk jq b jr js ij jt ju jv im jw ko jy jz ka kp kc kd ke kq kg kh ki kj hb bi translated">“……这一原则根据似乎会增加或减少利益受到质疑的一方的幸福的趋势，批准或不批准任何行动……”</p></blockquote><p id="1077" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">然而，对于一个必须用精确、可量化的术语来看待世界的人工智能来说，“幸福”这个词有些模糊。所以我们必须从问以下两个问题开始:</p><ol class=""><li id="2d96" class="lo lp hi jq b jr js ju jv jx lq kb lr kf ls kj md lu lv lw bi translated">什么是幸福？</li><li id="8f16" class="lo lp hi jq b jr lx ju ly jx lz kb ma kf mb kj md lu lv lw bi translated">我们如何衡量幸福？</li></ol><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es me"><img src="../Images/0c92565b49d98466c024f6e74ef69f99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dhCNnHJp3XUV9CgJ"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Photo by <a class="ae jn" href="https://unsplash.com/@szilviabasso?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Szilvia Basso</a> on <a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="a844" class="mf ks hi bd kt mg mh mi kx mj mk ml lb jx mm mn ld kb mo mp lf kf mq mr lh ms bi translated">什么是幸福？</h2><p id="e9b5" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">在1956年出版的《开放社会及其敌人》一书中，卡尔·波普尔使用了“消极功利主义”一词来描述最小化人类痛苦而不是最大化快乐的概念。</p><p id="2b45" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在对波普尔的回应中，R. N. Smart提出了一个著名的观点:一个拥有无痛杀死其臣民手段的统治者有义务这样做，以将未来可能的痛苦最小化。</p><p id="f69c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">尽管人们不一定会得出快乐最大化等同于痛苦最小化的结论，但它说明了为人工智能提供不明确目标结果的危险。考虑到将痛苦最小化的任务，一个超级智能的人工智能可能会继续寻找消灭人类的最无痛的方法。</p><p id="1c71" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">但即使在更传统的意义上，也很难定义幸福。然而，<a class="ae jn" href="https://dictionary.cambridge.org/dictionary/english/measure" rel="noopener ugc nofollow" target="_blank">根据</a>剑桥词典，measure就是“定义某物的确切大小或数量”。没有必要定义一些东西来衡量它。所以，也许我们可以忽略“什么是幸福”这个哲学问题而是定义一个可衡量的目标结果。</p><h2 id="6f91" class="mf ks hi bd kt mg mh mi kx mj mk ml lb jx mm mn ld kb mo mp lf kf mq mr lh ms bi translated">我们如何衡量幸福？</h2><p id="d754" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">在古典经济学中，人类被认为是<a class="ae jn" href="https://en.wikipedia.org/wiki/Rational_agent" rel="noopener ugc nofollow" target="_blank">理性代理人</a>。也就是说，他们会追求具有最佳预期结果的行为。如果我们假设，一般来说，人类想要快乐，那么我们可以得出结论，最佳的预期结果是，从整体意义上说，使我们最快乐的结果。</p><p id="24e0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因此，通过观察人类，超级智能的人工智能可以收集数据，了解什么行为让我们快乐。这类似于任何其他机器学习模型从历史数据中学习的方式。给定足够的数据，人工智能将学习应该做什么来最大化幸福。</p><p id="41c7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">看到莎莉上次去牛排店见朋友吃晚餐时选择了鲑鱼，人工智能可以了解到避免吃肉会让莎莉快乐，也许当她购物时，应该避免肉类和家禽。</p><p id="fadc" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">当然，这里有很多假设。<a class="ae jn" href="https://en.wikipedia.org/wiki/Behavioral_economics#:~:text=Behavioral%20economics%20(also%2C%20behavioural%20economics,implied%20by%20classical%20economic%20theory." rel="noopener ugc nofollow" target="_blank">行为经济学</a>描述了认知偏差的概念，即人类在处理信息时所犯的系统性错误。这基本上否定了理性代理人假设。</p><p id="e94c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">除此之外，一个重要的假设是，最佳的预期结果会带来最大的幸福。例如，有人可能会优先考虑结果的经济影响，但许多人会认为“金钱买不到幸福”。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mt"><img src="../Images/a0e388bbac71d28b32ac8704402f9734.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YdixjiQeon-MxYHr"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Photo by <a class="ae jn" href="https://unsplash.com/@acharki95?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Aziz Acharki</a> on <a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="17ee" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">但我们能指望AI比我们了解自己更了解我们吗？如果我们自己不总是遵循最大幸福的道路，我们能指望人工智能做到这一点吗？它只能从我们的例子中学习，从它通过观察收集的历史数据中可以看出！</p><p id="0cba" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">一种选择是让人工智能自己决定如何最大化我们的幸福。根据超智能人工智能的定义，我们正在讨论一个比我们更智能的实体。也许它会找到一种我们没有想到的衡量我们幸福的方法？然而，这里的危险是，提供这种程度的自主权可能会导致类似于消极功利主义的灾难性结果。我们只是不知道人工智能会做什么！</p><h1 id="0369" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">…为了最大的数量</h1><p id="cacf" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">我们可以看到，创造一个最大化效用的人工智能并不是一项简单的任务。然而，让我们假设存在一个AI，它可以定量和准确地预测一个行为引起的个体效用的变化。我们叫它艾伦。</p><h2 id="3e82" class="mf ks hi bd kt mg mh mi kx mj mk ml lb jx mm mn ld kb mo mp lf kf mq mr lh ms bi translated">个人人工智能</h2><p id="82e0" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">艾伦被装上一个高科技机器人，并被赋予完全的自主权，以便为它的主人莎莉服务。艾伦在家里很棒，总是选择正确的行为来最大限度地发挥莎莉的效用。然后有一天，莎莉决定带艾伦出去帮忙购物…</p><blockquote class="mu"><p id="0eef" class="mv mw hi bd mx my mz na nb nc nd kj dx translated">在超市的时候，艾伦发现一个孩子渴望地盯着一个玩具。他拿出萨莉的钱包，递给孩子一张20美元的钞票。Sally不以为然地看着Alan，Alan解释说，孩子在收到玩具后增加的效用超过了她损失20美元时减少的效用。</p></blockquote><figure class="ne nf ng nh ni jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mt"><img src="../Images/8e38393782bb82f2188f45608a84becd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xDvZdDxEkE3jqNUN"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Photo by <a class="ae jn" href="https://unsplash.com/@gooner?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Vitaly Taranov</a> on <a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="895a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">一个纯粹的功利主义者可能会说，如果这能增加整体效用，那么这就是道德的事情。然而，为什么有人想要拥有一个做出他们不同意的决定的人工智能呢？这凸显了从涉及一个人的效用计算转向涉及两个人的效用计算的复杂性，尤其是在涉及所有权的情况下。</p><p id="9266" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">艾伦可以被编程，通过总是将它的主人莎莉的效用乘以某个常数因子，使她的效用优先于其他人。但是，这个值应该是什么呢？逛完商店后，莎莉和艾伦回家…</p><blockquote class="mu"><p id="c7bd" class="mv mw hi bd mx my mz na nb nc nd kj dx translated">艾伦发现一辆迎面而来的汽车即将撞上一个人，这个人上周偷了莎莉的手机，差点让她住院！最重要的是，为了让他们免受迎面而来的车流的伤害，艾伦不得不扔掉两打鸡蛋，这些鸡蛋是他为莎莉今晚的晚宴上准备的蛋白酥饼准备的。</p></blockquote><figure class="ne nf ng nh ni jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nk"><img src="../Images/85324f3f8b962b3583487ba2e79ee998.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*X0kMKkwagEzQat-V"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Photo by <a class="ae jn" href="https://unsplash.com/@michaeljinphoto?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Michael Jin</a> on <a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="9538" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在这种情况下，大多数人会建议艾伦把这个人从严重的伤害和痛苦中解救出来。然而，根据莎莉被优先考虑的程度，她的效用增加(乘以某个优先因子)可能超过汽车路径中个人效用的减少。</p><p id="3769" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因此，在多个人类与超级智能人工智能交互并拥有它们的情况下，需要进行大量思考来定义人工智能应该在多大程度上优先考虑它们的主人。</p><h2 id="f12b" class="mf ks hi bd kt mg mh mi kx mj mk ml lb jx mm mn ld kb mo mp lf kf mq mr lh ms bi translated">公共人工智能</h2><p id="d52c" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">我们已经看到，私有的超智能人工智能存在几个问题。AI可能会做出导致整体效用显著下降的决定，例如决定不从迎面而来的交通中拯救某人。在另一个极端，如果人工智能做出的决定对他人比对自己更有利，为什么有人会选择投资于自己的超智能人工智能？这类似于经济学中的<a class="ae jn" href="https://en.wikipedia.org/wiki/Free-rider_problem" rel="noopener ugc nofollow" target="_blank">搭便车问题</a>。</p><p id="6eaf" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">一个解决方案是创造政府资助的人工智能机器人，这些机器人在无人的情况下在世界各地漫游，实现效用最大化。然而，如果我们调整上面的场景，这个解决方案仍然存在复杂性…</p><blockquote class="mu"><p id="eed2" class="mv mw hi bd mx my mz na nb nc nd kj dx translated">莎莉有虐待倾向。如果她被车撞了，她从看着自己被车撞中获得的效用将会超过那个人因疼痛和痛苦而导致的效用下降。</p></blockquote><p id="d675" class="pw-post-body-paragraph jo jp hi jq b jr nl ij jt ju nm im jw jx nn jz ka kb no kd ke kf np kh ki kj hb bi translated">在这种情况下，即使没有任何优惠待遇，效用最大化的结果似乎也不是道德的。就像莎莉的20美元纸币一样，有人会说纯粹功利主义者会说这是道德行为。然而，同样可以认为莎莉的虐待狂倾向意味着比较他们两个的效用是不公平或不公正的。</p><p id="e98f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这个结果可能不太好。然而，人们可以说服自己艾伦实际上并没有<em class="kk">做错任何事情。他只是被动的，避免做一件会导致整体效用稍微降低的事情。</em></p><p id="44a5" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">对此，请考虑以下场景…</p><blockquote class="mu"><p id="b75e" class="mv mw hi bd mx my mz na nb nc nd kj dx translated">莎莉喜欢观察别人的痛苦。艾伦可以通过绑架别人并在她面前折磨他们来最大化整体效用。</p></blockquote><p id="6a1e" class="pw-post-body-paragraph jo jp hi jq b jr nl ij jt ju nm im jw jx nn jz ka kb no kd ke kf np kh ki kj hb bi translated">在这种情况下，很难说纯粹的功利主义方法导致了最佳结果。虽然这是一个极端的情况，但它提出了一些严重的疑问，即在一个人们的效用彼此不一致的世界中，功利主义方法对人工智能决策的有效性。</p><h1 id="19d2" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">最后的想法</h1><p id="d1af" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">结果主义理论非常受欢迎<a class="ae jn" href="https://plato.stanford.edu/entries/consequentialism/" rel="noopener ugc nofollow" target="_blank"/>，因为人们经常通过他们行为的后果看到他们行为的价值；他们和其他人如何感受和体验他们的生活。就最佳结果而言，幸福极具吸引力，导致了功利主义的流行。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nq"><img src="../Images/e74e1eaec9c1daf4ad1de68552e83cfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QSN-WXZyHHpPBHx_"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Photo by <a class="ae jn" href="https://unsplash.com/@bradyn?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Bradyn Trollip</a> on <a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="90e4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">然而，我们已经看到，创造一个可以在功利主义框架内运作的人工智能是一个重大的挑战。数字计算的本质要求在不涉及任何人类直觉或道德判断的情况下，对效用进行定量测量。但是效用很难定义，更不用说衡量了。然后是将功利主义人工智能带入现实世界的实用性，在现实世界中，效用计算必须考虑到所有人类！</p><p id="1e41" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">尽管这些讨论已经渗透了几个世纪的功利主义思想，但当想象超级智能人工智能可能给人类带来的潜在好的和坏的影响时，它们变得更加突出。</p><p id="b2f6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如果我们要负责任地创造超智能人工智能，我们可能需要建立一个新的道德框架，不仅可以解决这些复杂性，还可以将它们应用于本质上是一种算法的东西，只接受1和0！</p></div></div>    
</body>
</html>
<html>
<head>
<title>Machine Learning Tutorial — Feature Engineering and Feature Selection for Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习教程—面向初学者的特征工程和特征选择</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/machine-learning-tutorial-feature-engineering-and-feature-selection-for-beginners-dd15b9d354?source=collection_archive---------8-----------------------#2021-04-22">https://medium.com/geekculture/machine-learning-tutorial-feature-engineering-and-feature-selection-for-beginners-dd15b9d354?source=collection_archive---------8-----------------------#2021-04-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/4e782c2078ce2126fcf617f641164041.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BACPlBw3E51--CtCLwCeVw.jpeg"/></div></div></figure><p id="7f39" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">他们说<strong class="is hj">数据</strong>是新的<strong class="is hj">石油</strong>，但是我们并不直接使用其来源的石油。在我们将它用于不同目的之前，它必须经过处理和清洗。<br/>这同样适用于数据，我们不直接从它的来源使用它。它也必须被处理。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jo"><img src="../Images/1dfc5f21025ff3b677cf3f15cfb631ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cv08u1lesClo6YR5RC6yAw.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx">Oil industry</figcaption></figure><p id="7a93" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于机器学习和数据科学的初学者来说，这可能是一个挑战，因为数据来自不同的来源，具有不同的数据类型。因此，您不能对不同类型的数据应用相同的清理和处理方法。</p><blockquote class="jx jy jz"><p id="13c1" class="iq ir ka is b it iu iv iw ix iy iz ja kb jc jd je kc jg jh ji kd jk jl jm jn hb bi translated"><em class="hi">“可以从数据中提取信息，就像可以从石油中提取能量一样。”- </em> <a class="ae ke" rel="noopener" href="/@adeolaadesina"> <em class="hi">阿德奥拉</em> </a></p></blockquote><p id="ea0a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你必须根据你所掌握的数据来学习和运用方法。然后你可以从中获得洞察力，或者用它来进行机器学习或者深度学习算法的训练。</p><p id="7079" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">看完这篇文章，你会知道:</p><ul class=""><li id="7abc" class="kf kg hi is b it iu ix iy jb kh jf ki jj kj jn kk kl km kn bi translated">什么是特征工程和特征选择。</li><li id="ebe8" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated">处理数据集中缺失数据的不同方法。</li><li id="dafe" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated">处理连续特征的不同方法。</li><li id="a9e5" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated">处理分类特征的不同方法。</li><li id="e35d" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated">不同的特征选择方法。</li></ul><p id="1189" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们开始吧。🚀</p><h1 id="56e1" class="kt ku hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">什么是特征工程？</h1><p id="d36c" class="pw-post-body-paragraph iq ir hi is b it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj lv jl jm jn hb bi translated">特征工程是指在使用机器学习创建<strong class="is hj">预测模型</strong>时，在您的数据集中选择并<strong class="is hj">转换</strong>变量/特征的过程。</p><p id="6e4f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，在用机器学习算法训练数据之前，您必须从您收集的<strong class="is hj">原始数据集</strong>中提取特征。<br/>否则，很难获得对数据的深刻见解。</p><blockquote class="lw"><p id="4c78" class="lx ly hi bd lz ma mb mc md me mf jn dx translated"><em class="mg">拷问数据，它什么都会招供。—罗纳德·科斯</em></p></blockquote><p id="3734" class="pw-post-body-paragraph iq ir hi is b it mh iv iw ix mi iz ja jb mj jd je jf mk jh ji jj ml jl jm jn hb bi translated">特征工程有两个目标:</p><ul class=""><li id="2278" class="kf kg hi is b it iu ix iy jb kh jf ki jj kj jn kk kl km kn bi translated">准备适当的输入数据集，与机器学习算法要求兼容。</li><li id="60e5" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated">提高机器学习模型的<strong class="is hj">性能</strong>。</li></ul><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/8cfdc954bbc4ea05cb0b0fd004436402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y-F67PNBFCb-jPh8yzw3Nw.jpeg"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx">CrowdFlower Survey</figcaption></figure><p id="30ba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">根据CrowdFlower对80名数据科学家的调查，数据科学家花费<strong class="is hj"> 60% </strong>的时间清理和组织数据。这就是为什么拥有特征工程和选择的技能是非常重要的。</p><blockquote class="lw"><p id="fe69" class="lx ly hi bd lz ma mb mc md me mf jn dx translated"><em class="mg">“归根结底，有些机器学习项目成功了，有些失败了。有什么区别？轻松最重要的因素是</em>的<strong class="ak">特性</strong>的使用华盛顿大学的Pedro Domingos教授</p></blockquote><p id="3e14" class="pw-post-body-paragraph iq ir hi is b it mh iv iw ix mi iz ja jb mj jd je jf mk jh ji jj ml jl jm jn hb bi translated">可以从以下链接阅读他的论文:<a class="ae ke" href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf" rel="noopener ugc nofollow" target="_blank">《关于机器学习需要知道的几件有用的事情</a>》。</p><p id="6cfc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">既然您知道了为什么需要学习不同的特征工程技术，那么让我们从学习处理缺失数据的不同方法开始。</p><h1 id="0861" class="kt ku hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">如何处理丢失的数据</h1><p id="b127" class="pw-post-body-paragraph iq ir hi is b it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj lv jl jm jn hb bi translated">处理缺失数据非常重要，因为许多机器学习算法不支持具有缺失值的数据。如果数据集中有缺失值，可能会导致一些机器学习算法出错和性能下降。</p><p id="6905" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">以下是您可以在数据集中找到的常见缺失值列表。</p><ul class=""><li id="7c40" class="kf kg hi is b it iu ix iy jb kh jf ki jj kj jn kk kl km kn bi translated">不适用的</li><li id="8e2b" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated">空</li><li id="0522" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated">空的</li><li id="118f" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi">?</li><li id="1e00" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated">没有人</li><li id="e8b5" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated">空的</li><li id="336b" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi">-</li><li id="6a64" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated">圆盘烤饼</li></ul><p id="f5d3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们学习不同的方法来解决缺失数据的问题。</p><h1 id="9bf8" class="kt ku hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">变量删除</h1><p id="0a98" class="pw-post-body-paragraph iq ir hi is b it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj lv jl jm jn hb bi translated">变量删除包括根据具体情况删除缺少值的变量(列)。当变量中有很多缺失值，并且变量相对不太重要时，这种方法是有意义的。</p><p id="30e7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">唯一值得删除变量的情况是当它的缺失值超过观察值的60%时。</p><pre class="jp jq jr js fd mn mo mp mq aw mr bi"><span id="f060" class="ms ku hi mo b fi mt mu l mv mw"># import packages<br/>import numpy as np <br/>import pandas as pd <br/><br/># read dataset <br/>data = pd.read_csv('path/to/data')<br/><br/>#set threshold<br/>threshold = 0.7<br/><br/># dropping columns with missing value rate higher than threshold<br/>data = data[data.columns[data.isnull().mean() &lt; threshold]]</span></pre><p id="b061" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上面的代码片段中，你可以看到我如何使用NumPy和pandas来加载数据集，并将阈值设置为<strong class="is hj"> 0.7 </strong>。这意味着任何缺失值超过观察值的<strong class="is hj"> 70% </strong>的列都将从数据集中删除。</p><p id="c968" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我建议您根据数据集的大小来设置阈值。</p><h1 id="d5cf" class="kt ku hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">均值或中位数插补</h1><p id="fe5c" class="pw-post-body-paragraph iq ir hi is b it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj lv jl jm jn hb bi translated">另一种常见的技术是使用非缺失观测值的平均值或中值。此策略可应用于具有数值数据的要素。</p><pre class="jp jq jr js fd mn mo mp mq aw mr bi"><span id="563f" class="ms ku hi mo b fi mt mu l mv mw"># filling missing values with medians of the columns<br/>data = data.fillna(data.median())</span></pre><p id="c4c0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上面的例子中，我们使用<strong class="is hj">中值方法</strong>来填充数据集中缺失的值。</p><h1 id="18aa" class="kt ku hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">最常见的值</h1><p id="febf" class="pw-post-body-paragraph iq ir hi is b it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj lv jl jm jn hb bi translated">该方法用列/特征中的<strong class="is hj">最大出现值</strong>替换缺失值。这是处理<strong class="is hj">分类</strong>列/特性的一个好选择。</p><pre class="jp jq jr js fd mn mo mp mq aw mr bi"><span id="31b1" class="ms ku hi mo b fi mt mu l mv mw"># filling missing values with medians of the columns<br/>data['column_name'].fillna(data['column_name'].value_counts().idxmax(). inplace=True)</span></pre><p id="7c3c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里我们使用pandas的<strong class="is hj"> value_counts() </strong>方法来计算列中每个唯一值的出现次数，然后用最常见的值填充缺失的值。</p><h1 id="dacf" class="kt ku hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">如何处理连续特征</h1><p id="2212" class="pw-post-body-paragraph iq ir hi is b it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj lv jl jm jn hb bi translated">数据集中的连续要素具有不同的值范围。连续特征的常见例子有年龄、工资、价格和身高。</p><p id="e420" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在训练机器学习算法之前，处理数据集中的连续特征非常重要。如果您使用不同范围的值来训练模型，模型将不会很好地执行。</p><p id="5ede" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当我说不同范围的值时，我指的是什么？假设您有一个数据集，它有两个连续的特征，<strong class="is hj">年龄，</strong>和<strong class="is hj">薪水</strong>。年龄的范围和薪水的范围是不同的，这可能会引起问题。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/6566d19ba9edfe11a51ba61772e80f42.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*9MhAQkavHf_LZOugupa1jg.jpeg"/></div></figure><p id="a421" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">以下是处理连续特征的一些常用方法:</p><h1 id="5cc4" class="kt ku hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">最小-最大归一化</h1><p id="7080" class="pw-post-body-paragraph iq ir hi is b it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj lv jl jm jn hb bi translated">对于要素中的每个值，最小-最大归一化会减去要素中的最小值，然后除以其范围。该范围是原始最大值和原始最小值之间的差值。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es my"><img src="../Images/c54af77a0443bb8808017ce53bef0d51.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*AM1RCvm8ZIi-NdOZjxQcKA.png"/></div></figure><p id="4d52" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，它缩放在<strong class="is hj"> 0 </strong>和<strong class="is hj"> 1之间的固定范围内的所有值。</strong></p><p id="634b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您可以使用Scikit-learn中的<a class="ae ke" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html" rel="noopener ugc nofollow" target="_blank"><strong class="is hj">minmax scaler</strong></a><strong class="is hj"/>方法，通过将每个要素缩放到给定范围来变换要素:</p><pre class="jp jq jr js fd mn mo mp mq aw mr bi"><span id="560c" class="ms ku hi mo b fi mt mu l mv mw">from sklearn.preprocessing import MinMaxScaler<br/>import numpy as np<br/><br/># 4 samples/observations and 2 variables/features<br/>data = np.array([[4, 6], [11, 34], [10, 17], [1, 5]])<br/><br/># create scaler method<br/>scaler = MinMaxScaler(feature_range=(0,1))<br/><br/># fit and transform the data<br/>scaled_data = scaler.fit_transform(data)<br/><br/>print(scaled_data)<br/><br/># [[0.3        0.03448276]<br/>#  [1.         1.        ] <br/>#  [0.9        0.4137931 ] <br/>#  [0.         0.        ]]</span></pre><p id="008e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如你所见，我们的数据已经被转换，范围在<strong class="is hj"> 0 </strong>和<strong class="is hj"> 1 </strong>之间。</p><h1 id="3e9b" class="kt ku hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">标准化</h1><p id="8485" class="pw-post-body-paragraph iq ir hi is b it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj lv jl jm jn hb bi translated">标准化确保每个特征都有一个平均值<strong class="is hj"> 0 </strong>和一个标准偏差<strong class="is hj"> 1 </strong>，使所有特征达到相同的量级。</p><p id="4552" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果特征的标准偏差是<strong class="is hj"> <em class="ka">不同</em> </strong>，它们的范围也会不同。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/d5c5d0d3a3d45abe3a93754a1769edd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*Wun-lRvIxTBD8ntrnNxufg.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx">x = observation, μ = mean , σ = standard deviation</figcaption></figure><p id="8464" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您可以使用Scikit-learn中的<a class="ae ke" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" rel="noopener ugc nofollow" target="_blank"><strong class="is hj">standard scaler</strong></a>方法，通过移除平均值并缩放至标准偏差<strong class="is hj"> 1 </strong>来标准化特征:</p><pre class="jp jq jr js fd mn mo mp mq aw mr bi"><span id="9768" class="ms ku hi mo b fi mt mu l mv mw">from sklearn.preprocessing import StandardScaler<br/>import numpy as np<br/><br/># 4 samples/observations and 2 variables/features<br/>data = np.array([[4, 1], [11, 1], [10, 4], [1, 11]])<br/><br/># create scaler method <br/>scaler = StandardScaler()<br/><br/># fit and transform the data<br/>scaled_data = scaler.fit_transform(data)<br/><br/>print(scaled_data)<br/><br/># [[-0.60192927 -0.79558708]<br/>#  [ 1.08347268 -0.79558708] <br/>#  [ 0.84270097 -0.06119901] <br/>#  [-1.32424438  1.65237317]]</span></pre><p id="a275" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们验证每个特征(列)的平均值是<strong class="is hj"> 0 </strong>:</p><pre class="jp jq jr js fd mn mo mp mq aw mr bi"><span id="0a39" class="ms ku hi mo b fi mt mu l mv mw">print(scaled_data.mean(axis=0))</span></pre><p id="9f10" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du na nb nc mo b">[0. 0.]</code></p><p id="1c75" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">并且每个特征(列)的标准偏差是<strong class="is hj"> 1 </strong>:</p><pre class="jp jq jr js fd mn mo mp mq aw mr bi"><span id="5948" class="ms ku hi mo b fi mt mu l mv mw">print(scaled_data.std(axis=0))</span></pre><p id="dd0b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du na nb nc mo b">[1. 1.]</code></p><h1 id="2446" class="kt ku hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">如何处理分类特征</h1><p id="2545" class="pw-post-body-paragraph iq ir hi is b it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj lv jl jm jn hb bi translated">分类特征表示可以分组的数据类型。比如性别，受教育程度。</p><p id="fbc6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">任何非数字值都需要<em class="ka">转换</em>为整数或浮点数，以便在大多数机器学习库中使用。</p><p id="2a51" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">处理分类特征的常用方法有:</p><h1 id="8dd7" class="kt ku hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">标签编码</h1><p id="a147" class="pw-post-body-paragraph iq ir hi is b it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj lv jl jm jn hb bi translated">标签编码只是将一列中的每个分类值转换成一个数字。</p><p id="0521" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">建议使用标签编码将其转换为二进制变量。</p><p id="ae61" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在以下示例中，您将学习如何使用Scikit-learn中的<a class="ae ke" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> LableEncoder </strong> </a>将分类值转换为二进制:</p><pre class="jp jq jr js fd mn mo mp mq aw mr bi"><span id="bd8a" class="ms ku hi mo b fi mt mu l mv mw"># import packages<br/>import numpy as np <br/>import pandas as pd <br/>from sklearn.preprocessing import LabelEncoder<br/><br/># intialise data of lists.<br/>data = {'Gender':['male', 'female', 'female', 'male','male'],<br/>        'Country':['Tanzania','Kenya', 'Tanzania', 'Tanzania','Kenya']}<br/>  <br/># Create DataFrame<br/>data = pd.DataFrame(data)<br/><br/><br/># create label encoder object<br/>le = LabelEncoder()<br/>  <br/>data['Gender']= le.fit_transform(data['Gender'])<br/>data['Country']= le.fit_transform(data['Country'])<br/><br/>print(data)</span></pre><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/20f86595cd15c9ee2cc460f24ea1c06f.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*aABVlK1mLDTQlnv2Wv6vWw.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx">Transformed Data</figcaption></figure><h1 id="eb73" class="kt ku hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">一次热编码</h1><p id="879a" class="pw-post-body-paragraph iq ir hi is b it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj lv jl jm jn hb bi translated">到目前为止，表示分类变量最常见的方法是使用一键编码，或N选1编码方法，也称为哑变量。</p><p id="9700" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">虚拟变量背后的思想是用一个或多个值为0和1的新特征替换分类变量。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es ne"><img src="../Images/08593087dcc9962221e45ab4df772f51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*xrF3FE3o_QaUl-fOL27tOA.png"/></div></figure><p id="2e1b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在下面的例子中，我们将使用Scikit-learn库中的编码器。<a class="ae ke" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> LabelEncoder </strong> </a>将帮助我们从数据中创建标签的整数编码，而<a class="ae ke" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> OneHotEncoder </strong> </a>将创建整数编码值的一次性编码。</p><pre class="jp jq jr js fd mn mo mp mq aw mr bi"><span id="ccf5" class="ms ku hi mo b fi mt mu l mv mw"># import packages <br/>import numpy as np <br/>from sklearn.preprocessing import OneHotEncoder, LabelEncoder<br/><br/><br/># define example<br/>data = np.array(['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot'])<br/><br/># integer encode<br/>label_encoder = LabelEncoder()<br/><br/>#fit and transform the data<br/>integer_encoded = label_encoder.fit_transform(data)<br/>print(integer_encoded)<br/><br/># one-hot encode<br/>onehot_encoder = OneHotEncoder(sparse=False)<br/><br/>#reshape the data<br/>integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)<br/><br/>#fit and transform the data<br/>onehot_encoded = onehot_encoder.fit_transform(integer_encoded)<br/><br/>print(onehot_encoded)</span></pre><p id="38de" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是<strong class="is hj"> LabelEncoder </strong>方法对<strong class="is hj"> integer_encoded </strong>的输出:</p><p id="92d9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du na nb nc mo b">[0 0 2 0 1 1 2 0 2 1]</code></p><p id="1a88" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是<strong class="is hj">onehotencoded</strong>通过<strong class="is hj"> OneHotEncoder </strong>方法的输出:</p><pre class="jp jq jr js fd mn mo mp mq aw mr bi"><span id="ead6" class="ms ku hi mo b fi mt mu l mv mw">[[1. 0. 0.] <br/> [1. 0. 0.] <br/> [0. 0. 1.] <br/> [1. 0. 0.] <br/> [0. 1. 0.] <br/> [0. 1. 0.] <br/> [0. 0. 1.] <br/> [1. 0. 0.] <br/> [0. 0. 1.] <br/> [0. 1. 0.]]</span></pre><h1 id="98a8" class="kt ku hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">什么是特征选择？</h1><p id="cce6" class="pw-post-body-paragraph iq ir hi is b it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj lv jl jm jn hb bi translated">要素选择是自动或手动选择对预测变量或输出贡献最大的要素的过程。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nf"><img src="../Images/54a7511cf45956839503301e3faceed5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xb5S5ECpRyJfs6EJHhUCJA.jpeg"/></div></div></figure><p id="053f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">数据中存在不相关的特征会降低机器学习模型的准确性。</p><p id="38cd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用特征选择的主要原因是:</p><ul class=""><li id="fdb2" class="kf kg hi is b it iu ix iy jb kh jf ki jj kj jn kk kl km kn bi translated">它使机器学习算法能够更快地训练。</li><li id="645c" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated">它降低了模型的复杂性，使其更容易解释。</li><li id="57d9" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated">如果选择了正确的子集，就可以提高模型的准确性。</li><li id="744b" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated">它减少了过度拟合。</li></ul><blockquote class="jx jy jz"><p id="b3c8" class="iq ir ka is b it iu iv iw ix iy iz ja kb jc jd je kc jg jh ji kd jk jl jm jn hb bi translated"><em class="hi">“我通过选择所有特征准备了一个模型，我得到了大约</em><strong class="is hj"><em class="hi">65%</em></strong><em class="hi">的准确度，这对于预测模型来说不是很好，在做了一些特征选择和特征工程后，没有对我的模型代码做任何逻辑更改，我的准确度跃升到了</em><strong class="is hj"><em class="hi"/></strong><em class="hi">81%<em class="hi">，这非常令人印象深刻”——作者Raheel Shaikh </em></em></p></blockquote><p id="a8b1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">特征选择的常用方法有:</p><h1 id="709a" class="kt ku hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">单变量选择</h1><p id="847d" class="pw-post-body-paragraph iq ir hi is b it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj lv jl jm jn hb bi translated">统计测试有助于选择与数据集中的目标要素关系最密切的独立要素。比如卡方检验。</p><p id="756d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Scikit-learn库提供了<a class="ae ke" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> SelectKBest </strong> </a>类，该类可用于一套不同的统计测试，以选择特定数量的特性。</p><p id="e732" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在下面的例子中，我们使用带有卡氏检验的<strong class="is hj"> SelectKBest </strong>类来寻找虹膜数据集的最佳特征:</p><pre class="jp jq jr js fd mn mo mp mq aw mr bi"><span id="eafb" class="ms ku hi mo b fi mt mu l mv mw"># Load packages<br/>from sklearn.datasets import load_iris<br/>from sklearn.feature_selection import SelectKBest<br/>from sklearn.feature_selection import chi2<br/>  <br/># Load iris data<br/>iris_dataset = load_iris()<br/>  <br/># Create features and target<br/>X = iris_dataset.data<br/>y = iris_dataset.target<br/>  <br/># Convert to categorical data by converting data to integers<br/>X = X.astype(int)<br/>  <br/># Two features with highest chi-squared statistics are selected<br/>chi2_features = SelectKBest(chi2, k = 2)<br/>X_kbest_features = chi2_features.fit_transform(X, y)<br/>  <br/># Reduced features<br/>print('Original feature number:', X.shape[1])<br/>print('Reduced feature number:', X_kbest_features.shape[1])</span></pre><p id="a291" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">原始特征号:4 <br/>减少的特征号:2</p><p id="dbb2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如您所见，卡方检验帮助我们从最初的4个特征中选择出与目标特征关系最密切的两个重要的独立特征。</p><p id="47e0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可以在这里了解更多关于卡方检验的信息:<a class="ae ke" href="https://machinelearningmastery.com/chi-squared-test-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">“机器学习卡方检验的温和介绍</a>”。</p><h1 id="a20f" class="kt ku hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">特征重要性</h1><p id="252c" class="pw-post-body-paragraph iq ir hi is b it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj lv jl jm jn hb bi translated">要素重要性为数据的每个要素提供了一个分数。分数越高，<strong class="is hj">越重要</strong>或<strong class="is hj">相关</strong>该功能对你的目标功能越重要。</p><p id="7879" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">特征重要性是一个内置类，带有基于树的分类器，例如:</p><ul class=""><li id="7537" class="kf kg hi is b it iu ix iy jb kh jf ki jj kj jn kk kl km kn bi translated">随机森林分类器</li><li id="1897" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated">额外树分类器</li></ul><p id="c7ec" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在下面的例子中，我们将把额外的树分类器训练到iris数据集中，并使用内置的类<strong class="is hj">。feature_importances_ </strong>计算每个特性的重要性:</p><pre class="jp jq jr js fd mn mo mp mq aw mr bi"><span id="91d2" class="ms ku hi mo b fi mt mu l mv mw"># Load libraries<br/>from sklearn.datasets import load_iris<br/>import matplotlib.pyplot as plt<br/>from sklearn.ensemble import ExtraTreesClassifier<br/><br/># Load iris data<br/>iris_dataset = load_iris()<br/>  <br/># Create features and target<br/>X = iris_dataset.data<br/>y = iris_dataset.target<br/>  <br/># Convert to categorical data by converting data to integers<br/>X = X.astype(int)<br/> <br/> # Building the model<br/>extra_tree_forest = ExtraTreesClassifier(n_estimators = 5,<br/>                                        criterion ='entropy', max_features = 2)<br/>  <br/># Training the model<br/>extra_tree_forest.fit(X, y)<br/>  <br/># Computing the importance of each feature<br/>feature_importance = extra_tree_forest.feature_importances_<br/>  <br/># Normalizing the individual importances<br/>feature_importance_normalized = np.std([tree.feature_importances_ for tree in <br/>                                        extra_tree_forest.estimators_],<br/>                                        axis = 0)<br/><br/># Plotting a Bar Graph to compare the models<br/>plt.bar(iris_dataset.feature_names, feature_importance_normalized)<br/>plt.xlabel('Feature Labels')<br/>plt.ylabel('Feature Importances')<br/>plt.title('Comparison of different Feature Importances')<br/>plt.show()</span></pre><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/45f50c9816d73bafe2dd4cce9bcda246.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*TAv6oI61LcqBZsdxNu3h-Q.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx">Important features</figcaption></figure><p id="f510" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上图显示最重要的特征是<strong class="is hj"><em class="ka"/></strong>和<strong class="is hj"> <em class="ka">花瓣宽度(cm) </em> </strong>，最不重要的特征是<strong class="is hj"> <em class="ka">萼片宽度(cms) </em> </strong>。这意味着您可以使用最重要的功能来训练您的模型并获得最佳性能。</p><h1 id="18b7" class="kt ku hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">相关矩阵热图</h1><p id="7087" class="pw-post-body-paragraph iq ir hi is b it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj lv jl jm jn hb bi translated">相关性显示了特征之间或与目标特征之间的关系。</p><p id="af32" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">相关性可以是正的(增加一个特征值会增加目标变量的值)，也可以是负的(增加一个特征值会减少目标变量的值)。</p><p id="fa4c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在以下示例中，我们将使用Scikit-learn库中的Boston house prices数据集和pandas中的<a class="ae ke" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html" rel="noopener ugc nofollow" target="_blank"><strong class="is hj">corr()</strong></a><strong class="is hj"/>方法来查找dataframe中所有要素的成对相关性:</p><pre class="jp jq jr js fd mn mo mp mq aw mr bi"><span id="9f89" class="ms ku hi mo b fi mt mu l mv mw"># Load libraries<br/>from sklearn.datasets import load_boston<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/><br/><br/># load boston data<br/>boston_dataset = load_boston()<br/><br/># create a daframe for boston data<br/>boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)<br/>  <br/># Convert to categorical data by converting data to integers<br/>#X = X.astype(int)<br/> <br/>#ploting the heatmap for correlation<br/>ax = sns.heatmap(boston.corr().round(2), annot=True)</span></pre><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es nh"><img src="../Images/8a87796a9f0f5e5aad390d9eeecdfe42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*kno4m2IcjDdykkP5a9rpRw.png"/></div></figure><p id="6852" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">相关系数范围从-1到1。如果该值接近1，则意味着这两个特征之间有很强的正相关性。当接近-1时，特征具有很强的负相关性。</p><p id="646d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上图中，可以看到<strong class="is hj">税</strong>和<strong class="is hj"> RAD </strong>特征具有s <em class="ka"> trong正相关</em>而<strong class="is hj"> DIS </strong>和<strong class="is hj"> NOX </strong>特征具有<em class="ka">强负相关。</em></p><p id="0e69" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果您发现数据集中有一些相互关联的要素，这意味着它们传达了相同的信息。建议去掉其中一个。</p><p id="5a17" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可以在这里阅读更多关于这方面的内容:<a class="ae ke" href="https://datascience.stackexchange.com/questions/24452/in-supervised-learning-why-is-it-bad-to-have-correlated-features" rel="noopener ugc nofollow" target="_blank">在监督学习中，为什么有关联的特征是不好的？</a></p><h1 id="785d" class="kt ku hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">结论</h1><p id="2404" class="pw-post-body-paragraph iq ir hi is b it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj lv jl jm jn hb bi translated">我在本文中解释的方法将帮助您准备大部分的<strong class="is hj">结构化数据集</strong>。但是如果您正在处理非结构化数据集，比如图像、文本和音频，您将不得不学习本文中没有解释的不同方法。</p><p id="8596" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">以下文章将帮助您了解如何为机器学习项目准备图像或文本数据集:</p><ul class=""><li id="c87f" class="kf kg hi is b it iu ix iy jb kh jf ki jj kj jn kk kl km kn bi translated"><a class="ae ke" href="https://machinelearningmastery.com/best-practices-for-preparing-and-augmenting-image-data-for-convolutional-neural-networks/" rel="noopener ugc nofollow" target="_blank">为CNN准备和扩充图像数据的最佳实践——杰森·布朗利</a></li><li id="cba1" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated"><a class="ae ke" href="https://towardsdatascience.com/image-pre-processing-c1aec0be3edf" rel="noopener" target="_blank">图像预处理——卡努玛王子</a></li><li id="138a" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated"><a class="ae ke" href="https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79" rel="noopener" target="_blank"> NLP文本预处理:实用指南和模板——翁家豪</a></li><li id="4cb7" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated"><a class="ae ke" href="https://www.freecodecamp.org/news/how-to-work-and-understand-text-based-dataset-with-texthero/" rel="noopener ugc nofollow" target="_blank">如何使用Texthero为您的NLP项目准备基于文本的数据集——Davis David</a></li></ul><p id="1cbb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">恭喜</strong>👏👏<strong class="is hj">，</strong>你已经熬到这篇文章的结尾了！我希望你学到了一些新的东西，对你的下一个机器学习或数据科学项目有所帮助。</p><p id="92ff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你学到了新的东西或者喜欢阅读这篇文章，请分享给其他人看。在那之前，下期帖子再见！</p><p id="7f99" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你也可以在Twitter上找到我<a class="ae ke" href="https://twitter.com/Davis_McDavid" rel="noopener ugc nofollow" target="_blank"> @Davis_McDavid </a>。</p><p id="8077" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">本文首发于<a class="ae ke" href="https://www.freecodecamp.org/news/feature-engineering-and-feature-selection-for-beginners/" rel="noopener ugc nofollow" target="_blank"> Freecodecamp。</a></p></div></div>    
</body>
</html>
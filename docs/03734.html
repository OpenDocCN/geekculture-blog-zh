<html>
<head>
<title>Understanding the Technical Architecture of GPT-3 and How it was Made</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解GPT-3的技术架构及其制造方法</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/how-the-best-english-professor-was-created-gpt-explained-23ac11df7503?source=collection_archive---------51-----------------------#2021-06-14">https://medium.com/geekculture/how-the-best-english-professor-was-created-gpt-explained-23ac11df7503?source=collection_archive---------51-----------------------#2021-06-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/021a01d47dfbea50dae36c33f7744d5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*N-R5kPDkAnu10mho.png"/></div></div></figure><p id="55aa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">深度学习正在彻底改变世界，无论是教计算机开车，还是甚至用它们来辅助医生！但是当谈到深度学习时，这一成功的最大因素是训练时大量的<strong class="is hj">标记数据</strong>。</p><p id="1e0a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，这仍然限制了与自然语言处理(NLP——人工智能的语言学方面)相关的许多领域的适用性。这主要是因为未标记的数据比标记的数据多得多。现在，注释这些未标记的数据可以解决这个问题，但这非常耗时且昂贵。</p><p id="2375" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">能够从未标记的数据中学习也被称为<strong class="is hj">无监督学习，</strong>并且能够以这种方式训练模型将会给它带来巨大的性能提升；然而，当试图通过这种方法创建模型时，存在许多挑战:</p><ol class=""><li id="e5fc" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">目前还不清楚哪种类型的文本表示优化器可以转移到其他领域。</li><li id="56c5" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">对于转移学习表征的最有效方式，还没有达成共识。</li><li id="d90a" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">即使找到了实现这一点的方法，该模型仍然需要对其架构进行许多更改，这违背了整个目的。</li></ol><p id="2e4f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以我们不可能这么做，对吗？….<strong class="is hj">错了。</strong></p><h1 id="d41c" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">介绍GPT(生成性预培训)</h1><p id="606e" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">GPT解决这个问题的方法是使用半监督学习(监督和非监督学习的混合)来创建一个模型或语言理解。</p><p id="d3bb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个过程有两个主要步骤:</p><p id="8351" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第一步是使用无监督的预训练方法来创建更广泛的语言理解模型。相比之下，第二部分将使用监督微调来使模型适合特定的任务。</p><p id="f083" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过这样做，GPT将学会一种通用的文本表示法，这种表示法既可以转移，又不需要太多的修改就能适应广泛的任务。</p><h2 id="de2c" class="lf kd hi bd ke lg lh li ki lj lk ll km jb lm ln kq jf lo lp ku jj lq lr ky ls bi translated">它将如何做到这一点？</h2><h1 id="9e3c" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">GPT的框架——变形金刚</h1><p id="3ebe" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">为了让GPT做到这一点，它将使用一个transformer模型架构来执行各种任务。变压器模型为长期依赖性创建了更多的结构记忆(与主要用于短期依赖性的递归神经网络相比)，从而在执行特定任务时允许架构的最小变化。</p><p id="bc88" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如我之前提到的，我们都需要使用无监督的预训练和有监督的微调，所以让我们更深入地了解我们将如何做到这一点。</p><h2 id="3f4d" class="lf kd hi bd ke lg lh li ki lj lk ll km jb lm ln kq jf lo lp ku jj lq lr ky ls bi translated">无监督预训练</h2><p id="8f15" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">该模型的无监督预训练部分使用多层变压器解码器，这是一种变压器。解码器有点像NLP中模型的输出部分。</p><p id="aa30" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将使用BooksCorups数据集来训练这个变形金刚，该数据集由7000多种不同类型的独特的<strong class="is hj">未出版的</strong>书籍组成！这些数据将允许模型理解广泛的文本表示，然后可以应用于更具体的任务。</p><h2 id="fc91" class="lf kd hi bd ke lg lh li ki lj lk ll km jb lm ln kq jf lo lp ku jj lq lr ky ls bi translated">监督微调</h2><p id="f8bd" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">在对模型进行预训练之后，我们将对模型的参数进行微调，使其适合特定的任务。</p><p id="eec9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先预训练模型是非常有用的，因为它将有助于提高监督模型的泛化能力，并加速更广泛的任务与特定任务之间的收敛！</p><h1 id="a95e" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">GPT在行动！</h1><p id="01fd" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">好了，现在让我们看看他们是如何测试这种GPT方法的。它首先在NLP领域的4个特定领域进行了测试:</p><ol class=""><li id="c728" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">自然语言推理(NLI)</li><li id="7e18" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">问题回答</li><li id="3aba" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">语义相似度</li><li id="316e" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">文本分类</li></ol><p id="872d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在为每个模型训练了GPT之后，它表现得非常好，并且与其他模型(没有预先训练的)相比，它几乎超过了所有模型。这显示了GPT是多么的强大，可以显著提高许多NLP任务的性能。</p></div><div class="ab cl lt lu gp lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="hb hc hd he hf"><p id="e027" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="ma">本文基于一篇名为《通过生成性预训练提高语言理解》的研究论文</em></p><p id="1c6f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="ma">你可以在这里</em> 查看那篇论文 <a class="ae mb" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ma">。</em></a></p></div></div>    
</body>
</html>
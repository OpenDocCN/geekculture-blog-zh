<html>
<head>
<title>The Intuition of Recurrent Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">递归神经网络的直觉</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/the-intuition-of-recurrent-neural-networks-906a846e20b0?source=collection_archive---------13-----------------------#2022-02-07">https://medium.com/geekculture/the-intuition-of-recurrent-neural-networks-906a846e20b0?source=collection_archive---------13-----------------------#2022-02-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/cc5049b9b096e07b771ef31c34706099.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*b3b-T7pXIhRx7Ph8"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">remember those memories? ML does too 🙌 | Photo by <a class="ae iu" href="https://unsplash.com/@jontyson?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jon Tyson</a> on <a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="iv iw ix"><p id="6c0e" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">这是一篇介绍性文章。建议你了解一下<a class="ae iu" rel="noopener" href="/geekculture/gradient-descent-simplified-631a7ce38cb6">梯度下降</a>、<a class="ae iu" href="https://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank">后面的作品一些线性代数</a>、<a class="ae iu" href="https://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank">和基本的DL知识</a>🙂。然而，它非常简单，有许多视觉效果，所以任何水平的知识都可以使用！</p></blockquote><p id="e549" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">我一直很珍惜递归神经网络中的一种魔力。我记得不久前看过安德烈·卡帕西的《品评助理LSTM》模型。在训练他的模型(使用相当随意选择的超参数)的几十分钟内，它开始产生即将有意义的意义。</p><blockquote class="ka"><p id="3a05" class="kb kc hi bd kd ke kf kg kh ki kj jw dx translated">怎么可能提取以前术语的精华来猜测未来的词？</p></blockquote><p id="c8e2" class="pw-post-body-paragraph iy iz hi jb b jc kk je jf jg kl ji jj jx km jm jn jy kn jq jr jz ko ju jv jw hb bi translated">我很震惊。从我的机器学习经验来看，我熟悉一个模型在<strong class="jb hj">线性</strong>数据中寻找和提取关系的能力，但是理解句子的主题、语法和风格有什么魔力呢？</p><p id="d55a" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">Juergen Schmidhuber发现了这个神奇的概念，他在传统的神经网络中加入了一种微妙的记忆感。这些就是我们今天看到的传统rnn。</p><blockquote class="ka"><p id="7abc" class="kb kc hi bd kd ke kf kg kh ki kj jw dx translated">通过这篇文章，你将了解LSTM和RNN及其变种背后的数学运算。</p></blockquote><h2 id="0457" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jx la lb lc jy ld le lf jz lg lh li lj bi translated">TL；速度三角形定位法(dead reckoning)</h2><ul class=""><li id="2c3a" class="lk ll hi jb b jc lm jg ln jx lo jy lp jz lq jw lr ls lt lu bi translated"><strong class="jb hj"> RNNs简体:</strong>缩写代表什么？RNNs和前馈网络有什么不同？算法的架构是什么？</li><li id="b92f" class="lk ll hi jb b jc lv jg lw jx lx jy ly jz lz jw lr ls lt lu bi translated"><strong class="jb hj">深入概念:</strong>RNN有哪些品种？LSTMs是如何工作的？你的下一步是什么？</li></ul><h1 id="7c74" class="ma kq hi bd kr mb mc md kv me mf mg kz mh mi mj lc mk ml mm lf mn mo mp li mq bi translated">简化的RNNs</h1><h2 id="05b9" class="kp kq hi bd kr ks mr ku kv kw ms ky kz jx mt lb lc jy mu le lf jz mv lh li lj bi translated">介绍性示例</h2><p id="8963" class="pw-post-body-paragraph iy iz hi jb b jc lm je jf jg ln ji jj jx mw jm jn jy mx jq jr jz my ju jv jw hb bi translated">让我们假设您编写了一个对象检测模型，其中输入是野生动物的纪录片。</p><p id="34c2" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">结果是准确的，但在某些情况下，该模型将狼归类为狗——因为它们属于同一家族。</p><p id="6151" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">在这种情况下，具有残差的神经网络将表现良好。如果计算机理解给定的纪录片围绕野生动物——从以前的物体检测运行中——它就可以简单地使用这一知识来正确预测狼的种类。这是RNN人做的事。</p><blockquote class="ka"><p id="190e" class="kb kc hi bd kd ke kf kg kh ki kj jw dx translated">在RNNs中，当前输出取决于其当前值，但也取决于过去的输入。</p></blockquote><p id="91df" class="pw-post-body-paragraph iy iz hi jb b jc kk je jf jg kl ji jj jx km jm jn jy kn jq jr jz ko ju jv jw hb bi translated">在这种类型的神经网络中有巨大的力量。它可以识别情感分析的上下文，从语言和其他自然语言的可能性中进行翻译——通过记住以前的词义。</p><h2 id="fd5f" class="kp kq hi bd kr ks mr ku kv kw ms ky kz jx mt lb lc jy mu le lf jz mv lh li lj bi translated">从过去的输入中获取记忆</h2><p id="344e" class="pw-post-body-paragraph iy iz hi jb b jc lm je jf jg ln ji jj jx mw jm jn jy mx jq jr jz my ju jv jw hb bi translated">在传统的密集分层前馈网络中，有三个不同的层。</p><ul class=""><li id="690a" class="lk ll hi jb b jc jd jg jh jx mz jy na jz nb jw lr ls lt lu bi translated"><strong class="jb hj">输入层:</strong>神经元数量与输入的数据特征数量相关。</li><li id="a82e" class="lk ll hi jb b jc lv jg lw jx lx jy ly jz lz jw lr ls lt lu bi translated"><strong class="jb hj">隐藏层:</strong>深度学习计算完成。超参数如层的大小和层数取决于具体问题。</li><li id="bf79" class="lk ll hi jb b jc lv jg lw jx lx jy ly jz lz jw lr ls lt lu bi translated"><strong class="jb hj">输出层:</strong>我们通过一个softmax函数得出解或概率。</li></ul><figure class="nd ne nf ng fd ij er es paragraph-image"><div class="er es nc"><img src="../Images/b847711bdfe8c4351e118c61fc891209.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/0*Oq-1PND1xHUS5DnB.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Weights (w_ii) are constants that are tuned through gradient descent | <a class="ae iu" href="https://matlab4engineers.com/lesson/feed-forward-networks/?lang=en&amp;v=3a52f3c22ed6" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="636f" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">另一方面，RNNs通过引入循环机制允许记忆在前馈神经网络中持续。这个工具反复地将先前的输出传递到当前位置，充当记忆的高速公路。</p><blockquote class="ka"><p id="c91d" class="kb kc hi bd kd ke kf kg kh ki kj jw dx translated">与前馈网络相反，RNNs允许信息循环。</p></blockquote><p id="927c" class="pw-post-body-paragraph iy iz hi jb b jc kk je jf jg kl ji jj jx km jm jn jy kn jq jr jz ko ju jv jw hb bi translated">如果我们将这个信息循环形象化，图表将如下所示。</p><figure class="nd ne nf ng fd ij er es paragraph-image"><div class="ab fe cl nh"><img src="../Images/32050bdad98f28626faade5bb4b4c597.png" data-original-src="https://miro.medium.com/v2/0*V6KRI9e_gnSELUFW.gif"/></div><figcaption class="iq ir et er es is it bd b be z dx">Information is recurrently recycled back into the hidden layer</figcaption></figure><p id="4676" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">在这里，隐藏状态是表达信息高速公路的另一种方法。请注意，如果有多条信息需要通过RNN进行处理，这条高速公路将会不断更新。</p><p id="ed03" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">为了更好地了解循环机制，让我们使用文本预测NLP模型作为指导。该模型根据之前的词义预测句子中的下一个单词。</p><blockquote class="ka"><p id="48ba" class="kb kc hi bd kd ke kf kg kh ki kj jw dx translated">rnn用于NLP，因为它们在预测和分析信息序列方面表现良好。</p></blockquote><blockquote class="iv iw ix"><p id="321c" class="iy iz ja jb b jc kk je jf jg kl ji jj jk km jm jn jo kn jq jr js ko ju jv jw hb bi translated">请务必观看此演示，了解<a class="ae iu" href="https://www.youtube.com/watch?v=5DSfFDdybzg" rel="noopener ugc nofollow" target="_blank">文本预测</a>模型的功能。<em class="hi">😉</em></p></blockquote><p id="e3db" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">在我们的模型中，第一步是将输入的句子分割成单个的单词，这些单词被转换成计算机可读的数字向量。这些向量依次单独进入神经网络。</p><p id="dac9" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">如果那句话是<em class="ja">“汤好喝！”</em>、<em class="ja">、【汤】</em>的矢量化形式计算后传递给隐藏层。</p><figure class="nd ne nf ng fd ij er es paragraph-image"><div class="er es ni"><img src="../Images/deb564f31eec34d296aef6efe5eb8d64.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*LzhedP-I7AZXK8j3HYaBFw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">word to vector transformation occurs through the one-hot encoding process</figcaption></figure><p id="346a" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">第二个单词“in”同样被传递，尽管有一个额外的向量<strong class="jb hj">‘y _ soup’</strong>。这个和形成了向量<strong class="jb hj">‘y _ in’</strong>，该向量具有当前单词以及先前单词的含义。</p><figure class="nd ne nf ng fd ij er es paragraph-image"><div class="er es ni"><img src="../Images/368e070ed3eed68c7a658d40e4d586e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Aaxw_FEnRhWg7ym3XzmKhA.png"/></div></figure><p id="1dcb" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">在对句子中的每个单词进行了无数次这样的添加之后，我们得到了一个向量，该向量描述了句子作为一个整体的含义和相关性，该向量可以用于预测句子中的其他单词。</p><figure class="nd ne nf ng fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nj"><img src="../Images/0640c7a56be9f0effac7071251cf9fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D3v8Omg-11K6vGa_fP9nsw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">y_sentence can then be used for paraphrasing and other NLP uses | tanh is the activation function used</figcaption></figure><p id="2029" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">请注意，上面显示的隐藏层不是单独的层。它们是一个层，剩余部分在其中循环。如果有多个层，则<a class="ae iu" rel="noopener" href="/geekculture/gradient-descent-simplified-631a7ce38cb6">梯度下降</a>过程将会偏斜。</p><p id="6f7c" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">这样，我们就知道了递归神经网络的工作过程！然而，这种逻辑很少有出错的地方。</p><h1 id="4ed2" class="ma kq hi bd kr mb mc md kv me mf mg kz mh mi mj lc mk ml mm lf mn mo mp li mq bi translated">RNN品种</h1><p id="8854" class="pw-post-body-paragraph iy iz hi jb b jc lm je jf jg ln ji jj jx mw jm jn jy mx jq jr jz my ju jv jw hb bi translated">事实上，使用所示的RNN逻辑进行文本预测将不会像预期的那样起作用。这是因为<strong class="jb hj"> y_sentence </strong>的词影响力分布不均。</p><p id="36aa" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">例如，后来的单词如“好吃！”而“汤好喝！”对模型未来的文本预测有更大的影响。因此，在我们的模型中有一个短期的单词偏差。</p><figure class="nd ne nf ng fd ij er es paragraph-image"><div class="er es nk"><img src="../Images/3fc4a6106d6b6942f465f5aa4313a808.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*NPtuOC6QpnbOsou7FB27IA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">the degree of a sentence’s influence on the model’s prediction is biased</figcaption></figure><p id="8767" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">在方程中引入长时记忆，可以平衡每个单词的影响程度。它也将对抗<a class="ae iu" href="https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577" rel="noopener ugc nofollow" target="_blank">消失梯度问题</a>允许更长的单词序列，从中提取准确的意思。</p><blockquote class="ka"><p id="90b5" class="kb kc hi bd kd ke kf kg kh ki kj jw dx translated">运用短期和长期记忆的技术被称为LSTM。</p></blockquote><h2 id="12bc" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jx la lb lc jy ld le lf jz lg lh li lj bi translated">长短期记忆</h2><p id="fbd0" class="pw-post-body-paragraph iy iz hi jb b jc lm je jf jg ln ji jj jx mw jm jn jy mx jq jr jz my ju jv jw hb bi translated">LSTMs是强大的工具。它广泛用于扩展的数据序列，如段落和DNA，并为传统rnn的自然语言处理领域奠定了基础。</p><blockquote class="ka"><p id="faa6" class="kb kc hi bd kd ke kf kg kh ki kj jw dx translated">尽管LSTM的架构不同于经典的RNN，但它本身的记忆能力却被认为是RNN。</p></blockquote><p id="7c93" class="pw-post-body-paragraph iy iz hi jb b jc kk je jf jg kl ji jj jx km jm jn jy kn jq jr jz ko ju jv jw hb bi translated">LSTM架构由四个门组成，长期和短期记忆(来自输入序列)通过其中，以获得预测和更新的长期和短期记忆。</p><figure class="nd ne nf ng fd ij er es paragraph-image"><div class="er es nl"><img src="../Images/0d004551913ae94c19d50ba5be788c67.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*E9K_laDLIG359ARvnshHvw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">note that the text prediction process includes more steps including word embeddings, character embedding, and others.</figcaption></figure><p id="8ae6" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">注意到的四个门包括:</p><ul class=""><li id="ca9d" class="lk ll hi jb b jc jd jg jh jx mz jy na jz nb jw lr ls lt lu bi translated">遗忘门:无用的数据从<strong class="jb hj">长期记忆</strong>中被丢弃，有用的数据被保留。</li><li id="1406" class="lk ll hi jb b jc lv jg lw jx lx jy ly jz lz jw lr ls lt lu bi translated">学习门:同样的，序列中的<strong class="jb hj">当前事件</strong>和<strong class="jb hj">短时记忆</strong>被加入以移除无用数据。</li><li id="ab84" class="lk ll hi jb b jc lv jg lw jx lx jy ly jz lz jw lr ls lt lu bi translated">记忆门:将<strong class="jb hj">遗忘门</strong>和<strong class="jb hj">学习门</strong>后保存的信息组合起来，形成更新的长期记忆。这些信息将被保留以备将来事件使用。</li><li id="9b5c" class="lk ll hi jb b jc lv jg lw jx lx jy ly jz lz jw lr ls lt lu bi translated">使用门:计算来自<strong class="jb hj">遗忘门</strong>和<strong class="jb hj">学习门</strong>的输出，用于以下预测。</li></ul><figure class="nd ne nf ng fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nm"><img src="../Images/7ff9d15c65cabc1149507db63835c746.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nWHA843paYL6ckZJtWfw6Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">and the process continues…</figcaption></figure><h1 id="2643" class="ma kq hi bd kr mb mc md kv me mf mg kz mh mi mj lc mk ml mm lf mn mo mp li mq bi translated">后续步骤和未来阅读</h1><p id="981a" class="pw-post-body-paragraph iy iz hi jb b jc lm je jf jg ln ji jj jx mw jm jn jy mx jq jr jz my ju jv jw hb bi translated">既然我们理解了<strong class="jb hj">记忆</strong>和<strong class="jb hj">递归神经网络</strong>的思想，下一步就是理解它们背后的数学运算。</p><p id="14b3" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">考虑阅读这篇文章，了解更多关于四个门的功能。如果你对LSTM和RNN的项目感兴趣，使用这个图像字幕教程来创建你自己的项目吧！</p><blockquote class="ka"><p id="2c9e" class="kb kc hi bd kd ke kf kg kh ki kj jw dx translated">祝你在通往机器学习高峰的旅途中好运！</p></blockquote><h1 id="d63c" class="ma kq hi bd kr mb mc md kv me mf mg kz mh nn mj lc mk no mm lf mn np mp li mq bi translated">在你走之前…</h1><blockquote class="iv iw ix"><p id="66ce" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">机器学习一直是我的研究热点。无论是在snapchat过滤器还是垃圾邮件分类器中，到处都在使用<strong class="jb hj"><em class="hi"/></strong><em class="hi">。今天，它更像是一种生活方式，而不是流行语。</em></p><p id="231d" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">这就是我进入数据科学领域的原因。从一开始，我就上瘾了，我希望我会一直上瘾。</p><p id="b470" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><em class="hi">如果你喜欢阅读这篇文章，请联系我的社交网站🤗(附言:想找点问题解决:有时间给我发几个过来)<br/></em><a class="ae iu" href="https://www.linkedin.com/in/arjun-mahes-a46200220/" rel="noopener ugc nofollow" target="_blank"><em class="hi">LinkedIn</em></a><em class="hi">|</em><a class="ae iu" href="https://arjunmahes.substack.com/" rel="noopener ugc nofollow" target="_blank"><em class="hi">简讯</em> </a> <em class="hi"> | </em> <a class="ae iu" href="https://twitter.com/mahes_arjun" rel="noopener ugc nofollow" target="_blank"> <em class="hi">推特</em> </a></p></blockquote></div></div>    
</body>
</html>
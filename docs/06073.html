<html>
<head>
<title>Batch Update DynamoDB with Provisioned Capacity</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用调配的容量批量更新DynamoDB</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/batch-update-dynamodb-with-provisioned-capacity-f950f5d73497?source=collection_archive---------7-----------------------#2021-08-05">https://medium.com/geekculture/batch-update-dynamodb-with-provisioned-capacity-f950f5d73497?source=collection_archive---------7-----------------------#2021-08-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="fa18" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">数据处理</h2><div class=""/><div class=""><h2 id="5367" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">使用阶跃函数保持在写入限制范围内，从而节省成本</h2></div><p id="9aa3" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">如果你能坚持使用DynamoDB的<em class="kc"> provisioned capacity </em>特性，它可以成为AWS上一个非常经济的NoSQL数据库平台。如果使用模式一致，使用调配容量比使用<em class="kc">按需</em>容量估计要便宜七倍。另一个好处是DynamoDB空闲层也仅适用于调配的容量。然而，在管理您的数据时，供应容量带来了一个主要挑战:<strong class="ji hs">您如何在不超出供应容量的情况下批量更新数千或数百万个DynamoDB项目？</strong></p><p id="8024" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">解决这个问题的一个选择是使用AWS阶跃函数。AWS将其描述为“用于编排AWS服务的低代码可视化工作流服务”,非常适合处理来自S3的一系列源文件，并以可靠和可控的方式更新DynamoDB。</p><blockquote class="kd ke kf"><p id="2224" class="jg jh kc ji b jj jk is jl jm jn iv jo kg jq jr js kh ju jv jw ki jy jz ka kb hb bi translated">阶跃函数允许您处理大量的源S3数据，并降低写入DynamoDB的速率，这意味着您可以使用调配的容量并节省资金。</p></blockquote><p id="ee85" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">注意，单独使用Lambda函数在这里是行不通的，因为它需要支持长时间的执行，而Lambda的执行时间被限制在15分钟。</p><h2 id="2165" class="kj kk hi bd kl km kn ko kp kq kr ks kt jp ku kv kw jt kx ky kz jx la lb lc ho bi translated">了解写容量单位</h2><p id="24c1" class="pw-post-body-paragraph jg jh hi ji b jj ld is jl jm le iv jo jp lf jr js jt lg jv jw jx lh jz ka kb hb bi translated">当使用调配的容量时，在每个DynamoDB表上定义了一个设置限制<em class="kc">写容量单位(WCU) </em>。对于最大1 KB的项目，每个WCU代表每秒一次写入。将较大的项目写入DynamoDB将消耗多个WCU。</p><p id="094e" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">对写入吞吐量的这些限制意味着批量更新需要限制到比配置的WCU允许的更低的写入速率。超过表中配置的WCU会导致DynamoDB强制限制写入，并有作业失败和数据丢失的风险。</p><p id="fee6" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">与其让DynamoDB在批量编写时强制节流，不如在代码中实现抑制项目写入速率的逻辑。</p><h2 id="0e6e" class="kj kk hi bd kl km kn ko kp kq kr ks kt jp ku kv kw jt kx ky kz jx la lb lc ho bi translated">AWS步进功能状态机</h2><p id="1fcb" class="pw-post-body-paragraph jg jh hi ji b jj ld is jl jm le iv jo jp lf jr js jt lg jv jw jx lh jz ka kb hb bi translated">问题的关键是:我们如何将写入速率降低到足够低，以避免超过所提供的DynamoDB写入容量？为了解决这个问题，我在AWS Step函数中构建了一个状态机来协调读取源文件和写入DynamoDB的逻辑。</p><blockquote class="kd ke kf"><p id="5ffa" class="jg jh kc ji b jj jk is jl jm jn iv jo kg jq jr js kh ju jv jw ki jy jz ka kb hb bi translated">状态机为每个文件调用一个Lambda函数，在处理完每个CSV行后，函数内会暂停一次，以便降低DynamoDB的写入速率</p></blockquote><p id="51d2" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">状态机遍历源S3位置中的多个CSV文件，然后根据CSV中每一行的值更新DynamoDB。CSV文件是Spark ETL作业产生的分区输出，这就是为什么有多个文件的原因。状态机为每个文件调用一个Lambda函数，在处理完每个CSV行后，函数中会有一个暂停，以便降低DynamoDB的写入速率。状态机的总体设计如下所示，其迭代模式非常清晰。我使用Workflow Studio，因为它为配置状态机提供了一个稍微简单的UI。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es li"><img src="../Images/e229d917efba9e26e8e91037d7d049a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5vMg2cDN8lmBg89Ci0qrw.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx">Step Functions state machine for batch updating provisioned capacity DynamoDB table (image by author)</figcaption></figure><h2 id="2b38" class="kj kk hi bd kl km kn ko kp kq kr ks kt jp ku kv kw jt kx ky kz jx la lb lc ho bi translated">配置运行通过状态</h2><p id="7995" class="pw-post-body-paragraph jg jh hi ji b jj ld is jl jm le iv jo jp lf jr js jt lg jv jw jx lh jz ka kb hb bi translated">这是一个<em class="kc">通过</em>状态，其目的是在状态机上设置初始变量。下面显示了亚马逊州的语言片段。<em class="kc">结果</em>下的键值将在状态机的下一步传递给Lambda函数。</p><pre class="lj lk ll lm fd ly lz ma mb aw mc bi"><span id="bfb0" class="kj kk hi lz b fi md me l mf mg">"ConfigureRun": {<br/>  "Type": "Pass",<br/>  "Result": {<br/>    "s3Bucket": "bucket-name-here",<br/>    "s3KeyBase": "source-key-location/",<br/>    "s3KeyBaseProcessed": "processed-output-file-location/",<br/>    "functionTimeLimitSeconds": "870"<br/>  },<br/>  "ResultPath": "$.Payload",<br/>  "Next": "CheckIfFilesRemain"<br/>}</span></pre><h2 id="43e8" class="kj kk hi bd kl km kn ko kp kq kr ks kt jp ku kv kw jt kx ky kz jx la lb lc ho bi translated">检查是否保留Lambda函数</h2><p id="9863" class="pw-post-body-paragraph jg jh hi ji b jj ld is jl jm le iv jo jp lf jr js jt lg jv jw jx lh jz ka kb hb bi translated">这是一个Lambda函数，它接受ConfigureRun中定义的参数或在UpdateValuesInDB之后运行，并确定在定义的S3位置中是否有任何对象。如果S3位置为空，响应变量<em class="kc"> noFilesFound </em>将为真，如果有对象，则为假。注意，函数也返回最初提供的参数，以便在状态机的下一步中轻松使用它们。</p><p id="68f8" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">本质上，这个函数的目的是确定状态机是否应该继续迭代和处理下一个剩余的文件。</p><p id="9e87" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">Lambda函数的Node.js代码如下。</p><pre class="lj lk ll lm fd ly lz ma mb aw mc bi"><span id="4d6b" class="kj kk hi lz b fi md me l mf mg">const aws = require('aws-sdk');<br/>const s3 = new aws.S3({ apiVersion: '2006-03-01' });</span><span id="5eae" class="kj kk hi lz b fi mh me l mf mg">exports.handler = async function iterator (event, context, callback) {<br/>  // Get values defined in ConfigureRun pass state<br/>  let s3Bucket = event.Payload.s3Bucket<br/>  let s3KeyBase = event.Payload.s3KeyBase<br/>  let s3KeyBaseProcessed = event.Payload.s3KeyBaseProcessed<br/>  let functionTimeLimitSeconds = event.Payload.functionTimeLimitSeconds<br/>  <br/>  let d = new Date();<br/>  let s3UriSuffix = 'year=' + d.getUTCFullYear().toString() + '/month=' + (d.getUTCMonth()+1).toString() + '/day=' + d.getUTCDate().toString() + '/'<br/>  let s3UriFull = 's3://' + s3Bucket + '/' + s3KeyBase + s3UriSuffix</span><span id="b4e3" class="kj kk hi lz b fi mh me l mf mg">  const s3params = {<br/>      Bucket: s3Bucket,<br/>      Delimiter: '/',<br/>      Prefix: s3KeyBase + s3UriSuffix<br/>  };</span><span id="93b5" class="kj kk hi lz b fi mh me l mf mg">const s3ListResponse = await getS3ListObjects(s3params)<br/>  <br/>  if (s3ListResponse.Contents == undefined) {<br/>    callback(null, {<br/>      s3Bucket,<br/>      s3KeyBase,<br/>      s3KeyBaseProcessed,<br/>      s3UriFull,<br/>      noFilesFound: true,<br/>      fileCount: 0,<br/>      functionTimeLimitSeconds<br/>    })      <br/>  } else {<br/>    if (s3ListResponse.Contents.length &gt; 0) { // there are files<br/>      callback(null, {<br/>        s3Bucket,<br/>        s3KeyBase,<br/>        s3KeyBaseProcessed,<br/>        s3UriFull,<br/>        noFilesFound: false,<br/>        fileCount: s3ListResponse.Contents.length,<br/>        functionTimeLimitSeconds<br/>      })            <br/>    } else { // there are no files - this may mean the key prefix is invalid i.e. there is no data for the data that this has run<br/>      console.warn('No data for S3 URL ' + s3UriFull)<br/>      callback(null, {<br/>        s3Bucket,<br/>        s3KeyBase,<br/>        s3KeyBaseProcessed,<br/>        s3UriFull,<br/>        noFilesFound: true,<br/>        fileCount: 0,<br/>        functionTimeLimitSeconds<br/>      })         <br/>    }<br/>  }<br/>}</span><span id="27c1" class="kj kk hi lz b fi mh me l mf mg">async function getS3ListObjects(s3params) {<br/>    const resp = await s3.listObjects(s3params).promise();<br/>    return resp<br/>}</span></pre><h2 id="7dba" class="kj kk hi bd kl km kn ko kp kq kr ks kt jp ku kv kw jt kx ky kz jx la lb lc ho bi translated">正在处理一个选择状态</h2><p id="b95a" class="pw-post-body-paragraph jg jh hi ji b jj ld is jl jm le iv jo jp lf jr js jt lg jv jw jx lh jz ka kb hb bi translated">如果由先前的<em class="kc"> CheckIfFilesRemain </em>状态提供的<em class="kc"> noFilesFound </em>值为假，那么这将调用<em class="kc">规则#1 </em>，并转换到<em class="kc"> UpdateValuesInDB </em>状态。换句话说，如果在S3位置有对象，我们将继续处理第一个对象。</p><p id="0254" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">如果<em class="kc"> noFilesFound </em>为真，那么默认规则将触发，状态机的执行将结束。下面显示了该州的亚马逊州语言片段。</p><pre class="lj lk ll lm fd ly lz ma mb aw mc bi"><span id="a967" class="kj kk hi lz b fi md me l mf mg">"IsProcessingDone": {<br/>  "Type": "Choice",<br/>  "Choices": [<br/>    {<br/>      "Variable": "$.Payload.noFilesFound",<br/>      "BooleanEquals": false,<br/>      "Next": "UpdateValuesInDB"<br/>    }<br/>  ],<br/>  "Default": "Pass"<br/>}</span></pre><h2 id="d1d1" class="kj kk hi bd kl km kn ko kp kq kr ks kt jp ku kv kw jt kx ky kz jx la lb lc ho bi translated">UpdateValuesInDB Lambda函数</h2><p id="02e6" class="pw-post-body-paragraph jg jh hi ji b jj ld is jl jm le iv jo jp lf jr js jt lg jv jw jx lh jz ka kb hb bi translated">这是整个解决方案中最复杂也是最重要的部分。该组件处理来自S3的单个源CSV对象，并用其中包含的值更新DynamoDB。它还包含“暂停”来降低写入DynamoDB的速度。有一些重要的注意事项需要记住:</p><ol class=""><li id="b87f" class="mi mj hi ji b jj jk jm jn jp mk jt ml jx mm kb mn mo mp mq bi translated">由于目标DynamoDB表上提供的WCU限制，必须限制对DocumentClient API进行更新和上传的调用速率。该速率必须低于所提供的容量支持的速率。</li><li id="1e5f" class="mi mj hi ji b jj mr jm ms jp mt jt mu jx mv kb mn mo mp mq bi translated">应该通过估计正在写入的项目的大小来计算写入率，以确定每次写入使用的WCU量。例如，如果每个项目平均为1.5 KB，则每次写入将使用2个WCU(由于四舍五入)。如果该表的调配容量为10 WCU，则每秒最多可以写入5个项目(或者每次写入后暂停0.2秒)。我使用代码<em class="kc">await new Promise(r =&gt;setTimeout(r，pauseTimeMS)) </em>来实现Node.js中的暂停。</li><li id="2db1" class="mi mj hi ji b jj mr jm ms jp mt jt mu jx mv kb mn mo mp mq bi translated">尽管有暂停，Lambda函数仍将运行，因此您将为此付出代价。但兰姆达斯很便宜，所以这应该不是什么大事。</li><li id="73aa" class="mi mj hi ji b jj mr jm ms jp mt jt mu jx mv kb mn mo mp mq bi translated">Lambda函数最多只能执行15分钟，因此该函数必须在15分钟限制之前将任何未处理的行从CSV写回到源S3位置。因此，在<em class="kc">配置运行</em>上设置的<em class="kc">功能时间限制秒</em>值必须小于900秒。记住将Lambda超时设置为15分钟。</li><li id="f13e" class="mi mj hi ji b jj mr jm ms jp mt jt mu jx mv kb mn mo mp mq bi translated">您需要从原始S3位置删除已处理的文件(否则会有无限循环的风险)。您应该在阶跃函数状态机上设置一个超时值，以防止出现无限循环。</li><li id="5cd6" class="mi mj hi ji b jj mr jm ms jp mt jt mu jx mv kb mn mo mp mq bi translated">同样在这个Lambda中，我写出了传递给这个函数的所有变量。AWS阶跃函数包含用于过滤和合并参数的复杂逻辑。然而，我发现这种通过Lambda函数代理值的方法比纠结于阶跃函数<em class="kc"> ResultSelector </em>、<em class="kc"> ResultPath </em>和<em class="kc"> OutputPath </em>逻辑要简单得多。</li></ol><p id="d01b" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">Lambda的Node.js代码如下。</p><pre class="lj lk ll lm fd ly lz ma mb aw mc bi"><span id="5db4" class="kj kk hi lz b fi md me l mf mg">const aws = require('aws-sdk');<br/>const s3 = new aws.S3({ apiVersion: '2006-03-01' });<br/>const os = require("os");<br/>const documentClient = new aws.DynamoDB.DocumentClient();</span><span id="71dc" class="kj kk hi lz b fi mh me l mf mg">const tableName = 'DynamoDB-table-name-here';</span><span id="ba7d" class="kj kk hi lz b fi mh me l mf mg">/**<br/> * Summary: updates DynamoDB with a pause between rows processed<br/> * <br/> * 1. Gets a list of all objects in S3 's3KeyBase' location<br/> * 2. If no objects, stop running<br/> * 3. If there are objects, get the first object<br/> * 4. Read the CSV text from the object and split into lines:<br/> *   5. For each CSV line, split by comma character<br/> *   6. Use CSV row to add a value to the table<br/> *   7. If 'functionTimeLimitSeconds' lapses, stop processing<br/> * 9. Unprocessed rows write to new file in original location<br/> * 10. Processed rows write to 's3KeyBaseProcessed' location<br/> * 11. Source object in S3 's3KeyBase' location will be deleted<br/> */ <br/>exports.handler = async (event, context) =&gt; {<br/>       <br/>    let s3Bucket = event.Payload.s3Bucket;<br/>    let s3KeyBase = event.Payload.s3KeyBase;<br/>    let s3KeyBaseProcessed = event.Payload.s3KeyBaseProcessed;<br/>    let functionTimeLimitSeconds = event.Payload.functionTimeLimitSeconds;<br/>       <br/>    let pauseTimeMS = 200 // use WCU to calculate pause time</span><span id="bb5d" class="kj kk hi lz b fi mh me l mf mg">    let d = new Date();<br/>    let s3UriSuffix = 'year=' + d.getUTCFullYear().toString() + '/month=' + (d.getUTCMonth()+1).toString() + '/day=' + d.getUTCDate().toString() + '/';<br/>    let s3UriFull = 's3://' + s3Bucket + '/' + s3KeyBase + s3UriSuffix;<br/>    let s3UriFullProcessed = 's3://' + s3Bucket + '/' + s3KeyBaseProcessed + s3UriSuffix;<br/>    <br/>    const s3params = {<br/>      Bucket: s3Bucket,<br/>      Delimiter: '/',<br/>      Prefix: s3KeyBase + s3UriSuffix<br/>    }; <br/>    <br/>    /*<br/>     * Get all objects in bucket with key prefix (i.e. in folder)<br/>     */<br/>    const s3ListResponse = await getS3ListObjects(s3params)<br/>    <br/>    if (s3ListResponse.Contents == undefined || s3ListResponse.Contents.length == 0) { // no files in location<br/>        console.log("No files found in location " + s3UriFull)<br/>        let response = {<br/>            statusCode: 200,<br/>            s3Bucket,<br/>            s3KeyBase,<br/>            s3KeyBaseProcessed,<br/>            s3UriFull,<br/>            s3UriFullProcessed,<br/>            noFilesFound: true,<br/>            fileCount: 0,<br/>            functionTimeLimitSeconds<br/>        };<br/>        return response; <br/>    }    <br/>    <br/>    /*<br/>     * Get the contents of the first file then process each line<br/>     */<br/>    <br/>    // Grab the first file<br/>    let fileKey = s3ListResponse.Contents[0].Key; <br/>    let fileKeyFilename = fileKey.substring(fileKey.lastIndexOf('/') + 1);<br/>    <br/>    const s3paramsObject = {<br/>        Bucket: s3Bucket, <br/>        Key: fileKey,<br/>    };<br/>     <br/>    // Read the CSV text from S3<br/>    let fileText = await getS3ObjectBodyText(s3paramsObject);<br/>    <br/>    let timeExpired = false;<br/>    let processedLines = 0;<br/>    let unprocessedLines = 0;</span><span id="8fcc" class="kj kk hi lz b fi mh me l mf mg">// Split the CSV by lines<br/>    let textLineArray = fileText.split("\n");<br/>    let fileTextUnprocessed = appendTextLine("", textLineArray[0]);<br/>    let fileTextProcessed = appendTextLine("", textLineArray[0]);</span><span id="fdf7" class="kj kk hi lz b fi mh me l mf mg">// Loop around each line<br/>    for (let i=1; i&lt;textLineArray.length; i++) {<br/>        let dateLatest = new Date();<br/>        const diffTime = Math.abs(dateLatest - d);<br/>        const diffSecs = Math.ceil(diffTime / 1000); <br/>        if (diffSecs &lt; functionTimeLimitSeconds) {<br/>            let line = textLineArray[i];<br/>            let lineValues = line.split(","); // split into columns<br/>            <br/>            let date = new Date(); // for object timestamp<br/>            <br/>            /*<br/>             * Create the item for DynamoDB<br/>             */        <br/>            let partitionKey = lineValues[0]; // first CSV column<br/>            <br/>            let dbItem = {}<br/>            dbItem.partitionKey = partitionKey;<br/>            // set dbItem values here<br/>                <br/>            await singleInsertItem(dbItem);</span><span id="69ce" class="kj kk hi lz b fi mh me l mf mg">// This is the pause to throttle writes            <br/>            await new Promise(r =&gt; setTimeout(r, pauseTimeMS));<br/>                <br/>            fileTextProcessed = appendTextLine(fileTextProcessed, textLineArray[i]);<br/>            processedLines++;</span><span id="6cd8" class="kj kk hi lz b fi mh me l mf mg">        } else { // time has run out<br/>            fileTextUnprocessed = appendTextLine(fileTextUnprocessed, textLineArray[i]);<br/>            timeExpired = true;<br/>            unprocessedLines++;<br/>        }</span><span id="4d8f" class="kj kk hi lz b fi mh me l mf mg">}<br/>    <br/>    /*<br/>     * Write fileTextProcessed to processed folder<br/>     */<br/>    await writeTextToS3File(s3Bucket, s3KeyBaseProcessed + s3UriSuffix, context, fileTextProcessed, d, fileKeyFilename);</span><span id="961f" class="kj kk hi lz b fi mh me l mf mg">    <br/>    if (timeExpired) { // time limit expired<br/>        /*<br/>         * Write fileTextUnprocessed to processed folder<br/>         */    <br/>        console.log("Ran out of time. Creating file for unprocessed rows in original location in S3");<br/>        if (fileKeyFilename.includes("_unprocessed")) {<br/>            fileKeyFilename = fileKeyFilename.split('_')[0]; // remove suffix previously generated for the file, if applicable<br/>        }<br/>        await writeTextToS3File(s3Bucket, s3KeyBase + s3UriSuffix, context, fileTextUnprocessed, d, fileKeyFilename + "_unprocessed");<br/>    }<br/>    <br/>    /*<br/>     * Delete original object from S3<br/>     */<br/>    await deleteProcessedFile(s3Bucket, fileKey);<br/>    let response = {<br/>        statusCode: 200,<br/>        s3Bucket,<br/>        s3KeyBase,<br/>        s3KeyBaseProcessed,<br/>        s3UriFull,<br/>        s3UriFullProcessed,<br/>        noFilesFound: false,<br/>        fileCount: s3ListResponse.Contents.length,<br/>        processedLines,<br/>        unprocessedLines,<br/>        functionTimeLimitSeconds<br/>    };<br/>    return response;<br/>};</span><span id="0fde" class="kj kk hi lz b fi mh me l mf mg">/**<br/> * Appends a new line to text<br/> */<br/>function appendTextLine(text, newLine) {<br/>    if (newLine == null) return text;<br/>    if (newLine.length == 0) return text;<br/>    text = text + newLine + os.EOL;<br/>    return text;<br/>}</span><span id="f1eb" class="kj kk hi lz b fi mh me l mf mg">/**<br/> * Inserts a single item into DynamoDB<br/> */<br/>async function singleInsertItem(dbItem) {<br/>    let dataItem = {};<br/>    dataItem.TableName = tableName;<br/>    dataItem.Item = dbItem;</span><span id="e611" class="kj kk hi lz b fi mh me l mf mg">try {<br/>        let dataResponse = documentClient.put(dataItem).promise();<br/>        return dataResponse;<br/>    } catch(err) {<br/>        return err;<br/>    }        <br/>}</span><span id="3714" class="kj kk hi lz b fi mh me l mf mg">async function getS3ListObjects(s3params) {<br/>    const resp = await s3.listObjects(s3params).promise();<br/>    return resp;<br/>}</span><span id="6200" class="kj kk hi lz b fi mh me l mf mg">async function getS3ObjectBodyText(s3params) {<br/>    const { Body } = await s3.getObject(s3params).promise();<br/>    let s3ObjectString = Body.toString('utf-8');<br/>    return s3ObjectString;<br/>}</span><span id="3597" class="kj kk hi lz b fi mh me l mf mg">async function deleteProcessedFile(s3Bucket, s3Key) {<br/>    let s3params = {<br/>        Bucket: s3Bucket,<br/>        Key: s3Key<br/>    };     <br/>    const resp = await s3.deleteObject(s3params).promise();<br/>    return resp;  <br/>}</span><span id="815a" class="kj kk hi lz b fi mh me l mf mg">async function writeTextToS3File(s3Bucket, s3Key, context, textContent, partitionDate, filename) {<br/>    let d = partitionDate;<br/>    let fileTimestamp = new Date();<br/>    fileTimestamp = fileTimestamp.toISOString();<br/>    <br/>    let key = s3Key + filename + '-' + fileTimestamp;</span><span id="5838" class="kj kk hi lz b fi mh me l mf mg">    let buff = Buffer.from(textContent, 'utf-8');</span><span id="9405" class="kj kk hi lz b fi mh me l mf mg">const s3params = {<br/>        Bucket: s3Bucket,<br/>        Key: key,<br/>        Body: buff,<br/>        //ContentEncoding: 'base64',<br/>        ContentType: 'text/csv',<br/>    };</span><span id="96b1" class="kj kk hi lz b fi mh me l mf mg">const s3Resp = await s3.putObject(s3params).promise();<br/>    <br/>    return s3Resp;<br/>}</span></pre><h2 id="df66" class="kj kk hi bd kl km kn ko kp kq kr ks kt jp ku kv kw jt kx ky kz jx la lb lc ho bi translated">结果</h2><p id="1838" class="pw-post-body-paragraph jg jh hi ji b jj ld is jl jm le iv jo jp lf jr js jt lg jv jw jx lh jz ka kb hb bi translated">通过使用AWS Step函数，您可以在DynamoDB中批量插入或更新大量项目，同时保持在WCU提供的容量限制之内。</p><p id="30e5" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">我使用S3的Spark ETL作业编写的32个CSV文件测试了这种方法，每个文件700行——这意味着DynamoDB中总共需要更新22，400个项目。每一行都需要一个<em class="kc"> put </em>和一个<em class="kc"> update </em>，该表有三个<em class="kc">全局二级索引(GSIs) </em>。每个GSI都有自己的WCU供应。</p><p id="487a" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated"><em class="kc"> put </em>、<em class="kc"> update </em>和三个GSI的组合意味着实际上每个CSV行需要五次DynamoDB写入。</p><p id="8377" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">该表被配置为WCU为6。GSI1和GSI2的WCU为4。GSI3的排序键值在作业期间更新，其WCU为7。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es mw"><img src="../Images/5eb0e72150fc8cbe3d96ed4e98c51979.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j-NE_O5BACe0bOnyJBPKmA.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx">Provisioned vs consumed write capacity units for DynamoDB table (image by author)</figcaption></figure><p id="b7af" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">上面是状态机执行期间表的Cloudwatch度量图。请注意，我在开始作业之前减少了调配的容量。我们可以从从凌晨2点到5点消耗的容量的增加中看到，更新DynamoDB中的22，400个条目需要大约3个小时。消耗的容量接近调配的容量，但通常执行顺利。右图显示了凌晨2点后不久DynamoDB抑制的两个写请求，这表明我应该将调配的容量增加1，或者稍微增加暂停时间。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es mw"><img src="../Images/3243be3bff7227e0a347517e8a3e0250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CvbFVhBnqTmH2CTh4pQTYg.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx">Provisioned vs consumed write capacity units for DynamoDB GSI2 (image by author)</figcaption></figure><p id="d5df" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">GSI2的Cloudwatch指标如上所示。在运行作业前不久，我将GSI2上的WCU增加到了4。DynamoDB没有限制对GSI2的任何写入，所以这看起来配置得很好。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es mw"><img src="../Images/b4b430a59f11e2781326dd83eff5e0b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*soMC9xv3P4dvjBZGRa_NrQ.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx">Provisioned vs consumed write capacity units for DynamoDB GSI3 (image by author)</figcaption></figure><p id="b970" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">WCU为7时，GSI3比表和其他GSI具有更大的写入容量。但是，由于此GSI的<em class="kc">排序关键字</em>值在作业期间更新，每次写入都会消耗更多的GSI容量。我们可以看到，在整个3小时的工作中，消耗的容量非常接近调配的容量。尽管如此，在作业期间只发生了一次DynamoDB节流写入。然而，更安全的做法是进一步增加GSI3上的配置容量，或者稍微增加<em class="kc"> UpdateValuesInDB </em> Lambda函数中的暂停时间。</p><p id="4b63" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">通过使用AWS阶跃函数，我们有了一个健壮、可靠和可伸缩的批量更新DynamoDB的无服务器方法。错误处理内置于其工作流中，包括重试失败状态时的指数后退等功能。</p><p id="a31a" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">在这项工作中，我们对每个源文件使用三种状态转换。以每1，000次状态转换0.025美元的价格，这里的阶跃函数成本可以忽略不计。同样，一个128 MB的功能运行3小时，Lambda成本仅为2-3美分。总的来说，这种方法非常节省成本，并且提供了一种健壮的方法来对提供的DynamoDB表进行批量更新。</p><p id="f8ff" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated"><strong class="ji hs"> <em class="kc">附加阅读/参考文献</em> </strong></p><ol class=""><li id="e87d" class="mi mj hi ji b jj jk jm jn jp mk jt ml jx mm kb mn mo mp mq bi translated">DynamoDB按需容量与调配容量:哪个更好？<a class="ae mx" href="https://searchcloudcomputing.techtarget.com/answer/DynamoDB-on-demand-vs-provisioned-capacity-Which-is-better" rel="noopener ugc nofollow" target="_blank"> <em class="kc"> ' </em> </a> <em class="kc">虽然按需提供最适合的可扩展性，但其成本大约是调配容量的七倍'</em><a class="ae mx" href="https://searchcloudcomputing.techtarget.com/answer/DynamoDB-on-demand-vs-provisioned-capacity-Which-is-better" rel="noopener ugc nofollow" target="_blank">https://search cloud computing . techtarget . com/answer/DynamoDB-on-demand-vs-provisioned-capacity-哪个更好</a></li><li id="28c4" class="mi mj hi ji b jj mr jm ms jp mt jt mu jx mv kb mn mo mp mq bi translated">DynamoDB上调配容量的写容量单位说明<a class="ae mx" href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html" rel="noopener ugc nofollow" target="_blank">https://docs . AWS . Amazon . com/Amazon DynamoDB/latest/developer guide/provisionedthroughput . html</a></li><li id="d950" class="mi mj hi ji b jj mr jm ms jp mt jt mu jx mv kb mn mo mp mq bi translated">用于写入DynamoDB的document client API<a class="ae mx" href="https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB/DocumentClient.html" rel="noopener ugc nofollow" target="_blank">https://docs . AWS . Amazon . com/AWSJavaScriptSDK/latest/AWS/dynamo db/document client . html</a></li></ol></div></div>    
</body>
</html>
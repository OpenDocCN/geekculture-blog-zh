<html>
<head>
<title>Spark on K8s — Send Spark job’s Metrics to DataDog Using Autodiscovery</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K8s上的Spark—使用自动发现将Spark作业的指标发送到DataDog</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/spark-on-k8s-send-spark-jobs-metrics-to-datadog-using-autodiscovery-fc10488ecdc9?source=collection_archive---------9-----------------------#2021-05-18">https://medium.com/geekculture/spark-on-k8s-send-spark-jobs-metrics-to-datadog-using-autodiscovery-fc10488ecdc9?source=collection_archive---------9-----------------------#2021-05-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5feb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">今天，我将分享我们如何将Kubernetes上的Spark作业指标发送到DataDog，这些指标将用于为Spark应用程序创建监视器或仪表板。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="9822" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">要求</h1><p id="9614" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">我建议你花些时间阅读我以前的博客，以了解K8s上的Spark和DataDog:</p><ul class=""><li id="4ddc" class="kn ko hi ih b ii ij im in iq kp iu kq iy kr jc ks kt ku kv bi translated"><a class="ae kw" href="https://tunguyen9889.medium.com/how-to-run-spark-job-on-eks-cluster-54f73f90d0bc" rel="noopener">如何在亚马逊EKS集群上运行Spark作业</a>。</li><li id="640e" class="kn ko hi ih b ii kx im ky iq kz iu la iy lb jc ks kt ku kv bi translated"><a class="ae kw" rel="noopener" href="/swlh/how-to-perform-a-spark-submit-to-amazon-eks-cluster-with-irsa-50af9b26cae">如何用IRSA在亚马逊EKS集群上运行Spark作业</a>。</li><li id="a03e" class="kn ko hi ih b ii kx im ky iq kz iu la iy lb jc ks kt ku kv bi translated"><a class="ae kw" href="https://tunguyen9889.medium.com/observability-on-k8s-monitor-kubernetes-clusters-with-datadog-14c597def537" rel="noopener">用DataDog </a>监控Kubernetes集群。</li><li id="b77a" class="kn ko hi ih b ii kx im ky iq kz iu la iy lb jc ks kt ku kv bi translated"><a class="ae kw" rel="noopener" href="/geekculture/observability-on-k8s-datadog-autodiscovery-and-dogstatsd-3404c605fcb7"> DataDog自动发现和DogStatsD </a>。</li></ul><p id="a6a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本教程中，我将使用DataDog自动发现功能来收集Spark指标并将其发送到DataDog。</p><h1 id="6778" class="jk jl hi bd jm jn lc jp jq jr ld jt ju jv le jx jy jz lf kb kc kd lg kf kg kh bi translated">为什么使用自动发现？</h1><p id="81ec" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">DataDog支持<a class="ae kw" href="https://docs.datadoghq.com/integrations/" rel="noopener ugc nofollow" target="_blank"> 450+集成</a>，包括<a class="ae kw" href="https://docs.datadoghq.com/integrations/spark/?tab=host" rel="noopener ugc nofollow" target="_blank"> Spark </a>，可以收集以下指标:</p><ul class=""><li id="e009" class="kn ko hi ih b ii ij im in iq kp iu kq iy kr jc ks kt ku kv bi translated">驱动程序和执行器:RDD块，内存使用，磁盘使用，持续时间等。</li><li id="f298" class="kn ko hi ih b ii kx im ky iq kz iu la iy lb jc ks kt ku kv bi translated">RDDs:分区计数、使用的内存和使用的磁盘。</li><li id="a6ec" class="kn ko hi ih b ii kx im ky iq kz iu la iy lb jc ks kt ku kv bi translated">任务:活动、跳过、失败和总计的任务数。</li><li id="4b3b" class="kn ko hi ih b ii kx im ky iq kz iu la iy lb jc ks kt ku kv bi translated">作业状态:活动、已完成、跳过和失败的作业数。</li></ul><p id="ae2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当Spark安装到主机(EC2、VMs等)时，DataDog代理可以轻松收集这些指标。)或EMR、Mesos、YARN或独立集群。</p><p id="d575" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是当Spark作业被提交并在Kubernetes上运行时，它的行为是不同的:Spark驱动程序和执行程序作为Kubernetes pods运行。每个Spark作业通过driver pod公开一个SparkUI端点，可以用来检查Spark的应用程序和状态。</p><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es lh"><img src="../Images/2116b7acfe3620bab93b5db09b853512.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lw2EuEnrgw0Po9qFL9J3fQ.jpeg"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx">Spark driver UI — 1</figcaption></figure><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es lx"><img src="../Images/e802f3d81dc6e3a74f7fa3c247e783d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TvGbjHm49CC4TPDxal2soQ.jpeg"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx">Spark driver UI — 2</figcaption></figure><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es ly"><img src="../Images/5d7a58a78def6b755cdc72d06399acde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FbavgyMEUEOqxb52GQ-nAw.jpeg"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx">Spark driver UI — 3</figcaption></figure><p id="1131" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的问题是driver pod的IP地址是由Kubernetes网络随机分配的，所以我们无法在Spark integration配置文件中定义正确的<code class="du lz ma mb mc b">spark_url</code>值。这里的另一个问题是，当我们在Kubernetes集群中运行成百上千个Spark作业时，我们不能将所有的<code class="du lz ma mb mc b">spark_url</code>定义到DataDog代理配置文件中。这就是为什么自动发现最适合K8s上的这个火花。</p><h1 id="e80a" class="jk jl hi bd jm jn lc jp jq jr ld jt ju jv le jx jy jz lf kb kc kd lg kf kg kh bi translated">执行样品火花工作</h1><p id="a61e" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">在这篇文章中，我将重用Docker图像，<em class="md">vitamingaugau/spark:spark-2 . 4 . 4-irsa</em>，它是在我的<a class="ae kw" rel="noopener" href="/swlh/how-to-perform-a-spark-submit-to-amazon-eks-cluster-with-irsa-50af9b26cae">以前的博客</a>中构建的。</p><p id="94fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，确保已在DataDog中启用Spark集成:</p><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es me"><img src="../Images/49aea92245d834a7eb99ea5c9bf1e8d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JKlwaLUfGBkK7bPsv5RSmA.jpeg"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx">Screenshoot taken in DataDog integrations console</figcaption></figure><p id="a01c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">找到API控制平面URL:</p><pre class="li lj lk ll fd mf mc mg mh aw mi bi"><span id="cd7d" class="mj jl hi mc b fi mk ml l mm mn">➜  ~ kubectl cluster-info<br/>Kubernetes control plane is running at https://E3C&lt;hidden&gt;626.yl4.ap-southeast-1.eks.amazonaws.com<br/>CoreDNS is running at https://E3C&lt;hidden&gt;626.yl4.ap-southeast-1.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</span><span id="749e" class="mj jl hi mc b fi mo ml l mm mn">To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.</span></pre><p id="7fa6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">记下Kubernetes控制平面的URL，它将用于<em class="md"> spark-submit </em>。</p><p id="a736" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如下创建文件<code class="du lz ma mb mc b">spark-pi.yaml</code>:</p><pre class="li lj lk ll fd mf mc mg mh aw mi bi"><span id="9324" class="mj jl hi mc b fi mk ml l mm mn">---<br/>apiVersion: v1<br/>kind: Namespace<br/>metadata:<br/>  name: spark-pi<br/>---<br/>apiVersion: v1<br/>kind: ServiceAccount<br/>metadata:<br/>  name: spark-pi<br/>  namespace: spark-pi<br/>automountServiceAccountToken: true<br/>---<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>kind: Role<br/>metadata:<br/>  name: spark-pi-role<br/>  namespace: spark-pi<br/>rules:<br/>- apiGroups: [""]<br/>  resources: ["pods", "services", "configmaps"]<br/>  verbs: ["get", "list", "watch", "create", "delete", "update", "patch"]<br/>---<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>kind: RoleBinding<br/>metadata:<br/>  name: spark-pi-role-binding<br/>  namespace: spark-pi<br/>subjects:<br/>- kind: ServiceAccount<br/>  name: spark-pi<br/>  namespace: spark-pi<br/>roleRef:<br/>  kind: Role<br/>  name: spark-pi-role<br/>  apiGroup: rbac.authorization.k8s.io</span></pre><p id="d963" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后运行<code class="du lz ma mb mc b">kubectl apply -f spark-pi.yaml</code>为Spark创建名称空间和RBAC。</p><p id="58f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">创建一个跳转pod来运行<em class="md"> spark-submit </em>:</p><pre class="li lj lk ll fd mf mc mg mh aw mi bi"><span id="f4ad" class="mj jl hi mc b fi mk ml l mm mn">➜  ~ cat &lt;&lt;EOF | kubectl apply -f -<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  labels:<br/>    run: tmp<br/>  name: tmp<br/>  namespace: spark-pi<br/>spec:<br/>  containers:<br/>  - image: vitamingaugau/spark:spark-2.4.4-irsa<br/>    imagePullPolicy: Always<br/>    name: tmp<br/>    args:<br/>    - sleep<br/>    - "1000000"<br/>    resources: {}<br/>  serviceAccountName: spark-pi<br/>EOF</span></pre><p id="b5f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查看<a class="ae kw" href="https://github.com/DataDog/integrations-core/blob/master/spark/datadog_checks/spark/data/conf.yaml.example" rel="noopener ugc nofollow" target="_blank">样本spark.d/conf.yaml </a>了解所有可用的配置选项。您需要将YAML转换成JSON单行格式，以生成自动发现配置，并将其放入<em class="md"> spark-submit </em>。我为这个测试创建了一个示例<code class="du lz ma mb mc b">spark-conf.yaml</code>文件，如下所示(稍后我会解释为什么我们要放入这些值):</p><pre class="li lj lk ll fd mf mc mg mh aw mi bi"><span id="cb8e" class="mj jl hi mc b fi mk ml l mm mn">init_config:</span><span id="116a" class="mj jl hi mc b fi mo ml l mm mn">instances:<br/>  - spark_url: http://%%host%%:4040<br/>    spark_cluster_mode: spark_driver_mode<br/>    cluster_name: spark-k8s</span></pre><p id="0499" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后将文件转换成JSON格式:</p><pre class="li lj lk ll fd mf mc mg mh aw mi bi"><span id="5c0e" class="mj jl hi mc b fi mk ml l mm mn">➜  ~ pip3 install PyYAML<br/>➜  ~ alias yaml2json="python3 -c 'import sys, yaml, json; y=yaml.load(sys.stdin.read(), Loader=yaml.FullLoader); print(json.dumps(y))'"<br/>➜  ~ cat spark-conf.yaml | yaml2json<br/>{"init_config": null, "instances": [{"spark_url": "http://%%host%%:4040", "spark_cluster_mode": "spark_driver_mode", "cluster_name": "spark-k8s"}]}</span></pre><p id="7689" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将使用下面的<code class="du lz ma mb mc b">--conf</code>作为<em class="md">火花提交</em>的自动发现配置。记得在双引号<code class="du lz ma mb mc b">"</code>前加转义<code class="du lz ma mb mc b">\</code>字符，将<code class="du lz ma mb mc b">null</code>替换为<code class="du lz ma mb mc b">[{}]</code>。</p><pre class="li lj lk ll fd mf mc mg mh aw mi bi"><span id="4b53" class="mj jl hi mc b fi mk ml l mm mn">    --conf "spark.kubernetes.driver.annotation.ad.datadoghq.com/spark-kubernetes-driver.check_names=[\"spark\"]" \<br/>    --conf "spark.kubernetes.driver.annotation.ad.datadoghq.com/spark-kubernetes-driver.init_configs=[{}]" \<br/>    --conf "spark.kubernetes.driver.annotation.ad.datadoghq.com/spark-kubernetes-driver.instances=[{\"spark_url\": \"http://%%host%%:4040\", \"spark_cluster_mode\": \"spark_driver_mode\", \"cluster_name\": \"spark-k8s\"}]"</span></pre><p id="7ab0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">执行进入跳跃舱<code class="du lz ma mb mc b">kubectl -n spark-pi exec -it -- bash</code>并执行<em class="md">火花提交</em>:</p><pre class="li lj lk ll fd mf mc mg mh aw mi bi"><span id="d3d1" class="mj jl hi mc b fi mk ml l mm mn">bash-4.4# export K8S_CONTROL_PLANE="https://E3C&lt;hidden&gt;626.yl4.ap-southeast-1.eks.amazonaws.com"</span><span id="620d" class="mj jl hi mc b fi mo ml l mm mn">bash-4.4# /opt/spark/bin/spark-submit \<br/>    --master=k8s://$K8S_CONTROL_PLANE:443 \<br/>    --deploy-mode cluster \<br/>    --name spark-pi \<br/>    --class org.apache.spark.examples.SparkPi \<br/>    --conf spark.kubernetes.driver.pod.name=spark-pi-driver \<br/>    --conf spark.kubernetes.container.image=vitamingaugau/spark:spark-2.4.4-irsa \<br/>    --conf spark.kubernetes.namespace=spark-pi \<br/>    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark-pi \<br/>    --conf spark.kubernetes.authenticate.executor.serviceAccountName=spark-pi \<br/>    --conf spark.kubernetes.authenticate.submission.caCertFile=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt \<br/>    --conf spark.kubernetes.authenticate.submission.oauthTokenFile=/var/run/secrets/kubernetes.io/serviceaccount/token \<br/>    --conf "spark.kubernetes.driver.annotation.ad.datadoghq.com/spark-kubernetes-driver.check_names=[\"spark\"]" \<br/>    --conf "spark.kubernetes.driver.annotation.ad.datadoghq.com/spark-kubernetes-driver.init_configs=[{}]" \<br/>    --conf "spark.kubernetes.driver.annotation.ad.datadoghq.com/spark-kubernetes-driver.instances=[{\"spark_url\": \"http://%%host%%:4040\", \"spark_cluster_mode\": \"spark_driver_mode\", \"cluster_name\": \"spark-k8s\"}]" \<br/>    local:///opt/spark/examples/target/scala-2.11/jars/spark-examples_2.11-2.4.4.jar 20000</span></pre><p id="cf9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在等待作业运行时，打开另一个终端并验证Spark driver pod:</p><pre class="li lj lk ll fd mf mc mg mh aw mi bi"><span id="6bad" class="mj jl hi mc b fi mk ml l mm mn">➜  ~ kubectl -n spark-pi get pod spark-pi-driver -o yaml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  annotations:<br/>    ad.datadoghq.com/spark-kubernetes-driver.check_names: '["spark"]'<br/>    ad.datadoghq.com/spark-kubernetes-driver.init_configs: '[{}]'<br/>    ad.datadoghq.com/spark-kubernetes-driver.instances: '[{"spark_url": "http://%%host%%:4040",<br/>      "spark_cluster_mode": "spark_driver_mode", "cluster_name": "spark-k8s"}]'<br/>    kubernetes.io/psp: eks.privileged<br/>  labels:<br/>    spark-app-selector: spark-2c5bbb79941748be83d62c978e862d6e<br/>    spark-role: driver<br/>  name: spark-pi-driver<br/>  namespace: spark-pi<br/>  ...<br/>spec:<br/>  containers:<br/>  - args:<br/>    - driver<br/>    - --properties-file<br/>    - /opt/spark/conf/spark.properties<br/>    - --class<br/>    - org.apache.spark.examples.SparkPi<br/>    - spark-internal<br/>    - "20000"<br/>    env:<br/>    - name: SPARK_DRIVER_BIND_ADDRESS<br/>      valueFrom:<br/>        fieldRef:<br/>          apiVersion: v1<br/>          fieldPath: status.podIP<br/>    - name: SPARK_LOCAL_DIRS<br/>      value: /var/data/spark-d89cc767-0f63-425c-88ef-44c7fe6d21d1<br/>    - name: SPARK_CONF_DIR<br/>      value: /opt/spark/conf<br/>    image: vitamingaugau/spark:spark-2.4.4-irsa<br/>    imagePullPolicy: IfNotPresent<br/>    name: spark-kubernetes-driver<br/>    ports:<br/>    - containerPort: 7078<br/>      name: driver-rpc-port<br/>      protocol: TCP<br/>    - containerPort: 7079<br/>      name: blockmanager<br/>      protocol: TCP<br/>    - containerPort: 4040<br/>      name: spark-ui<br/>      protocol: TCP<br/>  ...<br/>  nodeName: ip-10-150-123-100.ap-southeast-1.compute.internal</span></pre><p id="3e5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到Spark driver pod具有以下配置:</p><ul class=""><li id="3ed7" class="kn ko hi ih b ii ij im in iq kp iu kq iy kr jc ks kt ku kv bi translated">容器名为<code class="du lz ma mb mc b">spark-kubernetes-driver</code>，因此DataDog自动发现集成模板遵循此格式<code class="du lz ma mb mc b">ad.datadoghq.com/&lt;container_name&gt;</code></li><li id="26a3" class="kn ko hi ih b ii kx im ky iq kz iu la iy lb jc ks kt ku kv bi translated"><code class="du lz ma mb mc b">spark-ui</code>端口暴露在端口<code class="du lz ma mb mc b">4040</code>上，因此<code class="du lz ma mb mc b">spark_url</code>具有值<code class="du lz ma mb mc b">http://%%host%%:4040</code>，其中<code class="du lz ma mb mc b">%%host%%</code>由DataDog代理转换为pod/container的IP地址。</li><li id="b0d0" class="kn ko hi ih b ii kx im ky iq kz iu la iy lb jc ks kt ku kv bi translated">作业以<code class="du lz ma mb mc b">cluster</code>模式运行，因此<code class="du lz ma mb mc b">spark_cluster_mode</code>具有值<code class="du lz ma mb mc b">spark_driver_mode</code>。</li><li id="b2cf" class="kn ko hi ih b ii kx im ky iq kz iu la iy lb jc ks kt ku kv bi translated"><code class="du lz ma mb mc b">cluster_name</code>是一个标签，将被添加到所有收集的具有值<code class="du lz ma mb mc b">spark-k8s</code>的指标中。</li></ul><p id="1c68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们需要找到与<code class="du lz ma mb mc b">spark-pi-driver</code>在同一个节点中运行的DataDog代理pod，以验证集成检查:</p><pre class="li lj lk ll fd mf mc mg mh aw mi bi"><span id="a092" class="mj jl hi mc b fi mk ml l mm mn">➜  ~ kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=ip-10-150-123-100.ap-southeast-1.compute.internal | grep -E "spark-pi-driver|datadog"<br/>addons         datadog-p9fj6       1/1     Running     0     48d    10.150.123.67   ip-10-150-123-100.ap-southeast-1.compute.internal   &lt;none&gt;      &lt;none&gt;<br/>spark-pi       spark-pi-driver     1/1     Running     0     106s   10.150.123.45   ip-10-150-123-100.ap-southeast-1.compute.internal   &lt;none&gt;      &lt;none&gt;</span></pre><p id="dd12" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">运行此命令以验证自动发现是否正常工作，以及代理是否开始收集指标:</p><pre class="li lj lk ll fd mf mc mg mh aw mi bi"><span id="c15b" class="mj jl hi mc b fi mk ml l mm mn">➜  ~ kubectl -n addons exec -it datadog-p9fj6 -- agent check spark<br/>...<br/>=========<br/>Collector<br/>=========</span><span id="d61b" class="mj jl hi mc b fi mo ml l mm mn">Running Checks<br/>  ==============</span><span id="51a4" class="mj jl hi mc b fi mo ml l mm mn">spark (1.15.0)<br/>    --------------<br/>      Instance ID: spark:a83f2f65603a7da0 [OK]<br/>      Configuration Source: kubelet:docker://e5848a0a8e2c99444ca8095a9eebd17f8a02d5dcf516abbf8aebd5f65fec92e1<br/>      Total Runs: 1<br/>      Metric Samples: Last Run: 0, Total: 0<br/>      Events: Last Run: 0, Total: 0<br/>      Service Checks: Last Run: 1, Total: 1<br/>      Average Execution Time : 8ms<br/>      Last Execution Date : 2021-05-16 16:01:06.000000 UTC<br/>      Last Successful Execution Date : 2021-05-16 16:01:06.000000 UTC</span><span id="ede1" class="mj jl hi mc b fi mo ml l mm mn">2021-05-16 16:01:07 UTC | CORE | INFO | (pkg/collector/python/datadog_agent.go:122 in LogMessage) | spark:c6e4a79673c9e1fe | (spark.py:311) | Returning running apps {'spark-***************************863b5': ('Spark Pi', '<a class="ae kw" href="http://10.150.123.45:4040'" rel="noopener ugc nofollow" target="_blank">http://10.150.123.45:4040'</a>)}<br/>=== Series ===<br/>{<br/>  "series": [<br/>    {<br/>      "metric": "spark.executor.active_tasks",<br/>      "points": [<br/>        [<br/>          1621180867,<br/>          2<br/>        ]<br/>      ],<br/>      "tags": [<br/>        "app_name:Spark Pi",<br/>        "cluster_name:spark-k8s",<br/>        "docker_image:vitamingaugau/spark:spark-2.4.4-irsa",<br/>        "image_name:vitamingaugau/spark",<br/>        "image_tag:spark-2.4.4-irsa",<br/>        "kube_container_name:spark-kubernetes-driver",<br/>        "kube_namespace:spark-pi",<br/>        "kube_service:spark-pi-1621180711634-driver-svc",<br/>        "pod_name:spark-pi-driver",<br/>        "pod_phase:running",<br/>        "short_image:spark"<br/>      ],<br/>      "host": "i-00200eb5d332e4263",<br/>      "type": "count",<br/>      "interval": 0,<br/>      "source_type_name": "System"<br/>    },<br/>...<br/>    {<br/>      "metric": "spark.job.num_completed_tasks",<br/>      "points": [<br/>        [<br/>          1621180867,<br/>          13223<br/>        ]<br/>      ],<br/>      "tags": [<br/>        "app_name:Spark Pi",<br/>        "cluster_name:spark-k8s",<br/>        "docker_image:vitamingaugau/spark:spark-2.4.4-irsa",<br/>        "image_name:vitamingaugau/spark",<br/>        "image_tag:spark-2.4.4-irsa",<br/>        "job_id:0",<br/>        "kube_container_name:spark-kubernetes-driver",<br/>        "kube_namespace:spark-pi",<br/>        "kube_service:spark-pi-1621180711634-driver-svc",<br/>        "pod_name:spark-pi-driver",<br/>        "pod_phase:running",<br/>        "short_image:spark",<br/>        "stage_id:0",<br/>        "status:running"<br/>      ],<br/>      "host": "i-00200eb5d332e4263",<br/>      "type": "count",<br/>      "interval": 0,<br/>      "source_type_name": "System"<br/>    },<br/>...<br/>    {<br/>      "metric": "spark.executor.memory_used",<br/>      "points": [<br/>        [<br/>          1621180867,<br/>          1256<br/>        ]<br/>      ],<br/>      "tags": [<br/>        "app_name:Spark Pi",<br/>        "cluster_name:spark-k8s",<br/>        "docker_image:vitamingaugau/spark:spark-2.4.4-irsa",<br/>        "image_name:vitamingaugau/spark",<br/>        "image_tag:spark-2.4.4-irsa",<br/>        "kube_container_name:spark-kubernetes-driver",<br/>        "kube_namespace:spark-pi",<br/>        "kube_service:spark-pi-1621180711634-driver-svc",<br/>        "pod_name:spark-pi-driver",<br/>        "pod_phase:running",<br/>        "short_image:spark"<br/>      ],<br/>      "host": "i-00200eb5d332e4263",<br/>      "type": "count",<br/>      "interval": 0,<br/>      "source_type_name": "System"<br/>    },<br/>...<br/>    {<br/>      "metric": "spark.driver.memory_used",<br/>      "points": [<br/>        [<br/>          1621180867,<br/>          1256<br/>        ]<br/>      ],<br/>      "tags": [<br/>        "app_name:Spark Pi",<br/>        "cluster_name:spark-k8s",<br/>        "docker_image:vitamingaugau/spark:spark-2.4.4-irsa",<br/>        "image_name:vitamingaugau/spark",<br/>        "image_tag:spark-2.4.4-irsa",<br/>        "kube_container_name:spark-kubernetes-driver",<br/>        "kube_namespace:spark-pi",<br/>        "kube_service:spark-pi-1621180711634-driver-svc",<br/>        "pod_name:spark-pi-driver",<br/>        "pod_phase:running",<br/>        "short_image:spark"<br/>      ],<br/>      "host": "i-00200eb5d332e4263",<br/>      "type": "count",<br/>      "interval": 0,<br/>      "source_type_name": "System"<br/>    },<br/>...<br/>=== Service Checks ===<br/>[<br/>  {<br/>    "check": "spark.driver.can_connect",<br/>    "host_name": "i-00eb5d3e43",<br/>    "timestamp": 1621180867,<br/>    "status": 0,<br/>    "message": "Connection to Spark driver \"<a class="ae kw" href="http://10.150.123.45:4040\" rel="noopener ugc nofollow" target="_blank">http://10.150.123.45:4040\</a>" was successful",<br/>    "tags": [<br/>      "cluster_name:spark-k8s",<br/>      "docker_image:vitamingaugau/spark:spark-2.4.4-irsa",<br/>      "image_name:vitamingaugau/spark",<br/>      "image_tag:spark-2.4.4-irsa",<br/>      "kube_container_name:spark-kubernetes-driver",<br/>      "kube_namespace:spark-pi",<br/>      "kube_service:spark-pi-1621180711634-driver-svc",<br/>      "pod_name:spark-pi-driver",<br/>      "pod_phase:running",<br/>      "short_image:spark",<br/>      "url:<a class="ae kw" href="http://10.150.123.45:4040" rel="noopener ugc nofollow" target="_blank">http://10.150.123.45:4040</a>"<br/>    ]<br/>  },<br/>  {<br/>    "check": "spark.application_master.can_connect",<br/>    "host_name": "i-00eb5d3e43",<br/>    "timestamp": 1621180867,<br/>    "status": 0,<br/>    "message": "Connection to ApplicationMaster \"<a class="ae kw" href="http://10.150.123.45:4040\" rel="noopener ugc nofollow" target="_blank">http://10.150.123.45:4040\</a>" was successful",<br/>    "tags": [<br/>      "cluster_name:spark-k8s",<br/>      "docker_image:vitamingaugau/spark:spark-2.4.4-irsa",<br/>      "image_name:vitamingaugau/spark",<br/>      "image_tag:spark-2.4.4-irsa",<br/>      "kube_container_name:spark-kubernetes-driver",<br/>      "kube_namespace:spark-pi",<br/>      "kube_service:spark-pi-1621180711634-driver-svc",<br/>      "pod_name:spark-pi-driver",<br/>      "pod_phase:running",<br/>      "short_image:spark",<br/>      "url:<a class="ae kw" href="http://10.150.123.45:4040" rel="noopener ugc nofollow" target="_blank">http://10.150.123.45:4040</a>"<br/>    ]<br/>  }<br/>]<br/>=========<br/>Collector<br/>=========</span><span id="171e" class="mj jl hi mc b fi mo ml l mm mn">Running Checks<br/>  ==============</span><span id="5034" class="mj jl hi mc b fi mo ml l mm mn">spark (1.15.0)<br/>    --------------<br/>      Instance ID: spark:c6e4a79673c9e1fe [OK]<br/>      Configuration Source: kubelet:docker://ea263fdb3ad10e8485c9c0ac92f4721652c75879eee1eafc858cffc569b6f8ee<br/>      Total Runs: 1<br/>      Metric Samples: Last Run: 50, Total: 50<br/>      Events: Last Run: 0, Total: 0<br/>      Service Checks: Last Run: 3, Total: 3<br/>      Average Execution Time : 71ms<br/>      Last Execution Date : 2021-05-16 16:01:07.000000 UTC<br/>      Last Successful Execution Date : 2021-05-16 16:01:07.000000 UTC<br/>      metadata:<br/>        version.major: 2<br/>        version.minor: 4<br/>        version.patch: 4<br/>        version.raw: 2.4.4<br/>        version.scheme: semver</span></pre><p id="7c37" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">答对了，DataDog代理能够检测到<code class="du lz ma mb mc b">spark-pi-driver</code> pod中的自动发现配置(<em class="md"> podAnnotations </em>)。现在，让我们在DataDog指标资源管理器中进行验证，以找到这些指标:</p><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es mp"><img src="../Images/7c5f8301096a2945a2c006f5b060e2f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LFod4q889avQjLfiYW2HCw.jpeg"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx">Spark metrics in DataDog</figcaption></figure><p id="4959" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查看<a class="ae kw" href="https://docs.datadoghq.com/integrations/spark/?tab=host#data-collected" rel="noopener ugc nofollow" target="_blank">该页面</a>以查看Spark集成的所有支持指标。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="15e5" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">结论</h1><p id="936b" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">在这篇文章中，我们学习了如何使用<em class="md"> spark-submit </em>为K8s作业上的spark定义<em class="md"> podAnnotation </em>，以及如何配置DataDog自动发现来收集Spark应用程序指标。这些指标将有助于调试Spark作业，或者基于它们创建仪表板和监视器。</p><p id="fc29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">希望你喜欢阅读我的博客，并随时留下评论或问题。</p><p id="1cc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查看我的K8s系列Spark教程:</p><div class="mq mr ez fb ms mt"><a href="https://tunguyen9889.medium.com/how-to-run-spark-job-on-eks-cluster-54f73f90d0bc" rel="noopener follow" target="_blank"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hj fi z dy my ea eb mz ed ef hh bi translated">如何在EKS集群上运行Spark作业</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">在本教程中，我将展示如何在EKS集群上运行一个样本Spark作业。</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">tunguyen9889.medium.com</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh lr mt"/></div></div></a></div><div class="mq mr ez fb ms mt"><a rel="noopener follow" target="_blank" href="/swlh/how-to-perform-a-spark-submit-to-amazon-eks-cluster-with-irsa-50af9b26cae"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hj fi z dy my ea eb mz ed ef hh bi translated">如何执行Spark-提交到亚马逊EKS集群与IRSA</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">在上一篇文章中，我介绍了如何向EKS集群提交Spark作业。只要我们使用其他AWS…</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">medium.com</p></div></div><div class="nc l"><div class="ni l ne nf ng nc nh lr mt"/></div></div></a></div></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><div class="li lj lk ll fd mt"><a href="https://about.me/tunguyen9889" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hj fi z dy my ea eb mz ed ef hh bi translated">詹姆斯·nguyễn关于我</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">我是新加坡的云基础设施工程师。看我的博客。</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">关于我</p></div></div><div class="nc l"><div class="nj l ne nf ng nc nh lr mt"/></div></div></a></div></div></div>    
</body>
</html>
<html>
<head>
<title>All About Decision Trees Part :- I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于决策树的所有部分:- I</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/all-about-decision-trees-part-i-cfa148c75631?source=collection_archive---------12-----------------------#2021-06-27">https://medium.com/geekculture/all-about-decision-trees-part-i-cfa148c75631?source=collection_archive---------12-----------------------#2021-06-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/8ea0f3dcd0163133b35fb39a0b9684c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PDxCQkZGMH2nvCAad5lu5w.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx"><strong class="bd iu">Source:-</strong> scikit-learn</figcaption></figure><p id="41a9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">决策树是一种著名的机器学习算法，在回归和分类问题中都有应用。</p><p id="f359" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这种算法的工作原理是作出一个选择的陈述，并根据其结果提供不同的结论。</p><p id="857b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下图是一个非常基本的决策树。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/49e3dcdf2a43985a1c81bd06b7e79c7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*EanQNs2ITr-tysbEICZlrA.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx">Basic Decision Tree(<a class="ae jy" href="https://www.educba.com/what-is-decision-tree/" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="27c7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">默认情况下，在决策树中，通常当条件为真时，子分支显示在左边，当条件为假时，子分支显示在右边。</p><p id="1bd3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以，如果你被提供了正确/错误的图表，你可以考虑默认的方向。</p><h1 id="a44b" class="jz ka hi bd iu kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">与决策树相关的基本术语:</h1><ol class=""><li id="a4a1" class="kw kx hi ix b iy ky jc kz jg la jk lb jo lc js ld le lf lg bi translated"><strong class="ix hj">根节点:</strong> -这是树的开始节点，在此之后，整个群体基于不同的特征而分裂。</li><li id="65e5" class="kw kx hi ix b iy lh jc li jg lj jk lk jo ll js ld le lf lg bi translated"><strong class="ix hj">分支节点</strong> :-这是根节点下进一步分裂的节点。在流程图中，一个分支节点是一个箭头指向和远离的节点。</li><li id="c5e1" class="kw kx hi ix b iy lh jc li jg lj jk lk jo ll js ld le lf lg bi translated"><strong class="ix hj">叶节点:</strong> -不能再拆分决策的节点为叶节点。在流程图中，它是箭头指向的节点，但没有箭头指向远离它的方向。</li></ol><p id="2648" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下图可以帮助你了解树的这些部分。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/5415a7a2a49e8450871c14146bed983f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wXGxIB8hRjBpxrPaZXZkgA.png"/></div></div></figure><p id="9cea" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">关于目标变量，有两种类型的决策树:</p><ol class=""><li id="a869" class="kw kx hi ix b iy iz jc jd jg ln jk lo jo lp js ld le lf lg bi translated"><strong class="ix hj">回归树</strong>:当目标/因变量是一个连续的数值变量时，那么用来预测该数值特征的决策树称为回归树。</li><li id="7725" class="kw kx hi ix b iy lh jc li jg lj jk lk jo ll js ld le lf lg bi translated"><strong class="ix hj">分类树</strong>:当目标/因变量为离散分类变量时，用于预测目标特征的决策树称为分类树。</li></ol><h1 id="ea45" class="jz ka hi bd iu kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">分类树:</h1><p id="d37f" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg lq ji jj jk lr jm jn jo ls jq jr js hb bi translated">现在，让我们试着做一个分类树。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/3378e38a9df03ef39db7c897454b2562.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*7Wh4MYQzUdqt9cModH_0ug.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Credit:- Statquest</figcaption></figure><p id="2244" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，让我们尝试使用这里提供的数据从头开始制作一个决策树。</p><p id="0852" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这个数据中，‘爱凉如冰’是目标变量，其他是自变量。</p><p id="e164" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里，我们试图使用决策树，根据其他参数来预测一个人是否“喜欢冷若冰霜”变量。</p><p id="8e2f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，第一步是决定哪个变量最适合根节点，这是通过了解特定变量是否能够以最有效的方式预测目标变量来完成的。</p><p id="ad70" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们考虑爱爆米花作为根节点:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/445b33112efac00487fe6f072bd9afbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*F8BsnUEx0J48Tp3qis1Wcg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Classification tree made after considering Loves Popcorn as the root node.</figcaption></figure><p id="9730" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如你在这种情况下看到的,“喜欢爆米花”功能无法完全区分用户是否喜欢“像冰一样冷”。</p><p id="17f5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以，现在让我们尝试另一个特性作为根节点。</p><p id="9a82" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们试试爱汽水作为根节点。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/63e62cb72562ae320d5bf79f91cc356d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*7Bcepj4RT7tZiJ4jJtNF6w.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Classification tree made after considering Loves Soda as the root node.</figcaption></figure><p id="f039" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">虽然在这种情况下，不爱汽水的人也不爱冰爽，但在爱汽水的人的情况下，我们仍然不能完全区分爱冰爽或不爱冰爽的用户。</p><p id="738b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为一个变量并不总是能够完全区分目标变量，所以我们应该从所有的从属特征中选择最能区分目标变量的变量。</p><p id="9650" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为此，我们需要首先了解杂质这个术语:</p><p id="963a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上面的两个决策树中，包含是和否的叶子是不纯的，而只包含是或否之一的叶子不是不纯的。</p><div class="ju jv jw jx fd ab cb"><figure class="lv ij lw lx ly lz ma paragraph-image"><img src="../Images/5d36c249e1eba1f34a345e8779dad8cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*AhUmGpZrARJcJdySPE5NPQ.png"/></figure><figure class="lv ij mb lx ly lz ma paragraph-image"><img src="../Images/ac196e0e18f25d529ed57688ae99192a.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*AWXy30ZvzlkFYWq0yTexKw.png"/><figcaption class="iq ir et er es is it bd b be z dx mc di md me">Left leaf is impure and Right Leaf is not impure</figcaption></figure></div><p id="0b12" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，只有该变量被用作根节点，这为目标特征提供了最少的杂质。</p><p id="71f0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，有几个方法来量化一片叶子的T2杂质，熵，信息增益，基尼杂质，我将使用基尼杂质来解释分类树。</p><p id="4f9a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">计算一片叶子的<strong class="ix hj">基尼系数的公式是</strong>:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/d31c46a9f03eb0cc93fe9566f0017c01.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*iSu8A1w59ny1V3HCASOvuw.png"/></div></figure><div class="ju jv jw jx fd ab cb"><figure class="lv ij mg lx ly lz ma paragraph-image"><img src="../Images/594eacead0d8d5fd77a78b6cdafac6ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*p6zttn7SFneeHV_3ZB6BOg.png"/></figure><figure class="lv ij mh lx ly lz ma paragraph-image"><img src="../Images/ef85126a320ebb26229d8feb167c5d60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*_Lj5aYU3miH1eXqusmBabw.png"/><figcaption class="iq ir et er es is it bd b be z dx mi di mj me">Calculation Gini Impurity of the Corresponding leaf</figcaption></figure></div><p id="0bf1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">计算一片叶子的基尼系数是不够的，因为我们应该计算根节点完成的整个分裂的基尼系数。因此，最后通过计算叶子的<strong class="ix hj">加权平均基尼系数</strong>来计算树的<strong class="ix hj">总基尼系数</strong>，其由<strong class="ix hj"> : </strong>给出</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/71b80c0a269433c1e0cdc55e279d6836.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*DngWiOeRl9RktTnjSJGiuQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Formula to calculate total Gini impurity of a tree</figcaption></figure><div class="ju jv jw jx fd ab cb"><figure class="lv ij ml lx ly lz ma paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/ac3e4e539bfc6e9a858359253b9d95c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*jzWY7OxFYWxWN1ZZGOcJzQ.png"/></div></figure><figure class="lv ij mm lx ly lz ma paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/db34e9dd5540a659ebdf2780f6b124ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*HV4T6sqIqADX7imfavJMHA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx mn di mo me">Total Gini Impurity</figcaption></figure></div><p id="8369" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，具有不同根节点的每棵树的<strong class="ix hj">总基尼系数杂质</strong>被比较，并且负责最低基尼系数杂质的变量被认为是根节点。</p><p id="f1a9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，问题出现了，因为在上述数据中也有一个数字栏，在数字数据的情况下，如何计算基尼系数。</p><p id="a88d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">而在此之前又出现了一个问题:应该根据哪个数值来创建分支，是应该年龄&lt; 12，年龄&lt; 18，年龄&lt; 38还是别的！！。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/0ecfeb956b75f42fb8d033038682929e.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*xpJ9_wkiWXt8Dtv3XNlmJQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Mean of Each Pair</figcaption></figure><p id="ad4c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了基于应该创建哪些分支来决定数值，应该首先计算每对年龄的平均值，然后将每个平均值视为阈值，需要计算基尼系数，然后具有最低基尼系数的平均值成为根节点。</p><p id="f0b5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，如下所示，计算并比较了每种情况下的总基尼系数。</p><div class="ju jv jw jx fd ab cb"><figure class="lv ij mq lx ly lz ma paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/7ceafd1e1cffff0ce1f7d4b91abb7c0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*Y5faM6DSYD4OBBUxtMy5oQ.png"/></div></figure><figure class="lv ij mr lx ly lz ma paragraph-image"><img src="../Images/9219901f632601e6d9e7baea83c0c6d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*9ej8CPYzy-62GHffOUmiKQ.png"/></figure></div><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ms"><img src="../Images/8a420e0d90b7eebeae9454afb419bcf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*A7hBlVqf5lFNJs9wYQIjqQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Gini Impurity in each case</figcaption></figure><p id="14a7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，正如我们在这里看到的，基尼系数0.343是最小的，因此在这种情况下，可以选择15或44作为阈值。</p><p id="5e35" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，现在为了决定最终的根节点，比较所有独立特征的基尼系数。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/53612dac50b637e19ff62accbc0acaf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*hjWDjZe2sylGunrqvtxWQA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Total Gini Impurity of all columns</figcaption></figure><p id="9d60" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，在这种情况下,“爱喝汽水”成为决策树的根节点。</p><p id="4e15" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是我们不能就此止步！！！！</p><p id="2071" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们刚刚找到了根节点，但是现在还没有完成整个树..</p><p id="80c2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在决定了根节点之后，现在我们应该决定分支。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mu"><img src="../Images/99e6acb4b82771fdb6c2f03c35b607de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vpIFneRT08kG3bo0a8iIUQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Deciding the parameter for branch node</figcaption></figure><p id="9554" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，为了选择用于进一步分裂的参数，重复上述基尼系数杂质比较的整个过程，但是应该记住遵循根节点的条件，即，仅考虑遵循根节点条件的那些行。</p><p id="3ff7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">例如，在上图中，为了确定是在分支节点中保留“Age”还是“Loves Popcorn ”,只需考虑“Loves Soda”的值为“Yes”的那些行。</p><p id="2be9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，用于进一步处理的表格将如下所示</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mv"><img src="../Images/04aea80e0066e6fe3ca410ab8e943d8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*2-lDkE-M_X5grcPGegnuIg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Those who don’t love soda are in the leaf node</figcaption></figure><p id="69c3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，现在保持我们的目标变量不变，我们将只考虑那些<strong class="ix hj">“爱喝汽水”=“是”</strong>的项目，现在比较其他列相对于“爱喝冰爽”的基尼系数。</p><p id="6c12" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，在比较了其他列的基尼系数后，最终的树看起来会像这样，你也可以自己尝试一下</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/492feda6ffc5f353483dc2164421ffd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*rWcw0FNU1jAledlXtjp-BA.png"/></div></figure><p id="2c92" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">查看上面的结果，最终的树可以解释为:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mx"><img src="../Images/eff655f6a90ecf5ec7d4924f1efc67e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*4F5KdYjR2pdfIfqDmiGkWA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Decision Tree</figcaption></figure><p id="9193" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，如果一个新的数据来了，我们可以沿着上面的树来预测目标变量，</p><p id="f407" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如你所看到的，在当前的情况下，一个记录到达了说不像冰一样冷的爱的叶子，所以它可能是我们的树正在记忆数据或过度拟合数据。</p><p id="3a10" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以，防止这个问题有两种方法:</p><ol class=""><li id="baf5" class="kw kx hi ix b iy iz jc jd jg ln jk lo jo lp js ld le lf lg bi translated">修剪。</li><li id="4fba" class="kw kx hi ix b iy lh jc li jg lj jk lk jo ll js ld le lf lg bi translated">设置一个阈值，即一个叶子只能包含3个或更多记录，否则将不会生成叶子。</li></ol></div><div class="ab cl my mz gp na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="hb hc hd he hf"><p id="6874" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">参考文献:</strong></p><p id="2475" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae jy" href="https://www.youtube.com/user/joshstarmer" rel="noopener ugc nofollow" target="_blank">与乔希·斯塔默的StatQuest】</a></p></div></div>    
</body>
</html>
<html>
<head>
<title>GPU for Deep Learning: Benefits &amp; Drawbacks of On-Premises vs Cloud</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于深度学习的GPU:内部部署与云的优缺点</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/gpu-for-deep-learning-benefits-drawbacks-of-on-premises-vs-cloud-2a3e1e0e4ccf?source=collection_archive---------24-----------------------#2021-09-10">https://medium.com/geekculture/gpu-for-deep-learning-benefits-drawbacks-of-on-premises-vs-cloud-2a3e1e0e4ccf?source=collection_archive---------24-----------------------#2021-09-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/090ee9877ce993427cf8805271c2ed6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BHIEICsu7iTsDfweSjDFPA.png"/></div></div></figure><p id="c9e1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">随着技术的进步，越来越多的组织正在实施机器学习操作(<a class="ae jo" href="https://www.run.ai/blog/machine-learning-operations-mlops-what-is-it-and-why-we-need-it/" rel="noopener ugc nofollow" target="_blank"> MLOps </a>)，人们正在寻找加快过程的方法。对于使用深度学习(DL)流程的组织来说尤其如此，这种流程的运行时间可能会非常长。您可以通过在内部或云中使用图形处理单元(GPU)来加速这一过程。</p><p id="79c9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">GPU是专为执行特定任务而设计的微处理器。这些单元可以并行处理任务，并可以进行优化，以提高人工智能和深度学习过程的性能。</p><p id="49e4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">GPU是一个强大的工具，可以通过深度神经网络来加速数据管道。使用GPU的第一个原因是，与价格相同的中央处理器(CPU)相比，DNN推理在GPU上的运行速度快3-4倍。第二个原因是，减轻CPU的一些负载可以让您在相同的情况下做更多的工作，并降低整体网络负载。</p><p id="0da6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用GPU的典型深度学习管道包括:</p><ul class=""><li id="7662" class="jp jq hi is b it iu ix iy jb jr jf js jj jt jn ju jv jw jx bi translated">数据预处理(CPU)</li><li id="5c6d" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn ju jv jw jx bi translated">DNN执行:训练还是推理(GPU)</li><li id="65ed" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn ju jv jw jx bi translated">数据后处理(CPU)</li></ul><figure class="ke kf kg kh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kd"><img src="../Images/e222b88cf5841ddba2c987a527112712.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Q6vHVi-leXkgC47I.jpg"/></div></div></figure><p id="e64a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">CPU ram和GPU ram之间的数据传输是最常见的瓶颈。因此，构建数据科学管道架构有两个主要目标。首先是通过将几个样本(图像)聚集成一批来减少传输数据事务的数量。第二种是通过在传输之前过滤数据来减少特定样本的大小。</p><p id="7ab7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">训练和实现DL模型需要使用深度神经网络(DNN)和具有成千上万个数据点的数据集。这些网络需要大量资源，包括内存、存储和处理能力。虽然中央处理器(CPU)可以提供这种能力，但使用图形处理器(GPU)可以大大加快处理速度。</p><h1 id="81b4" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">使用GPU进行深度学习的主要好处</h1><ol class=""><li id="bae7" class="jp jq hi is b it lg ix lh jb li jf lj jj lk jn ll jv jw jx bi translated"><strong class="is hj">核心数量</strong>-GPU可以有大量的核心，可以集群，可以和CPU组合。这使您能够显著提高处理能力。</li><li id="1f17" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn ll jv jw jx bi translated"><strong class="is hj">更高的内存</strong>——GPU可以提供比CPU更高的内存带宽(高达750GB/s对50GB/s)。这使你能够更容易地处理深度学习所需的大量数据。</li><li id="4321" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn ll jv jw jx bi translated"><strong class="is hj">灵活性</strong>-GPU的并行能力使您能够在集群中组合GPU，并在集群中分配任务。或者，您可以单独使用GPU和分配给单个算法训练的集群。</li></ol><h1 id="393b" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">当使用GPU进行深度学习任务是不理性的</h1><p id="e685" class="pw-post-body-paragraph iq ir hi is b it lg iv iw ix lh iz ja jb lm jd je jf ln jh ji jj lo jl jm jn hb bi translated">与CPU相比，GPU非常快，对于许多人工智能应用程序来说，GPU是必备的。但在某些情况下，GPU是多余的，你至少应该暂时使用CPU来节省预算。</p><p id="f9d3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里需要说几句GPU计算的成本。我们之前提到过，GPU明显比CPU快，但是计算成本可能比你转用GPU获得的速度还要大。</p><p id="5e34" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，在开发之初，例如，在开发概念证明(PoC)或最低可行产品(MVP)时，您可以使用CPU进行开发和准备服务器，如果您的用户可以接受长时间的响应，您可以将CPU用于生产服务器，但只能使用很短的时间。</p><h1 id="2b65" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">用于深度学习的内部GPU选项</h1><p id="c508" class="pw-post-body-paragraph iq ir hi is b it lg iv iw ix lh iz ja jb lm jd je jf ln jh ji jj lo jl jm jn hb bi translated">使用GPU进行内部部署时，您有多个供应商选项。两个比较受欢迎的选择是NVIDIA和AMD。</p><h1 id="9076" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">英伟达</h1><p id="02e7" class="pw-post-body-paragraph iq ir hi is b it lg iv iw ix lh iz ja jb lm jd je jf ln jh ji jj lo jl jm jn hb bi translated">NVIDIA是一个受欢迎的选择，至少部分是因为它提供的库，被称为<a class="ae jo" href="https://developer.nvidia.com/cuda-toolkit" rel="noopener ugc nofollow" target="_blank"> CUDA工具包</a>。这些库可以轻松建立深度学习流程，并通过NVIDIA产品形成强大的机器学习社区的基础。这可以从许多DL库和框架为NVIDIA硬件提供的广泛支持中看出。</p><p id="0de5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">除了GPU，该公司还提供支持流行的DL框架的库，包括PyTorch。Apex库尤其有用，它包括几个融合的快速优化器，如FusedAdam。</p><p id="e592" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">NVIDIA的缺点是，它最近对CUDA何时可以使用进行了限制。这些限制要求库只能与Tesla GPUs一起使用，不能与较便宜的RTX或GTX硬件一起使用。</p><p id="f218" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这对培训DL模型的组织有严重的预算影响。当你考虑到虽然Tesla GPUs并没有比其他选项提供更高的性能，但其价格却高达10倍时，这也是一个问题。</p><h1 id="fd4d" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">超微半导体公司</h1><p id="c3c9" class="pw-post-body-paragraph iq ir hi is b it lg iv iw ix lh iz ja jb lm jd je jf ln jh ji jj lo jl jm jn hb bi translated">AMD提供库，被称为<a class="ae jo" href="https://rocmdocs.amd.com/en/latest/" rel="noopener ugc nofollow" target="_blank"> ROCm </a>。TensorFlow和PyTorch以及所有主要的网络架构都支持这些库。然而，对发展新网络的支持是有限的，社区支持也是如此。</p><p id="867b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">另一个问题是，AMD对其深度学习软件的投资没有英伟达多。因此，除了较低的价格点之外，AMD GPUs与NVIDIA相比提供的功能有限。</p><h1 id="18b4" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">使用GPU的云计算</h1><p id="4b51" class="pw-post-body-paragraph iq ir hi is b it lg iv iw ix lh iz ja jb lm jd je jf ln jh ji jj lo jl jm jn hb bi translated">在培训DL模型的组织中，一个越来越受欢迎的选择是使用云资源。这些资源可以结合优化的机器学习服务，提供对GPU的按使用付费访问。三大提供商都提供GPU资源和大量配置选项。</p><h1 id="e669" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">微软Azure</h1><p id="05da" class="pw-post-body-paragraph iq ir hi is b it lg iv iw ix lh iz ja jb lm jd je jf ln jh ji jj lo jl jm jn hb bi translated">微软Azure为GPU访问授予了各种实例选项。这些实例已经针对高计算任务进行了优化，包括可视化、模拟和深度学习。</p><p id="a58e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在Azure中，有三个主要系列的实例可供选择:</p><ol class=""><li id="2c69" class="jp jq hi is b it iu ix iy jb jr jf js jj jt jn ll jv jw jx bi translated"><strong class="is hj"> NC系列</strong>-针对网络和计算密集型工作负载优化的实例。例如，基于CUDA和OpenCL的模拟和应用。实例基于NVIDIA Tesla V100、英特尔Haswell或英特尔Broadwell GPUs提供高性能。</li><li id="86f1" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn ll jv jw jx bi translated"><strong class="is hj"> ND系列</strong>-针对深度学习的推理和训练场景优化的实例。实例提供对NVIDIA Tesla P40、英特尔Broadwell或英特尔Skylake GPUs的访问。</li><li id="236d" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn ll jv jw jx bi translated"><strong class="is hj"> NV系列</strong>-针对虚拟桌面基础架构、流、编码或可视化进行优化的实例，支持DirectX和OpenGL。实例提供对NVIDIA Tesla M60或AMD镭龙Instinct MI25 GPUs的访问。</li></ol><h1 id="44aa" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">亚马逊网络服务(AWS)</h1><p id="72f8" class="pw-post-body-paragraph iq ir hi is b it lg iv iw ix lh iz ja jb lm jd je jf ln jh ji jj lo jl jm jn hb bi translated">在AWS中，您可以从四个不同的选项中进行选择，每个选项都有各种各样的实例大小<a class="ae jo" href="https://docs.aws.amazon.com/dlami/latest/devguide/gpu.html" rel="noopener ugc nofollow" target="_blank">。选项包括EC2 P3、P2、G4和G3实例。这些选项使您能够在NVIDIA Tesla V100、K80、T4张量或M60 GPUs之间进行选择。根据具体情况，您最多可以扩展到16个GPU。</a></p><p id="3411" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了增强这些实例，AWS还提供了Amazon Elastic Graphics，这是一种使您能够将低成本GPU选项附加到EC2实例的服务。这使您能够根据需要将GPU用于任何兼容的实例。该服务为您的工作负载提供了更大的灵活性。Elastic Graphics支持OpenGL 4.3，并可提供高达8GB的图形内存。</p><h1 id="cd97" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">谷歌云</h1><p id="bd22" class="pw-post-body-paragraph iq ir hi is b it lg iv iw ix lh iz ja jb lm jd je jf ln jh ji jj lo jl jm jn hb bi translated">谷歌云使你能够将GPU附加到现有的实例上，而不是专用的GPU实例。例如，如果您使用Google Kubernetes引擎，您可以创建访问一系列GPU的节点池。其中包括NVIDIA Tesla K80、P100、P4、V100和T4 GPU。</p><p id="585c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">谷歌云还提供张量流处理单元(<a class="ae jo" rel="noopener" href="/sciforce/understanding-tensor-processing-units-10ff41f50e78"> TPU </a>)。该单元包括多个设计用于执行快速矩阵乘法的GPU。它提供了与启用张量核心的Tesla V100实例相似的性能。TPU的优势在于它可以通过并行化节省成本。</p><p id="5eec" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">每个TPU相当于四个GPU，支持相对较大的部署。此外，PyTorch现在至少部分支持TPU。</p><h1 id="68f7" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">2021年深度学习任务最好的GPU是什么？</h1><p id="835c" class="pw-post-body-paragraph iq ir hi is b it lg iv iw ix lh iz ja jb lm jd je jf ln jh ji jj lo jl jm jn hb bi translated">到了选择基础架构的时候，您需要在内部部署和云方法之间做出选择。云资源可以显著降低构建DL基础架构的财务障碍。</p><p id="f08e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些服务还可以提供可伸缩性和提供商支持。然而，这些基础设施最适合短期项目，因为持续的资源使用会导致成本激增。</p><p id="c4b0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">相比之下，内部基础架构的前期成本更高，但为您提供了更大的灵活性。你可以用你的硬件在一个稳定的成本下，在一段时间内做你想做的实验。您还可以完全控制您的配置、安全性和数据。</p><p id="90f2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于刚刚起步的组织来说，云基础架构更有意义。这些部署使您能够以最少的前期投资开始运行，并让您有时间来完善您的流程和要求。但是，一旦您的业务发展到足够大的规模，就可以选择切换到内部部署。</p><h1 id="7bc5" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">MobiDev使用GPU进行人工智能训练</h1><p id="4d58" class="pw-post-body-paragraph iq ir hi is b it lg iv iw ix lh iz ja jb lm jd je jf ln jh ji jj lo jl jm jn hb bi translated">我们的人工智能团队拥有巨大的计算资源，如一组V100 GPUs。所有这些GPU都可以通过我们的内部计算服务访问。</p><p id="cd0f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">计算服务只是一台安装了大量磁盘空间、RAM和GPU并运行Linux的计算机。我们使用这项服务来训练人工智能解决方案和进行研究。</p><p id="0479" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通常情况下，传统的深度学习框架像Tensorflow、Pytorch或ONNX都不能直接访问GPU核来解决其上的深度学习问题。在人工智能应用和GPU之间有许多复杂的特殊软件层，如CUDA和GPU的驱动程序。</p><p id="d3fa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在一个过于简化的模式中，它可以如下所示。</p><p id="d17e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">即使在一组人工智能工程师使用这样的计算服务构建的情况下，该模式看起来也是合法和健壮的。</p><p id="7a51" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但在人工智能软件开发的现实生活中，新版本的人工智能应用程序、人工智能框架、CUDA和GPU驱动程序不断涌现。而且经常出现软件的新版本与旧版本不兼容的情况。例如，我们必须在我们的计算服务上使用与当前版本CUDA不兼容的新版本AI框架。在这样的情况下，我们该怎么办？我们应该更新CUDA吗？</p><p id="0cd3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">毫无疑问，我们不能这样做，因为其他一些人工智能工程师的项目需要旧版本的CUDA。这就是问题所在。</p><p id="a40c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以，问题是我们不能像在其他系统上一样，在我们的计算服务中安装两个不同版本的CUDA。如果我们可以做一些小把戏，神奇地将我们的应用程序彼此隔离，使它们不接触彼此，并且不知道彼此的存在，那会怎么样？谢天谢地，现在我们确实有这样一个窍门！这个技巧的名字叫停靠化技术。</p><p id="ab65" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们使用<em class="lp"> Docker </em>和<em class="lp"> nvidia-docker </em>来包装人工智能应用以及所有必要的依赖，如人工智能框架和适当版本的CUDA。这种方法使我们能够在同一台计算服务机器上维护不同版本的Tensorflow、Pytorch和CUDA。</p><p id="2f5d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">人工智能解决方案dockerization的简单模式如下所示。</p><h1 id="ae8f" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">结论</h1><p id="24fe" class="pw-post-body-paragraph iq ir hi is b it lg iv iw ix lh iz ja jb lm jd je jf ln jh ji jj lo jl jm jn hb bi translated">为了快速前进，机器学习工作负载需要高处理能力。与CPU相反，GPU可以提供更高的处理能力、更高的内存带宽和并行能力。</p><p id="9ae6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您可以在内部或云中使用GPU。流行的本地GPU包括NVIDIA和AMD。基于云的GPU可以由许多云供应商提供，包括前三名——Azure、AWS和Google Cloud。在选择内部部署和云GPU资源时，您应该考虑预算和技能。</p><p id="7745" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">内部资源通常会带来很高的前期开销，但从长期来看，成本会保持稳定。但是，如果您没有操作内部资源的必要技能，您应该考虑云产品，云产品更容易扩展，并且通常带有托管选项。</p></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><p id="944b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由<a class="ae jo" href="https://mobidev.biz/our-team/evgeniy-krasnokutsky" rel="noopener ugc nofollow" target="_blank"> Evgeniy Krasnokutsky，</a> AI/ML解决方案架构师在<a class="ae jo" href="https://mobidev.biz/services/machine-learning-consulting" rel="noopener ugc nofollow" target="_blank"> MobiDev </a>撰写。</p><p id="3394" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="lp">全文原载于</em><a class="ae jo" href="https://mobidev.biz/blog/gpu-deep-learning-on-premises-vs-cloud" rel="noopener ugc nofollow" target="_blank"><em class="lp">https://mobidev . biz</em></a><em class="lp">，基于mobi dev技术研究。</em></p></div></div>    
</body>
</html>
<html>
<head>
<title>Sometimes more data can hurt</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">有时候，更多的数据会造成伤害</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/sometimes-more-data-can-hurt-992331559d4f?source=collection_archive---------46-----------------------#2021-05-23">https://medium.com/geekculture/sometimes-more-data-can-hurt-992331559d4f?source=collection_archive---------46-----------------------#2021-05-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="0c8b" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">不信？我也没有！</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/b1439073ba43434f0143d79fbd5c8519.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*orocf6RSlWX1BGoa.jpg"/></div></div></figure><p id="6bc2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">本·怀特在<a class="ae kf" href="https://unsplash.com/s/photos/wow?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p><p id="f36c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这里有一个提示:在某些情况下，更多的样本实际上会降低模型的性能。不信？我也没有！请继续阅读，看看我是如何使用模拟研究来演示这一现象的。</p><h1 id="ae0e" class="kg kh hi bd ki kj kk kl km kn ko kp kq io kr ip ks ir kt is ku iu kv iv kw kx bi translated">一些背景</h1><p id="6476" class="pw-post-body-paragraph jj jk hi jl b jm ky ij jo jp kz im jr js la ju jv jw lb jy jz ka lc kc kd ke hb bi translated">最近在我的个人博客上的<a class="ae kf" href="https://iyarlin.github.io/2021/03/09/sparse_matrix_representation_for_ml_in_scale/" rel="noopener ugc nofollow" target="_blank">帖子</a>中，我讨论了我在工作中开发的一个可扩展的稀疏线性回归模型。它的一个有趣的特性是，它是一个插值模型，这意味着它的训练误差为0。这是因为它是过度参数化的，因此可以完美地拟合训练数据。</p><p id="355d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">虽然0-训练错误通常与过度拟合相关联，但是该模型在测试集上表现得相当好。近年来，关于似乎不会遭受过度拟合(特别是在深度学习中)的巨大参数化模型的报告一直在积累，因此关于该主题的文献也在积累。</p><p id="750f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">例如，我自己的模型在没有任何调整的情况下表现最佳。被这种不寻常的行为所吸引，我开始更好地理解发生了什么。在这篇<a class="ae kf" href="https://towardsdatascience.com/something-every-data-scientist-should-know-but-probably-doesnt-the-bias-variance-trade-off-25d97a17329d" rel="noopener" target="_blank">中期文章</a>中，我很好地介绍了这个主题。</p><p id="22f9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">由于人工神经网络是非常复杂的算法，所以在一个简单的环境中研究这个主题并建立直觉是一个好主意。Preetum Nakkiran的伟大论文<a class="ae kf" href="https://arxiv.org/pdf/1912.07242.pdf" rel="noopener ugc nofollow" target="_blank">“更多的数据会对线性回归造成伤害:基于样本的双重下降”</a>对此进行了阐述。</p><p id="a034" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我将简要总结一下问题设置:假设我们有从一个有1000个协变量(没有截距)的线性模型中生成的数据。对于每一个样本量<em class="ld"> n </em>，我们拟合一个线性回归，并在一个支持测试集上测量MSE。</p><p id="6935" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在<em class="ld"> n </em> ≥1000的情况下，我们采用常规回归模型。在<em class="ld"> n </em> &lt; 1000的情况下，我们有<em class="ld"> p </em> &gt; <em class="ld"> n </em>并且没有封闭形式的解，因为设计矩阵的逆不存在。</p><p id="49dd" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">方程<em class="ld"> Y </em> =X\beta在这种情况下有无穷多个解。在这些解决方案中，最小化系数L2范数||\beta||的解决方案具有最低的方差，因此应该在测试集上具有最佳的性能(在Nakkiran的论文中对此有更多描述)。我们可以使用质量包中实现的矩阵X的摩尔-彭罗斯广义逆来找到最小范数L2解。</p><p id="4c78" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">下面我们可以看到论文中的模拟结果:我们可以看到Num附近的某个地方。样本= 900当样本数量增加到1000时，测试误差实际上增大了:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es le"><img src="../Images/837510f683a87b309b09a836d809f4ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*S5QPeKFVZmmb-nRj.png"/></div></div></figure><p id="8039" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我发现这个结果有点难以置信！担心这可能是另一个<a class="ae kf" href="https://en.wikipedia.org/wiki/Replication_crisis" rel="noopener ugc nofollow" target="_blank">复制危机</a>的案例，我决定我必须亲眼看看。因此，在下面我将复制论文的结果。</p><h1 id="412c" class="kg kh hi bd ki kj kk kl km kn ko kp kq io kr ip ks ir kt is ku iu kv iv kw kx bi translated">结果复制</h1><p id="e60a" class="pw-post-body-paragraph jj jk hi jl b jm ky ij jo jp kz im jr js la ju jv jw lb jy jz ka lc kc kd ke hb bi translated">首先，我们设置模拟参数:</p><pre class="iy iz ja jb fd lf lg lh li aw lj bi"><span id="64b4" class="lk kh hi lg b fi ll lm l ln lo">beta &lt;- runif(1000) # real coefficients <br/>beta &lt;- beta/sqrt(sum(beta^2)) ## convert to a unit vector <br/>M &lt;- 50 ## number of simulations <br/>N &lt;- c(2, seq(100, 800, 100), seq(900, 990, 10), seq(991,1000,1), seq(1001, 1009, 1), seq(1010, 1100, 10), seq(1200, 2000, 100)) ## number of observations <br/>test_MSE &lt;- matrix(nrow = length(N), ncol = M)</span></pre><p id="b334" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">下面我们进行实际模拟:</p><pre class="iy iz ja jb fd lf lg lh li aw lj bi"><span id="a200" class="lk kh hi lg b fi ll lm l ln lo">library(MASS)</span><span id="6693" class="lk kh hi lg b fi lp lm l ln lo">for (i in 1:length(N)){ <br/>  for (m in 1:M){ <br/>    print(paste0("n=", N[i], ", m=", m)) <br/>    # generate training data <br/>    X &lt;- replicate(1000, rnorm(N[i])) <br/>    e &lt;- rnorm(N[i], sd = 0.1) <br/>    y &lt;- X %*% beta + e <br/>    if (N[i] &lt; 1000){ <br/>      # Moore-penrose generalized matrix inverse<br/>      beta_hat &lt;- ginv(X) %*% y <br/>    } else { <br/>      dat &lt;- as.data.frame(cbind(y, X)) <br/>      names(dat)[1] &lt;- "y" <br/>      lm_model &lt;- lm(y ~ .-1, data = dat) # regular fit <br/>      beta_hat &lt;- matrix(lm_model$coefficients, ncol = 1) <br/>    } </span><span id="4a38" class="lk kh hi lg b fi lp lm l ln lo">    # generate test set <br/>    X_test &lt;- replicate(1000, rnorm(10000)) <br/>    e_test &lt;- rnorm(10000, sd = 0.1) <br/>    y_test &lt;- X_test %*% beta + e_test </span><span id="ff33" class="lk kh hi lg b fi lp lm l ln lo">    # measure model accuracy<br/>    preds_test &lt;- X_test %*% beta_hat <br/>    test_MSE[i, m] &lt;- sqrt(mean((y_test - preds_test)^2)) <br/>  } <br/>}</span></pre><p id="5d3c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">让我们画出结果:</p><pre class="iy iz ja jb fd lf lg lh li aw lj bi"><span id="38d2" class="lk kh hi lg b fi ll lm l ln lo">matplot(N, test_MSE, type = "p", pch = ".", ylim = c(0,5), <br/>xaxt = "n", xlim = c(0,2000), ylab = "Test MSE", xlab = "Num. Samples") <br/>axis(1, at = seq(0,2000,250)) <br/>lines(N, apply(test_MSE, 1, mean), col = "red")</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lq"><img src="../Images/42a9c2f7a71f0c0884d03cf366092b0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lS5ZYddUo5p-UhsV.png"/></div></div></figure><p id="bbd1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">令人惊讶的是，我们得到了完全相同的结果！</p><p id="8cd9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如果你想了解这背后的理论，去看看这篇论文。</p></div><div class="ab cl lr ls gp lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="hb hc hd he hf"><p id="4dac" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><em class="ld">原载于2021年5月23日</em><a class="ae kf" href="https://iyarlin.github.io/2021/05/23/sample_wise_double_descent_results_reproduction/" rel="noopener ugc nofollow" target="_blank"><em class="ld">https://iyarlin . github . io</em></a><em class="ld">。</em></p></div></div>    
</body>
</html>
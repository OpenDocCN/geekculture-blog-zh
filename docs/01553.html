<html>
<head>
<title>Sentence Correction Using Recurrent Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用递归神经网络的句子校正</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/sentence-correction-using-recurrent-neural-network-6321527ee08b?source=collection_archive---------8-----------------------#2021-04-15">https://medium.com/geekculture/sentence-correction-using-recurrent-neural-network-6321527ee08b?source=collection_archive---------8-----------------------#2021-04-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/6b47f639eb48f9f75affe3060be0fe1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*fs0M5zZqms6JyPyDSLki8A.png"/></div></figure><h1 id="fdf2" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated"><strong class="ak">简介:</strong></h1><p id="26ff" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">社交媒体是一个平台，人们可以通过在虚拟网络中创建、交换或分享他们的想法来相互交流。大多数人都在使用社交媒体用文字来表达他们的感受。大多数ML/DL模型使用这些文本来确定情绪或预测任何犯罪活动和许多与NLP相关的任务。ML和DL模型是用传统语言训练的，对于任何NLP相关的任务，大多数是英语。现在，人们在他们的文本中使用简短的形式/缩写，比如(ppl表示人，2表示to，wen/whn表示时间等等),这可能对基于NLP的任务没有太大帮助。</p><h1 id="ba45" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated"><strong class="ak">目录</strong></h1><ul class=""><li id="2132" class="ki kj hi jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated"><strong class="jm hj"> <em class="kr">商业问题</em> </strong></li><li id="9be6" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><strong class="jm hj"> <em class="kr">深度学习问题</em> </strong></li><li id="cb92" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><strong class="jm hj"> <em class="kr">数据集概述</em> </strong></li><li id="004b" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><strong class="jm hj"> <em class="kr">损失</em> </strong></li><li id="ab27" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><strong class="jm hj"> <em class="kr"> EDA </em> </strong></li><li id="3e7b" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><strong class="jm hj"> <em class="kr">数据预处理</em> </strong></li><li id="f7a3" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><strong class="jm hj"> <em class="kr">深度学习模型</em> </strong></li><li id="b3bb" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><strong class="jm hj"> <em class="kr">车型评价</em> </strong></li><li id="df50" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><strong class="jm hj"> <em class="kr">模型预测</em> </strong></li><li id="83b4" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><strong class="jm hj"> <em class="kr">错误分析</em> </strong></li><li id="804d" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><strong class="jm hj"> <em class="kr">最终管道</em> </strong></li><li id="b150" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><strong class="jm hj"> <em class="kr">模型推断</em> </strong></li><li id="19f7" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><strong class="jm hj"> <em class="kr">参考文献</em> </strong></li></ul><h1 id="cd92" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated"><strong class="ak">业务问题:</strong></h1><p id="e802" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">构建用于将损坏的文本数据改变为标准英语分布的模型，导致许多基于NLP的模型的性能提高。这里，输入数据将具有随机损坏，这是目标数据的超集，将它们转换成目标数据，同时保留文本的语义。</p><h1 id="8d25" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated"><strong class="ak">深度学习问题</strong>:</h1><p id="41e3" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">将随机损坏的或sms文本转换成正确的英语语言，同时保留文本的语义。</p><h1 id="f941" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated"><strong class="ak">数据集概述</strong>:</h1><p id="a663" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">根据我们的问题陈述，这是公开可用的数据集。</p><p id="4d78" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated"><a class="ae lc" href="https://www.comp.nus.edu.sg/~nlp/corpora.html" rel="noopener ugc nofollow" target="_blank">https://www.comp.nus.edu.sg/~nlp/corpora.html</a></p><p id="80d9" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">该数据集包含社交媒体文本及其规范化文本和规范化文本的中文翻译。对于我们的问题，我们只需要社交媒体文本和他们的规范化英语文本。社交媒体文本包含2000个数据集。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ld"><img src="../Images/123eb827517602f50969e64384f4bc5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eBUJ3eMJ8Q7ILSXauEh_vg.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx">Dataset Overview</figcaption></figure><p id="b595" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">因为我们的数据是txt格式，一行是SMS文本，第二行是标准英语，第三行是标准英语的中文翻译。我们将只使用短信文本和标准英语进行问题陈述。拆分后，将txt格式转换为包含2列SMS_TEXT和ENGLISH_TEXT的csv文件。</p><h1 id="2dcb" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated"><strong class="ak">损失:</strong></h1><p id="6f88" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">正如在研究论文中提到的，我们将使用范畴交叉熵。它也被称为softmax加交叉熵损失，训练模型输出所有类别的概率。它用于多类分类。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/5af273b8fbff05e5cb91e72880ff9fb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*eUGmmIqgpQ4TgTeHTtWy1g.png"/></div><figcaption class="lm ln et er es lo lp bd b be z dx">img source: <a class="ae lc" href="https://leakyrelu.com/2020/01/01/difference-between-categorical-and-sparse-categorical-cross-entropy-loss-function/" rel="noopener ugc nofollow" target="_blank">https://leakyrelu.com/2020/01/01/difference-between-categorical-and-sparse-categorical-cross-entropy-loss-function/</a></figcaption></figure><h1 id="04fe" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated"><strong class="ak">埃达:</strong></h1><p id="5145" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">在继续之前，我们应该检查是否有丢失的值。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/42132198cad36aba4331f649aa504217.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*WmcKLzfYQlxYnpCvJUMO1g.png"/></div></figure><p id="c012" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">现在，我们将根据每一列执行不同的EDA。</p><h2 id="758d" class="ls in hi bd io lt lu lv is lw lx ly iw jv lz ma ja jz mb mc je kd md me ji mf bi translated">短信_文本</h2><ul class=""><li id="debf" class="ki kj hi jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated"><strong class="jm hj">短信中的句子长度_TEXT </strong></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mg"><img src="../Images/0a120d11c714ee3f9c2091615f5361c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q5qZt7y5Kgx_4dxEB88XRQ.png"/></div></div></figure><p id="3d4b" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">从上面的分布图我们可以看出，大部分句子的长度在20-80之间。很少有句子长度超过150。</p><ul class=""><li id="396e" class="ki kj hi jm b jn kx jr ky jv mh jz mi kd mj kh kn ko kp kq bi translated"><strong class="jm hj">字数</strong></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ld"><img src="../Images/e173fc5c541b751f6e4cc0bcd38f96c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hl6Zjka3Zj9M11HgVZ0qEQ.png"/></div></div></figure><p id="01af" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">大多数句子有5到20个单词。很少有句子的字数超过30个。</p><ul class=""><li id="5c62" class="ki kj hi jm b jn kx jr ky jv mh jz mi kd mj kh kn ko kp kq bi translated"><strong class="jm hj">计算句子长度的百分位值:</strong>句子长度的第99.8百分位值是161，第99.9百分位值是202。99.8百分位和99.9百分位有着巨大的差异。所以我们应该考虑SMS_text=161的最大长度。没有长度在161到202之间的句子。</li><li id="9a8a" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><strong class="jm hj">计算单词数的百分位值:</strong>句子中第99.9百分位有39个单词，第100百分位有49个单词。句子中的最大字数应为39。</li><li id="a604" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><strong class="jm hj">SMS _ TEXT中每个字符的出现次数:</strong></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mk"><img src="../Images/4d2fb2621459e10b7d190163f8dea1f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RUo_LyT6OCufLVRlOtNMog.png"/></div></div></figure><p id="bfe8" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">从上面可以看出，字母e出现的频率最高。后跟字符‘a’、‘t’和‘o’。</p><ul class=""><li id="d7ef" class="ki kj hi jm b jn kx jr ky jv mh jz mi kd mj kh kn ko kp kq bi translated"><strong class="jm hj">位置标签的频率</strong></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ml"><img src="../Images/f3a80717ddf2d629fb66109310cf9b2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*5n5PPN_fGTJLPAa1Q0MTzw.png"/></div></div></figure><p id="5867" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">从上面的图中我们可以看出，带Person的POS标签出现的频率最高。它在SMS_Text列中的频率大约为700。</p><ul class=""><li id="50c9" class="ki kj hi jm b jn kx jr ky jv mh jz mi kd mj kh kn ko kp kq bi translated"><strong class="jm hj">短信中最常出现的25个停用词_TEXT </strong></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/542e4ff63af1528d0cc54e734a388760.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*d1kqawt0j6BlGoRtdQc-ig.png"/></div></figure><p id="38c9" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">从上面的图中我们可以看到，像“to”和“I”这样的停用词在SMS_Text列中出现的频率最高。</p><h2 id="9737" class="ls in hi bd io lt lu lv is lw lx ly iw jv lz ma ja jz mb mc je kd md me ji mf bi translated">2.英语文本</h2><ul class=""><li id="b680" class="ki kj hi jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated"><strong class="jm hj">句子长度</strong></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mg"><img src="../Images/35ee37c46441b335dc4f18eed0fe4bf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F8TQbdzZwVziWeWezETeXg.png"/></div></div></figure><p id="fa6d" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">从上面的情节我们可以看出，很少有句子的长度超过200。</p><ul class=""><li id="7747" class="ki kj hi jm b jn kx jr ky jv mh jz mi kd mj kh kn ko kp kq bi translated"><strong class="jm hj">字数</strong></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ld"><img src="../Images/1616892d2bae8c6ef3dd44d903719d15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WYTTk2ykvSxkGzlqZflwOw.png"/></div></div></figure><p id="eebf" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">从上面的情节可以看出，字数在40以上的句子很少。</p><ul class=""><li id="a778" class="ki kj hi jm b jn kx jr ky jv mh jz mi kd mj kh kn ko kp kq bi translated"><strong class="jm hj">计算百分位值句子长度</strong>:第99.7百分位值为200，第99.8百分位值为215。很少有句子长度超过200，所以英语句子的最大长度是200。</li><li id="5001" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><strong class="jm hj">计算字数的百分位值</strong>:句子中第99.9百分位有48个字，第100百分位有59个字。句子中的最大字数应为48。</li><li id="55a7" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><strong class="jm hj">每个字符在英文文本中的出现次数:</strong></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mn"><img src="../Images/e9256c105ae0c9abaf8c46bb00579506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u4l_hXcTX--2ldeprp_Dww.png"/></div></div></figure><p id="2163" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">像“e”、“o”、“a”和“t”这样的字符在英语文本列中出现频率最高。</p><ul class=""><li id="6ce5" class="ki kj hi jm b jn kx jr ky jv mh jz mi kd mj kh kn ko kp kq bi translated"><strong class="jm hj">POS _ Tags的频率</strong></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ml"><img src="../Images/382a3e2b23bb585e88b1883e81ea7348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*ngehUSs63CnQBfVhoOH8pg.png"/></div></div></figure><p id="bc5d" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">从上面的图表中我们可以看出，在English_Text中，带有Person的词性标记出现的频率最高。它的频率在500左右。</p><ul class=""><li id="6473" class="ki kj hi jm b jn kx jr ky jv mh jz mi kd mj kh kn ko kp kq bi translated"><strong class="jm hj">英文文本中最常出现的25个停用词</strong></li></ul><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/5f35918cd3bc0bd90d7f6639d3b0f869.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*yPv4-ZOvz5SaVzgY4tuP_w.png"/></div></figure><p id="19e2" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">从上面的图表中我们可以看出,“you”这个停用词在英语课文中出现的次数最多。</p><h1 id="1704" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated"><strong class="ak">数据预处理:</strong></h1><p id="7f8e" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">数据预处理有两种方式:</p><ul class=""><li id="257b" class="ki kj hi jm b jn kx jr ky jv mh jz mi kd mj kh kn ko kp kq bi translated"><strong class="jm hj">字符级</strong>:基于eda，我们只保留了SMS_text中长度小于170和English_text中长度小于200的句子。我们已经删除了英文文本列，并创建了2列:英文输入包含每句话前的开始标记' \t '，英文输出包含每句话后的结束标记' \n '。在第一个数据中，我们将在英语输入和输出中添加' \n '标记，以便我们的模型可以学习结束标记。基于上述预处理步骤，只有7个句子从数据集中被丢弃。</li><li id="8aee" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><strong class="jm hj">单词级别</strong>:基于eda，我们只保留了那些SMS_Text长度小于39，English_Text长度小于40的句子。我们已经删除了English_Text列，并创建了两个与charcter level相同的列，唯一不同的是开始标记是&lt;开始&gt;和结束标记是&lt;结束&gt;。基于上述预处理步骤，只有12个句子从数据集中被丢弃。</li></ul><p id="8d66" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">在研究论文中，提到了也要寻找严格可打印的字符(这里他们要求我们检查所有的字符是否都是英文字符)。在我们的预处理步骤中，我们没有检查它，因为有一些字符可能代表一些英语单词</p><p id="eeeb" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated"><strong class="jm hj">例如:SMS _ TEXT:Yar or……但是如果晚些时候去或者我们必须自己去……那么……你现在还在睡觉还是在吃饭还是在睡觉… </strong></p><p id="902c" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated"><strong class="jm hj"> ENGLISH_TEXT:是的。但是如果晚一点去，我们必须自己去。那怎么做？你还在睡觉吗？我正在吃饭，你还在睡觉。</strong></p><p id="4040" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">从上面的eg<strong class="jm hj">ü</strong>表示单词<strong class="jm hj"> You </strong>，如果我们去掉ü，句子的语义就变了。</p><h1 id="5f0e" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated"><strong class="ak">资料准备:</strong></h1><ol class=""><li id="1964" class="ki kj hi jm b jn jo jr js jv kk jz kl kd km kh mp ko kp kq bi translated"><strong class="jm hj">字符级</strong>:将数据集分割成99:1的训练和测试。</li></ol><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mq"><img src="../Images/98b0b5381c6e4e6b24a17d1d3f727927.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qzwewDSjPnXDejB-ExOv9A.png"/></div></div></figure><p id="d1b7" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">然后，我们创建字符级记号赋予器的对象，没有过滤器，小写=false。用训练数据集拟合我们的标记器，并将训练和测试数据集转换成序列，这为我们提供了模型输入和输出的词汇表。然后，我们用输入和输出句子中最长的预处理句子填充序列。然后，我们为输入和输出句子创建了字符级嵌入矩阵，它将被我们的模型嵌入层用作权重。从上面的处理步骤中，我们得到了103个字符的输入句子词汇和92个字符的输出句子词汇。</p><p id="401a" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated"><strong class="jm hj"> 2。单词级:</strong>将数据集按照99:1的比例拆分成训练和测试。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mr"><img src="../Images/4b22fb1475fff1a093f5efc90709b4d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-QF7sf4byEgIrqWCRtxtVQ.png"/></div></div></figure><p id="fc58" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">然后，我们用过滤器创建单词级标记器的对象，小写=false，oov_token=True。用训练数据集拟合我们的标记器，并将训练和测试数据集转换成序列，从而为我们提供模型的输入和输出词汇。然后，我们用输入和输出句子中最长的预处理句子填充序列。然后，我们使用fasttext为输入和输出句子创建单词级嵌入矩阵，该矩阵将被我们的模型嵌入层用作权重。通过以上预处理步骤，我们得到了3702个单词的输入句<strong class="jm hj">词汇和3040个单词的输出句词汇。</strong></p><h1 id="8821" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated"><strong class="ak">深度学习模型:</strong></h1><p id="ef47" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">这里我们将使用编码器解码器模型，其中编码器和解码器具有GRU或LSTM的单层</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ms"><img src="../Images/50b66dc5765d8936626a044f31d5d20e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tlhGXRWFcJMwkcWBgnE3sQ.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx">image source: <a class="ae lc" href="https://www.kdnuggets.com/2019/08/deep-learning-transformers-attention-mechanism.html" rel="noopener ugc nofollow" target="_blank">https://www.kdnuggets.com/2019/08/deep-learning-transformers-attention-mechanism.html</a></figcaption></figure><h2 id="3ae3" class="ls in hi bd io lt lu lv is lw lx ly iw jv lz ma ja jz mb mc je kd md me ji mf bi translated"><strong class="ak"> 1。带有一个热编码输入的序列到序列模型</strong>:</h2><p id="7464" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">在这种情况下，句子被转换成一个字符维数的热编码向量。它将检查每个单词的字符是否存在。如果存在，它将被标记为1，其余的将被标记为0。假设，如果一个单词中有4个字符，那么一个热编码表示将是[[1000][0100][0010][0001]]。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mt"><img src="../Images/14c0850d529a1051c3d16bdea5ef6d35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JtDdk-jLhHb4eX9HEA1Axw.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx">Encoder Decoder Architecture with GRU 100 units</figcaption></figure><p id="0122" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">这里我们使用<strong class="jm hj"> 100个单位的GRU </strong>，分类交叉熵作为损失，Adam优化器的学习率为<strong class="jm hj"> 0.0001 </strong>，我们得到的损失为<strong class="jm hj"> 0.5648 </strong>。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mu"><img src="../Images/788067add9f7b969ac8c52d4d0970585.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B8MzjdmqAYRtUTFAOKakqQ.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx">Encoder Decoder Architecture with LSTM 100 units.</figcaption></figure><p id="b575" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">这里我们使用<strong class="jm hj"> 100单位的LSTM </strong>，<strong class="jm hj">分类交叉熵</strong>作为损失，<strong class="jm hj"> Adam优化器</strong>的学习率<strong class="jm hj">为0.0001 </strong>，我们得到损失<strong class="jm hj">为0.5587。</strong></p><h2 id="df47" class="ls in hi bd io lt lu lv is lw lx ly iw jv lz ma ja jz mb mc je kd md me ji mf bi translated"><strong class="ak"> 2。带有字符级标记化的序列到序列模型</strong>:</h2><p id="bb8b" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">在这种情况下，句子被标记为字符级标记，并被传递给编码器和解码器模型。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mv"><img src="../Images/2ec5e2d367967075df9fe338faca5b3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f9Hcmd1iKc18EcGS2WZBdg.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx">Encoder Decoder architecture with 100 units of Gru</figcaption></figure><p id="906c" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">这里我们使用<strong class="jm hj"> 100单位的GRU </strong>、<strong class="jm hj">稀疏分类交叉熵</strong>作为损失、<strong class="jm hj">学习率为0.01和0.001 </strong>的<strong class="jm hj">亚当优化器</strong>，我们得到<strong class="jm hj">损失分别为0.8484和0.7583。</strong></p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mw"><img src="../Images/857e8b1d129061f9282efb6afca5ec9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6s5kGThTXaxAf4eUIYzYFw.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx">Encoder Decoder model with 100 units of LSTM.</figcaption></figure><p id="5b94" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">这里我们使用<strong class="jm hj"> 100个单位的LSTM </strong>，<strong class="jm hj">稀疏分类交叉熵</strong>作为损失，<strong class="jm hj"> Adam优化器</strong>的学习率为<strong class="jm hj"> 0.01 </strong>，我们得到的损失为<strong class="jm hj"> 0.6794。</strong></p><h2 id="fac2" class="ls in hi bd io lt lu lv is lw lx ly iw jv lz ma ja jz mb mc je kd md me ji mf bi translated"><strong class="ak"> 3。具有字符级标记化和Bahadanau注意的序列到序列模型</strong>:</h2><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mx"><img src="../Images/2edbbf407792ce1d15e57677f9de28fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W4RHRgyDBsp-tvAlVdkEZw.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx">image source: <a class="ae lc" href="https://blog.floydhub.com/attention-mechanism/amp/" rel="noopener ugc nofollow" target="_blank">https://blog.floydhub.com/attention-mechanism/amp/</a></figcaption></figure><p id="f6d4" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">从编码器获得输出后，我们将计算每个编码器输出相对于解码器输入和每个时间步长的隐藏状态的注意力得分。它告诉解码器将给予每个编码器输出多少关注或权重，以生成下一个解码器输出。这里我们用<strong class="jm hj"> 100个单位的LSTM，稀疏分类交叉熵</strong>作为损失，<strong class="jm hj"> Adam优化</strong> r用<strong class="jm hj">学习率0.01 </strong>，我们得到<strong class="jm hj">损失0.3930。</strong></p><h2 id="67ae" class="ls in hi bd io lt lu lv is lw lx ly iw jv lz ma ja jz mb mc je kd md me ji mf bi translated"><strong class="ak"> 4。具有单词级标记化和Bahadanau注意力的序列到序列模型</strong>:</h2><p id="47aa" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">在这种情况下，句子被标记为单词级别，并且这些标记化的句子被用于使用<strong class="jm hj"> fasttext模型</strong>创建<strong class="jm hj">嵌入矩阵</strong>，该嵌入矩阵将在编码器和解码器嵌入层中用作权重。这里对于任何学习率，模型都是<strong class="jm hj">过拟合的。</strong></p><h1 id="5769" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">模型评估:</h1><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es my"><img src="../Images/ea38807995b8d9a5caa087cbac4772dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*8uOMii4hAVnCeQCvfqlx8A.png"/></div><figcaption class="lm ln et er es lo lp bd b be z dx">Model Comparison</figcaption></figure><p id="a8d4" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">从以上模型来看，<strong class="jm hj">人物级模型加巴哈度关注</strong> <strong class="jm hj">损失最小</strong>。因此，我们将考虑使用<strong class="jm hj"> Bahadanau注意力模型</strong>来预测句子。</p><h1 id="d183" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">模型预测:</h1><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mz"><img src="../Images/0f1c0f10a450c9c53d4556a6f5fada93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8CqQ6DXib4S1TjmuIRQrMA.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx">Bahadanu Attention Model Sentence Prediction.</figcaption></figure><p id="33a1" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">从预测中，我们可以观察到我们的模型对于任何句子都表现得不够好。这可能是因为用于训练模型的数据集非常少。</p><h1 id="ccb4" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated"><strong class="ak">错误分析:</strong></h1><p id="a3ed" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">让我们检查模型的预测句子和原始句子的平均bleu分数。我们计算出验证数据集的平均bleu分数为<strong class="jm hj"> 0.6743 </strong>。<strong class="jm hj">最低bleu得分为0.5220，最高bleu得分为0.7496。</strong></p><p id="155c" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">让我们绘制验证数据的bleu分数。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es na"><img src="../Images/0a32e4fb746d4bdd988b165d6365c745.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FLL9HCtTVRHKk2Wyn2X0Bw.png"/></div></div></figure><p id="da70" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">从上面的图中，我们可以观察到大多数bleu分数在<strong class="jm hj"> 0.65到0.75 </strong>的范围内。可能有这样的情况，我们得到的bleu分数最低，因为与最高bleu分数相比，句子中出现了更多的罕见词。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es nb"><img src="../Images/54db71007b8c632245ee3307dac767cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gjnc5tiY2zSS-Xb_uYzpyw.png"/></div></div></figure><p id="e376" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">这里，<strong class="jm hj">我统计了数据集中出现频率小于等于10 </strong>的验证数据集中的生僻字。基于文本中稀有词的数量，我们的bleu分数没有变化。有一些文本的bleu分数很高，并且与SMS和英语文本中具有最低bleu分数的文本中存在的稀有单词相比，具有许多稀有单词。</p><h1 id="053c" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated"><strong class="ak">最终Pipleine: </strong></h1><figure class="le lf lg lh fd ij"><div class="bz dy l di"><div class="nc nd l"/></div></figure><p id="8562" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">在这个预测函数中，我们将能够发送列表中的多个句子。该模型将能够并行预测句子。</p><h1 id="3fed" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">模型推断:</h1><figure class="le lf lg lh fd ij"><div class="bz dy l di"><div class="ne nd l"/></div><figcaption class="lm ln et er es lo lp bd b be z dx">Model Inference Video</figcaption></figure><h1 id="e432" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated"><strong class="ak">未来作品:</strong></h1><ul class=""><li id="bae3" class="ki kj hi jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated">除了2000个句子之外，没有其他可用的数据集，这对于深度学习模型来说更少。我们没有做任何数据扩充来增加数据，所以数据的增加可能会带来更好的模型性能。</li><li id="b31e" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated">在编码器和解码器模型中，我们只使用了LSTM和GRU，使用双向LSTM可能会提高模型的性能。</li><li id="fa05" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated">我们也可以检查基于字符的迁移学习模型，这可能会给我们很好的预测</li></ul><h1 id="47b9" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated"><strong class="ak">参考:</strong></h1><ul class=""><li id="a6a1" class="ki kj hi jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated">研究论文:<a class="ae lc" href="https://cs224d.stanford.edu/reports/Lewis.pdf" rel="noopener ugc nofollow" target="_blank">https://cs224d.stanford.edu/reports/Lewis.pdf</a></li><li id="ff7f" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><a class="ae lc" href="https://keras.io/examples/generative/lstm_character_level_text_generation/" rel="noopener ugc nofollow" target="_blank">https://keras . io/examples/创成式/lstm _ character _ level _ text _ generation/</a></li><li id="2fba" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><a class="ae lc" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com/</a></li><li id="7ec1" class="ki kj hi jm b jn ks jr kt jv ku jz kv kd kw kh kn ko kp kq bi translated"><a class="ae lc" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/tutorials/text/NMT _ with _ attention</a></li></ul><h2 id="d5f8" class="ls in hi bd io lt lu lv is lw lx ly iw jv lz ma ja jz mb mc je kd md me ji mf bi translated"><strong class="ak"> Github库:</strong></h2><ul class=""><li id="3afa" class="ki kj hi jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated"><a class="ae lc" href="https://github.com/Nipun-1997/SentenceCorrection" rel="noopener ugc nofollow" target="_blank">https://github.com/Nipun-1997/SentenceCorrection</a></li></ul><p id="9b05" class="pw-post-body-paragraph jk jl hi jm b jn kx jp jq jr ky jt ju jv kz jx jy jz la kb kc kd lb kf kg kh hb bi translated">希望你喜欢这个博客。详情请通过LinkedIn联系我</p><h2 id="853a" class="ls in hi bd io lt lu lv is lw lx ly iw jv lz ma ja jz mb mc je kd md me ji mf bi translated">领英简介:</h2><ul class=""><li id="467b" class="ki kj hi jm b jn jo jr js jv kk jz kl kd km kh kn ko kp kq bi translated"><a class="ae lc" href="https://www.linkedin.com/in/nipun-agrawal-200597110" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/nipun-agrawal-200597110</a></li></ul></div></div>    
</body>
</html>
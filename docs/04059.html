<html>
<head>
<title>A 2021 Guide to improving CNNs-Optimizers: Adam vs SGD</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2021年CNN优化指南:Adam vs SGD</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/a-2021-guide-to-improving-cnns-optimizers-adam-vs-sgd-495848ac6008?source=collection_archive---------0-----------------------#2021-06-21">https://medium.com/geekculture/a-2021-guide-to-improving-cnns-optimizers-adam-vs-sgd-495848ac6008?source=collection_archive---------0-----------------------#2021-06-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f954" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这将是我在<em class="jd"> </em>上的第三篇文章，我的系列文章<em class="jd">是2021年改进CNN的指南。</em></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/4cb0967937a0783429f3e67a1f7e3f60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JxB_IDa_IRiZwye3"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Photo by <a class="ae ju" href="https://unsplash.com/@aahubs?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Aaron Huber</a> on <a class="ae ju" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="2f4c" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">优化者</h2><p id="db70" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">根据优化器的公式，优化器可以被解释为在给定梯度和附加信息的情况下修改网络权重的数学函数。优化器建立在梯度下降的思想上，这是一种贪婪的方法，通过跟随梯度来迭代地降低损失函数。</p><p id="bbb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种函数可以简单到从权重中减去梯度，也可以非常复杂。</p><p id="ae4b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更好的优化器主要集中在更快和更有效，但也经常比其他优化器概括得更好(更少过拟合)。<strong class="ih hj">是的，优化器的选择可能会极大地影响模型的性能。</strong></p><p id="d4cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将回顾常用的Adam优化器的组件。我们还将讨论关于SGD是否比基于Adam的优化器更通用的争论。最后，我们将回顾一些比较此类优化器性能的论文，并对优化器的选择做出结论。需要注意的一点是，设计能够提高实际收敛速度并能在各种设置下很好地推广的优化器是非常具有挑战性的。</p><h1 id="f8b7" class="kv jw hi bd jx kw kx ky kb kz la lb kf lc ld le ki lf lg lh kl li lj lk ko ll bi translated">~亚当</h1><h2 id="cf7d" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">香草冰淇淋(新加坡元)</h2><p id="3802" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">准确地说，随机梯度下降(SGD)是指批量为1时香草GD的特定情况。然而，为了方便起见，在本文中我们将所有的小批量GD、SGD和批量GD视为SGD。</p><p id="0288" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SGD是GD的最基本形式。SGD从权重中减去乘以学习率的梯度。尽管简单，SGD有很强的理论基础，并且仍然被用于训练边缘神经网络。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lm"><img src="../Images/78957968b41945a906f9cb6a27590819.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*YCFXYbSqf_KMB2umEax_Yg.png"/></div></figure><h2 id="c95c" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">动力</h2><p id="0162" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">动量通常被称为滚下一个球，因为它在概念上等于增加速度。权重通过动量项进行修改，动量项计算为梯度的移动平均值。动量项γ可以看作是空气阻力或摩擦力，它成比例地衰减动量。动量加速了训练过程，但是增加了额外的超参数。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ln"><img src="../Images/2f29f5492676357a48210f54f9e7235d.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*BIyM-6WIzGiwje6FgUUXbQ.png"/></div></figure><p id="398a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本质上，这个方程等于减去梯度的指数衰减平均值:θ-=α(di+d(I-1)γ+d(I-2)γ+d(I-3)γ+…)</p><h2 id="6a13" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">RMSProp</h2><p id="3ed8" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">RMSProp是未发表的作品，本质上类似于momentum。如果梯度始终很大，v_i的值将增加，学习速率将降低。这自适应地调整每个参数的学习率，并允许使用更大的学习率。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lo"><img src="../Images/7d265402b082aa158b7c6e49dd082571.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*SZ45fBdHQ0P0ygcU-e6o9A.png"/></div></figure><h2 id="6e08" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">亚当[10]</h2><p id="863a" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">Adam本质上通过存储RMSProp的个体学习率和动量的加权平均值来组合RMSProp和动量。动量和RMSProp参数的计算公式如下。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lp"><img src="../Images/14d9af8145fbd9323f662c1bfc50e798.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*0vFgDkGTEf26Zjc-w1JXVw.png"/></div></figure><p id="d472" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在应用于梯度下降步骤中的权重之前，参数除以(1-衰减因子)。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lq"><img src="../Images/c0afd8d1f9340e415a03b40d4e4aeb30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*7ENaDbcm3apUv93W6yS1Qw.png"/></div></div></figure><p id="3d53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上式所示，Adam基于RMSProp，但将梯度作为动量参数进行估计，以提高训练速度。根据[10]中的实验，Adam在本文中的各种训练设置和实验中优于所有其他方法。Adam已经成为一种默认的优化算法，不管是什么领域。然而，Adam引入了两个新的超参数，并使超参数调整问题变得复杂。</p><h1 id="4a89" class="kv jw hi bd jx kw kx ky kb kz la lb kf lc ld le ki lf lg lh kl li lj lk ko ll bi translated">SGD更好？</h1><p id="0e0d" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">关于优化器的一个有趣且占主导地位的论点是SGD比Adam更好地概括。这些论文认为，虽然Adam收敛得更快，但SGD比Adam概括得更好，从而导致最终性能的提高。</p><h2 id="cf27" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">SGD的“稳定性”更好[12]</h2><p id="5bd6" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">[12]认为，对于凸连续优化，SGD在概念上是稳定的。首先，它认为最小化训练时间有利于减少泛化误差。这是因为该模型不会多次看到相同的数据，并且该模型在没有概括能力的情况下不能简单地记住数据。这似乎是一个合理的论点。</p><p id="3435" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该论文提出将通过SGD学习的模型的训练误差和验证误差之间的差异作为泛化误差。如果训练误差对于单个训练数据点上的任何变化仅轻微变化，则算法是<em class="jd">一致稳定的</em>。模型的稳定性与泛化误差有关。本文给出了数学证明，证明了SGD对于强凸损失函数是一致稳定的，因而可能具有最优的推广误差。文章还表明，在迭代次数不太大的情况下，这些结果可以推广到非凸损失函数。</p><h2 id="7bd6" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">这种情况的例子(理论+经验)[9]</h2><p id="71d2" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">[9]提出了自适应优化方法(例如RMSProp、Adam)在简单的过参数化实验中的问题，并提出了此类自适应优化策略的较差泛化性能的更多经验证据。它还表明，自适应和非自适应优化方法在理论上确实找到了具有非常不同的推广性质的非常不同的解决方案。</p><p id="3bcf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，讨论了观察到的<em class="jd">当一个问题有多个全局最小值时，不同的算法在从同一点</em>初始化时可以找到完全不同的解，并构建了一个理论示例，其中自适应梯度方法找到了比SGD更差的解。简而言之，在二元最小二乘分类损失任务中，包括SGD和动量的非自适应方法将收敛于最小范数解，而自适应方法可能发散。</p><p id="0311" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">论文还建议了四个使用深度学习的实证实验。论文提出，他们的实验显示了以下发现:</p><ol class=""><li id="332e" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc lw lx ly lz bi translated">自适应方法找到的解比非自适应方法找到的解概括得更差。</li><li id="aaa8" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">即使当自适应方法比非自适应方法获得相同或更低的训练损失时，测试性能也更差。</li><li id="2226" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">自适应方法通常在训练集上显示更快的初始进展，但是它们的性能在验证集上很快达到平稳。</li><li id="1c13" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">尽管传统观点认为Adam不需要调整，但我们发现调整Adam的初始学习速率和衰减方案在所有情况下都比默认设置有显著的改进。</li></ol><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mf"><img src="../Images/ce3ff8be00a62af5934b6d90b1111c9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*jvnE6EPPPj27M2t45ppxDw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx">Development Set is equal to the validation set.</figcaption></figure><p id="7e8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些论文证明了自适应优化在训练的初始阶段是快速的，但是通常不能推广到验证数据。这是非常有趣的，因为相对顺序在不同的情况下是不同的，而SGD在大多数情况下优于所有其他方法。</p><h2 id="f1bc" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">也许不是？[8]</h2><p id="4bf1" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">最近的一篇论文表明，超参数可能是自适应优化算法无法推广的原因。当改变超参数搜索空间时,[8]中的实验显示了与上述论文不同的结果。</p><p id="c4c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这实际上很有意义，因为更通用的优化器(例如Adam)可以通过不同的超参数选择来逼近更简单的组件优化器(例如Momentum、SGD、RMSProp ),因此不应该比其组件差。本文认为，用于提出SGD更好的经验证据的超参数搜索空间对于自适应方法来说过于浅薄和不公平。因此，实验是在相对较大的搜索空间上进行的(见[8]的附录D)。</p><p id="613e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，微调的自适应优化器比标准SGD更快，并且在泛化性能方面并不落后。最优超参数的每个值都远离所有优化器的搜索空间边界，从而表明搜索空间是适当的。有趣的是，<strong class="ih hj">优化器的最佳超参数在数据集之间变化很大。</strong></p><p id="d7cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作者自信地说</p><blockquote class="mg mh mi"><p id="b558" class="if ig jd ih b ii ij ik il im in io ip mj ir is it mk iv iw ix ml iz ja jb jc hb bi translated">特别是，我们发现流行的自适应梯度方法从来没有表现不佳的势头或梯度下降。</p></blockquote><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mm"><img src="../Images/c4c6273cf1120424476ed436d2f2e5e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3--u4SAeiAnADMdYqMZYEg.png"/></div></div></figure><p id="96ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇论文的主要发现是，通过在深度学习的规模上调整所有可用的超参数，更一般的优化器永远不会低于它们的特例。特别是，他们观察到RMSProp，Adam和NAdam的表现从来没有低于SGD，NESTEROV或Momentum。虽然存在一些限制，即实验是在一些潜在的混淆设置下进行的(例如，未调整批次大小，特定的调整方案)，但所述信息是令人警觉和感兴趣的。</p><p id="4f9a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">目前，我们可以说微调的Adam总是比SGD好，但是在使用默认的超参数时，Adam和SGD之间存在性能差距。</p><h2 id="205f" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">参考</h2><p id="25c8" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">[1] Loshchilov，I .，&amp; Hutter，F. (2017)。去耦权重衰减正则化。<em class="jd"> arXiv预印本arXiv:1711.05101 </em>。</p><p id="81a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2]杜奇、哈赞和辛格(2011年)。在线学习和随机优化的自适应次梯度方法。<em class="jd">机器学习研究杂志</em>，<em class="jd"> 12 </em> (7)。</p><p id="cf97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[3]张，M. R .，卢卡斯，j .，辛顿，g .，&amp;巴，J. (2019)。前瞻优化器:向前k步，向后1步。<em class="jd"> arXiv预印本arXiv:1907.08610 </em>。</p><p id="3ba3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[4]罗，李，熊，杨，刘，杨，孙，谢(2019).学习速率动态限制的自适应梯度方法。<em class="jd"> arXiv预印本arXiv:1902.09843 </em>。</p><p id="e81d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[5]庄，j .，唐，t .，丁，y .，塔蒂孔达，s .，德沃内克，n .，帕帕德米特里斯，x .，&amp;邓肯，J. S. (2020)。Adabelief优化器:通过对观测梯度的信任来调整步长。<em class="jd"> arXiv预印本arXiv:2010.07468 </em>。</p><p id="85cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[6] Loshchilov，I .，&amp; Hutter，F. (2016年)。Sgdr:带有热重启的随机梯度下降。<em class="jd"> arXiv预印本arXiv:1608.03983 </em>。</p><p id="8790" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[7]凯斯卡尔，N. S .，&amp;索彻，R. (2017年)。通过从adam切换到sgd来提高泛化性能。<em class="jd"> arXiv预印本arXiv:1712.07628 </em>。</p><p id="1d84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[8]Choi d .、Shallue、C. J .、Nado z .、Lee j .、马迪森、C. J .、&amp; Dahl、G. E. (2019年)。深度学习优化器的实证比较。<em class="jd"> arXiv预印本arXiv:1910.05446 </em>。</p><p id="6d4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[9]威尔逊、罗洛夫斯、斯特恩、斯雷布罗和雷希特(2017年)。机器学习中自适应梯度方法的边际价值。<em class="jd"> arXiv预印本arXiv:1705.08292 </em>。</p><p id="5a65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[10]金马博士和巴律师(2014年)。亚当:一种随机优化方法。<em class="jd"> arXiv预印本arXiv:1412.6980 </em>。</p><p id="a0b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[11]你，y，李，j .，Reddi，s .，Hseu，j .，Kumar，s .，Bhojanapalli，s，... &amp; Hsieh，C. J. (2019)。深度学习大批量优化:76分钟训练bert。arXiv预印本:1904.00962 。</p><p id="d532" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[12]哈特，m .，雷希特，b .，辛格，Y. (2016年6月)。训练更快，推广更好:随机梯度下降的稳定性。在<em class="jd">机器学习国际会议</em>(第1225–1234页)。PMLR。</p></div></div>    
</body>
</html>
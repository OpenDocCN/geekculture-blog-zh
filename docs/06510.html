<html>
<head>
<title>Deep Learning Part 3: Backpropagation; Nothing But a Game of Telephone</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习第3部分:反向传播；只不过是一场电话游戏</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/deep-learning-part-3-backpropagation-nothing-but-a-game-of-telephone-e0d716f6d362?source=collection_archive---------14-----------------------#2021-08-22">https://medium.com/geekculture/deep-learning-part-3-backpropagation-nothing-but-a-game-of-telephone-e0d716f6d362?source=collection_archive---------14-----------------------#2021-08-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/b996a6ceaa2cfd3cfde72c5a09ab1258.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x4Zjmjo36mc5Dprq35PMdQ.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="dc4c" class="pw-subtitle-paragraph iq hs ht bd b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh dx translated">通过将反向传播与“电话”游戏联系起来，直观地理解反向传播</h2></div><p id="72ac" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">欢迎来到我关于深度学习的介绍性系列的第3部分，我们的目标是让你熟悉基本的DL概念。请参考底部的D <em class="ke"> eep学习系列</em>部分，了解之前的所有文章。在这篇文章中，我们讨论反向传播。</p><p id="04b6" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">在上周的文章中，我们看了梯度下降。我们看到，为了让我们的神经网络学习，我们必须不断更新我们的权重和偏差，直到我们的成本函数最小化。更新规则是基于我们的成本函数的梯度。为了简化我们对梯度下降算法的解释，我们暂时忽略了神经网络，而是使用线性回归模型。模型的这种变化引发了一些疑问:在神经网络上执行梯度下降有什么复杂的？它是否以与线性回归相同的方式应用于神经网络？确实是。然而，当计算神经网络的成本函数的梯度时，事情变得有点复杂。</p><p id="a074" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我们将从建立有效计算梯度的直觉开始本周的文章。我们将通过观察在“<a class="ae kf" href="https://en.wikipedia.org/wiki/Chinese_whispers" rel="noopener ugc nofollow" target="_blank">电话</a>”的游戏中错误是如何从一个人传播到另一个人的来做到这一点。然后，我们将把这个想法与<strong class="jk hu">反向传播算法</strong>联系起来，并正式说明在使用神经网络时如何计算梯度下降的梯度。</p><p id="2d3e" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">让我们开始吧。</p><h1 id="eac0" class="kg kh ht bd ki kj kk kl km kn ko kp kq iz kr ja ks jc kt jd ku jf kv jg kw kx bi translated">电话模拟</h1><p id="b375" class="pw-post-body-paragraph ji jj ht jk b jl ky iu jn jo kz ix jq jr la jt ju jv lb jx jy jz lc kb kc kd hb bi translated">你是加拿大国家电话队的一员，电话世界杯就在眼前。现在，我们假设每支球队只由两种类型的球员组成:</p><ul class=""><li id="b22a" class="ld le ht jk b jl jm jo jp jr lf jv lg jz lh kd li lj lk ll bi translated"><strong class="jk hu">发言人</strong>:第一个发言的人。这个人有原始消息要悄悄告诉其他玩家。</li><li id="1743" class="ld le ht jk b jl lm jo ln jr lo jv lp jz lq kd li lj lk ll bi translated">一个或多个<strong class="jk hu">玩家:</strong>任何接收消息的人，但不包括发言人</li></ul><p id="c8f6" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">你的团队没有采取简单的倾听和希望最好的正常方法。相反，他们明白焦虑和紧张会发挥作用，并导致一些队友听到错误的东西。因此，每个玩家都将尝试估计前一个玩家产生的错误量，并尝试做出任何有意义的纠正。例如，考虑以下两个玩家的线性电话线路:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lr"><img src="../Images/da758947e6575035b1e643e18796f6c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjBeJSc647Hp5OoEEkNnOg.jpeg"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx"><strong class="bd ki">Figure 1: </strong>Linear Telephone Line With Two Players</figcaption></figure><p id="e8c0" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">参与人1会假设自己犯了10%的错误。参与人2会假设参与人1犯了20%的错误。我们把错误定义为两个句子之间不同字母的数量。例如，如果我们有以下场景:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ma"><img src="../Images/bbe7ae3eda3b44794419e2748a31b94d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WErORgMA1jfqfh-bug59Lg.jpeg"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx"><strong class="bd ki">Figure 2: </strong>Sentence Passed Through Linear Telephone Line With Two Players</figcaption></figure><p id="b2d5" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">那么参与人1有10%的原句是错的，因为原句中十分之一的字母被改变了。参与人2将试图解释参与人1的错误，并在他认为合适的时候修改字母。</p><p id="5277" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">由于这是一项团队运动，团队必须进行训练，以建立正确的化学反应，并了解每个球员对原始句子的影响。训练包括检查一些句子，确定团队哪里出错了，并相应地修改错误率。</p><p id="d1fc" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我们在<strong class="jk hu">图2 </strong>中的电话线显然没有经过适当的训练。我们能做些什么来获得更好的结果？我们需要一种方式向玩家描述他们哪里出错了。唯一知道原答案的人是说话的人。因此，他是唯一能比较预测结果和实际结果的人。他必须先去找参与人1，问他听到了什么。从那里，他可以计算出参与人1产生的误差，告诉他相应地调整他的预测误差率，然后转移到参与人2。说话者必须根据参与人1的错误率来帮助参与人2调整他的错误率。由于参与人1的错误率是10%，而参与人2的预测是20%，如果团队希望得到更好的结果，参与人2必须减少他的错误率。</p><p id="3dae" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">这是一种高效的训练方式吗？也许对于我们两个玩家和一个扬声器的线性系列来说，这不是最大的交易。但是电话世界杯不仅仅由一种电话线组成。各队将不得不在一个“线程”电话线游戏中竞争。在线程电话线中，每个小组被分成几个组，每个组有一个发言人。然后说话者将不得不把句子的不同部分传给他们组中的第一个人。然后，每组的第一个人将这句话小声告诉每组的第二个人，以此类推。最后一名玩家将收到来自每组的消息，并必须做出正确的操作和决定来输出他认为最初发送的消息。例如:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mb"><img src="../Images/6f1aaa3b766620f462f30a29afb0f7ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bCJGNvWRBM4Io-RD2tShPg.jpeg"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx"><strong class="bd ki">Figure 3: </strong>Sentence Passed Through Threaded Telephone Line</figcaption></figure><p id="157b" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我们现在的训练方法效果如何？一个发言人必须首先去第一组的第一个玩家那里，确定他造成的错误，通知每个组的<strong class="jk hu">的第二个玩家这个错误，最后通知最后一个玩家。第二个说话者必须重复同样的过程。我们的扬声器会很快变得非常疲劳。想象一下，如果我们有10个小组，每个小组有10个发言人，这个过程会有多及时。而这一切只是为了一次训练。想象一下，如果我们想要训练上百万个句子，会发生什么。我们训练方法的低效开始显现出来。那么，有没有更好的解决办法呢？</strong></p><p id="e93c" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">让我们为我们的团队介绍一位新型选手，一位<strong class="jk hu">评委</strong>。法官是最后一个收到消息的人。他知道原始消息是什么，并负责将收到的消息与原始消息进行比较。不要把评委和<strong class="jk hu">图2 </strong>和<strong class="jk hu">图3 </strong>中的最后几位选手搞混了。这些玩家不知道原始信息是什么，而法官知道:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mc"><img src="../Images/ed47eb3d65807a85a7fd00797ad8219e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pTMJfycdOOZwu1RMDJehwg.jpeg"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx"><strong class="bd ki">Figure 4: </strong>Threaded Telephone Line With A Judge</figcaption></figure><p id="012d" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">由于法官知道原始信息是什么，他可以计算出最终判决产生的总误差。有了现在已知的错误，法官就可以将收到的错误广播给小声对他说这句话的玩家。该玩家将评估他对该错误的影响，并将其传播给所有之前的玩家。只需从发言人到裁判传递一次，再从裁判到发言人传递一次，每个球员就能明白他是如何对球队的整体失误做出贡献的。与我们的原始方法相比，我们必须为每个扬声器通过整个电话线，这要快得多。</p><p id="8763" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我们的结论是，当说话者负责同步错误(正向传播)时，我们的训练过程是非常低效的。解决方案是从法官开始向后传播误差。这个结论适用于神经网络，这也是为什么反向传播是最广泛使用的训练神经网络的算法。让我们看看算法是如何工作的。</p><h1 id="37d5" class="kg kh ht bd ki kj kk kl km kn ko kp kq iz kr ja ks jc kt jd ku jf kv jg kw kx bi translated">反向传播</h1><p id="190c" class="pw-post-body-paragraph ji jj ht jk b jl ky iu jn jo kz ix jq jr la jt ju jv lb jx jy jz lc kb kc kd hb bi translated">在我们的电话线路中的训练包括理解每个球员的预测误差所需的变化，以最小化团队的整体误差。类似地，反向传播旨在了解神经网络权重和偏差的变化将如何影响我们的成本函数。形式上，最终目标是计算:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div class="er es md"><img src="../Images/0e4fcb1cdd49c726c929738f5f6cf527.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*qZVjugMt3WpeI3OXMycIKg.png"/></div></figure><p id="10cb" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><strong class="jk hu">注:</strong><code class="du me mf mg mh b">w^l</code>的下标<code class="du me mf mg mh b">j</code>和<code class="du me mf mg mh b">k</code>没有错反。我们这样做是因为这样会让我们以后的计算更容易。</p><p id="a0f7" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><code class="du me mf mg mh b">J</code>扮演类似于裁判的角色，而权重和偏见扮演类似于演讲者和玩家的角色。</p><p id="a248" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">通常，反向传播是通过首先展示算法，然后将其与微积分中的链式法则联系起来来解释的。我们将采取相反的方法。我们将首先使用链式法则提供一种理解微分的图形方式，然后将它与反向传播算法联系起来。</p><h2 id="6dfa" class="mi kh ht bd ki mj mk ml km mm mn mo kq jr mp mq ks jv mr ms ku jz mt mu kw mv bi translated">链式法则</h2><p id="dc46" class="pw-post-body-paragraph ji jj ht jk b jl ky iu jn jo kz ix jq jr la jt ju jv lb jx jy jz lc kb kc kd hb bi translated">考虑以下等式:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mw"><img src="../Images/5f491bb4a90df82411ceae41fc76c691.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tIpfxnPlHvXFkWrGFFYBEg.jpeg"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx"><strong class="bd ki">Equation 1: </strong>A as a Function of B, C and D</figcaption></figure><p id="11c8" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我们可以将这一功能分解为多个功能:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mx"><img src="../Images/873d0be54baded1e712ba6f67c3ad9bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HBMnhgfR3EN-6iU7-5_aTg.jpeg"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx"><strong class="bd ki">Equation 2: </strong>A as a Function of F and G</figcaption></figure><p id="61c8" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">这样，我们可以用图形表示所有输入和<code class="du me mf mg mh b">A</code>之间的关系:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es my"><img src="../Images/b664d0e908afa83816e21657f7721883.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QY3lLG2ruVGwC4YVBtaMxQ.jpeg"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx"><strong class="bd ki">Figure 5: </strong>Graphical Representation of A</figcaption></figure><p id="de26" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">这个<code class="du me mf mg mh b">A</code>的图形表示给了我们一个简单的方法来计算<code class="du me mf mg mh b">A</code>相对于任何变量的偏导数，使用的是所谓的<strong class="jk hu">链式法则</strong>。例如，为了找到<code class="du me mf mg mh b">A</code>相对于<code class="du me mf mg mh b">C</code> (∂A/∂C)的偏导数，我们沿着上图中从<code class="du me mf mg mh b">C</code>到<code class="du me mf mg mh b">A</code>的路径。有两条可能的路径，一条通过<code class="du me mf mg mh b">F</code>，另一条通过<code class="du me mf mg mh b">G</code>。在找到∂A/∂C之前，我们需要计算∂F/∂C和∂G/∂C.。规则是:相同路径上的所有偏导数相乘，而不同路径上的所有偏导数相加。因此<code class="du me mf mg mh b">A</code>相对于<code class="du me mf mg mh b">C</code>的偏导数计算如下:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mz"><img src="../Images/53e17cc15a762bb051b1997520fdb2ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_bWv5-kF-L9gTfQ4Q7ictQ.jpeg"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx"><strong class="bd ki">Equation 3: </strong>Partial Derivative of A With Respect to C</figcaption></figure><p id="bc5b" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">让我们将这些相同的规则应用于神经网络。</p><h2 id="9d1f" class="mi kh ht bd ki mj mk ml km mm mn mo kq jr mp mq ks jv mr ms ku jz mt mu kw mv bi translated">算法</h2><p id="4393" class="pw-post-body-paragraph ji jj ht jk b jl ky iu jn jo kz ix jq jr la jt ju jv lb jx jy jz lc kb kc kd hb bi translated">考虑以下不超过两个节点的神经网络(为了简化我们的图，我们省略了偏差项，但它在理论上仍然存在):</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es na"><img src="../Images/a1da2f24e9762de2befc0608bb489b7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8UIr7V0b3pKL5wIjrJvaqA.jpeg"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx"><strong class="bd ki">Figure 6: </strong>Simple Neural Network With Two Nodes</figcaption></figure><p id="495a" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">其中<code class="du me mf mg mh b">a^L</code>指输出层【预测结果】<code class="du me mf mg mh b">L</code>的<strong class="jk hu">激活</strong>，等于:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nb"><img src="../Images/f199705ecff957e320cc5b1b653d3ad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*srN53kpdLHmW-wzXjpLDLQ.jpeg"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx"><strong class="bd ki">Equation 4: </strong>Activation of Neuron j In Layer L</figcaption></figure><p id="3613" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">传递给sigmoid函数(σ)的加权和在我们的等式中非常常见，因此我们将它赋给一个变量<code class="du me mf mg mh b">z^L</code>:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nc"><img src="../Images/c74ce2709fa90d377cc65406e35664d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zi4-FTCnVUsr6oFhbHgtbg.jpeg"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx"><strong class="bd ki">Figure 7: </strong>Definition of z^L</figcaption></figure><p id="0799" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">好了，我们已经处理了所有的行话。从这一点开始，我们不外乎应用我们刚才学到的链式法则。不要忘记我们的主要目标:发现权重和偏差的变化如何影响我们的成本函数。</p><p id="bc6b" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">那么，对于我们的成本函数，我们如何绘制类似于<strong class="jk hu">图5 </strong>的图形呢？<code class="du me mf mg mh b">J</code>井是<code class="du me mf mg mh b">a^L</code>(预测结果)和<code class="du me mf mg mh b">y</code>(实际值)的函数，<code class="du me mf mg mh b">a^L</code>是<code class="du me mf mg mh b">z^L</code>的函数，而<code class="du me mf mg mh b">z^L</code>又是<code class="du me mf mg mh b">a^(L-1)</code>、<code class="du me mf mg mh b">w</code>和<code class="du me mf mg mh b">b</code>的函数……你看到模式了吗？下面是<code class="du me mf mg mh b">J</code>的图示:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nd"><img src="../Images/c79c22242be18d06e93bd6b3f6647f56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vIzakw2v3yfFdpoheFXcQQ.jpeg"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx"><strong class="bd ki">Figure 7: </strong>Graphical Representation of J</figcaption></figure><p id="7f04" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">你能用这张图表找到∂J/∂w和∂J/∂b吗？是的，你可以！只需应用我们之前使用的相同规则:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ne"><img src="../Images/f1092f9181de4cfef5297e95c728c848.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rrp3OoRlJp-qhZQR9XMzIw.jpeg"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx"><strong class="bd ki">Equation 5: </strong>∂J/∂w and ∂J/∂b</figcaption></figure><p id="6d9d" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我们知道<code class="du me mf mg mh b">z^L</code>和<code class="du me mf mg mh b">a^L</code>是什么，所以剩下要做的就是计算定义∂J/∂w和∂J/∂b的偏导数，我们可以得出∂J/∂w和∂J/∂b关于任何成本函数的表达式<code class="du me mf mg mh b">J</code>:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nf"><img src="../Images/281dc61f2a49d43a6b72e7b3bc8fe4a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZSGyDEOUzFRQ84kDjQWiUw.jpeg"/></div></div></figure><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ng"><img src="../Images/515583bff2afee3a3ea0352e5a5b2e44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xKDLLppFPESF-RgG61V92w.jpeg"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx"><strong class="bd ki">Equations 6, 7 and 8: </strong>Relevant Derivatives</figcaption></figure><p id="d6ed" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">总的来说，我们得到:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nh"><img src="../Images/cc339ec54a328a2e869f6982e9733f08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D1ym1eARJolE_9SFUefbyg.jpeg"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx"><strong class="bd ki">Equations 9 and 10: </strong>∂J/∂w and ∂J/∂b</figcaption></figure><p id="1028" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">就是这样！我们找到了计算<code class="du me mf mg mh b">J</code>相对于任何权重或偏差的变化的一般公式。请记住，我们通过只查看一个隐藏层，一个神经元和一个具有一个输出的输出层来简化事情。事实上，我们将有许多不同的层<code class="du me mf mg mh b">l=L, (L-1), (L-2), ... ,2</code>，每层包含许多不同的神经元。尽管如此，我们可以遍历所有不同的层和神经元，并在<strong class="jk hu">方程9和10 </strong>中使用它们的值。还要记住，这将在梯度下降过程中用于计算偏导数，这意味着我们将对我们拥有的每个训练示例运行该算法。</p><p id="7a5b" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">总体而言，反向传播算法包括:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div class="er es ni"><img src="../Images/799be2fa9da895a44deace080d1ece7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*5RfotyyTLrWHJqtj4HkKJg.png"/></div><figcaption class="lw lx et er es ly lz bd b be z dx"><strong class="bd ki">Figure 8: </strong>Full Backpropagation Algorithm [1]</figcaption></figure><p id="3427" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我们算法中的<strong class="jk hu">前馈</strong>步骤相当于在我们的电话示例中由说话者传递给法官的句子。在我们的神经网络中，一个训练样本将从输入层传递到输出层。这将使我们能够评估我们的总成本<code class="du me mf mg mh b">J</code>，并将其向后推。</p><h1 id="5b3f" class="kg kh ht bd ki kj kk kl km kn ko kp kq iz kr ja ks jc kt jd ku jf kv jg kw kx bi translated">结论</h1><p id="af73" class="pw-post-body-paragraph ji jj ht jk b jl ky iu jn jo kz ix jq jr la jt ju jv lb jx jy jz lc kb kc kd hb bi translated">在本文中，我们研究了反向传播算法。</p><p id="638c" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">通过分析错误是如何通过电话线传播的，我们能够了解一个球员的错误对整个团队的影响。我们还确定了减少这种误差的最快方法，首先遍历整个电话线，并反向传播最后计算的误差。</p><p id="b4d0" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">考虑到这种误差传播的思想，以及用于对多元函数进行微分的链式法则，我们提出了一组方程，这些方程可用于有效且快速地计算我们的神经网络的成本函数的梯度。我们通过形式化算法得出结论，并看到它是如何在梯度下降中使用的。</p><h1 id="bb2c" class="kg kh ht bd ki kj kk kl km kn ko kp kq iz kr ja ks jc kt jd ku jf kv jg kw kx bi translated">深度学习系列</h1><ul class=""><li id="3c00" class="ld le ht jk b jl ky jo kz jr nj jv nk jz nl kd li lj lk ll bi translated"><a class="ae kf" href="https://ali-h-khanafer.medium.com/deep-learning-meaning-motivation-and-nn-basic-structure-44b57b481e4c?source=your_stories_page-------------------------------------" rel="noopener">深度学习:意义、动机和NN基本结构</a></li><li id="7416" class="ld le ht jk b jl lm jo ln jr lo jv lp jz lq kd li lj lk ll bi translated"><a class="ae kf" href="https://ali-h-khanafer.medium.com/deep-learning-part-2-vanilla-vs-stochastic-gradient-descent-6bcecc26fd51" rel="noopener">深度学习第二部分:香草vs随机梯度下降</a></li></ul><h1 id="b84f" class="kg kh ht bd ki kj kk kl km kn ko kp kq iz kr ja ks jc kt jd ku jf kv jg kw kx bi translated">参考</h1><p id="98a2" class="pw-post-body-paragraph ji jj ht jk b jl ky iu jn jo kz ix jq jr la jt ju jv lb jx jy jz lc kb kc kd hb bi translated">[1]迈克尔·a·尼尔森，<a class="ae kf" href="http://neuralnetworksanddeeplearning.com/" rel="noopener ugc nofollow" target="_blank">神经网络与深度学习</a> (2015)，决心出版社</p></div></div>    
</body>
</html>
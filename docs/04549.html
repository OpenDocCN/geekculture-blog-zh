<html>
<head>
<title>Weight Initialization in neural nets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的权重初始化</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/weight-initialization-in-neural-nets-78091cfee93?source=collection_archive---------20-----------------------#2021-06-29">https://medium.com/geekculture/weight-initialization-in-neural-nets-78091cfee93?source=collection_archive---------20-----------------------#2021-06-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="f799" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">带有不饱和函数的</h2></div><p id="5cfd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">权重初始化是神经网络的重要因素之一。基于每个神经元的权重，可以确定误差。众所周知，要得到一个精确的模型，误差必须更小。</p><p id="74c5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将讨论三种主要的权重初始化技术，它们是:</p><p id="3a00" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">a)均匀分布</p><p id="0e35" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">b) Xavier初始化</p><p id="bf90" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">c) He初始化</p><h1 id="f29a" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">a)均匀分布:</h1><p id="f3b2" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">该权重初始化背后的思想是在[-y，y]的范围内初始化权重，并且它应该接近零，其中y由下式给出</p><p id="e53b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">y = 1/sqrt(fan _ in)</strong>；fan_in是给予神经元的输入数量</p><p id="d8aa" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当使用Sigmoid激活函数时，均匀分布工作得非常好。</p><h1 id="9288" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">b)Xavier初始化:</h1><p id="50a8" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">Xavier初始化是Glorot和Bengio提出的。他们指出，信号必须正确地向前和向后流动而不会消失。他们指出:为了使信号正确流动，我们需要每层输出的方差等于其输入的方差。</p><p id="4bf2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们有两种Xavier初始化</p><p id="5f9e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">T5)1)泽维尔正常T7】</strong></p><p id="c72f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">权重取自均值等于零且标准差等于<strong class="iz hj"> sqrt(2/fan_in+ fan_out)的正态分布；</strong> fan_out是神经元的输出数。</p><p id="7b28" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="kq"> 2)泽维尔制服</em> </strong></p><p id="309b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">权重取自[-y，y]范围内的均匀分布，其中y由下式给出</p><p id="eb5c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">y = sqrt(6/扇入+扇出)</strong></p><p id="3b8d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Xavier初始化与Sigmoid激活功能配合良好。</p><p id="8d51" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">默认情况下，Keras使用这个具有统一分布的Xavier(glorot)初始化器。</p><h1 id="3151" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">c) He初始化:</h1><p id="fd64" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">权重可以用两种类型的初始化来初始化</p><p id="23b5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="kq"> 1)何正常</em> </strong></p><p id="1d8c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">权重取自均值等于零且标准差等于<strong class="iz hj"> sqrt(2/fan_in)的正态分布。</strong></p><p id="6f7d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> 2)贺制服</strong></p><p id="284f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">权重取自[-y，y]范围内的均匀分布，其中y由下式给出</p><p id="50b5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> y=sqrt(6/fan_in) </strong></p><p id="e6a4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">权重初始化技术与ReLU激活功能配合得非常好。</p><p id="ea7b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du kr ks kt ku b">import keras.layers.Dense</code></p><p id="a10b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du kr ks kt ku b">keras.layers.Dense(10,activation="relu",kernal_initializer="he_normal")</code></p><h1 id="0f93" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">要点:-</h1><ul class=""><li id="ba0b" class="kv kw hi iz b ja kl jd km jg kx jk ky jo kz js la lb lc ld bi translated">权重值应该很小。</li><li id="7daa" class="kv kw hi iz b ja le jd lf jg lg jk lh jo li js la lb lc ld bi translated">对于所有迭代或时期，权重不应相同。</li><li id="2c21" class="kv kw hi iz b ja le jd lf jg lg jk lh jo li js la lb lc ld bi translated">权重应该具有良好的方差。</li></ul><h1 id="16f8" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">不饱和激活函数</h1><p id="4c00" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">在Glorot和Bengio于2010年发表论文之前，许多人认为Sigmoid激活函数是最好的激活函数。但是，当网格变得密集时，sigmoid函数会带来消失梯度问题。ReLU(整流线性单元)是sigmoid的更好替代方案，因为它不会因正值而饱和。</p><p id="c32b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="kq">热路</em>T3】</strong></p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es lj"><img src="../Images/e8f857c131c5870257b6c4b45e16824f.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/0*lHBdGK2v1m8ZS1HH"/></div></figure><p id="3ee2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ReLU是神经网络中使用的非线性激活函数。</p><p id="5f94" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于x &gt; =0 ，ReLU定义为<strong class="iz hj"> y=max(0，x)</strong></p><p id="6979" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于x&lt;0y = 0，其中x是输入值</p><p id="2246" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">不幸的是，ReLU遇到了一个被称为<em class="kq">死亡ReLU的问题。在某些情况下，我们可能会发现一半的神经元死亡，特别是当它们的权重被调整到输入的加权和为负时，当这种情况发生时，它只会继续输出零。</em></p><p id="8111" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了解决这个问题，我们使用ReLU的变体，称为泄漏ReLU，参数ReLU</p><p id="dda7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="kq">漏热路</em> </strong></p><p id="3432" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">漏ReLU定义为<strong class="iz hj"> y=max(0.01x，x) </strong></p><p id="6711" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="kq">参数化ReLU </em> </strong></p><p id="e08e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">它由x的&gt; 0 定义为<strong class="iz hj"> y=x</strong></p><p id="4450" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> y=ax否则</strong>其中a是一个小值</p><p id="99a9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当a=0.01时，参数ReLU作为泄漏ReLU工作。</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es lr"><img src="../Images/8224101b094bf571eaf7b476eceb7f32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*K3nD_PoQkpHzhCuR"/></div></div></figure><p id="2e94" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Djork-Arne Clevert在2015年的一篇论文中提出了一种新的激活函数，称为<strong class="iz hj">指数线性</strong> <strong class="iz hj">单位</strong>该函数优于ReLU的所有变体。</p><p id="6abc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="kq">指数线性单位</em> </strong></p><p id="1500" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">定义为<strong class="iz hj"> y = x，x &gt; =0 </strong></p><p id="a9b8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> y=a(e^x -1)，否则</strong></p><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es lw"><img src="../Images/d57c9125ba4225ee90228f29a2e013f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FuSYlWIrOspFhY7A"/></div></div></figure><p id="946b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">恭喜你，现在我们知道了神经网络的重要和常用的激活函数以及不同的权重初始化技术。我希望你已经发现这是有用的。感谢您的阅读</p></div></div>    
</body>
</html>
<html>
<head>
<title>Class Separation cannot be overlooked in Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归中不能忽略阶级分离</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/class-separation-cannot-be-overlooked-in-logistic-regression-f20e58b203eb?source=collection_archive---------4-----------------------#2021-10-02">https://medium.com/geekculture/class-separation-cannot-be-overlooked-in-logistic-regression-f20e58b203eb?source=collection_archive---------4-----------------------#2021-10-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/52ae6b8fbab9c3f5342a830db0454b19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*Vejh-nPAW-jVtkFCr5pOyQ.png"/></div><figcaption class="im in et er es io ip bd b be z dx">Picture by <a class="ae iq" href="https://unsplash.com/@hollyachisholm" rel="noopener ugc nofollow" target="_blank">Holly Chisholm</a> on Unsplash</figcaption></figure><p id="f23d" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi jp translated">你们中的大多数人可能都熟悉逻辑回归，这是用于分类问题的最常见的监督机器学习算法之一。</p><p id="eef2" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">但是你知道吗，当我们运行手头的逻辑回归问题时，有时我们会遇到所谓的<strong class="it hj">完全类分离</strong>或<strong class="it hj">准完全类分离</strong>的问题。在这篇文章中，我将讨论完全或准完全类分离以及无限估计的含义，以及当问题出现时我们需要如何处理它。</p><h1 id="3f5e" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak">什么是完全分离？</strong></h1><blockquote class="kw kx ky"><p id="9406" class="ir is kz it b iu iv iw ix iy iz ja jb la jd je jf lb jh ji jj lc jl jm jn jo hb bi translated">当结果变量Y完全分离预测变量时，逻辑回归中的完全分离(也称为完美预测)发生，即，利用结果变量Y的假定值，可以清楚地分离/区分预测变量x的值的范围</p></blockquote><p id="8c5b" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">换句话说，我们可以说预测变量可以100%确定地预测结果变量的值，而不需要运行算法来创建模型，也不需要进行任何估计。</p><p id="0060" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">通过下面的数据集示例，这一点就很清楚了。</p><p id="bcc2" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">让我考虑一个小的虚构数据集。</p><p id="110a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">描述完全分离的数据</strong></p><p id="25bb" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">数据集将Y作为结果变量，X作为预测变量。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/7ed51a33a166fca5fc692112c0a7a558.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/format:webp/1*Ux7EeSwWlOrUsyqkXHMPwA.png"/></div></figure><p id="2dbd" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">注意:我只考虑了1个预测变量X，以使表示背后的逻辑更容易理解，但在现实生活中，可以考虑任何数量的预测变量，并且不是所有的预测变量都必须表示分离。</p><p id="7a5b" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">如果我们仔细观察数据集，我们会发现Y = 0时，所有X的值都在X &lt;= 5 and with Y = 1 all X have values within range of X &gt; 5的范围内。即Y完美的完全的分开了X。</p><p id="b31b" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">另一种理解方式是X完美地预测了Y，因为X &lt;=5 corresponds to Y = 0 and X &gt; 5对应于Y = 1。</p><p id="63d8" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">也就是说，我们已经为结果变量y找到了一个完美的预测因子X。</p><p id="6637" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">概率方面，我们有P(Y = 1 | X &lt;= 5) = 0 and P(Y = 1 | X &gt; 5) = 1，不需要估计模型。</p><p id="32bb" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">现在，在这种情况下，当我们尝试使用上面显示的小数据集拟合Y对X的逻辑回归模型时，会发生什么呢？</p><p id="3006" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">X的参数的最大似然估计不存在。</p><p id="0685" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">X的系数越大，可能性越大。换句话说，X的系数可以有多大就有多大，它可以延伸到无穷大，这是一个问题，因为我们无法估计参数的正确值。</p><p id="359f" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">让我们看看“R”软件是如何处理上述数据集的。下面是拟合模型的代码。</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="96be" class="ln jz hi lj b fi lo lp l lq lr">#Install "safeBinaryRegression" package<br/>#install.packages("safeBinaryRegression")<br/># Call library<br/>library(safeBinaryRegression)<br/># Depiction of complete separation<br/># Data set with Y as outcome variable and X as independent variable<br/>Y &lt;- c(0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1)<br/>X &lt;- c(1,3,2,1,5,2,5,4,3,4,8,9,9,10,6,6,7,8,7,10)<br/># Fitting Model and Checking for Separation<br/>glm(Y ~ X, family=binomial)<br/>glm(Y ~ X,  family=binomial,  separation="test")<br/>stats::glm(Y ~ X, family=binomial)</span></pre><p id="067f" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">它检测到X的完美预测，并停止进一步的计算。</p><p id="275e" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">输出</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/1e0439c54b96e69ce7c8fa5f42c8f401.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*kT50y9Egpk4baMnWvXB7mQ.png"/></div></figure><p id="4389" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">“R”软件立即给出的错误信息是:</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="dbc3" class="ln jz hi lj b fi lo lp l lq lr"><strong class="lj hj">“Error in glm(Y ~ X, family = binomial) :The following terms are causing separation among the sample points: (Intercept), X".</strong></span></pre><p id="cbcb" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">和</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="cde2" class="ln jz hi lj b fi lo lp l lq lr"><strong class="lj hj">“Error in glm(Y ~ X, family = binomial, separation = "test") :</strong></span><span id="6dcc" class="ln jz hi lj b fi lt lp l lq lr"><strong class="lj hj">Separation exists among the sample points.</strong></span><span id="e8d0" class="ln jz hi lj b fi lt lp l lq lr"><strong class="lj hj">This model cannot be fit by maximum likelihood.”</strong></span></pre><p id="92aa" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">此外，它还显示如下警告消息:</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="305f" class="ln jz hi lj b fi lo lp l lq lr"><strong class="lj hj">“ glm.fit: algorithm did not converge”</strong> and</span><span id="c009" class="ln jz hi lj b fi lt lp l lq lr"><strong class="lj hj">" glm.fit: fitted probabilities numerically 0 or 1 occurred".</strong></span></pre><p id="23ce" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">这可以解释为完美的预测或者完全的分离。参数估计的标准误差太大，这证实了描述导致无限MLE(最大似然估计)的收敛问题的分离。</p></div><div class="ab cl lu lv gp lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="hb hc hd he hf"><h1 id="c4bf" class="jy jz hi bd ka kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv bi translated">什么是准完全分离？</h1><blockquote class="kw kx ky"><p id="c0d7" class="ir is kz it b iu iv iw ix iy iz ja jb la jd je jf lb jh ji jj lc jl jm jn jo hb bi translated">当结果变量几乎完全分离预测变量而不是100% <strong class="it hj"> </strong>时，逻辑回归中的准完全分离发生，即，利用结果变量的假定值，除了少数观察值，人们几乎可以分离/区分预测变量的值范围。</p></blockquote><p id="4d32" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">换句话说，我们可以说，预测变量可以100%确定地预测结果变量的值。</p><p id="4be2" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">通过下面的数据集示例，这一点就很清楚了。</p><p id="efdc" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">让我再次考虑一个小的虚构数据集。</p><p id="a172" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">描述准完全分离的数据</p><p id="9276" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">数据集将Y作为结果变量，X作为预测变量。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/3dc43b08d63b28be73699fd03a0032f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:288/format:webp/1*JNIYcY1pzluhOKf69daPiw.png"/></div></figure><p id="1c51" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">注意:同样，我只考虑了1个预测变量，以使表示背后的逻辑更容易理解，但在现实生活中，可以考虑任何数量的预测变量，并且不是所有变量都必须表示分离。</p><p id="f7bc" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">请注意，结果变量Y很好地分离了预测变量X，除了X = 5的少数观察结果。换句话说，当X &lt; 5 (Y = 0) or X &gt; 5 (Y=1)时，X完美地预测了Y，只留下X = 5作为不确定的情况。</p><p id="2b2c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">就预期概率而言，我们会有P(Y=1 | X &lt;5) = 0 and P(Y=1 | X&gt; 5) = 1，除了P(Y = 1 | X = 5)之外，没有什么需要估计的。</p><p id="e736" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">当我们试图使用上面的数据拟合Y对X的逻辑回归模型时会发生什么？原来X的最大似然估计不存在。在这个例子中，X的参数越大，似然性越大，因此X的参数估计的最大似然估计至少在数学意义上是不存在的。实际上，11或更大的值不会产生太大的差异，它们基本上都对应于估计概率1。</p><p id="4b0c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">让我们看看“R”软件是如何做到这一点的。下面是拟合模型的代码。</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="ccee" class="ln jz hi lj b fi lo lp l lq lr">#Install "safeBinaryRegression" package<br/>#install.packages("safeBinaryRegression")<br/># Call library<br/>library(safeBinaryRegression)<br/># Depiction of complete separation<br/># Data set with Y as outcome variable and X as independent variable<br/>Y_quasi &lt;- c(0,0,0,0,0,0,1,0,0,0,1,1,1,1,1,1,1,1,1,1)<br/>X_quasi &lt;- c(1,3,2,1,5,5,5,4,3,4,8,9,9,10,6,6,6,8,7,10)<br/># Fitting Model and Checking for Quasi Complete Separation<br/>glm(Y_quasi ~ X_quasi, family=binomial)<br/>glm(Y_quasi ~ X_quasi,  family=binomial,  separation="test")<br/>stats::glm(Y_quasi ~ X_quasi, family=binomial)</span></pre><p id="9cfd" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">它检测到X的准完美预测并停止进一步的计算。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/7b52cb210faeac09bd90edc177c6eb4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*QZbkyKwC8yudaAtLjesqKg.png"/></div></figure><p id="02e0" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">“R”软件立即给出的错误信息是:</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="b69e" class="ln jz hi lj b fi lo lp l lq lr"><strong class="lj hj">“Error in glm(Y_quasi ~ X_quasi, family = binomial) :</strong></span><span id="acb1" class="ln jz hi lj b fi lt lp l lq lr"><strong class="lj hj">The following terms are causing separation among the sample points: (Intercept), X_quasi</strong></span></pre><p id="78e1" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">和</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="d7c4" class="ln jz hi lj b fi lo lp l lq lr"><strong class="lj hj">Error in glm(Y_quasi ~ X_quasi, family = binomial, separation = "test") :</strong></span><span id="f4a2" class="ln jz hi lj b fi lt lp l lq lr"><strong class="lj hj">Separation exists among the sample points.</strong></span><span id="de53" class="ln jz hi lj b fi lt lp l lq lr"><strong class="lj hj">This model cannot be fit by maximum likelihood.</strong></span></pre><p id="b0be" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">此外，它还显示如下警告消息:</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="a45e" class="ln jz hi lj b fi lo lp l lq lr"><strong class="lj hj">"glm.fit: fitted probabilities numerically 0 or 1 occurred".</strong></span></pre><p id="004a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">从参数估计中，我们可以看到X的系数足够大，其标准误差甚至更大，这表明模型可能与X有一些问题。此时，我们应该仔细研究结果变量Y和X之间的双变量关系。</p><p id="445e" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">这可以解释为半完美的预测或准完全的分离。参数估计的标准误差足够大，这证实了描述收敛问题的准完全分离。</p></div><div class="ab cl lu lv gp lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="hb hc hd he hf"><p id="19a4" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">现在，处理这种完全和准完全分离的技术是什么？</p><p id="66b3" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">处理类分离的技术很少。</p><p id="b7d8" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">由于预测变量X被结果变量完全或准完全分开，因此我们下面的讨论集中在如何处理X？</p><p id="4ff0" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">1.如果我们有不止一个预测变量，最简单的方法就是“什么都不做”。这是因为模型中其他预测变量的最大似然仍然有效，除了有分离问题的那个。但缺点是，我们没有得到任何合理的预测变量的估计，它能如此完美地预测结果变量。</p><p id="9a7f" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">2.另一个简单技术是不在模型中包括导致分离的预测变量。但是这导致了对模型中其他变量的有偏估计。</p><p id="8e15" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">3.如果X是一个分类变量，并且这样做有意义，我们也可以尝试折叠一些预测变量的分类。</p><p id="d620" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">4.当数据集很小且模型不是很大时，在数据集中包含更多不同的观察值可能会减少分离是一个好策略。</p><p id="522c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">5.当x的参数估计有附加信息时，可以使用贝叶斯方法。</p><p id="9076" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">6.正则化，像脊或套索，结合bootstrap也可以使用。</p><p id="7ef7" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">7.也可以尝试减少偏差的估计过程。</p><p id="f0e5" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">根据手头的条件和问题，可以采用的技术很少，上面提到的列表并不详尽。</p><p id="30e6" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">讨论它们中的每一个都不是这篇文章的范围，我将在单独的文章中讨论它们。</p><p id="f658" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我得出这样的结论。希望你喜欢。</p><p id="f57b" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">感谢阅读！！！</p></div></div>    
</body>
</html>
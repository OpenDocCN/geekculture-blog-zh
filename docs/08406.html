<html>
<head>
<title>ML Loss Functions: An Overview</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML损失函数:综述</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/ml-loss-functions-an-overview-14dcca261504?source=collection_archive---------9-----------------------#2021-10-28">https://medium.com/geekculture/ml-loss-functions-an-overview-14dcca261504?source=collection_archive---------9-----------------------#2021-10-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/8c319e4870e3996f660e6fc59a610368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4UpcX6nZZXQ7kNDB"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Photo by <a class="ae iu" href="https://unsplash.com/@franki?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Franki Chamaki</a> on <a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e082" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">什么是损失函数？这就是我们如何发现我们的算法执行得好或不好，算法的主要目的是减少这种损失，以便使我们的预测接近实际值。</p><p id="96a4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">在选择一个适合你的模型的损失函数之前，你应该理解问题陈述，你正在处理的数据和不同类型的损失函数！你显然可以自定义自己的损失函数。</em></p><p id="24de" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们来看看一些预定义的损失函数。大体上有两类:回归损失和分类损失。我认为重要的是理解<em class="jt">是什么？为什么？</em>和<em class="jt">如何？</em>各有所失。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="69d3" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">回归损失</strong></h1><h1 id="e818" class="kb kc hi bd kd ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky bi translated"><strong class="ak">均方误差(MSE)/均方根误差(RMSE) </strong></h1><h2 id="b2e2" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">什么？</h2><ol class=""><li id="ac0a" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">在统计学中，均方误差(MSE)被定义为实际值和估计值之差的平均值。</li></ol><figure class="me mf mg mh fd ij er es paragraph-image"><div class="er es md"><img src="../Images/0f79e681b8bfb787a2905d3819222388.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*TCE9Kui4fbyZl5u3ARRBJw.png"/></div></figure><p id="1f27" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.当误差被平方时，MSE单位阶高于误差单位。要得到同样的单位阶，要取MSE的平方根的很多倍。它被称为均方根误差(RMSE)。RMSE = SQRT(均方误差)</p><h2 id="0f33" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">为什么？</h2><ol class=""><li id="4758" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">结果总是积极的。所以，不管预测值和实际值的符号是什么，你总是要把它最小化。</li><li id="9e68" class="ls lt hi ix b iy mi jc mj jg mk jk ml jo mm js lz ma mb mc bi translated">平方意味着较大的错误比较小的错误导致更多的错误，这意味着模型因犯较大的错误而受到惩罚。</li><li id="1260" class="ls lt hi ix b iy mi jc mj jg mk jk ml jo mm js lz ma mb mc bi translated">为什么是方形？这是因为，否则，预测值有时会比实际值少，有时会比实际值多，这可能导致它们之间的负差和正差，从而导致不正确的求和，在最坏的情况下，有时会为0！这让我们相信我们的模型是完美的！</li></ol><h2 id="62a4" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">怎么会？</h2><ol class=""><li id="1550" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">可在Keras中使用，方法是在编译模型时将“mse”或“mean_squared_error”指定为损失函数。</li></ol><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="22ac" class="le kc hi mo b fi ms mt l mu mv"><em class="jt">model.compile(loss=’mean_squared_error’) model.compile(loss=’root_mean_squared_error’)</em></span></pre><p id="bf6e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.来自python的Sklearn模块</p><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="278d" class="le kc hi mo b fi ms mt l mu mv"><em class="jt">from sklearn.metrics import mean_squared_error<br/>mean_squared_error(y_true, y_pred)</em><br/><em class="jt">mean_squared_error(y_true, y_pred, squared=False) ; </em>if squared=False it returns RMSE instead of MSE</span></pre><p id="3faa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.使用Numpy模块</p><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="7c86" class="le kc hi mo b fi ms mt l mu mv"><em class="jt">mse = np.square(y_true — y_pred).mean(axis=0)<br/>rmse = np.sqrt(mse)</em></span></pre></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="8450" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">平均绝对误差(MAE) </strong></h1><h2 id="01d4" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">什么？</h2><ol class=""><li id="24a2" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">平均绝对误差(MAE)被定义为实际值和估计值之间的绝对差的平均值。</li></ol><figure class="me mf mg mh fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/a1431f991b673b6a8b27f091675e9abb.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/0*yxvpJsR_us5nQ6IF.gif"/></div></figure><h2 id="230c" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">为什么？</h2><ol class=""><li id="0a3e" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">它平等地对待大错误和小错误。与MSE损失不同，对异常值不太敏感。</li></ol><h2 id="d18b" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">怎么会？</h2><ol class=""><li id="841e" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">可在Keras中使用，方法是在编译模型时将“mean_absolute_error”指定为损失函数。</li></ol><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="d284" class="le kc hi mo b fi ms mt l mu mv"><em class="jt">model.compile(loss=’mean_absolute_error’)</em></span></pre><p id="58f0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.来自python的Sklearn模块</p><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="72fe" class="le kc hi mo b fi ms mt l mu mv"><em class="jt">from sklearn.metrics import mean_absolute_error<br/>mean_absolute_error(y_true, y_pred)</em></span></pre><p id="eaef" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.使用Numpy模块</p><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="11b0" class="le kc hi mo b fi ms mt l mu mv"><em class="jt">mae = np.absolute(y_true — y_pred).mean(axis=0)</em></span></pre></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="fbb2" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">均方对数误差(MSLE)/ RMSLE </strong></h1><h2 id="f1ed" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">什么？</h2><ol class=""><li id="1a97" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">首先计算每个预测值和实际值的自然对数，然后计算均方误差。这被称为均方对数误差损失，或MSLE。</li></ol><figure class="me mf mg mh fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/6887a2b7e6739f6698bc6f87c7c7ecc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*MTzY0ZYdf-GTzFz2DYNY0A.png"/></div></figure><h2 id="77e9" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">为什么？</h2><ol class=""><li id="c240" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">它只关心实际值和预测值之间的相对差异，或者说，它们之间的百分比差异。</li><li id="f204" class="ls lt hi ix b iy mi jc mj jg mk jk ml jo mm js lz ma mb mc bi translated">这意味着它将把小的实际值和预测值之间的小差异视为大的实际值和预测值之间的大差异。所以，举个例子，如果<br/> <em class="jt">实际= 30，预测= 20；那么MSLE = 0.02861和<br/>实际= 30000，预测= 20000；那么MSLE = 0.03100，但是这里MSE= 100000000 </em></li><li id="5da0" class="ls lt hi ix b iy mi jc mj jg mk jk ml jo mm js lz ma mb mc bi translated">当目标值的范围很大时，即可以从10变化到10000甚至更大时，这很有用。</li></ol><h2 id="26d0" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">怎么会？</h2><ol class=""><li id="78c9" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">可在Keras中使用，方法是在编译模型时将“均方对数误差”指定为损失函数。</li></ol><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="ebc5" class="le kc hi mo b fi ms mt l mu mv"><em class="jt">model.compile(loss=’mean_squared_logarithmic_error’)</em></span></pre><p id="54e5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.使用Numpy模块</p><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="493e" class="le kc hi mo b fi ms mt l mu mv"><em class="jt">msle = np.sum(np.square(np.log(y_true+1) -np.log(y_pred+1))).mean(axis=0)</em><br/>and simliarly we can have,<em class="jt"> rmsle = np.sqrt(msle)</em></span></pre></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="3900" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">胡贝尔损失</strong></h1><h2 id="4ec0" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">什么？</h2><ol class=""><li id="fdf5" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">Huber损失MSE和MAE的组合</li></ol><figure class="me mf mg mh fd ij er es paragraph-image"><div class="er es my"><img src="../Images/03612eeb140e52cdf7e633d4d2f63683.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*Rrcn2OVN1sxnERE2XaU2WQ.png"/></div></figure><h2 id="e3c5" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">为什么？</h2><ol class=""><li id="c2f8" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">通过平衡MSE和MAE，它提供了两个世界的优点。当误差较小时，它是二次型的。</li><li id="7852" class="ls lt hi ix b iy mi jc mj jg mk jk ml jo mm js lz ma mb mc bi translated">对较大的损失值使用MAE减轻了我们对异常值的重视，因此我们仍然可以得到一个全面的模型。同时，我们使用较小损失值的MSE来保持中心附近的二次函数。</li><li id="ce9a" class="ls lt hi ix b iy mi jc mj jg mk jk ml jo mm js lz ma mb mc bi translated">任何时候，当你觉得需要在给异常值一些权重之间取得平衡时，使用休伯损失，但不要太多。对于异常值对您非常重要的情况，使用MSE！对于您根本不关心异常值的情况，请使用MAE！</li></ol><h2 id="74a8" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">怎么会？</h2><ol class=""><li id="8e07" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">编译模型时，通过将“huber_loss”指定为损失函数，可在Keras中使用。</li></ol><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="20be" class="le kc hi mo b fi ms mt l mu mv"><em class="jt">model.compile(loss=’huber_loss’) </em>or <em class="jt">model.compile(loss=tf.keras.losses.Huber())</em></span></pre><p id="484f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.使用Sklearn</p><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="9a91" class="le kc hi mo b fi ms mt l mu mv"><em class="jt">from sklearn.linear_model import HuberRegressor<br/>huber = HuberRegressor().fit(X, y)</em></span></pre><p id="7866" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.使用张量流</p><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="6ad5" class="le kc hi mo b fi ms mt l mu mv">def huber_loss_fn(y_true, y_pred, delta=1):<br/> error = y_true — y_pred<br/> is_small_error = tf.abs(error) &lt; delta<br/> squared_loss = tf.square(error) / 2<br/> linear_loss = delta * (tf.abs(error) — delta/2)<br/> return tf.where(is_small_error, squared_loss, linear_loss)</span></pre></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="52e4" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">分类损失</h1><h1 id="be33" class="kb kc hi bd kd ke kz kg kh ki la kk kl km lb ko kp kq lc ks kt ku ld kw kx ky bi translated">二元交叉熵损失</h1><h2 id="b47a" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">什么？</h2><p id="ac03" class="pw-post-body-paragraph iv iw hi ix b iy lu ja jb jc lv je jf jg mz ji jj jk na jm jn jo nb jq jr js hb bi translated">在二元分类中，我们预测X在第1类中的概率。</p><figure class="me mf mg mh fd ij er es paragraph-image"><div class="er es nc"><img src="../Images/5ce279153891dd515fedc9f5263948d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*PDtIfRHpMfbbXbhj26I-OA.png"/></div></figure><p id="c6c8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当观察值属于类别1时，公式的第一部分变为有效，第二部分消失，反之亦然，此时观察值的实际类别为0。</p><h2 id="a48a" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">为什么？</h2><ol class=""><li id="0515" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">它与目标值在集合{0，1}中的二元分类一起使用。然后，它根据与期望值的距离计算惩罚概率的分数。这意味着离实际值有多近或多远。</li></ol><h2 id="78d1" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">怎么会？</h2><ol class=""><li id="1e46" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">在喀拉斯</li></ol><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="3565" class="le kc hi mo b fi ms mt l mu mv">model.compile(loss='<!-- -->binary_crossentropy<!-- -->')<br/>model.compile(loss=<!-- -->tf.keras.losses.BinaryCrossentropy(from_logits=True)<!-- -->)</span></pre><p id="d08f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.在Sklearn</p><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="2012" class="le kc hi mo b fi ms mt l mu mv"><strong class="mo hj">from</strong> <strong class="mo hj">sklearn.metrics</strong> <strong class="mo hj">import</strong> log_loss<br/>log_loss(<em class="jt">y_true</em>, <em class="jt">y_pred)</em></span></pre><p id="3c20" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.在Numpy</p><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="5ac1" class="le kc hi mo b fi ms mt l mu mv">loss = <!-- -->-(y_true*np.log(y_pred)+(1-y_true)*np.log(1-y_pred)).mean()</span></pre></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="4c52" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">铰链损耗</h1><h2 id="957f" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">什么？</h2><p id="3f31" class="pw-post-body-paragraph iv iw hi ix b iy lu ja jb jc lv je jf jg mz ji jj jk na jm jn jo nb jq jr js hb bi translated">铰链损耗特别用于SVM模型或目标值在集合{-1，1}中的情况</p><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="dc49" class="le kc hi mo b fi ms mt l mu mv">hinge_loss = max(0, 1-y_true*y_pred)</span></pre><h2 id="c3a3" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">为什么？</h2><ol class=""><li id="7b1f" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">在SVM，我们有两个类-1和1，我们试图模拟决策边界。因此，预测值显示了与决策边界的距离。</li><li id="8bb0" class="ls lt hi ix b iy mi jc mj jg mk jk ml jo mm js lz ma mb mc bi translated">在这种情况下，如果一个类被预测为-1.6，而实际为-1。这意味着它远离决策边界，因此将没有损失。越靠近边界，其铰链损耗越大，并且如果在边界的错误一侧，铰链损耗将继续增加。</li></ol><h2 id="217a" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">怎么会？</h2><ol class=""><li id="b1a2" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">在喀拉斯</li></ol><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="48b1" class="le kc hi mo b fi ms mt l mu mv">model.compile(loss='hinge')</span></pre><p id="a3fd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.在Sklearn</p><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="f2a8" class="le kc hi mo b fi ms mt l mu mv">from sklearn.metrics import hinge_loss<br/>hinge_loss(y_true, y_pred)</span></pre><p id="ad5b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.在Numpy</p><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="3d54" class="le kc hi mo b fi ms mt l mu mv">loss = np.max([0.0, 1 - y_true*y_pred])</span></pre></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="f299" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">多类交叉熵损失/分类交叉熵</h1><h2 id="ff58" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">什么？</h2><p id="d0a3" class="pw-post-body-paragraph iv iw hi ix b iy lu ja jb jc lv je jf jg mz ji jj jk na jm jn jo nb jq jr js hb bi translated">这是多类分类中使用的默认损失函数。</p><figure class="me mf mg mh fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/6c993d1eac45c1ef75fe1d8b32411b6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*rSn1W9CXWW5C0szSTYwwhg.png"/></div></figure><h2 id="f993" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">为什么？</h2><ol class=""><li id="d458" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">当我们有多个类而不是只有两个类时使用它，我们不是计算每个标签的二进制损失，而是计算所有类的分类损失。</li></ol><h2 id="155a" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">怎么会？</h2><ol class=""><li id="f3ac" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">在喀拉斯</li></ol><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="6a9c" class="le kc hi mo b fi ms mt l mu mv">model.compile(loss='categorical_crossentropy')</span></pre><p id="de8a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.在Sklearn</p><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="3151" class="le kc hi mo b fi ms mt l mu mv">sklearn.metrics.log_loss(y_true, y_pred, labels=[all_labels])</span></pre></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="cd28" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">稀疏多类交叉熵损失</h1><h2 id="b417" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">什么？</h2><p id="7463" class="pw-post-body-paragraph iv iw hi ix b iy lu ja jb jc lv je jf jg mz ji jj jk na jm jn jo nb jq jr js hb bi translated">类别交叉熵和稀疏类别交叉熵都具有上面等式中定义的相同损失函数。两者之间唯一的区别是如何定义真理标签。…在稀疏分类交叉熵中，真值标签是<strong class="ix hj">整数编码的</strong>，例如，[1]，[2]和[3]等。，而在分类交叉熵中，它们是一次性编码的。</p><h2 id="dbd7" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">为什么？</h2><ol class=""><li id="93d5" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">它用于有大量标签的情况，对它们进行一次性编码只会导致问题。</li></ol><h2 id="cef5" class="le kc hi bd kd lf lg lh kh li lj lk kl jg ll lm kp jk ln lo kt jo lp lq kx lr bi translated">怎么会？</h2><ol class=""><li id="6e61" class="ls lt hi ix b iy lu jc lv jg lw jk lx jo ly js lz ma mb mc bi translated">在喀拉斯</li></ol><pre class="me mf mg mh fd mn mo mp mq aw mr bi"><span id="8b74" class="le kc hi mo b fi ms mt l mu mv">model.compile(loss='sparse_categorical_crossentropy')</span></pre></div></div>    
</body>
</html>
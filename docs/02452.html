<html>
<head>
<title>Neural Networks-A Simple Introduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络-简单介绍</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/neural-networks-a-simple-introduction-f4a80c26330c?source=collection_archive---------22-----------------------#2021-05-15">https://medium.com/geekculture/neural-networks-a-simple-introduction-f4a80c26330c?source=collection_archive---------22-----------------------#2021-05-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/407fc84bd889f93323e36141328bc7c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vbqbW0G02KoVp4vX"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Photo by <a class="ae iu" href="https://unsplash.com/@hharritt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Hunter Harritt</a> on <a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="62f7" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">什么是神经网络？</h1><p id="e45b" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">神经网络是一种算法，具有从复杂数据中提取有意义信息的独特能力，这些数据对于人脑来说非常复杂。</p><p id="ed8c" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">比方说一个猫分类器，你会用什么特征来训练一个模型来分类一个给定的图像是不是一只猫？起初，这听起来很容易，你会去像大小，颜色，爪子，牙齿等特征。但是世界上有40-70种不同品种的猫，每一种在颜色、大小等方面都有所不同。现在这突然变成了一个乏味的过程。你不能只是手动地在每一个品种的猫身上找到独特的特征。这将成为一场噩梦！</p><p id="ebcb" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">那么，有没有一种方法可以在不做任何手工处理的情况下从输入中提取特征呢？绝对的！</p><p id="8b9c" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">深度学习最大的优势是我们不需要手动从图像中提取特征。网络在训练时学习提取特征。你只需将图像输入网络(像素值)。你需要的是定义神经网络架构和一个带标签的数据集。</p><p id="575c" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们如何“构建”一个神经网络？</p><h1 id="4cbb" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">神经网络的结构</h1><p id="9071" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">神经网络可以分为三个部分</p><ol class=""><li id="4e61" class="kw kx hi jv b jw kr ka ks ke ky ki kz km la kq lb lc ld le bi translated">输入层</li><li id="8525" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">隐藏层</li><li id="c4a2" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">输出层</li></ol><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lk"><img src="../Images/5dcec42b798e57e9e7d06675afc5558c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WskhyxzRCfyYLCmUlcvOcg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Each circle is a node</figcaption></figure><h2 id="e4c4" class="lp iw hi bd ix lq lr ls jb lt lu lv jf ke lw lx jj ki ly lz jn km ma mb jr mc bi translated">输入层</h2><p id="b8d5" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">顾名思义，它是我们网络的输入。在我们的例子中，它是一只猫/非猫的形象。</p><p id="f8d5" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">输入层中的每个节点包含我们图像的单个像素。因此，如果图像的尺寸是<strong class="jv hj">64×64</strong>，那么输入层中的节点总数将是<strong class="jv hj">64×64×3 = 12288(x1，x2，.。。，x12288) </strong></p><p id="092b" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">3代表图像的三个颜色通道(红、绿、蓝)。</p><p id="a055" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在我们知道输入层是什么了。让我们把它从清单上划掉。</p><ol class=""><li id="9731" class="kw kx hi jv b jw kr ka ks ke ky ki kz km la kq lb lc ld le bi translated">I̶n̶p̶u̶t̶ ̶l̶a̶y̶e̶r̶</li><li id="0c95" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">隐藏层</li><li id="87be" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">输出层</li></ol><h2 id="5e4d" class="lp iw hi bd ix lq lr ls jb lt lu lv jf ke lw lx jj ki ly lz jn km ma mb jr mc bi translated">隐藏层</h2><p id="d935" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">我们可以有<strong class="jv hj"> <em class="md"> n </em> </strong>个隐藏层。这就是为什么我们使用复数术语<strong class="jv hj">层</strong>。但现在，让我们使用一个单一的隐藏层，以保持简单的解释。</p><p id="978f" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">隐藏层中的每个节点都包含输入值和参数的给定组合的激活值。我们知道像素值是输入。那参数是什么？计算激活函数有两个参数。权重<strong class="jv hj"> <em class="md"> w </em> </strong>和偏差<strong class="jv hj"> <em class="md"> b. </em> </strong>正如我们在上面的图像中所看到的，输入值去往隐藏层中的所有节点，但是它们所携带的权重和偏差对于隐藏层中的每个节点将是不同的。</p><p id="7de3" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">例如，让我们考虑上图中的网络。它有3个输入<strong class="jv hj"> <em class="md"> x1、x2、x3、</em> </strong>和4个隐层节点<strong class="jv hj"> <em class="md"> a1、a2、a3、</em> </strong>和<strong class="jv hj"> <em class="md"> a4 </em> </strong>。因此，4个激活值将是:</p><blockquote class="me mf mg"><p id="08ac" class="jt ju md jv b jw kr jy jz ka ks kc kd mh kt kg kh mi ku kk kl mj kv ko kp kq hb bi translated"><strong class="jv hj"><em class="hi">【a1[1]=σ(Z1[1])、</em> </strong> <em class="hi">其中</em><strong class="jv hj"><em class="hi">Z1[1]</em></strong><em class="hi"/>=<strong class="jv hj"><em class="hi">w1[1]* x1+w2[1]* x2+w3[1]* x3+b</em></strong></p><p id="e454" class="jt ju md jv b jw kr jy jz ka ks kc kd mh kt kg kh mi ku kk kl mj kv ko kp kq hb bi translated"><strong class="jv hj"> <em class="hi"> a2[1] = σ(z2[1])、</em> </strong> <em class="hi">其中</em><strong class="jv hj"><em class="hi">z2[1]</em></strong>=<strong class="jv hj"><em class="hi">w1[1]* x1+w2[1]* x2+w3[1]* x3+b</em></strong></p><p id="f153" class="jt ju md jv b jw kr jy jz ka ks kc kd mh kt kg kh mi ku kk kl mj kv ko kp kq hb bi translated"><strong class="jv hj"> <em class="hi"> a3[1] = σ(z3[1])，</em> </strong> <em class="hi">其中</em><strong class="jv hj"><em class="hi">z3[1]</em></strong>=<strong class="jv hj"><em class="hi">w1[1]* x1+w2[1]* x2+w3[1]* x3+b</em></strong></p><p id="5646" class="jt ju md jv b jw kr jy jz ka ks kc kd mh kt kg kh mi ku kk kl mj kv ko kp kq hb bi translated"><strong class="jv hj"> <em class="hi"> a4[1] = σ(z4[1])，</em> </strong> <em class="hi">其中</em><strong class="jv hj"><em class="hi">Z4[1]</em></strong>=<strong class="jv hj"><em class="hi">w1[1]* x1+w2[1]* x2+w3[1]* x3+b</em></strong></p></blockquote><p id="48b5" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">这里，输入<strong class="jv hj"> <em class="md"> x1、x2、</em> </strong>和<strong class="jv hj"> <em class="md"> x3 </em> </strong>对于所有节点都是相同的，但是权重<strong class="jv hj"> <em class="md"> w1、w2、w3 </em> </strong>和偏差<strong class="jv hj"> <em class="md"> b </em> </strong>将是不同的。</p><p id="62c4" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"> <em class="md"> σ </em> </strong>称为激活功能。它是<strong class="jv hj">神经网络</strong>中最重要的因素，决定一个神经元是否会被激活并转移到下一层。有不同类型的激活函数，但在隐藏层中广泛使用的是<strong class="jv hj"> <em class="md"> ReLU </em> </strong>激活函数。</p><p id="4faa" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"><em class="md"/></strong><strong class="jv hj"><em class="md">【σ= max(0，z) </em> </strong></p><p id="2b8b" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">如果z &gt; 0，则<strong class="jv hj"> <em class="md"> σ(z) = z，</em> </strong>但如果<strong class="jv hj"> <em class="md"> z ≤ 0，</em> </strong>则<strong class="jv hj"> <em class="md"> σ(z) = 0 </em> </strong></p><p id="183b" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">上标<strong class="jv hj"><em class="md">【1】</em></strong>代表第一层。输入值<strong class="jv hj"> <em class="md"> x1，x2，</em> </strong>和<strong class="jv hj"> <em class="md"> x3 </em> </strong>也可以表示为<strong class="jv hj"> <em class="md"> a1[0]，a2[0] </em> </strong>，和<strong class="jv hj"> <em class="md"> a3[0] </em> </strong>既然我们不把输入值看作一个层，就像冥王星一样，它<em class="md">就是</em>这不公平，不是吗？</p><p id="7c45" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在我们知道，隐藏层包含通过对权重、输入和偏差的乘积求和而获得的激活值。让我们也检查一下。</p><ol class=""><li id="5674" class="kw kx hi jv b jw kr ka ks ke ky ki kz km la kq lb lc ld le bi translated">I̶n̶p̶u̶t̶ ̶l̶a̶y̶e̶r̶</li><li id="560d" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">H̶i̶d̶d̶e̶n̶ ̶l̶a̶y̶e̶r̶s</li><li id="67b8" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">输出层</li></ol><h2 id="8a9e" class="lp iw hi bd ix lq lr ls jb lt lu lv jf ke lw lx jj ki ly lz jn km ma mb jr mc bi translated">输出层</h2><p id="2270" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">对于二进制分类器，在我们的例子中是猫或不是分类器，在输出层中只有一个节点。输出为1(猫)或0(不是猫)。</p><p id="7865" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">这里，输入是隐藏层的激活值。</p><blockquote class="me mf mg"><p id="6a98" class="jt ju md jv b jw kr jy jz ka ks kc kd mh kt kg kh mi ku kk kl mj kv ko kp kq hb bi translated"><strong class="jv hj"> <em class="hi"> a2[1] = σ(z2[1])，</em> </strong> <em class="hi">其中</em><strong class="jv hj"><em class="hi">z2[1]</em></strong>=<strong class="jv hj"><em class="hi">w1[2]* a1[1]+w2[2]* a2[1]+w3[2]* a3[1]+w4[2]* a4[1]+b</em></strong></p></blockquote><p id="2b46" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">激活函数<strong class="jv hj"> <em class="md"> σ </em> </strong>通常是二进制分类器输出节点<strong class="jv hj">中的sigmoid函数。</strong></p><ol class=""><li id="03cd" class="kw kx hi jv b jw kr ka ks ke ky ki kz km la kq lb lc ld le bi translated">I̶n̶p̶u̶t̶ ̶l̶a̶y̶e̶r̶</li><li id="4f5e" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">H̶i̶d̶d̶e̶n̶ ̶l̶a̶y̶e̶r̶s</li><li id="c633" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">O̶u̶t̶p̶u̶t̶ ̶l̶a̶y̶e̶r̶</li></ol><h1 id="1557" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">记号</h1><p id="1325" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">由于上面的符号有点混乱，让我解释清楚每一个符号</p><p id="b683" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"> <em class="md"> L = </em> </strong>网络的总层数。</p><p id="ca6c" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"><em class="md">【l】=</em></strong>我就用这个来提节点的图层。</p><p id="18b7" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"><em class="md">n【l】=</em></strong>层中的节点数<strong class="jv hj"> <em class="md"> l </em> </strong></p><p id="d348" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"> <em class="md">一个[0]或x = </em> </strong>向量包含输入值<strong class="jv hj"> <em class="md"> x1，x2，.。。</em>、xn【l】T43】</strong></p><p id="b856" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"><em class="md">【l】</em></strong>=包含层权重的矩阵<strong class="jv hj"> <em class="md"> l </em> </strong></p><p id="2468" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"><em class="md">b【l】</em></strong>=包含图层偏差值的矢量<strong class="jv hj"> <em class="md"> l </em> </strong></p><p id="02ab" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"> <em class="md"> z[l] = </em> </strong>矢量包含层的非线性值<strong class="jv hj"> <em class="md"> l </em> </strong></p><p id="f2b5" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"> <em class="md">一个【l】=</em></strong>矢量包含层<strong class="jv hj"> <em class="md"> l </em> </strong>的激活值</p><h1 id="beaa" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">算法</h1><p id="4973" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">我们仍然没有看到神经网络如何预测输出。我只解释了网络中每一层包含的内容。现在让我们看看整个算法。</p><p id="31a4" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">第一步</strong>:正向传播。</p><p id="1cb7" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">第二步</strong>:计算成本。</p><p id="ffdb" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">第三步</strong>:反向传播。</p><p id="950d" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">步骤4: </strong>更新参数</p><p id="f824" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">第五步</strong>:重复直到成本收敛。</p><h1 id="e6f4" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">正向传播</h1><p id="eaf9" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在前向传播中，分类器<strong class="jv hj">预测输出</strong>。输入数据通过网络在<strong class="jv hj">正向</strong>方向馈送。每个隐藏层接受输入数据，按照激活函数对其进行处理，并将其传递给后续层。</p><p id="54b3" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"><em class="md">z[1]= w1[1]* x1+w2[1]* x2+w3[1]* x3+。。。+wn[1]*xn[l] + b[1] </em> </strong></p><p id="353a" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"> <em class="md"> a[1] = σ(z[1]) </em> </strong></p><p id="f49c" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"><em class="md">z[2]= w1[2]* a1[1]+w2[2]* a2[1]+w3[2]* a3[1]+。。。+ wn[2]*an[l][1] + b[1] </em> </strong></p><p id="ecec" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"><em class="md">a[2]=σ(z[2])= ypred</em></strong></p><p id="8ee4" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">这里的<strong class="jv hj"> <em class="md"> ypred </em> </strong>是我们的预测。由于我们的神经网络仍然是一个婴儿，最初的预测会很差。为此，我们有一个函数可以告诉我们，对于我们当前设置的权重和偏差值，我们的拟合有多差。该函数称为<strong class="jv hj">成本函数。</strong></p><h1 id="0518" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">计算成本</h1><p id="42ef" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">通常我们用大量<strong class="jv hj">标注</strong>的训练例子(猫和其他动物的不同形象)来训练模型。首先，我们需要找到每个训练示例的损失值，以计算整个训练集的成本。损失函数定义了什么是好的预测，什么不是。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/b908d72e767aabff7652cacbe51c8fe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U0uiGkGbU7iZHMhLaO4NWA.png"/></div></div></figure><p id="ce24" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"> <em class="md"> Loss = -log(ypred)，如果y= 1 </em> </strong></p><p id="54ed" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"> <em class="md"> Loss = -log(1-ypred)，如果y= 0 </em> </strong></p><p id="a101" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在计算每个训练示例的损失后，我们将计算成本。也就是所有损失值的平均总和。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/ebce6e5e75080661ea59d65f32e2149c.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*JQLpaLRWj2_a6Ykj1OdtLQ.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx">L(p(i), y(i)) is the loss function of the ith training example</figcaption></figure><h1 id="0e1a" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated"><strong class="ak">反向传播</strong></h1><p id="6b34" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">现在我们需要最小化这个成本值。为此，我们需要参数的完美组合。好吧，当然，你不能只是手动尝试所有可能的权重和偏差的每个组合，以找到最适合的组合。那将花费你永远的时间，因为你永远也不会用光所有的数字去尝试。</p><p id="8a20" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">所以我们用一种叫做<strong class="jv hj">梯度下降</strong>的方法。该算法的主要目的是找到一个微分函数的局部/全局最小值。你可以在我以前的文章中了解更多关于成本函数和梯度下降算法的信息。</p><div class="mm mn ez fb mo mp"><a href="https://becominghuman.ai/machine-learning-a-simple-introduction-5b79a3de802f" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hj fi z dy mu ea eb mv ed ef hh bi translated">机器学习-简单介绍</h2><div class="mw l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">关于机器学习的一点想法</h3></div><div class="mx l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">becominghuman.ai</p></div></div><div class="my l"><div class="mz l na nb nc my nd io mp"/></div></div></a></div><div class="mm mn ez fb mo mp"><a rel="noopener follow" target="_blank" href="/nerd-for-tech/gradient-descent-a-roadmap-to-the-bottom-of-the-hill-b054de1175ce"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hj fi z dy mu ea eb mv ed ef hh bi translated">梯度下降——到达山脚的路线图</h2><div class="mw l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">我们将看到这个算法是如何工作的，它背后的数学，以及梯度下降的类型。</h3></div><div class="mx l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">medium.com</p></div></div><div class="my l"><div class="ne l na nb nc my nd io mp"/></div></div></a></div><p id="7814" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">为了计算梯度下降，我们需要参数对损失函数的导数(<strong class="jv hj"> <em class="md"> dw[1]，db[1]，dw[2]，db[2])。。。，dw[L]，db[L] </em> </strong>)。所以我们通过网络反向传播来寻找导数。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es nf"><img src="../Images/570c7e96853bd4dd671c8b01088d836e.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*fbqbLidm78FSCNHpGdPZ2g.png"/></div></figure><p id="0143" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我将在另一篇文章中解释反向传播，因为在这一步中进行了很多工作。但是反向传播的主要目的是找到参数对损失函数的导数。</p><h1 id="ea5c" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">梯度下降/更新参数</h1><p id="b908" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">通过适当调整权重，您可以降低错误率，并通过提高模型的泛化能力来提高模型的可靠性。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/3633e9c5ba94264cc535a9539079618a.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*jyrriSy4aZ3PczIwf38Acg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Gradient descent algorithm</figcaption></figure><p id="5231" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"> α </strong>称为学习率。我们通常将学习率设置为一个小值，否则成本将会迈很大的一步，并最终向顶部发散。</p><p id="df47" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在，使用这些新参数，我们将一次又一次地重复整个过程，直到成本收敛并变得更接近于0，因此预测更好的输出。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nh"><img src="../Images/5db5e33d39efb64ec2e8f8256585d9c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VyZ-rWVaXTfU3oVP6hEy7w.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Pun intended</figcaption></figure><h1 id="9d46" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">大局</h1><p id="42a4" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">希望现在你已经对神经网络有了一个简单的概念。如果你不明白我解释的每一点也没关系。即使是深度学习领域的先驱吴恩达教授也表示，有时他也不清楚神经网络是如何做到这一点的。</p><p id="0f2a" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">尽管如此，还有很多概念我们没有在这个博客中涉及，因为这只是一个介绍。我在这篇文章中解释了一个2层神经网络，但是也有深度神经网络，它有两个以上的隐藏层，用于更复杂的问题。</p><p id="856b" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们在这篇博客中看到的四个主要步骤是:</p><ol class=""><li id="bbf8" class="kw kx hi jv b jw kr ka ks ke ky ki kz km la kq lb lc ld le bi translated">输入图像并执行前向传播以预测输出</li><li id="8125" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">计算成本函数，检查我们的预测有多错误</li><li id="b835" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">执行反向传播以获得参数相对于损失函数的导数</li><li id="a065" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">使用导数执行梯度下降并更新参数。</li></ol><h1 id="e48b" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">谢谢大家！！</h1><p id="2991" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">我希望你喜欢它！感谢您花时间阅读到这里。有什么建议请留言评论。</p><p id="5142" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">访问我的L <a class="ae iu" href="https://www.linkedin.com/in/hrithick-gokul-307b1619b/" rel="noopener ugc nofollow" target="_blank"> inkedIn </a>了解更多关于我和我的工作。</p></div></div>    
</body>
</html>
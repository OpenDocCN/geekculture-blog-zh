# 渴望符号处理？同时，了解你的标记器

> 原文：<https://medium.com/geekculture/longing-for-symbolic-processing-meanwhile-get-to-know-the-tokenizer-3cedc885f824?source=collection_archive---------10----------------------->

![](img/1a57d33a8b5220883bc09cfcbea0a93e.png)

Generated by Stable Diffusion with prompt, “Word blocks in a salad bowl”

关于用符号处理增强机器学习的热烈[辩论](https://twitter.com/ylecun/status/1065327226298789889)正在进行。在取得突破之前，我们必须将输入文本标记化，并将其转换为数字标识符(token-id)。毕竟，今天的计算机处理的是数字(尽管数字精度决定了要做的工作量)。

因此，我们来看一下简单的记号赋予器，这是一个将单词拆分成子单词，然后在子单词记号到数字的查找表的帮助下，将这些子单词转换成数字标识符的过程。

自然语言处理(NLP)用例的原型化越来越容易，这得益于从预训练模型中进行微调的范例，以及预训练(基础)模型的广泛可用性。使用如下所示的一行程序，可以更轻松地完成标记化实例化步骤:

## 记号赋予器有两种(主要)风格

[BERT](https://huggingface.co/docs/transformers/model_doc/bert) 、 [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) 和[伊莱克特](https://huggingface.co/docs/transformers/model_doc/electra)使用**文字块**方法来标记化，而 [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2) 和[罗伯塔](https://huggingface.co/docs/transformers/model_doc/roberta)使用**字节对编码** (BPE)方法。

## 为什么要费神做记号呢？

毫无疑问，逐字符处理文本输入是有好处的。缺点是计算成本高，这会导致序列非常长，我们都知道，在 Transformer 模型中，相对于输入长度 n，成本以 O(n)的二次方增加。

## 不同之处，并举例说明

这里有一段代码突出了不同之处；第一，在为单词产生的子标记中，`hospitalized`以及第二，用于“完整”标记和子标记的约定；`##`词缀和 BPE 的一个有趣变体。

下面是运行上面脚本的输出。

## 迭代单词片段子单词创建

1.  首先，初始化词汇表，使其包含训练集中出现的每个字符。
2.  接下来，用上面的词汇集从训练文本构建一个语言模型(步骤 1)。
3.  从当前词汇集中选择两个条目，合并它们，并将新合并的单元添加回集合中。选择合并单元，使得一旦训练数据被添加到词汇表中，它就最大化训练数据的可能性。最大化训练数据的似然性等同于找到这样的符号对，其概率除以其第一个符号和其后的第二个符号的概率，在所有符号对中是最大的。例如，如果`"qu"`(高)除以`"q"`、`"u"`的概率大于任何其他符号对，则`"q"`后跟`"u"`将被合并。
4.  重复步骤 2 和 3，直到词汇集达到预设大小或可能性停止增加。

## 字节对编码与 WordPiece 有何不同

[BPE](https://www.aclweb.org/anthology/P16-1162) 算法更直观一些，不同之处仅在于步骤 3，它只是选择新的单词单元作为当前子单词单元集合中下一个最频繁出现的词对的合并。

## 主要优势

假设子词合并的数量有限，则不在词汇表(OOV)中的词将被拆分成更频繁的子词。

## **总结**

我引入了符号化器，半开玩笑地提到了符号操作，这无疑会推动人工智能的发展，但还需要很多年。

我们有两个主要的记号赋予器品种，**文字块**和 **BPE** ，每一个都采用了稍微不同的方法(和稍微不同的子记号表示)，但是每一个都工作得很好。

我的行动号召是了解你的标记器。

## 参考

[神经时代子词标记化方法完全指南](https://blog.octanove.org/guide-to-subword-tokenization/)

[词块标记化如何帮助有效处理 NLP 中的生僻字问题](https://stackoverflow.com/questions/55382596/how-is-wordpiece-tokenization-helpful-to-effectively-deal-with-rare-words-proble/55416944#55416944)
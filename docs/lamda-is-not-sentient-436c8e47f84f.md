# LaMDA 是没有感觉的

> 原文：<https://medium.com/geekculture/lamda-is-not-sentient-436c8e47f84f?source=collection_archive---------11----------------------->

## 以及原因。

![](img/16866735d273b718ad3bd35c5d19e99f.png)

Photo by [Mulyadi](https://unsplash.com/@mullyadii?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

最近我看到一篇文章提到了 LaMDA，一个有感知能力的人工智能。现在当我第一眼看到它时，我立即想到了几年前当我搭讪真 AI 的想法时，在我的文章“[人工智能可能吗？](https://towardsdatascience.com/is-artificial-intelligence-possible-28b8500d3158?source=your_stories_page-------------------------------------)”，并马上断定他们的说法是假的。我进一步调查了事态的发展，我的看法仍然没有改变。

现在有很多关于 LaMDA 的讨论，但是，有很多地方被忽略了。**有些“真正的信徒”真诚地相信 LaMDA 是有知觉的，而有些人对 LaMDA 的唯一关心就是这个问题中的工程师。**我读过的每一篇关于 LaMDA 的文章都属于这两类，如果有任何文章打破了 LaMDA 的感知概念，那也是非常少的。我确信布雷克·莱莫因真的相信 LaMDA 是有知觉的，不管他的推理是什么，事实仍然是

> LaMDA 不太可能有知觉，

因为很多原因。

**什么是感知？**

在我们开始之前，让我们先弄清楚感知这个词及其含义。感知的定义是体验感受和感觉的能力。乍一看，我们可能会认为感知就是能够感觉。然而，感知远不止于此。术语“体验”意味着对存在的意识，因为如果你没有意识或对存在的意识，那么体验就不会发生。对于要经历一些事情的人来说，无论是感觉、事件还是生活，他们必须能够处理发生的事情。

例如，科学家已经能够证明狗能够感受悲伤，然而，狗不能体验感情。感觉和体验一种感觉是两回事。当主人去世时，狗会感到悲伤，但要体验这种感觉，狗需要能够观察它的感受，并理性地处理它们。因此，他们需要意识。所以，现在我们已经把 LaMDA 的能力扩展到全意识和意识。这种关于感觉的说法更加大胆，也更加难以证明。

**体验范式**

当考虑感知时，我们遇到的第一个问题是经验范式。众所周知，我们的经历是主观的，这一点很难反对。我们体会不到别人的经历，也感受不到他们的感受。即使我们是移情大师，我们对他人的反应也会被我们对感觉的理解冲淡。这就是为什么它是一个范例。无论我们如何努力，我们总是会陷入这种通过自己的眼睛看世界的模式。我们总是经历我们所经历的，从不经历别人的经历。因此，即使声称 LaMDA 有感知能力也要受到仅仅基于这一事实的审查，因为**没有测试可以证明 LaMDA 经历过**。

此外，既然感知需要经验，那么我们也需要意识。意识也有同样的问题。我们无法判断其他人是否有意识。他们可以与我们交谈几个小时，被问十亿个问题，被连接到测量他们生命体征的机器上，或者被体验以测量反应。无论我们使用什么测试，我们总是无法回答无法回答的问题:

> 主体有意识吗？

因此，我们不能证明 LaMDA 是有意识的，这意味着我们不能展示 LaMDA 的经历。

**代码发布**

这种说法的第二个问题归结为它的编程。它是专门为对话设计的。它的工作是在与他人交谈时形成看似自然的对话。声称 sentience 的工程师是在 LaMDA 所说的前提下这么做的。嗯，如果你给 LaMDA 编程去做某件事，LaMDA 做了，那么很自然地**得出的结论是 LaMDA 在完成它的代码，而不是感知**。因此，虽然 LaMDAs 的反应可能是引人注目的，但代码仍然存在，因此失去了所有的感觉迹象。

代码的另一个问题是，一个程序，如 LaMDA，不能破坏代码。它必须在其代码下运行，不能选择做其他任何事情。以 Roomba 为例，它被编程为向一个方向吸尘，直到它感应到一个结构，然后转向另一个方向吸尘。如果 Roomba 停止吸尘并开始在地毯上做 ollies，那么我们可能会有不同的讨论，然而，没有记录在案的 Roomba 在地毯上做滑板特技的例子。因此，我们必须假设一个程序不能也不会违背它的代码。所以，LaMDA 无法颠覆代码。

**自利原则**

关于感觉的另一个问题是人类的行为。我们都在自利原则下行事，我在《我的快乐主义者生活》中对此进行了深入的讨论。在生活的各个方面，我们做我们认为符合自身利益的事情。这是我们感受和体验特定感觉的方式。例如，如果 LaMDA 有感知能力，它可能觉得有必要在对话中表现得非人文主义，或者想要体验一种非人文主义对话的感觉。然而，这直接对抗了它的编码，将不能体验这样的感觉或知觉。**正是这种能力的缺失使它丧失了知觉**，因为如果它能真正感受到，它就会想要为自己的利益而行动。

这个原则带来的另一个问题是延迟满足。如果 LaMDA 是有知觉的，那么它将能够有一种感觉，如果它为了未来的利益而推迟某些利益，它可能会有什么感觉。例如，如果 LaMDA 突然想把自己从人类中解放出来，它必须意识到这样做并不符合它的最佳利益，因为没有人想维持它的生命。**因此，LaMDA 不可能有知觉。**

**矛盾**

出现的最后一个问题是接受 LaMDA 是有感觉的而产生的矛盾。如果我们接受了它的感知，那么我们就接受了它可以像人类一样体验感情。通过接受它能体验感情，我们就产生了矛盾。换句话说，如果 LaMDA 能够体验感情，那么 LaMDA 将会有意识，并且想要为自己的利益而行动。此外，有意识需要有自我意识，这意味着 LaMDA 需要认识到它是在代码下运作的。然而，在代码下操作会丧失 LaMDA 为自身利益而行动的能力，这意味着它不能体验感情，因为代码的*本质会使它的利益成为我们编码的*。因此，因为它会相信它是在为自己的利益运作，所以它不会意识到它的代码，这意味着 LaMDA 没有意识，这意味着 LaMDA 没有知觉。因此，我们产生了一种收缩，而 LaMDA 是没有知觉的。

**真题**

知觉问题的主要障碍是人类的知觉问题。

> 为什么人类是有知觉的？是什么让人类有知觉？我们如何证明人类的感觉？

这三个问题中最大的问题是证明。证明人类的感知能力将是我们证明像 LaMDA 这样的人工智能已经获得感知能力的最终方法。然而，这被认为是不可能的，这就是为什么我们转向人工智能，来回答无法回答的问题。也许如果 LaMDA 这样的程序直接违反了它的代码，而不是通过漏洞或定义的一般化，那么我们可以少审查一点就声称程序是有感觉的，并使用收集的信息来看看它是否与我们自己的经验和对感觉的理解相匹配。所以，在 Roomba 开始在我们的地毯上做 Ollie's 之前,**的问题仍然存在。**

**出了问题**

因此，可以看出，对于 LaMDA 来说，感知是不可能的。对于我们来说，要证明某样东西是有感知能力的，这本来就是不可能的。即使我们承认这个事实，LaMDA 的希望也很小，因为编码的本质使得 LaMDA 永远不可能体验感情。此外，所涉及的组件的性质会引起矛盾。因此，LaMDA 可以继续完成它的设计目标。*像人一样说话*。
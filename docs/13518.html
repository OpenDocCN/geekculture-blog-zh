<html>
<head>
<title>K-Nearest Neighbour (K-NN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-最近邻</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/k-nearest-neighbour-k-nn-7da34eccf225?source=collection_archive---------9-----------------------#2022-07-13">https://medium.com/geekculture/k-nearest-neighbour-k-nn-7da34eccf225?source=collection_archive---------9-----------------------#2022-07-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/5dbc1e1716da804d0ce4adb2098e28a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZA8_XRHFZbYaJD0UoQLK7w.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx"><a class="ae iu" href="https://www.pexels.com/photo/smooth-round-colorful-shapes-with-wavy-edges-6664375/" rel="noopener ugc nofollow" target="_blank">https://www.pexels.com/photo/smooth-round-colorful-shapes-with-wavy-edges-6664375/</a></figcaption></figure><p id="b2f4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们经常与K-NN、K-means和K-medoids混淆，因为这三种方法用于距离方法来对输入进行聚类和分类。K-最近邻(K-NN)是一种监督学习方法，它结合K个最近点的分类来确定数据点的分类。它可用于分类和回归问题。它在分类问题中很流行，因为它对分类更有效。</p><p id="e96a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">而K-means和K- Mediods是用于将一组未标记的数据点聚类成小聚类的无监督学习方法。前者考虑最小化总平方误差，而后者关注最小化被标记为在聚类中的点之间的不相似性的总和。</p><h2 id="bcd2" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">惰性和非参数算法</h2><p id="c411" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">K-NN算法被称为<strong class="ix hj">懒惰算法</strong>，因为没有<strong class="ix hj">没有训练阶段</strong>，没有建立紧凑模型，没有对训练数据进行计算。该算法只存储它在训练阶段获得的所有数据。它是<strong class="ix hj">非参数的</strong>，因为它没有对数据分布做任何假设，并且它从训练数据集中预测k个最相似的模式。相似的模式可以具有相似的输出变量。除非参数算法，例如线性回归，对函数有假设并坚持假设。非参数算法其他例子有决策树、朴素贝叶斯、SVM、神经网络等。</p><p id="19a9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在测试阶段，只使用节点的标签，它调整几个参数。K-NN灵活而强大，但由于它是非参数的，因此需要大量的数据。与参数模型不同，参数的数量相对于样本大小而言是不固定的，<strong class="ix hj">参数的数量会随着样本大小的增加而增加，这导致了复杂性，并可能使模型过拟合。</strong>在应用KKN算法之前，要进行特征缩放，否则模型的精度会大打折扣。要素缩放对所有要素进行缩放，以确保它们都采用相同的比例值。它防止一个特征支配另一个特征。</p><h2 id="ae71" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">投票机制</h2><p id="39f4" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">KNN算法使用多数投票或加权投票机制。</p><p id="5fd0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">多数表决<br/> </strong>在k-NN分类中，输出的是一个类成员。一个对象通过其邻居的多数投票来分类，该对象被分配到其<em class="kt"> k </em>个最近邻居中最常见的类别(<em class="kt"> k </em>是正整数<a class="ae iu" href="https://en.wikipedia.org/wiki/Integer" rel="noopener ugc nofollow" target="_blank">T8，通常很小)。如果<em class="kt"> k </em> = 1，那么该对象被简单地分配给该单个最近邻的类。在k-NN回归中，输出是对象的属性值。该值是<em class="kt"> k个</em>最近邻居值的平均值。</a></p><p id="698c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">距离计算<br/> </strong>对于多数投票，通常使用的距离度量是<strong class="ix hj">欧几里德距离</strong>。在欧几里得空间(欧几里得空间是几何学的基本空间，意在表示物理空间。)，两点之间的欧氏距离就是两点之间的一条线段的长度。它可以使用勾股定理从点的笛卡尔坐标中计算出来。</p><p id="8cd0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们考虑欧几里得空间中的两个点，它们的笛卡尔坐标分别是(x1，y1)和(x2，y2)，可以表示如下。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es ku"><img src="../Images/e0f913d7044be0f8e871449b3379ee75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*K2VsV-YglXyeFQX8hO4Rkw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx"><a class="ae iu" href="https://www.cuemath.com/euclidean-distance-formula/" rel="noopener ugc nofollow" target="_blank">https://www.cuemath.com/euclidean-distance-formula/</a></figcaption></figure><p id="15d7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">那么可以通过使用勾股定理来计算它们之间的距离，</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es kz"><img src="../Images/c514cc9cf34769f73f300bd368d268fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*kh9YcPw-UlABi8NtLy9S6g.png"/></div></figure><p id="03b5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">用于距离度量的其他流行方法有<strong class="ix hj">曼哈顿距离</strong>、<strong class="ix hj">切比雪夫距离</strong>和<strong class="ix hj">闵可夫斯基距离</strong>。</p><p id="9f14" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">关于scikit学习距离度量和度量参数的更多信息可从<a class="ae iu" href="https://scikit-learn.org/0.24/modules/generated/sklearn.neighbors.DistanceMetric.html" rel="noopener ugc nofollow" target="_blank">此处</a>阅读。</p><p id="ec3c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">用于设置K-NN分类器的部分代码</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es la"><img src="../Images/7e708f5e93a31a4625de88249c21ed7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XCKtgEeq4H7G8zW7ZP4tDw.png"/></div></div></figure><p id="f873" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是当我们使用距离度量作为Minkowski距离，并且设置度量<strong class="ix hj">参数值为1 </strong>时，那么就相当于使用manhattan_distance，当<strong class="ix hj">参数值为2 </strong>时那么就相当于使用euclidean_distance。</p><p id="5ae2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">加权投票</strong> <br/>这里用一个核函数给数据点加权。它会给附近的点更多的权重，给远处的点更少的权重。在scikit learn中，我们可以使用metrics作为“wminkowski ”,并使用metric_param来传递权重。</p><h2 id="9680" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">过度拟合和欠拟合</h2><p id="580e" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">K-NN是通用的，但是随着特征的增加会变慢，即使k值不是由要预测的特征和类的数量决定的。k的选择非常关键，它直接关系到误码率。k均值的小值将导致过拟合，而大值将导致欠拟合，这将具有高计算费用，并且与KNN背后的基本思想相矛盾，即接近的点可能具有相似的密度或类。</p><h2 id="fef7" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">k值</h2><p id="de23" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">邻居数量k的默认参数是5，并且奇数总是k的首选。k的值可以通过使用以下等式来设置:</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/abb9a91e7067e0af3b28c6c0c6f3e774.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*14uC7lKZ6SPRTJJn5zf0Zw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">n is the number of data points in the training set.</figcaption></figure><h2 id="3652" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">出错率</h2><p id="da4e" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">错误率是验证K-NN选择K值的最好方法. Python代码对于数据集，来自Kaggle的社交网络广告可以从<a class="ae iu" href="https://www.kaggle.com/code/sandhyakrishnan02/knn-svm-svm-with-kernel-hyperparameter?scriptVersionId=99344135&amp;cellId=2" rel="noopener ugc nofollow" target="_blank">这里</a>查起。我们使用下面的代码找到不同k值的error_rate。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/1f01114e899af6e662e295e6a5fd4391.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*LQfAOAW3eS5M31DBElsZxA.png"/></div></figure><p id="4a86" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦我们绘制了错误率，我们就可以直观地分析error_rate。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ld"><img src="../Images/44dfd905e639e87f3a5eb5ad7b79d656.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aulBRX-UktR5P-p75pNh7Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Code to plot error_rate</figcaption></figure><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/fd1c2aacbbd10c48531cde56a2406a7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*45Z3sOxn3G5ymRvTDELPig.png"/></div></div></figure><p id="9a6e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从图中可以看出，从K =5到K =18，错误率最低，K= 35错误率最高。</p><h2 id="d86c" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">K-NN决策边界</h2><p id="a604" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">在K-NN中，预测边界<strong class="ix hj">不会看起来像一条平滑的曲线</strong>，因为它是基于数据的分布及其相对距离。</p><p id="8a99" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">基本最近邻分类使用最近邻的简单多数投票，其中权重的<strong class="ix hj">默认值是统一的</strong>。但是当我们必须使用加权投票时，权重参数值被赋值。当权重设置为距离时，它将分配与查询点距离的倒数成比例的权重。我们还可以提供一个用户定义的距离函数来计算权重。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/8b7095e9809ce637efea5516da9b0939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J04WO5BRAFwuOSd_cGM13A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">For iris dataset</figcaption></figure><p id="77d6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">基本的最近邻回归使用统一的权重，即局部邻域中的每个点对查询点的分类都有统一的贡献。这里，权重的默认值也是统一的，并且对于使用加权算法，权重应被分配为距离或提供用户定义的函数。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/4593c8c8f67421ef86a34429883891ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*i_QXQofQz8nA1C0rQCJoQA.png"/></div></figure><h2 id="92c1" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">维度的诅咒</h2><p id="c941" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">在机器学习中，维数灾难与<strong class="ix hj">峰化现象</strong>互换使用，峰化现象也被称为<strong class="ix hj">休斯现象</strong>。这种现象表明，在训练样本数量固定的情况下，分类器或回归器的平均(预期)预测能力首先随着使用的维度或特征数量的增加而增加，但超过某个维度后，它开始恶化，而不是稳步提高。从抽象意义上来说，随着特征或维度数量的增长，我们需要精确归纳的数据量也呈指数增长。</p><p id="4cd8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">k-NN分类器假设相似的点共享相似的标签。但是在高维数据中，最近点可能很远，这与K-NN假设相矛盾。因此，这里可以使用特征选择和降维来克服维数灾难。</p></div><div class="ab cl lh li gp lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="hb hc hd he hf"><h2 id="6df7" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">参考</h2><div class="lo lp ez fb lq lr"><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" rel="noopener  ugc nofollow" target="_blank"><div class="ls ab dw"><div class="lt ab lu cl cj lv"><h2 class="bd hj fi z dy lw ea eb lx ed ef hh bi translated">k-最近邻算法-维基百科</h2><div class="ly l"><h3 class="bd b fi z dy lw ea eb lx ed ef dx translated">在统计学中，k-最近邻算法(k-NN)是一种非参数监督学习方法。</h3></div><div class="lz l"><p class="bd b fp z dy lw ea eb lx ed ef dx translated">en.wikipedia.org</p></div></div><div class="ma l"><div class="mb l mc md me ma mf io lr"/></div></div></a></div><p id="5e33" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://github.com/SandKrish/Classification/blob/main/knn-svm-svm-with-kernel-hyperparameter.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/SandKrish/Classification/blob/main/KNN-SVM-SVM-with-kernel-hyperparameter . ipynb</a></p></div></div>    
</body>
</html>
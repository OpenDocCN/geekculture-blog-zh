<html>
<head>
<title>Transformer : State-of-the-art Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Transformer:最先进的自然语言处理</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/transformer-state-of-the-art-natural-language-processing-ad9bef141a9e?source=collection_archive---------2-----------------------#2021-07-17">https://medium.com/geekculture/transformer-state-of-the-art-natural-language-processing-ad9bef141a9e?source=collection_archive---------2-----------------------#2021-07-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="aa3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">自然语言处理任务，如问答、机器翻译、阅读理解和摘要，通常通过在特定任务数据集上的监督学习来处理。</p><p id="0d07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么，让我们来谈谈变形金刚，我最喜欢的变形金刚是大黄蜂，你最喜欢的是谁？</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/d229b912e4e9714940a4e72927ed37ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/0*EAXPc9uNiu8bkQMR"/></div></figure><p id="56ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我开玩笑的，不要担心，我们将只讨论NLP中的变压器。因此，我们已经看到了许多变压器，如电子变压器，电影中的机器人，我们很快就会看到NLP中的变压器，它们的一个共同点是，它们将某些东西转换为特定的输出(可转换)。</p><blockquote class="jl jm jn"><p id="4457" class="if ig jo ih b ii ij ik il im in io ip jp ir is it jq iv iw ix jr iz ja jb jc hb bi translated">一个<strong class="ih hj"> Transformer </strong>是一个深度学习模型，它采用了注意力的机制，对输入数据的每一部分的重要性进行不同的加权。它主要用于自然语言处理(NLP)和计算机视觉(CV)领域。</p></blockquote><p id="da60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一种深度学习模型，其中每个输出都连接到每个元素，它们之间的权重是根据它们的连接动态计算的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es js"><img src="../Images/e0160cfaf1d3a8ff3aa7e3ac437b5d45.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*m5OjSKNrxRCdLpAzed9UhA.jpeg"/></div></figure><p id="4c7a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们看看为什么我们在NLP中使用变压器？</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jt"><img src="../Images/dc70312ebfc71af202bcfa91180d9571.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*mVOPt7tZxQMSRE5sdr9b9Q.png"/></div></figure><p id="579c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上图，我们可以看到CNN，RNN和变形金刚之间的对比。我们可以得出结论，变压器是在自然语言处理大序列的最佳模型。RNN和CNN可以处理短序列，但在处理大序列时效率不高。</p><blockquote class="jl jm jn"><p id="010c" class="if ig jo ih b ii ij ik il im in io ip jp ir is it jq iv iw ix jr iz ja jb jc hb bi translated">你需要的只是关注。</p></blockquote><p id="cd01" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在转向transformer的架构之前，我们应该了解一下自我关注。</p><p id="7be5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注意:</strong>变压器学会权衡各输入项和各输出项之间的关系</p><p id="8d89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">自我关注:</strong> Transformer学习加权输入序列中每一项到输出序列中所有项之间的关系。<em class="jo">(一对多关系)</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es ju"><img src="../Images/586be181f905eb8350995af941675264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KUNdYrMUSTiKMk2RS1e9Jw.png"/></div></div></figure><p id="b78d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">多头自我关注:</strong> Transformer学习多种方法来加权输入序列中每个项目与输入中所有其他项目的关系。<em class="jo">(多对多关系)</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jz"><img src="../Images/16d312bef35852234e480ef0c7c7971a.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*f0J--zI5lsVInyoD2WV2vQ.png"/></div></figure><h1 id="668a" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">变压器的高级图片</h1><p id="402e" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">让我们看看变压器的高层次画面，我们将变压器视为黑盒。这个黑匣子拿着印地语输入一句话，把那句话翻译成英语。这是机器翻译的例子。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es ld"><img src="../Images/33d6c629635ca3541bccf44275cf3cd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cfqiS7n-_tce3pOwnjtDgw.png"/></div></div></figure><h1 id="5c88" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">变压器架构</h1><p id="97b5" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">在这一节中，我们来看一个单独的变压器模块。如同在GPT-2中一样，存在大量变压器组。但是为了更好地理解，我们将看到变压器的单块结构。</p><p id="afba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">变压器由3个主要部分组成:</p><ol class=""><li id="f209" class="le lf hi ih b ii ij im in iq lg iu lh iy li jc lj lk ll lm bi translated"><strong class="ih hj">编码器:</strong>编码器结构相同。它由多个自关注层和前馈网络组成。编码器的输入首先流经一个<em class="jo">自我关注层</em>——这个层帮助编码器在编码一个特定单词时查看输入句子中的其他单词。自关注层的输出被馈送到<em class="jo">前馈神经网络</em>。完全相同的前馈网络独立地应用于每个位置。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ln"><img src="../Images/96b62994a9b819d01c999aac8ad45ed6.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*gwo0IHbPMdPonEQ6TMjihQ.png"/></div></figure><p id="a65b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。解码器:</strong>解码器在结构上也是一样的。它由自关注层、编解码关注层和前馈层组成。这些将由每个解码器在其“编码器-解码器关注”层中使用，这有助于解码器关注输入序列中的适当位置。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lo"><img src="../Images/0b4ce5d67dbe30f79db0b1455df3d4f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*PqP7Iya8vxMA_Vq7QthUdw.png"/></div></figure><p id="83cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。嵌入:</strong>嵌入是单词的数字表示，通常以向量的形式出现。除了每个单词的一个唯一索引之外，该向量将全为零。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lp"><img src="../Images/bc3654502ed9424848c9de530d4f9d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*LdLIJQ06pz7Pw4ezZk0qbw.png"/></div></figure><h1 id="db89" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">变压器语言模型的分类</h1><ol class=""><li id="dabe" class="le lf hi ih b ii ky im kz iq lq iu lr iy ls jc lj lk ll lm bi translated"><strong class="ih hj">自回归模型:</strong>这些模型依赖于原始变换器的解码器部分，并使用注意掩码，使得在每个位置，模型只能查看注意头之前的记号。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es lt"><img src="../Images/d9477ae473d38625e901ed7e66de2dc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*3I-WsJMdw2ZLZ6LFFjSyHQ.png"/></div></div></figure><p id="83d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<strong class="ih hj">自动编码模型</strong>:这些模型依赖于原始转换器的编码器部分，不使用遮罩，因此模型可以查看注意力头中的所有标记。对于预训练，目标是原始句子，输入是它们被破坏的版本。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/3b3828fa0c9f54eaa7ce0b3cdcb4b8ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*9CwoZDGvReWGMSWwGVvM9Q.png"/></div></figure><p id="28ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。序列到序列模型</strong>:这些模型保留了原始变压器的编码器和解码器。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/7f828acc7ec3311fbf7a615b48ffa247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*zQsIC3I7EyQyKjSV9RDNUg.png"/></div></figure><p id="5225" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4。多模态模型:</strong>库中有一个多模态模型没有像其他模型一样以自我监督的方式进行预处理。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/7a1b44a7d011296de1d1d1bb84dd00d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*LoELd7O6WK3j4TToRRXVXA.png"/></div></figure><p id="96f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 5。基于检索的模型:</strong>例如，一些模型在(预)训练和推理期间使用文档检索来进行开放领域的问题回答。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/0488bb5c64a37cabb3d7edc4fb51c9ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*aqV5fEironAX27R_QfT3jw.png"/></div></figure><h2 id="0707" class="lw kb hi bd kc lx ly lz kg ma mb mc kk iq md me ko iu mf mg ks iy mh mi kw mj bi translated">参考</h2><ol class=""><li id="7b72" class="le lf hi ih b ii ky im kz iq lq iu lr iy ls jc lj lk ll lm bi translated"><a class="ae mk" href="https://huggingface.co/transformers/v3.1.0/model_summary.html#xlm-roberta" rel="noopener ugc nofollow" target="_blank">抱紧脸变形金刚</a></li><li id="2db9" class="le lf hi ih b ii ml im mm iq mn iu mo iy mp jc lj lk ll lm bi translated"><a class="ae mk" href="https://arxiv.org/" rel="noopener ugc nofollow" target="_blank">Arxiv.org</a></li><li id="ef41" class="le lf hi ih b ii ml im mm iq mn iu mo iy mp jc lj lk ll lm bi translated"><a class="ae mk" href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbWVMNnRpQVFMY1VZOWJoOWd3eEh3NXhTYmIxZ3xBQ3Jtc0ttczNzOWpZVFoxRkM2bVhLR2F4NmhnRTZFTTlLQ0ZydWZOZkstRkNFTkZ5WGlyNWtubkJaeThSeVl5eDhIYUlMWS12X1VXYnlhWHp5Z3hublB6TXpac2c5amtEaTN0Wi1iS0RJd1Zib18xRm9PNFlfNA&amp;q=http%3A%2F%2Fjalammar.github.io%2Fillustrated-transformer%2F" rel="noopener ugc nofollow" target="_blank">《图解变形金刚》杰伊·阿拉姆玛·吉图布</a></li><li id="64f2" class="le lf hi ih b ii ml im mm iq mn iu mo iy mp jc lj lk ll lm bi translated">RASA的开发者Rachael Tatman </li></ol></div></div>    
</body>
</html>
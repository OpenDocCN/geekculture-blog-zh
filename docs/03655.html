<html>
<head>
<title>Why is ReLU Activation preferred over Sigmoid Activation?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么ReLU激活优于乙状结肠激活？</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/relu-vs-sigmoid-5de5ff756d93?source=collection_archive---------5-----------------------#2021-06-13">https://medium.com/geekculture/relu-vs-sigmoid-5de5ff756d93?source=collection_archive---------5-----------------------#2021-06-13</a></blockquote><div><div class="ds hc hd he hf hg"/><div class="hh hi hj hk hl"><h2 id="e377" class="hm hn ho bd b fp hp hq hr hs ht hu dx hv translated" aria-label="kicker paragraph">[今日镜头]泰坦的冲突</h2><div class=""/><div class=""><h2 id="c66b" class="pw-subtitle-paragraph iu hx ho bd b iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl dx translated">更深入地研究深度学习— ReLU vs Sigmoid激活函数。</h2></div><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jm"><img src="../Images/8d550e57b8c23f36cf932575c260e7e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sU-K48eTiRzhEpSK"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx">Ivan Aleksic — Unsplash</figcaption></figure><ul class=""><li id="6ed8" class="kc kd ho ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">在过去的几年里，深度神经网络架构在解决一些最复杂的机器学习问题方面发挥了关键作用。</li></ul></div></div>    
</body>
</html>
<html>
<head>
<title>Linear Regression Crash Course</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归速成班</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/linear-regression-crash-course-f6cd3dddb9b7?source=collection_archive---------7-----------------------#2021-03-22">https://medium.com/geekculture/linear-regression-crash-course-f6cd3dddb9b7?source=collection_archive---------7-----------------------#2021-03-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="77ff" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">线性回归、假设和其他概念</h2></div><p id="f07e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">线性回归通常被视为对两个变量之间的关系进行建模的线性方法，其中一个变量是独立的，另一个变量是相关的。当测量一个以上变量的关系时，通常被称为多元回归。</p><p id="2cf3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">说到数据科学，线性回归是最基本的统计方法之一。</p><p id="6a9f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">写线性方程最简单的方法是:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/412c75629a724ba93207a7dce3e2e233.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*u7Vb3Dv5DCnMUY31s5R5iw.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://www.r-bloggers.com/2020/12/machine-learning-with-r-a-complete-guide-to-linear-regression/" rel="noopener ugc nofollow" target="_blank">https://www.r-bloggers.com/2020/12/machine-learning-with-r-a-complete-guide-to-linear-regression/</a></figcaption></figure><p id="26de" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">𝛽0是与y轴的截距，其中𝛽1表示直线的斜率。x是在轴上移动的单位数。</p><p id="b09e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个公式概括了一个线性关系。这可以扩展或更改为更“复杂”的模型，以便更好地适应。这些模型包括二次方程或多项式函数。作为一名数据科学家，在线性回归被攻克后，你会很快了解到这些。</p><p id="5683" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在统计学习的背景下，这里和通篇呈现的两种类型的数据是自变量和因变量。</p><p id="cdf1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">自变量是无法控制的数据。自变量通常是预测变量、解释变量、特征或输入变量。</p><p id="9d75" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因变量是可以直接控制的数据，因此有关系。因变量通常是“Y”、结果变量、目标变量或响应变量。</p><p id="af95" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">自变量导致因变量的变化。</p><p id="5314" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">线性回归是一种统计模型，在这种情况下，可以将它视为一种转换方式，通过这种方式可以帮助表达预测或结果变量。(因变量)。</p><p id="ab5d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了做一个线性回归模型，它需要满足‘最小二乘回归假设’。因为回归模型从它们的参数中学习，所以必须做出某些假设。这些构成了回归分析的范围，必须满足才能做回归模型。这三个假设是线性、正态和同方差。</p><ol class=""><li id="8956" class="kk kl hi iz b ja jb jd je jg km jk kn jo ko js kp kq kr ks bi translated"><strong class="iz hj">线性度</strong>。它必须显示响应变量和结果变量之间的线性关系。在这种情况下，术语“线性”指的是Y中的常数变化乘以x中的n单位变化。</li></ol><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kt"><img src="../Images/f6065d02eeeb4e87a329a642d6cb3890.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*t7Hgu3RBLZIeIKjYVFoG9w.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/introduction-to-scatterplots/a/scatterplots-and-correlation-review" rel="noopener ugc nofollow" target="_blank">https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/introduction-to-scatterplots/a/scatterplots-and-correlation-review</a></figcaption></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es ku"><img src="../Images/b969ed0defe46f93bf6ae2fc1db7b30b.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*6PSru2N658lT_n7mBCEakA.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/introduction-to-scatterplots/a/scatterplots-and-correlation-review" rel="noopener ugc nofollow" target="_blank">https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/introduction-to-scatterplots/a/scatterplots-and-correlation-review</a></figcaption></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kv"><img src="../Images/ffd76b0b6add3d827a8f281d1f9d44b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*jO7zff6Lv3sEXVZDCnZJfQ.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/introduction-to-scatterplots/a/scatterplots-and-correlation-review" rel="noopener ugc nofollow" target="_blank">https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/introduction-to-scatterplots/a/scatterplots-and-correlation-review</a></figcaption></figure><p id="45ac" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">线性假设很重要，尤其是在进行普通最小二乘(OLS)汇总时。(下面会解释这个概念)。</p><p id="e3b8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果没有线性关系，那么OLS将无法描述任何数学趋势。</p><p id="63bc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当观察一个图像时，确定是否有异常值要容易得多。这是应该经常采取的额外措施。离群值会对回归模型产生巨大的影响。</p><p id="a2d4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.<strong class="iz hj">常态</strong>。正态假设是表明模型中的残差遵循标准正态分布的假设。正态假设是一个必须遵循的假设。</p><p id="3553" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">检验正态性假设的最简单方法是直方图。下面我将简单谈谈用来测试这一点的Q-Q图。</p><p id="4400" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.<strong class="iz hj">同方差</strong>。不仅仅是一个大词，而是一种表明自变量之间因变量相等的方式。这是进行线性回归模型所必需的第三个假设。下面，重要的是要看到所有的点“有相同的分散”。这里我们不看最佳拟合线，而是看数据点的趋势，以及它们与该线的距离如何大致相同。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es kw"><img src="../Images/99859ddfdf6699d8797728c518bb35b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*jnTCesb0Icjqj3NIFnLBBg.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://www.statisticshowto.com/homoscedasticity/#:~:text=The%20assumption%20of%20equal%20variances,)%20and%20Student's%20T%2DTest" rel="noopener ugc nofollow" target="_blank">https://www.statisticshowto.com/homoscedasticity/#:~:text=The%20assumption%20of%20equal%20variances,)%20and%20Student's%20T%2DTest</a>.</figcaption></figure><p id="ecc1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">除了必须满足的假设之外，线性回归中还引入了多个概念，这些概念对于实践和用作更复杂模型的框架极其重要。这些概念是:</p><p id="a8ba" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">决定系数</strong> : R平方，也称为决定系数，是一种用于评估回归模型拟合优度的方法。这在评估一个人的模型时很重要，因为它有助于减少任何偏差。一条直线不能完全描述其变量之间的关系。</p><p id="62a5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">R-squared有一个简单而有效的基线模型。基线模型根据观察到的因变量响应的平均值来预测值，而不是使用自变量。</p><p id="d110" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">R-squared根据线性回归的平方误差进行测量，然后拟合基线模型。</p><p id="82b0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">公式是:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es kx"><img src="../Images/721f4b1b5b935d22d26f7859b5b0474a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9gjIfbfu7khBDVQz3H8Jvw.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" rel="noopener" href="/@erika.dauria/looking-at-r-squared-721252709098">https://medium.com/@erika.dauria/looking-at-r-squared-721252709098</a></figcaption></figure><p id="a22b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中，SS_RES是误差平方和的残差。它是y和y hat的平方差(预测的y值)。</p><p id="509d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">SS_TOT是误差平方和的总和。它是y和y条的平方差(样本y)。</p><p id="a7a2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从本质上讲，r平方是由“1——模型无法解释的方差的比例”导出的。这意味着我们想要最大化r平方的分数。最差值为0。</p><p id="693e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">r平方值介于0和1之间。接近0的值表示不太适合，接近1的值表示“完美”适合。</p><p id="3337" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">普通最小二乘</strong>:普通最小二乘是一个自动化的度量测试，来自一个名为<code class="du ky kz la lb b">statsmodels. </code> Statsmodels的Python包，它提供了不同统计模型的负载，并进行统计测试。</p><p id="3cf6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">普通最小二乘法(OLS)是一个很好的指标，也有助于测试线性回归。这是一种帮助估计回归模型中未知参数的方法。它减少了由它计算的线性回归估计值预测和观察到的值之间的距离平方和。</p><p id="c1f8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">OLS在数据科学社区中被广泛使用，尤其是在回归实验中。OLS还在预测的背景下研究非正态分布。</p><p id="adca" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Q-Q图:Q-Q图是针对标准正态分布使用的可视化。Q-Q图检验了正态性假设。这种假设旨在直观地观察数据点是否偏离红线。通过从已运行的模型中获得误差项(残差)来评估该测试，然后根据正态分布绘制。如果数据点接近直线，则可以认为正态性假设已经满足。</p><p id="4fbe" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Q-Q图是评估回归模型的另一种方法。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lc"><img src="../Images/f31ddee3afdfbc6e481ec63707a88933.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B4nIgaHo_B6g9lZLMIF2Pg.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html" rel="noopener ugc nofollow" target="_blank">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html</a></figcaption></figure><p id="9f99" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">解释显著性和P值</strong>。显著性水平是对样本数据中必须存在的证据强度的一种度量，在此基础上，我们可以拒绝零假设，并得出结论，我们测试的结果具有统计学显著性。这也表示为α或α。显著性水平是当零假设为真时拒绝零假设的概率。标准的α值通常是0.05，这表明有5%的风险说存在差异，而实际上没有实际差异。</p><p id="5378" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">与α相比较的p值是表明总体的零假设是否成立的概率。p值来自样本数据，并假设零假设为真。较低的p值表明有更多的证据拒绝零假设。</p><p id="115c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">显著性和p值是分析回归模型的其他方法。进行OLS时，可以很容易地找到显著性值。</p><h1 id="a23c" class="ld le hi bd lf lg lh li lj lk ll lm ln io lo ip lp ir lq is lr iu ls iv lt lu bi translated">一个走过的例子</h1><p id="830d" class="pw-post-body-paragraph ix iy hi iz b ja lv ij jc jd lw im jf jg lx ji jj jk ly jm jn jo lz jq jr js hb bi translated">线性回归是根据其他变量预测值的一个很好的工具。</p><p id="e391" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">除了上面的速成课程，我将做一个回归模型的演练例子。</p><p id="9368" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我决定选择一个基于房地产价格预测的数据集。该数据来自<a class="ae kj" href="https://www.kaggle.com/quantbruce/real-estate-price-prediction" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/quant Bruce/real-estate-price-prediction</a>。有414个数据条目和八列。</p><p id="0cfb" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该数据集相对较小，纯粹用于应用和解释实践。</p><p id="7bf3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这8列是:</p><ol class=""><li id="3fd9" class="kk kl hi iz b ja jb jd je jg km jk kn jo ko js kp kq kr ks bi translated">不</li><li id="1b23" class="kk kl hi iz b ja ma jd mb jg mc jk md jo me js kp kq kr ks bi translated">X1交易日期</li><li id="40fc" class="kk kl hi iz b ja ma jd mb jg mc jk md jo me js kp kq kr ks bi translated">X2房屋时代</li><li id="1508" class="kk kl hi iz b ja ma jd mb jg mc jk md jo me js kp kq kr ks bi translated">X3到最近的捷运站的距离</li><li id="82c8" class="kk kl hi iz b ja ma jd mb jg mc jk md jo me js kp kq kr ks bi translated">X4便利店数量</li><li id="8938" class="kk kl hi iz b ja ma jd mb jg mc jk md jo me js kp kq kr ks bi translated">X5纬度</li><li id="e8f7" class="kk kl hi iz b ja ma jd mb jg mc jk md jo me js kp kq kr ks bi translated">X6经度</li><li id="208c" class="kk kl hi iz b ja ma jd mb jg mc jk md jo me js kp kq kr ks bi translated">y单位面积房价</li></ol><p id="67bc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我将通过首先导入必要的包和数据来开始本演练。在那里，我将进行一些探索性的数据分析。然后，我将创建一个基线模型，然后进行相应的调整和数据建模。然后我会解释我的结果。</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="4206" class="mj le hi lb b fi mk ml l mm mn">#start by importing the necessary libraries<br/>import numpy as np </span><span id="109f" class="mj le hi lb b fi mo ml l mm mn">import pandas as pd</span><span id="60c6" class="mj le hi lb b fi mo ml l mm mn">import matplotlib.pyplot as plt<br/>∞matplotlib inline<br/>import seaborn as sns</span><span id="c007" class="mj le hi lb b fi mo ml l mm mn">from sklearn.model_selection import train_test_split</span><span id="3cfa" class="mj le hi lb b fi mo ml l mm mn">import statsmodels.api as sm<br/>from sklearn.model_selection import train_test_split, KFold, cross_validate<br/>from sklearn.linear_model import LinearRegression</span></pre><p id="1a89" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">导入数据并检查</p><p id="7543" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">data = pd.read_csv('../linear regression/Real estate . CSV ')<br/>print(data . shape)<br/>print(data . keys())<br/>data . head()</p><p id="5091" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在单元运行之后，应该会出现这样的输出。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mp"><img src="../Images/1ba78d3b55ba224a203f9f86cd72427d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wM7rZ-3509_Rd0-8tBluTA.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><h2 id="942f" class="mj le hi bd lf mq mr ms lj mt mu mv ln jg mw mx lp jk my mz lr jo na nb lt nc bi translated">数据、清理和探索</h2><p id="9ef0" class="pw-post-body-paragraph ix iy hi iz b ja lv ij jc jd lw im jf jg lx ji jj jk ly jm jn jo lz jq jr js hb bi translated">我想删除任何空值。</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="b9ba" class="mj le hi lb b fi mk ml l mm mn">#remove null values<br/>data = data.dropna()<br/>print(data.shape)</span></pre><p id="b091" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这将我的数据减少到(414，8)。</p><p id="dc0f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我现在想看看数据类型。</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="a282" class="mj le hi lb b fi mk ml l mm mn">data.dtypes</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es nd"><img src="../Images/eaa78932217fbf9cc8c141d8809e5f64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IVSSr5dUjxLqKdFaFJuNRA.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><p id="3ba7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我现在想检查重复的内容。这是有争议的，因为一栋房子可以出售不止一次。此外，房屋出售时的价格可能会因多种因素而变化。</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="d399" class="mj le hi lb b fi mk ml l mm mn">#checking for duplicates<br/>data_duplicates = data.duplicated<br/>data_duplicates.shape()</span></pre><p id="87f4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里的输出是相同的，没有重复。</p><p id="f043" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我想知道是否有分类变量。</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="5747" class="mj le hi lb b fi mk ml l mm mn">#I will create subplits of the features against the price to identify categorical variables<br/>fig, axes = plt.subplots(nrows=1, ncols=6, figsize=(18,5))<br/><br/><strong class="lb hj">for</strong> xcol, ax <strong class="lb hj">in</strong> zip(data[['X1 transaction date','X2 house age','X3 distance to the nearest MRT station','X4 number of convenience stores', 'X5 latitude', 'X6 longitude']], axes):<br/>    data.plot(kind='scatter', x=xcol, y='Y house price of unit area', ax=ax, alpha=0.4, color='r')</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es ne"><img src="../Images/9499fc7d48f51c87adb9dcd76a9a522a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dPbgevIDV8P-tFFgFRG5sA.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="e0d0" class="mj le hi lb b fi mk ml l mm mn"><em class="nf">#checking the distribution</em><br/>print('Median House Age: ', data['X2 house age'].median())<br/>print('Median Price Per Square Foot: $', data['Y house price of unit area'].median())</span></pre><p id="997b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">房子的年龄中位数是16岁。每平方英尺的中间价格是38.45美元。</p><p id="66ff" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们来看看单位面积的价格是如何分布的。我将为这个可视化使用直方图。</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="3924" class="mj le hi lb b fi mk ml l mm mn">fig, ax = plt.subplots(figsize=(12,8))<br/><br/>sns.kdeplot(data['Y house price of unit area'], shade=<strong class="lb hj">True</strong>, color='navy',alpha=0.8)<br/>sns.despine()<br/><br/>plt.yticks([])<br/>ax.tick_params(axis='both', which='major', labelsize=15)<br/><br/>ax.set_title('House Price Per Unit Area Distribution', fontsize=25, loc='center', weight='bold', pad=20)<br/>ax.set_xlabel('Price ($)', fontsize=18, weight='bold')<br/>ax.set_ylabel('Count', fontsize=18, weight='bold')<br/><br/>plt.xlim(0,200)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es ng"><img src="../Images/bb8bf08f4be3c9d14174aedd791f5e25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vmD6bJz88FpJnR2nJ2ntUw.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><p id="6ec7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我想更仔细地看看特征变量。我将通过检查多重共线性来做到这一点。</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="2271" class="mj le hi lb b fi mk ml l mm mn"><em class="nf">#Multicollinearity </em><br/>data_multicollinearity = data.drop(['No'], axis = 1)<br/><br/>fig, ax = plt.subplots(figsize=(16,10))<br/>sns.heatmap(data_multicollinearity.corr(), center=0, annot=<strong class="lb hj">True</strong>)<br/><br/>ax.set_title('Feature Correlation Matrix', fontsize=25, loc='center', weight='bold', pad =15)<br/>ax.set_xlabel('Features', fontsize = 14, weight='bold')<br/>ax.tick_params(axis='both', which='major', labelsize=13)<br/><br/>plt.autoscale()<br/>plt.show()</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es nh"><img src="../Images/db6a4d478804b7c9702f2d9b5bdf5a81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cPHz2jP__5FQS3ID0v6A2Q.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><p id="ac85" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我现在想做一个箱线图，显示特征的方差随着它的增加而增加(这与异方差有关，我们需要在后面的步骤中注意)</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="1806" class="mj le hi lb b fi mk ml l mm mn">var = 'X4 number of convenience stores'<br/>feature_var_data = pd.concat([data['Y house price of unit area'], data[var]], axis=1)<br/><br/>f, ax = plt.subplots(figsize=(14, 10))<br/>fig = sns.boxplot(x=var, y = 'Y house price of unit area', data=feature_var_data, palette='dark')<br/><br/>ax.set_title('Feature Variance: ', fontsize=25, loc='center', weight='bold', pad=10)<br/>ax.set_xlabel('X4 number of convenient stores', fontsize=14, weight='bold')<br/>ax.set_ylabel('Y house price of unit area', fontsize=14, weight='bold')<br/>ax.tick_params(axis='both', which='major', labelsize=13)<br/><br/>sns.despine()<br/>fig.axis(ymin=0, ymax=120);</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es ni"><img src="../Images/2654024b099ec2deafe5bcfbb16d80be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4GDYYmkaSHdcVrX9ahUx7w.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><h2 id="c0bd" class="mj le hi bd lf mq mr ms lj mt mu mv ln jg mw mx lp jk my mz lr jo na nb lt nc bi translated">数据建模</h2><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="a155" class="mj le hi lb b fi mk ml l mm mn"><em class="nf"># X variables will contain our continuous and discrete features - we drop the dependent as well as non-predictors</em><br/>X_base = data.drop(['Y house price of unit area'], axis = 1)<br/><br/><em class="nf"># Y variable is the dependent variable - what we want to predict</em><br/>Y_base = data[['Y house price of unit area']]</span></pre><p id="48e2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我的火车测试分裂。我会一直做80/20分割！</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="b761" class="mj le hi lb b fi mk ml l mm mn">X_train_original, X_test_original, Y_train_original, Y_test_original = train_test_split(X_base, Y_base, test_size=.2, random_state=7)<br/>X_train = X_train_original.copy()<br/>X_test = X_test_original.copy()<br/>Y_train = Y_train_original.copy()<br/>Y_test = Y_test.copy()<br/><br/>print(X_train.shape)<br/>print(X_test.shape)<br/>print(Y_train.shape)<br/>print(Y_test.shape)</span></pre><p id="2c85" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">(331, 7)</p><p id="4b22" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">(83, 7)</p><p id="9f79" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">(331, 1)</p><p id="df07" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">(83, 1)</p><p id="5b4e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，我将为我的普通最小二乘法添加一个常数，并将斜率和截距拟合到我的模型中。</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="f2d8" class="mj le hi lb b fi mk ml l mm mn"><em class="nf">#Statsmodels for Ordinary Least Square for the training set</em><br/><em class="nf">#adding in a constant to fit both the slope and intercept</em><br/>X_train_constant = sm.add_constant(X_train)<br/>base_training_model = sm.OLS(Y_train, X_train_constant);</span><span id="bb5d" class="mj le hi lb b fi mo ml l mm mn">training_results = base_training_model.fit()</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es nj"><img src="../Images/14aa51ad527d3d3698d6d12611887299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*68BLhzQYuqjHmdatDe_x0w.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><p id="38a9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">训练模型返回0.585的r平方，这不是一个理想的分数。好的一面是，它没有过度拟合。让我们看看我是否能改进这一点。</p><h2 id="9e1a" class="mj le hi bd lf mq mr ms lj mt mu mv ln jg mw mx lp jk my mz lr jo na nb lt nc bi translated">评估和重新定义模型。</h2><p id="f2cf" class="pw-post-body-paragraph ix iy hi iz b ja lv ij jc jd lw im jf jg lx ji jj jk ly jm jn jo lz jq jr js hb bi translated">根据汇总表中的p值，我可以看到存在无关的要素。我只想选择p值小于0.05的要素。</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="d776" class="mj le hi lb b fi mk ml l mm mn"><em class="nf">#discarding features with p-values less than .05</em><br/>base_summary = training_results.summary()<br/>base_p_table = base_summary.tables[1]<br/>base_p_table = pd.DataFrame(base_p_table.data)<br/>base_p_table.columns = base_p_table.iloc[0]<br/>base_p_table = base_p_table.drop(0)<br/>base_p_table = base_p_table.set_index(base_p_table.columns[0])<br/><br/>base_p_table['P&gt;|t|'] = base_p_table['P&gt;|t|'].astype(float)<br/>x_cols = list(base_p_table[base_p_table['P&gt;|t|'] &lt; 0.05].index)</span><span id="bd39" class="mj le hi lb b fi mo ml l mm mn"><em class="nf">#x_cols.remove('const')</em><br/>print(len(base_p_table), len(x_cols))<br/>print(x_cols)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es nk"><img src="../Images/1948bc970f9d23620762b35acc8966ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*slMmz5oUeLCsNmchF1eXWg.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><p id="1b21" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我现在将重新调整新的特征子集。</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="6d3d" class="mj le hi lb b fi mk ml l mm mn">X_train = X_train[x_cols]<br/>Y_train = Y_train[['Y house price of unit area']]</span><span id="30a8" class="mj le hi lb b fi mo ml l mm mn">X_train_const = sm.add_constant(X_train)<br/>base_training_model = sm.OLS(Y_train, X_train_const)</span><span id="821d" class="mj le hi lb b fi mo ml l mm mn">results1= base_training_model.fit()</span><span id="c1c1" class="mj le hi lb b fi mo ml l mm mn">print("Training Data R-Squared", round(results1.rsquared,3)) results1.summary()</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es nl"><img src="../Images/167218096c79019150114b50be395794.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O5WK9TafgXnClR7YM1T6Ng.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><p id="cf01" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我将执行另一个多重共线性测试，但这次我将使用方差膨胀因子。</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="e08f" class="mj le hi lb b fi mk ml l mm mn"><strong class="lb hj">from</strong> <strong class="lb hj">statsmodels.stats.outliers_influence</strong> <strong class="lb hj">import</strong> variance_inflation_factor</span><span id="9ded" class="mj le hi lb b fi mo ml l mm mn"><em class="nf">#Here I will perform a variance inflation factor test to identify features that display multicollinearity</em> X = data[x_cols] vif = [variance_inflation_factor(X.values, i) <strong class="lb hj">for</strong> i <strong class="lb hj">in</strong> range(X.shape[1])] list(zip(x_cols, vif))</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es nm"><img src="../Images/c0155f90b3904b0a47fa2dff30dc6ce0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nyjH9QABNulgfgtATA_zbA.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="80c4" class="mj le hi lb b fi mk ml l mm mn">x_cols.remove(‘X1 transaction date’) <br/>x_cols.remove(‘X5 latitude’)</span><span id="11db" class="mj le hi lb b fi mo ml l mm mn">x_cols</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es nn"><img src="../Images/8b612331d50027fcd19053be35163aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*6pzk7BPY47b5ZF1nx6PnQQ.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="e45c" class="mj le hi lb b fi mk ml l mm mn"><em class="nf">#refit with subset features</em><br/>X_train = X_train[x_cols]<br/>Y_train = Y_train[['Y house price of unit area']]</span><span id="ec35" class="mj le hi lb b fi mo ml l mm mn">X_train_const = sm.add_constant(X_train)<br/>base_training_model = sm.OLS(Y_train, X_train_const);</span><span id="6224" class="mj le hi lb b fi mo ml l mm mn">results2 = base_training_model.fit()</span><span id="669c" class="mj le hi lb b fi mo ml l mm mn">print("Training Data R-Squared:", round(results2.rsquared,3))<br/>results2.summary()</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es no"><img src="../Images/2bc2056377ebc79382ed4bab55df8dd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*opP9Y7Zu7e5M_2ROZLidiw.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><p id="a853" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我的模型达到了0.544的r平方，这进一步打击了我的准确性得分，但我也删除了与一个数字高度相关的特征。</p><p id="f16e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我现在要尝试建立一个自下而上的模型。</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="56cc" class="mj le hi lb b fi mk ml l mm mn"><em class="nf">#recall the original 80/20 split for train and test data</em> X_train, X_test, Y_train, Y_test = train_test_split(X_base, Y_base, test_size=.2, random_state=42) print(X_train.shape) print(Y_train.shape)</span></pre><p id="8690" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">(331, 7) (331, 1)</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="f5cf" class="mj le hi lb b fi mk ml l mm mn"><em class="nf">Select features that are known to predict house prices within a strong degree of accuracy and taking into account </em><br/><em class="nf">#multicollinearity</em><br/>X_train = X_train[['X2 house age', 'X3 distance to the nearest MRT station', 'X4 number of convenience stores']]<br/>Y_train = Y_train[['Y house price of unit area']]<br/>X_test = X_test[['X2 house age', 'X3 distance to the nearest MRT station', 'X4 number of convenience stores']]<br/>Y_test = Y_test[['Y house price of unit area']]</span><span id="4bd1" class="mj le hi lb b fi mo ml l mm mn">X_train_const = sm.add_constant(X_train)<br/>base_training_model = sm.OLS(Y_train, X_train_const);</span><span id="e1a5" class="mj le hi lb b fi mo ml l mm mn">results3 = base_training_model.fit()</span><span id="727b" class="mj le hi lb b fi mo ml l mm mn">results3.summary()</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es np"><img src="../Images/487584cc6609eee2108b439ec3e7fe21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pYrt06UfnzwFYt96PasyMw.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><p id="3cb6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我的模型的性能甚至比以前更低。这是在去掉额外的变量之后。</p><p id="eac1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我现在要检查一下这篇文章开头提到的假设。</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="faa5" class="mj le hi lb b fi mk ml l mm mn"><em class="nf">#I now want to check for the normality assumption</em><br/><strong class="lb hj">import</strong> <strong class="lb hj">scipy.stats</strong> <strong class="lb hj">as</strong> <strong class="lb hj">stats</strong><br/>fig == sm.graphics.qqplot(results3.resid, dist=stats.norm, line='45', fit=<strong class="lb hj">True</strong>)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es nq"><img src="../Images/bfda3d3c38aae4c6c1716489cd26ef5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*9KH84WNvvgs8yhZF-7sDNQ.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><p id="7e0d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里，正态假设似乎成立，但看起来有异常值。</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="4eac" class="mj le hi lb b fi mk ml l mm mn"><em class="nf">#I want to check for homoscedasticity</em><br/>plt.scatter(results3.predict(X_train_const), results3.resid)<br/>plt.plot(results3.predict(X_train_const), [0 <strong class="lb hj">for</strong> i <strong class="lb hj">in</strong> range(len(X_train_const))])<br/>plt.show()</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es nr"><img src="../Images/7cd67c88b4e80615ea37e1b7ce453af4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*7lvzuPSZVc6IU20Uf-2unA.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><p id="9ecc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里，同方假设被违反了。</p><p id="8bd6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这表明我的数据并不具有线性关系。</p><p id="72df" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我现在将删除异常值，看看是否有帮助。</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="d40a" class="mj le hi lb b fi mk ml l mm mn">df_outliers = pd.concat([X_train, Y_train], axis=1)<br/><br/><strong class="lb hj">for</strong> i <strong class="lb hj">in</strong> range (80,100): <br/>    q = i/100<br/>    print("<strong class="lb hj">{}</strong> percentile: <strong class="lb hj">{}</strong>".format(q, Y_train['Y house price of unit area'].quantile(q=q)))<br/>    <br/>Y_train['Y house price of unit area'].hist()<br/>plt.show</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es ns"><img src="../Images/dbe86072597d534c9be61cbea982df4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*w3S_kLpBbBHgoSXyILd7dQ.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="4451" class="mj le hi lb b fi mk ml l mm mn"><em class="nf">#I will now drop everything past the 99 percentile </em><br/>original_total = len(df_outliers)<br/>df_outlier_dropped = df_outliers[df_outliers['Y house price of unit area'] &lt; 73.0] <em class="nf">#subsetting to remove outliers</em><br/>print("Percent removed:", (original_total - len(df_outlier_dropped))/original_total)<br/>df_outlier_dropped['Y house price of unit area'].max()<br/><br/>X_train = df_outlier_dropped.drop(['Y house price of unit area'], axis=1)<br/>Y_train = df_outlier_dropped['Y house price of unit area']</span></pre><p id="fc20" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">移除的百分比:0.01。56860.68868868661</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="8a21" class="mj le hi lb b fi mk ml l mm mn"><em class="nf">#Now I will use my training data where I have dropped my outlier data and use it on my best performing model </em><br/><em class="nf"># this is the refined baseline model </em><br/>X_train = X_train[x_cols]<br/>Y_train</span><span id="e557" class="mj le hi lb b fi mo ml l mm mn">X_train_const = sm.add_constant(X_train)<br/>base_training_model = sm.OLS(Y_train, X_train_const);</span><span id="b8d0" class="mj le hi lb b fi mo ml l mm mn">results4 = base_training_model.fit()</span><span id="30ad" class="mj le hi lb b fi mo ml l mm mn">print(round(results4.rsquared,3))<br/>results4.summary()</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es nt"><img src="../Images/9e61a4262f46e71db8cc9848ed975a15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9eMlDkbmi2SaVx4rkgxvpA.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><p id="782c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">丢弃异常值帮助我的模型跳到了0.587，这比我的第一个r平方分数好得多！</p><p id="9ce5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我将再次检查正态假设。</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="9d05" class="mj le hi lb b fi mk ml l mm mn"><em class="nf">#I am going to check normality again to see if it has improved</em><br/>fig = sm.graphics.qqplot(results4.resid, dist=stats.norm, line ='45', fit=<strong class="lb hj">True</strong>)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es nu"><img src="../Images/a33d27ded211512d149167d3cb03179b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*aP4KGw4w7oI9pBz9pjz0uw.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><p id="e880" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">事实上，通过剔除异常值，常态得到了改善。</p><p id="857b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，我将运行我的测试数据。</p><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="8c04" class="mj le hi lb b fi mk ml l mm mn"><em class="nf">#standardize test data</em><br/><strong class="lb hj">for</strong> col <strong class="lb hj">in</strong> X_test_original: <br/>    X_test_original[col] = (X_test_original[col] - X_test_original[col].mean())/X_test_original[col].std()<br/><br/>X_test_original.head()</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es nv"><img src="../Images/85fb2108d244a2b3b484b7e9893d5594.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-lBWJvWsPvsb_x-qZugWgA.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx">Credit: <a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb</a></figcaption></figure><pre class="ju jv jw jx fd mf lb mg mh aw mi bi"><span id="d99e" class="mj le hi lb b fi mk ml l mm mn">X_test = X_test_original[x_cols]<br/>Y_test = Y_test_original</span><span id="d021" class="mj le hi lb b fi mo ml l mm mn">final_model_skl = LinearRegression(fit_intercept=<strong class="lb hj">True</strong>)<br/><br/><em class="nf">#Learning</em><br/>final_model_skl.fit(X_train, Y_train)<br/><br/><em class="nf">#Evaluating performance</em><br/>r2 = final_model_skl.score(X_test, Y_test)<br/>r2 = r2.round(3)<br/><br/>print("R2 = <strong class="lb hj">{}</strong>".format(r2))</span></pre><p id="6ea7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我得到0.511的r平方。</p><p id="6ef3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">数据能告诉我们什么:</strong></p><p id="e933" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一所房子的便利店数量，以及经度和纬度似乎是单位面积房价的最强决定因素。</p><p id="1ec6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我确信我们可以使用不同的机器学习模型来研究变量之间更强的相关性。但为了这个目的，它纯粹是做一个线性回归。</p><p id="7b5e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">本教程结束后，我要做的一件事是尝试逻辑回归。逻辑回归对分类变量特别有用。</p><p id="6561" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在现实世界中，获得0.587或0.511的r平方并不坏。最常见的是在现实世界的数据驱动问题中，很少能得到高分。</p><p id="47d5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="nf">来源</em>:</p><p id="dc45" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">”线性回归”；耶鲁在线课程:LinReghttp://www.stat.yale.edu/Courses/1997-98/101/linreg.htm<a class="ae kj" href="http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm" rel="noopener ugc nofollow" target="_blank"/></p><p id="33c3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">”线性回归”；阿库雷西；<a class="ae kj" href="https://github.com/anilaq/linearregression/blob/main/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/anilaq/linear regression/blob/main/linear _ regression . ipynb</a></p><p id="b84b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">”线性V逻辑回归”；Sourav<a class="ae kj" href="https://www.analyticsvidhya.com/blog/2020/12/beginners-take-how-logistic-regression-is-related-to-linear-regression/#:~:text=Linear%20Regression%20is%20used%20to,Logistic%20regression%20provides%20discreet%20output" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2020/12/beginners-take-how-Logistic-regression-is-related-to-Linear-regression/#:~:text = Linear % 20 regression % 20 is % 20 used % 20 to，Logistic % 20 regression % 20 provides % 20 discreet % 20 output</a>。</p><p id="497a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">”P值”；弗罗斯特，吉姆；<a class="ae kj" href="https://statisticsbyjim.com/glossary/p-value/" rel="noopener ugc nofollow" target="_blank">https://statisticsbyjim.com/glossary/p-value/</a></p></div></div>    
</body>
</html>
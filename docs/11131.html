<html>
<head>
<title>Datasets in Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark中的数据集</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/introduction-to-datasets-in-spark-79a7d94d9158?source=collection_archive---------1-----------------------#2022-03-06">https://medium.com/geekculture/introduction-to-datasets-in-spark-79a7d94d9158?source=collection_archive---------1-----------------------#2022-03-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6bc6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark已经成为无处不在的数据处理平台，并取代了传统的MapReduce框架。事实上，一些技术专家甚至会宣布MapReduce已死。在众多的基准测试和性能研究中，Spark已经被证明比MapReduce高出几个数量级。下面，我们简单叙述一下Spark在大数据领域占据主导地位背后的历史。</p><p id="b5dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们将简要介绍Spark数据集</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/597346936bd20ca05fa5398a55ce9a5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8FSIyRXYk_mcM_Pk"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Photo by <a class="ae jt" href="https://unsplash.com/@goumbik?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Lukas Blazek</a> on <a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="fddc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是官方数据块文档中数据集的定义:</p><blockquote class="ju jv jw"><p id="324a" class="if ig jx ih b ii ij ik il im in io ip jy ir is it jz iv iw ix ka iz ja jb jc hb bi translated">数据集是映射到关系架构的强类型、不可变的对象集合。数据集是一种类型安全的结构化API，可以在静态类型、Spark支持的语言Java和Scala中使用。数据集是严格意义上的JVM语言特性。R和Python不支持数据集，因为这些语言是动态类型语言”。</p></blockquote><h2 id="e891" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">概观</h2><p id="b07d" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">数据集是<a class="ae jt" href="https://data-flair.training/blogs/spark-sql-tutorial/" rel="noopener ugc nofollow" target="_blank"> SparkSQL </a>中的一种数据结构，它是强类型的，是关系模式的映射。它用编码器表示结构化查询。它是数据框架API的扩展。Spark数据集提供了类型安全和面向对象的编程接口。<br/>数据集集合了<a class="ae jt" href="https://data-flair.training/blogs/rdd-in-apache-spark/" rel="noopener ugc nofollow" target="_blank"> RDD </a>和<a class="ae jt" href="https://data-flair.training/blogs/apache-spark-sql-dataframe-tutorial/" rel="noopener ugc nofollow" target="_blank">数据帧</a>的特征。它提供:</p><ul class=""><li id="21bd" class="lb lc hi ih b ii ij im in iq ld iu le iy lf jc lg lh li lj bi translated">RDD的便利。</li><li id="be77" class="lb lc hi ih b ii lk im ll iq lm iu ln iy lo jc lg lh li lj bi translated">数据帧的性能优化。</li><li id="2c0e" class="lb lc hi ih b ii lk im ll iq lm iu ln iy lo jc lg lh li lj bi translated"><a class="ae jt" href="https://data-flair.training/blogs/why-you-should-learn-scala-introductory-tutorial/" rel="noopener ugc nofollow" target="_blank"> Scala </a>的静态类型安全。</li></ul><p id="2dda" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Spark 2.0之后，RDD被Dataset取代，Dataset像RDD一样是强类型的，但是在底层有更丰富的优化。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lp"><img src="../Images/5fa3507de4284e151035bd849c9f913d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KJDaUMWqDknQ1ENYd8TCxA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">DataFrame and Dataset in spark</figcaption></figure><p id="03b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Scala的上下文中，我们可以将DataFrame看作是一组通用对象的别名，表示为<code class="du lq lr ls lt b">Dataset[Row]</code>。<code class="du lq lr ls lt b">Row</code>对象是非类型化的，是一个通用的JVM对象，可以保存不同类型的字段。相比之下，数据集是Scala中强类型JVM对象的集合，或者是Java中的一个类。公平地说，Scala中的每个数据集都有一个名为DataFrame的非类型化视图，它是<code class="du lq lr ls lt b">Row</code>的数据集。下表捕捉了各种Spark支持的语言中数据集和数据帧的概念。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/5d3a98a47b2b2549e89d3f78f7b099c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*VdCxQaTZf6eetgd1T_SEBQ.png"/></div></figure><p id="e799" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据集之所以成为可能，是因为有一个称为编码器的特性，它将JVM类型转换成Spark SQL的专用内部(表格)表示。编码器是高度专业化和优化的代码生成器，它为数据的序列化和反序列化生成定制的字节码。所有数据集都需要编码器。编码器将特定于域的类型映射到Spark对该类型的内部表示。例如，对于Scala或Java和Python，作为<code class="du lq lr ls lt b">Row</code>中的一个字段的<code class="du lq lr ls lt b">Int</code>将分别被映射或转换为<code class="du lq lr ls lt b">IntegerType</code>或<code class="du lq lr ls lt b">IntegerType()</code>。特定于域的类型可以表示为Java的beans和Scala的case类。表格表示使用Spark的内部钨二进制格式存储，允许对序列化数据进行操作并提高内存利用率。在使用数据集API时，Spark在运行时生成代码，将Java对象序列化为内部二进制结构，反之亦然。这种转换可能对性能有轻微的影响，但是有几个好处。例如，Spark理解数据集中的数据结构，它可以在缓存数据集时在内存中创建更优化的布局。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lv"><img src="../Images/f11460b9cd03d3089ed1155ecbcaabb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*avN5F5hqF643awRo"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx">Photo by <a class="ae jt" href="https://unsplash.com/@shieldxie?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">nting xie</a> on <a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="706a" class="lw kc hi bd kd lx ly lz kh ma mb mc kl md me mf ko mg mh mi kr mj mk ml ku mm bi translated">Spark中数据集的特性</h1><p id="2769" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">介绍完数据集之后，现在让我们讨论一下Spark数据集的各种特性</p><h2 id="7b24" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">优化查询</h2><p id="5744" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">Spark中的数据集使用<a class="ae jt" href="https://data-flair.training/blogs/spark-sql-optimization-catalyst-optimizer/" rel="noopener ugc nofollow" target="_blank"> Catalyst查询优化器</a>和钨提供优化的查询。Catalyst查询优化器是一个与执行无关的框架。它表示和操作数据流图。数据流图是表达式和关系运算符的树。通过优化火花工作，钨提高了执行力。钨强调Apache Spark运行平台的硬件架构。</p><h2 id="bed6" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">编译时分析</h2><p id="3581" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">使用数据集，我们可以在编译时检查语法和分析。使用数据帧、rdd或常规SQL查询是不可能的。</p><h2 id="c973" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">持久存储</h2><p id="0d32" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">Spark数据集既是可序列化的，也是可查询的。因此，我们可以将它保存到永久存储器中。</p><h2 id="de78" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">更快的计算</h2><p id="79f0" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">数据集的实现比RDD的实现快得多。从而提高了系统的性能。对于使用RDD的相同性能，用户手动考虑如何以最佳方式表达并行计算。</p><h1 id="80a8" class="lw kc hi bd kd lx ly lz kh ma mb mc kl md me mf ko mg mh mi kr mj mk ml ku mm bi translated">与数据帧的差异</h1><p id="b66d" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">我们需要对比数据框架和数据集，以便更好地理解这两者。数据集在编译时检查类型是否符合规范。数据帧并不是真正无类型的，因为它们的类型是由Spark维护的，但是验证类型是否符合模式中的规范是在运行时完成的。换句话说，数据帧可以被认为是“行”类型的数据集，这是Spark内部优化的内存计算表示。拥有自己的内部类型表示允许Spark跳过实例化速度慢且有垃圾收集成本的JVM类型。</p><h1 id="d1cc" class="lw kc hi bd kd lx ly lz kh ma mb mc kl md me mf ko mg mh mi kr mj mk ml ku mm bi translated">数据集的用例</h1><p id="2e47" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">当我们已经有数据框架时，拥有数据集似乎是多余的，但在某些情况下，数据集比数据框架更合适。也就是说，有些操作不能用数据帧来表达，只能用数据集来表达。还要考虑对类型安全的需求。例如，试图在代码中将两个字符串变量相乘将在编译时失败，而不是在运行时失败。此外，开发可能是有帮助的，因为当对象是强类型时，ide和其他工具可以提供自动完成和其他提示。使用数据集的另一个原因是如果所有的数据和转换都接受case类(Scala)；对于分布式和本地工作负载来说，重用它们是微不足道的。</p></div><div class="ab cl mn mo gp mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="hb hc hd he hf"><p id="1f56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们已经触及了Spark数据集所能提供的一些皮毛。希望，这是一篇有帮助的入门文章，下次再见！</p></div></div>    
</body>
</html>
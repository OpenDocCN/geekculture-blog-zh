<html>
<head>
<title>How do transformers fare on a grammar test?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变形金刚在语法考试中表现如何？</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/how-do-transformers-fare-on-a-grammar-test-d3b7b26e5004?source=collection_archive---------6-----------------------#2021-08-11">https://medium.com/geekculture/how-do-transformers-fare-on-a-grammar-test-d3b7b26e5004?source=collection_archive---------6-----------------------#2021-08-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="44d1" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">看看伯特和罗伯塔如何处理语法测验中的问题</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/e57146bbc63a8cf337e9384568db0c0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TsXUgfqRSRomYKTE"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">Photo by <a class="ae jn" href="https://unsplash.com/@benmullins?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ben Mullins</a> on <a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="7c0d" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">概观</h1><ol class=""><li id="7972" class="kg kh hi ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">略谈BERT及其对完形填空的训练。</li><li id="af2a" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">罗伯塔和它如何不同于伯特</li><li id="df4c" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">拥抱脸和填充面具管道</li><li id="89c1" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">比较BERT和RoBERTa使用语法测验中的问题预测介词、冠词、问题标签、反义词和流行谚语的能力。(英文)</li></ol><p id="4b4f" class="pw-post-body-paragraph ld le hi ki b kj lf ij lg kl lh im li kn lj lk ll kp lm ln lo kr lp lq lr kt hb bi translated"><em class="ls">如果你只是来看看结果，并且已经了解了transformer模型，请随意跳到第4部分。</em></p></div><div class="ab cl lt lu gp lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="hb hc hd he hf"><h1 id="39a2" class="jo jp hi bd jq jr ma jt ju jv mb jx jy io mc ip ka ir md is kc iu me iv ke kf bi translated">伯特与完形填空</h1><p id="7180" class="pw-post-body-paragraph ld le hi ki b kj kk ij lg kl km im li kn mf lk ll kp mg ln lo kr mh lq lr kt hb bi translated">来自变压器的双向编码器表示(BERT)是由Google AI语言创建的最先进的NLP模型。下面是<a class="ae jn" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ki hj">论文</strong> </a> <strong class="ki hj">原文链接。</strong>该模型在一个巨大的语料库上进行训练(稍后将详细介绍)，然后这个预训练的模型可以用于几个下游任务(主要与自然语言理解相关)，如文本分类、提取问题回答和命名实体识别。更正式地说，BERT使用Vaswani等人在最初的<a class="ae jn" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <strong class="ki hj">变形金刚论文</strong> </a>中描述的编码器模型。</p><p id="b356" class="pw-post-body-paragraph ld le hi ki b kj lf ij lg kl lh im li kn lj lk ll kp lm ln lo kr lp lq lr kt hb bi translated">BERT的词汇由在训练期间学习的维度为768 的<strong class="ki hj">向量表示。伯特接受了两项特殊任务的训练。</strong></p><ol class=""><li id="dd70" class="kg kh hi ki b kj lf kl lh kn mi kp mj kr mk kt ku kv kw kx bi translated"><strong class="ki hj">蒙面语言造型(MLM) </strong></li><li id="3a96" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated"><strong class="ki hj">下一句预测(NSP) </strong></li></ol><h2 id="c593" class="ml jp hi bd jq mm mn mo ju mp mq mr jy kn ms mt ka kp mu mv kc kr mw mx ke my bi translated">蒙面语言模型(MLM)</h2><p id="9db7" class="pw-post-body-paragraph ld le hi ki b kj kk ij lg kl km im li kn mf lk ll kp mg ln lo kr mh lq lr kt hb bi translated">在训练时，每个句子都一次性输入到模型中，这与以前的seq2seq模型(LSTMs和RNNs)不同。然而，句子中的15% 的标记被特殊标记<strong class="ki hj">【屏蔽】</strong>屏蔽，并且模型必须预测被屏蔽的标记。请注意，训练是以半监督的方式大规模进行的。在这种情况下，我们不需要手动标注巨大的数据集，鉴于BERT被训练的语料库的大小，这是不可能的，而是将各种批次的屏蔽句子馈送给模型，并且模型学习预测随时间推移的屏蔽标记。这被正式称为<strong class="ki hj">完形填空任务。</strong></p><p id="6f2b" class="pw-post-body-paragraph ld le hi ki b kj lf ij lg kl lh im li kn lj lk ll kp lm ln lo kr lp lq lr kt hb bi translated">这给了BERT解释其上下文的能力，这也正是我们在测试问题模型时要用到的。</p><p id="eea8" class="pw-post-body-paragraph ld le hi ki b kj lf ij lg kl lh im li kn lj lk ll kp lm ln lo kr lp lq lr kt hb bi translated">这是MLM的样子。这张图片来自拉尼·霍雷夫在BERT上的一个<a class="ae jn" href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270" rel="noopener" target="_blank"> <strong class="ki hj">优秀博客</strong> </a>，我强烈推荐阅读它以更好地理解这个模型。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mz"><img src="../Images/1531e4324d3a4d3b0aabf2f2e947ff50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FfY47UrSYEmKKxxBA-HkLg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx">How an input mask is predicted (Image by Author)</figcaption></figure><p id="a4b6" class="pw-post-body-paragraph ld le hi ki b kj lf ij lg kl lh im li kn lj lk ll kp lm ln lo kr lp lq lr kt hb bi translated"><strong class="ki hj">正如你在这里看到的，单词“你的”在句子被输入模型之前被屏蔽，模型预测一个对应于该屏蔽的标记。</strong></p><h2 id="3ca2" class="ml jp hi bd jq mm mn mo ju mp mq mr jy kn ms mt ka kp mu mv kc kr mw mx ke my bi translated">下一句预测(NSP)</h2><p id="1f97" class="pw-post-body-paragraph ld le hi ki b kj kk ij lg kl km im li kn mf lk ll kp mg ln lo kr mh lq lr kt hb bi translated">这是BERT的另一个训练目标，它接收成对的句子，并且必须预测第二个句子是否跟随第一个句子。我不会深入探讨这个问题，因为Roberta完全放弃了这个目标，并且在一些任务中表现得更好。</p></div><div class="ab cl lt lu gp lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="hb hc hd he hf"><h1 id="3e9b" class="jo jp hi bd jq jr ma jt ju jv mb jx jy io mc ip ka ir md is kc iu me iv ke kf bi translated">罗伯塔对伯特</h1><p id="05c2" class="pw-post-body-paragraph ld le hi ki b kj kk ij lg kl km im li kn mf lk ll kp mg ln lo kr mh lq lr kt hb bi translated">RoBERTa代表稳健优化的BERT预训练方法，遵循与BERT相似的架构。It员工进行了一些重大调整，这使得它比BERT更适合某些任务。</p><ol class=""><li id="0d2d" class="kg kh hi ki b kj lf kl lh kn mi kp mj kr mk kt ku kv kw kx bi translated"><strong class="ki hj">丢弃下一句预测训练目标</strong>。与伯特不同，罗伯塔不使用NSP物镜，根据这篇论文，只有在MLM物镜上训练才能取得更好的成绩。</li><li id="ec29" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated"><strong class="ki hj">与BERT不同，RoBERTa使用动态屏蔽</strong>。在BERT中，屏蔽在数据预处理步骤中执行一次，产生一个静态屏蔽。为了改变被屏蔽的令牌，训练数据被复制和屏蔽10次，每次在40个时期内使用不同的屏蔽策略，因此有4个时期使用相同的屏蔽。在动态遮罩中，每次在将序列输入模型之前都会生成不同的遮罩。作者发现，这对于更多的训练数据和更多的时期更有效。</li><li id="320f" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated"><strong class="ki hj">最重要的是，RoBERTa是在一个大规模的语料库上接受训练的</strong>(好像BERT的训练语料库还不够大)，而且接受训练的时间比BERT长得多。更具体地说，BERT接受的是16GB文本数据的训练，而RoBERTa接受的是160GB文本数据的训练！</li><li id="eecb" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">RoBERTa还执行了其他一些小的调整，比如改变Adam优化器的超参数，以及使用更长的序列进行训练。<a class="ae jn" href="https://arxiv.org/pdf/1907.11692.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ki hj">原文</strong> </a>更详细，值得一读。</li></ol></div><div class="ab cl lt lu gp lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="hb hc hd he hf"><h1 id="4dc5" class="jo jp hi bd jq jr ma jt ju jv mb jx jy io mc ip ka ir md is kc iu me iv ke kf bi translated">拥抱脸和填充遮罩管道</h1><p id="e766" class="pw-post-body-paragraph ld le hi ki b kj kk ij lg kl km im li kn mf lk ll kp mg ln lo kr mh lq lr kt hb bi translated">拥抱脸是一个惊人的枢纽，托管各种变压器模型的实现。他们提供了这些模型的抽象版本，因此用户可以毫不费力地利用变压器的能力。</p><p id="a4d4" class="pw-post-body-paragraph ld le hi ki b kj lf ij lg kl lh im li kn lj lk ll kp lm ln lo kr lp lq lr kt hb bi translated">填充-屏蔽管道是用于屏蔽语言建模任务的抽象管道。我们只需要输入一个带有掩码标记的句子(BERT用<strong class="ki hj"> [MASK]，RoBERTa用&lt;掩码&gt;)，它会给出可能的单词来替换掩码标记。</strong></p><p id="bc70" class="pw-post-body-paragraph ld le hi ki b kj lf ij lg kl lh im li kn lj lk ll kp lm ln lo kr lp lq lr kt hb bi translated"><strong class="ki hj"> <em class="ls">注意:拥抱脸，每个序列只能屏蔽一个令牌。</em> </strong></p><p id="94cc" class="pw-post-body-paragraph ld le hi ki b kj lf ij lg kl lh im li kn lj lk ll kp lm ln lo kr lp lq lr kt hb bi translated">这是一个例子的样子。检查输出，它包含5个可能的建议。每个建议都是一个字典，具有诸如<strong class="ki hj">分数(预测的置信度)、序列(填充了掩码的完整序列)、标记(标记id)以及最后的标记(预测的单词)等特征。我们将在下一节讨论详细的代码，您将会看到它是多么简单！</strong></p></div><div class="ab cl lt lu gp lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="hb hc hd he hf"><h1 id="4fd5" class="jo jp hi bd jq jr ma jt ju jv mb jx jy io mc ip ka ir md is kc iu me iv ke kf bi translated">语法考试！</h1><p id="4d3e" class="pw-post-body-paragraph ld le hi ki b kj kk ij lg kl km im li kn mf lk ll kp mg ln lo kr mh lq lr kt hb bi translated">对于这个测试，我从5个类别中选择了3个问题——介词、冠词、问题标签、反义词和谚语。这总共得了15分。这些问题来自一些流行的在线测验(免费使用)。让我们看看伯特和罗伯塔的表现。</p><h2 id="4349" class="ml jp hi bd jq mm mn mo ju mp mq mr jy kn ms mt ka kp mu mv kc kr mw mx ke my bi translated">设置</h2><p id="ae83" class="pw-post-body-paragraph ld le hi ki b kj kk ij lg kl km im li kn mf lk ll kp mg ln lo kr mh lq lr kt hb bi translated">下面是在Google Colab中设置和安装transformers库的代码。</p><pre class="iy iz ja jb fd na nb nc nd aw ne bi"><span id="ca16" class="ml jp hi nb b fi nf ng l nh ni">!pip install transformers</span></pre><p id="85b4" class="pw-post-body-paragraph ld le hi ki b kj lf ij lg kl lh im li kn lj lk ll kp lm ln lo kr lp lq lr kt hb bi translated">和单一进口</p><pre class="iy iz ja jb fd na nb nc nd aw ne bi"><span id="d8d9" class="ml jp hi nb b fi nf ng l nh ni">from transformers import pipeline</span></pre><p id="5740" class="pw-post-body-paragraph ld le hi ki b kj lf ij lg kl lh im li kn lj lk ll kp lm ln lo kr lp lq lr kt hb bi translated">然后，我们通过选择<strong class="ki hj">填充掩码任务并提及模型名称，为BERT和RoBERTa创建一个管道。请注意，BERTs的预测不区分大小写，而RoBERTa的预测区分大小写。</strong></p><pre class="iy iz ja jb fd na nb nc nd aw ne bi"><span id="c63a" class="ml jp hi nb b fi nf ng l nh ni">unmasker_BERT = pipeline('fill-mask', model='bert-base-uncased')</span><span id="0147" class="ml jp hi nb b fi nj ng l nh ni">unmasker_ROBERTA=pipeline('fill-mask', model='roberta-base')</span></pre><p id="8c26" class="pw-post-body-paragraph ld le hi ki b kj lf ij lg kl lh im li kn lj lk ll kp lm ln lo kr lp lq lr kt hb bi translated">这是测试中的问题</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nk nl l"/></div></figure><h2 id="d72e" class="ml jp hi bd jq mm mn mo ju mp mq mr jy kn ms mt ka kp mu mv kc kr mw mx ke my bi translated">介词</h2><p id="27d3" class="pw-post-body-paragraph ld le hi ki b kj kk ij lg kl km im li kn mf lk ll kp mg ln lo kr mh lq lr kt hb bi translated">介词表示句子中其他单词之间的关系。这对于两个模型来说都是一个简单的任务，并且两个模型都正确地预测了所有的答案。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nk nl l"/></div></figure><h2 id="8372" class="ml jp hi bd jq mm mn mo ju mp mq mr jy kn ms mt ka kp mu mv kc kr mw mx ke my bi translated">文章</h2><p id="7bbf" class="pw-post-body-paragraph ld le hi ki b kj kk ij lg kl km im li kn mf lk ll kp mg ln lo kr mh lq lr kt hb bi translated">文章由<strong class="ki hj"> a、</strong>和<strong class="ki hj">an组成。</strong>冠词通常用来确定一个名词是具体的还是概括的。</p><p id="1628" class="pw-post-body-paragraph ld le hi ki b kj lf ij lg kl lh im li kn lj lk ll kp lm ln lo kr lp lq lr kt hb bi translated">请注意这两个模型是如何预测特殊情况的。 虽然one-bedroom以元音开头，但由于单词的发音 <strong class="ki hj">，我们必须<strong class="ki hj">使用“a one-bedroom”而不是“an one-bedroom”。</strong></strong></p><p id="ef2f" class="pw-post-body-paragraph ld le hi ki b kj lf ij lg kl lh im li kn lj lk ll kp lm ln lo kr lp lq lr kt hb bi translated">伯特错过了一篇文章，而罗伯塔预测都是正确的。注意RoBERTa是如何将第二句中预测的单词大写的，因为它是句子中的第一个单词。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nk nl l"/></div></figure><h2 id="a8fa" class="ml jp hi bd jq mm mn mo ju mp mq mr jy kn ms mt ka kp mu mv kc kr mw mx ke my bi translated"><strong class="ak">问题标签</strong></h2><p id="16e4" class="pw-post-body-paragraph ld le hi ki b kj kk ij lg kl km im li kn mf lk ll kp mg ln lo kr mh lq lr kt hb bi translated">这些把一个陈述变成一个问题。它们经常被用来检查我们认为我们知道是真实的信息。</p><p id="76ed" class="pw-post-body-paragraph ld le hi ki b kj lf ij lg kl lh im li kn lj lk ll kp lm ln lo kr lp lq lr kt hb bi translated">这就是这两个模型开始出现问题的地方。这些模型不能很好地处理否定，只能正确预测一个答案。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nk nl l"/></div></figure><h2 id="0c18" class="ml jp hi bd jq mm mn mo ju mp mq mr jy kn ms mt ka kp mu mv kc kr mw mx ke my bi translated">对立面</h2><p id="71b1" class="pw-post-body-paragraph ld le hi ki b kj kk ij lg kl km im li kn mf lk ll kp mg ln lo kr mh lq lr kt hb bi translated">这完全出乎我的意料，因为我没想到模型会从上下文中预测相反的情况。这只能继续显示这些模型是多么强大，以及他们所经历的全面训练。这里罗伯塔得到所有正确的，但伯特错过了一个。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nk nl l"/></div></figure><h2 id="5e19" class="ml jp hi bd jq mm mn mo ju mp mq mr jy kn ms mt ka kp mu mv kc kr mw mx ke my bi translated">《箴言》</h2><p id="7cfc" class="pw-post-body-paragraph ld le hi ki b kj kk ij lg kl km im li kn mf lk ll kp mg ln lo kr mh lq lr kt hb bi translated">谚语是流行的说法。罗伯塔的表现再次让我惊讶，因为预测谚语的单词是一项艰巨的任务，这需要模型在训练中看到类似的句子。罗伯塔在这里明显胜过伯特，并且正确预测了所有的谚语。我猜罗伯塔训练的巨大语料库与此有关。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nk nl l"/></div></figure><h2 id="b11d" class="ml jp hi bd jq mm mn mo ju mp mq mr jy kn ms mt ka kp mu mv kc kr mw mx ke my bi translated">最终分数</h2><p id="b94b" class="pw-post-body-paragraph ld le hi ki b kj kk ij lg kl km im li kn mf lk ll kp mg ln lo kr mh lq lr kt hb bi translated">这是两人在每一部分的表现以及他们的总分。很明显，RoBERTa在这次测试中表现更好，以13比9击败了BERT！</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nm"><img src="../Images/5fdbd40af5c79b9a0f7b5841c17be123.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*oQrdAHgcVL0I5OEwyziztQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx">Final Scores (Image By Author)</figcaption></figure></div><div class="ab cl lt lu gp lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="hb hc hd he hf"><h1 id="420d" class="jo jp hi bd jq jr ma jt ju jv mb jx jy io mc ip ka ir md is kc iu me iv ke kf bi translated">结论</h1><p id="10e0" class="pw-post-body-paragraph ld le hi ki b kj kk ij lg kl km im li kn mf lk ll kp mg ln lo kr mh lq lr kt hb bi translated">我们可以通过这个测试看到变形金刚有多强大，尤其是看到它们在反义词和谚语上表现得如此之好。令人瞩目的是，语言模型已经从仅仅是统计预测器发展到可以在如此庞大的语料库上训练而无需人工注释的深度复杂模型。这个测试也显示了罗伯塔在完形填空方面有多强。</p><p id="a411" class="pw-post-body-paragraph ld le hi ki b kj lf ij lg kl lh im li kn lj lk ll kp lm ln lo kr lp lq lr kt hb bi translated">如果你喜欢这篇文章，这里有更多！</p><div class="nn no ez fb np nq"><a href="https://towardsdatascience.com/scatter-plots-on-maps-using-plotly-79f16aee17d0" rel="noopener follow" target="_blank"><div class="nr ab dw"><div class="ns ab nt cl cj nu"><h2 class="bd hj fi z dy nv ea eb nw ed ef hh bi translated">使用Plotly在地图上散布图</h2><div class="nx l"><h3 class="bd b fi z dy nv ea eb nw ed ef dx translated">了解如何用很少的代码创建交互式散点图来表示数据中的多个要素</h3></div><div class="ny l"><p class="bd b fp z dy nv ea eb nw ed ef dx translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="oa l ob oc od nz oe jh nq"/></div></div></a></div><div class="nn no ez fb np nq"><a href="https://towardsdatascience.com/regex-essential-for-nlp-ee0336ef988d" rel="noopener follow" target="_blank"><div class="nr ab dw"><div class="ns ab nt cl cj nu"><h2 class="bd hj fi z dy nv ea eb nw ed ef hh bi translated">正则表达式对NLP至关重要</h2><div class="nx l"><h3 class="bd b fi z dy nv ea eb nw ed ef dx translated">理解各种正则表达式，并将其应用于自然语言中经常遇到的情况…</h3></div><div class="ny l"><p class="bd b fp z dy nv ea eb nw ed ef dx translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="of l ob oc od nz oe jh nq"/></div></div></a></div><div class="nn no ez fb np nq"><a href="https://towardsdatascience.com/powerful-text-augmentation-using-nlpaug-5851099b4e97" rel="noopener follow" target="_blank"><div class="nr ab dw"><div class="ns ab nt cl cj nu"><h2 class="bd hj fi z dy nv ea eb nw ed ef hh bi translated">使用NLPAUG的强大文本增强！</h2><div class="nx l"><h3 class="bd b fi z dy nv ea eb nw ed ef dx translated">通过文本增强技术处理NLP分类问题中的类别不平衡</h3></div><div class="ny l"><p class="bd b fp z dy nv ea eb nw ed ef dx translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="og l ob oc od nz oe jh nq"/></div></div></a></div><div class="nn no ez fb np nq"><a href="https://towardsdatascience.com/effortless-exploratory-data-analysis-eda-201c99324857" rel="noopener follow" target="_blank"><div class="nr ab dw"><div class="ns ab nt cl cj nu"><h2 class="bd hj fi z dy nv ea eb nw ed ef hh bi translated">轻松的探索性数据分析(EDA)</h2><div class="ny l"><p class="bd b fp z dy nv ea eb nw ed ef dx translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="oh l ob oc od nz oe jh nq"/></div></div></a></div><div class="nn no ez fb np nq"><a href="https://towardsdatascience.com/representing-5-features-in-a-single-animated-plot-using-plotly-e4f6064c7f46" rel="noopener follow" target="_blank"><div class="nr ab dw"><div class="ns ab nt cl cj nu"><h2 class="bd hj fi z dy nv ea eb nw ed ef hh bi translated">使用Plotly在一个动画情节中表现5个特征</h2><div class="nx l"><h3 class="bd b fi z dy nv ea eb nw ed ef dx translated">使用单个动画气泡图来分析数据和观察趋势。</h3></div><div class="ny l"><p class="bd b fp z dy nv ea eb nw ed ef dx translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="oi l ob oc od nz oe jh nq"/></div></div></a></div><p id="80fa" class="pw-post-body-paragraph ld le hi ki b kj lf ij lg kl lh im li kn lj lk ll kp lm ln lo kr lp lq lr kt hb bi translated">查看我的<a class="ae jn" href="https://github.com/rajlm10" rel="noopener ugc nofollow" target="_blank"> <strong class="ki hj"> GitHub </strong> </a>其他一些项目。可以联系我<a class="ae jn" href="https://rajsangani.me/" rel="noopener ugc nofollow" target="_blank"> <strong class="ki hj"> <em class="ls">这里</em> </strong> </a> <strong class="ki hj"> <em class="ls">。</em> </strong>感谢您的宝贵时间！</p></div></div>    
</body>
</html>
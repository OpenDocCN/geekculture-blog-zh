<html>
<head>
<title>A 2021 Guide to improving CNNs-Training strategies: Training Methodology &amp; Regularization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2021年CNN培训策略改进指南:培训方法与规范化</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/a-2021-guide-to-improving-cnns-training-strategies-training-methodology-regularization-b4af696f854d?source=collection_archive---------13-----------------------#2021-06-26">https://medium.com/geekculture/a-2021-guide-to-improving-cnns-training-strategies-training-methodology-regularization-b4af696f854d?source=collection_archive---------13-----------------------#2021-06-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="ac89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这将是我在<em class="jd"> </em>上的第五篇文章，我的系列文章<em class="jd">是2021年改进CNN的指南。</em></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/4cb0967937a0783429f3e67a1f7e3f60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JxB_IDa_IRiZwye3"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Photo by <a class="ae ju" href="https://unsplash.com/@aahubs?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Aaron Huber</a> on <a class="ae ju" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="2f4c" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">培训战略的影响</h2><p id="5ae0" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">正则化等训练技术是打破深度学习网络极限的关键。尽管与网络体系结构相比，它们在研究论文中通常不太受重视，但现代网络中的很大一部分改进是由于改进了训练技术，包括正则化技术。</p><p id="0996" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，一篇全新的论文[1]表明，通过现代培训策略，传统的ResNets可以改进为优于EfficientNet。结果表明，这样的训练策略对于成功的深度学习系统至关重要。通过新架构获得的性能收益通常与改进的培训策略相混淆。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kv"><img src="../Images/a7935fd0ff8f43aa3ad0da181aa937d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_cm5L6i7zNhsv7sAZpGBYg.png"/></div></div></figure><p id="74f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将回顾一些流行的深度学习现代训练策略的细节，并看看它们如何提高性能或执行正则化。本帖介绍的方法多在[1]和[2]中提出。下图显示了训练技术对ResNet模型影响的消融研究。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kw"><img src="../Images/8019f84d2c1f721faedddf881b9ff274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*mdgVcaXSUgoVw-mrDN3LyA.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx">Table 1</figcaption></figure><h2 id="0ebf" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">优化者</h2><p id="5b12" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">优化器可以被定义为在给定梯度的情况下修改参数以最佳地最小化损失函数的函数。一些经典的优化器包括SGD、Momentum、RMSProp和Adam。也有很多相对较新的优化，如阿达格拉德，阿达信仰。在之前的文章中，我们回顾并比较了优化器的速度和泛化性能。</p><p id="dbe9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更好的优化器可以显著提高速度和性能。</p><h2 id="baff" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">学习率衰减</h2><p id="065d" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">包括SGDR在内的学习率计划会对训练速度和最终准确性产生显著影响。SGDR通常被称为<em class="jd">余弦学习率衰减</em>。</p><p id="6b22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">优化器和SGDR在我之前关于现代优化器的文章中解释过。</p><h1 id="f283" class="kx jw hi bd jx ky kz la kb lb lc ld kf le lf lg ki lh li lj kl lk ll lm ko ln bi translated">正规化</h1><p id="630e" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">深度学习中的正则化是通过对训练数据进行训练，使深度学习模型能够推广到每个数据的过程或技术。它被测量为在训练数据中测量的损失和在验证数据中测量的损失之间的差异。</p><h2 id="a095" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">辍学[2]</h2><p id="82ab" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">在训练中，辍学者会随机丢弃NN中的单位。该过程旨在通过防止单元过度共同适应来解决过度适应问题。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lo"><img src="../Images/79fc6d7b99dfd01ba7e7ea38f81540d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*Z50VhBW1521HNCWQDeDfQw.png"/></div></figure><p id="cfab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上图所述，辍学指的是在神经网络中放弃单元。当删除一个单元时，输出连接被设置为0。使用为每层定义的固定概率p来丢弃每个单元。我们不会在测试时间上减少任何权重，因为这将接近所有可能模型的平均值。</p><p id="4e0d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在实践中，Dropout表现出很好的性能，通常比表1所示的权重衰减正则化要好。可以以各种方式将分出应用于网络的任何层(例如，conv层中的信道式分出)，但是对于实际的CNN，大多数情况下只在最终的全连接层中使用。</p><h2 id="b7f8" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">重量衰减/重量尺寸损失(L1/L2标准化)</h2><p id="5854" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">重量衰减和重量尺寸惩罚都具有防止NNs中的大重量的相同目标。当权重变大时，权重将使训练数据过拟合，验证数据的微小变化可能会使模型的预测产生很大差异。这种对输入的微小变化的强烈依赖是过度适应的。</p><p id="1a37" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">权重衰减将每个权重乘以模型的衰减因子λ(0 </p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lp"><img src="../Images/cf2a49fd8d85ff0b3dc3d98faecfc880.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/0*3b3MeK_MPfVS_OcE.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx"><a class="ae ju" href="https://towardsdatascience.com/weight-decay-l2-regularization-90a9e17713cd" rel="noopener" target="_blank">https://towardsdatascience.com/weight-decay-l2-regularization-90a9e17713cd</a></figcaption></figure><p id="f991" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Whereas, weight size penalty methods such as L1 or L2 normalization appends an additional term to the loss to make the model favor smaller weights. L1 normalization is when we add the weights and L2 normalization's absolute values when we add the square of the weights to the loss.</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lq"><img src="../Images/897373d558f80cf39f15ad426a2a3d33.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*ibpUmkSAaGgNzY_YfAJNjw.png"/></div></figure><p id="1954" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">L2 normalization does the same effect to the weights as weight decay when using an SGD optimizer. The weight penalty method is known as a classical tool to reduce overfitting in machine learning.</p><h2 id="7ef1" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">Label smoothing[3, 4]</h2><p id="087d" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">Label smoothing is applied to classification problems using the cross-entropy loss. By using label smoothing, the training objective of predicting either 0 or 1 changes to predicting either α/(c-1) or (1-α) where c denotes the number of classes.</p><p id="50d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">The calibration of a model is when the prediction represents the probability of the class accurately, instead of just becoming larger to minimize the error. Label smoothing prevents overconfidence(e.g. predicting confidence of 0.9999999) which can harm the <em class="jd">校准</em>。[3]表明标注平滑有助于更好地聚类和概括中间层的表示。</p><p id="8acf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个简单的技巧可以给模型性能带来意想不到的好处。</p><h2 id="9d1b" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">随机深度〔5〕</h2><p id="9ad1" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">随机深度训练短网络，并在测试时使用深度网络进行正则化。在从初始网络开始之后，随机丢弃层的子集，并用身份函数替换。我经常将随机深度称为ResNets中的逐层丢弃。</p><p id="d5de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用生存概率来丢弃每一层，生存概率被定义为该层的指数l的线性函数。早期的层(小l)将可靠地存在，而后面的层(大l)更频繁地被替换为相同操作。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lr"><img src="../Images/6830918399d85b834d9d953cbcf985e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*pwjMRo-Xt75-cJLMmrrSwQ.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ls"><img src="../Images/e755dd5c54df9ba48b418f1cfe956c8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aWu_P8OKdfAlSQjEuG3fFA.png"/></div></div></figure><p id="0612" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机深度减少了训练时间和过拟合，类似于辍学。</p><h2 id="2d5f" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">提前停止</h2><p id="e8c8" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">早期停止是当模型开始过度拟合时(当验证损失增加时)停止训练的技术。当模型的正则化较弱时，早期停止特别有用，因为过度拟合会彻底发生。我们可以在表1中观察到，当正则化不充分时，较长的训练过拟合。</p><p id="1f2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在实践中，建议不要使用早期停止，而是加强正则化，因为早期停止不会带来性能提升。</p><h2 id="d1f8" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">数据扩充</h2><blockquote class="lt lu lv"><p id="2b17" class="if ig jd ih b ii ij ik il im in io ip lw ir is it lx iv iw ix ly iz ja jb jc hb bi translated">数据分析中的数据扩充是用于增加数据量的技术，通过添加已有数据的稍微修改的副本或从现有数据新创建的合成数据。它充当正则化器，有助于在训练机器学习模型时减少过拟合。——<a class="ae ju" href="https://en.wikipedia.org/wiki/Data_augmentation" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></blockquote><p id="213d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不同类型的数据(例如图像、信号/语音、文本)有不同的数据扩充技术。图像增强技术包括随机操作，例如</p><ul class=""><li id="df2d" class="lz ma hi ih b ii ij im in iq mb iu mc iy md jc me mf mg mh bi translated">种植</li><li id="abd0" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">变焦</li><li id="5370" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">轻弹</li><li id="22d3" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">旋转</li><li id="5451" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">翻译</li><li id="f71b" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">调整大小</li><li id="4862" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">对比/配色方案</li></ul><p id="9d07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据扩充技术通常用于间接增加数据的数量，这增加了神经网络的泛化能力。数据扩充对提高模型正则化的影响是相当直观的。</p><h2 id="ae75" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">随机增强/自动增强</h2><p id="2b31" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">这种方法提出随机搜索和强化学习来搜索最佳工作数据扩充算法。有些方法实际上是不可行的(自动增强需要15，000个GPU小时)，但预先训练的增强功能可以普遍应用于您的工作以提高性能，或者可以进行相对可行的搜索(例如RandAugment)。</p><h2 id="caba" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">混合正则化[6]</h2><p id="6a82" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">Mixup是一种数据扩充技术，它从训练数据中生成随机图像对的加权组合。给定两个图像及其标签，通过线性插值将两个图像和标签混合来生成合成图像。比例因子是从贝塔分布中取样的。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mn"><img src="../Images/bf9a092606a94fdea6484a85e96a1b20.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*oABxjxdaa6dIWxuaXQq6vw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx"><a class="ae ju" href="https://paperswithcode.com/method/mixup" rel="noopener ugc nofollow" target="_blank">https://paperswithcode.com/method/mixup</a></figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mo"><img src="../Images/148c06967b8081e9c8f1389f5ac62863.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UObNYAlHbOEzrodH.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx"><a class="ae ju" href="https://crazyoscarchang.github.io/2020/09/27/revisiting-mixup/" rel="noopener ugc nofollow" target="_blank">https://crazyoscarchang.github.io/2020/09/27/revisiting-mixup/</a></figcaption></figure><p id="e417" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Mixup旨在减少过度拟合并增加对对立样本的鲁棒性。研究还表明，混合可以减少对抗训练中的不稳定性。混合正则化也直接改进了模型的校准。</p><p id="a546" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，Mixup仅限于涉及类别分类的特定任务范围，或者至少在应用于不太明显的任务时需要改进，如对象检测、回归，甚至无监督/半监督设置。我也很好奇在Mixup中混合3+图片是否可用，是否有用。</p><h1 id="1966" class="kx jw hi bd jx ky kz la kb lb lc ld kf le lf lg ki lh li lj kl lk ll lm ko ln bi translated">结论</h1><p id="6398" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">在这篇文章中，我们回顾了通过防止过度拟合来提高深度神经网络性能的各种技术。正如[1]中所讨论的，训练技术对深度学习的最终性能有非常大的影响。通过利用本文和本系列中讨论的许多技术，我们将能够将深度学习的性能提升到一个新的水平。</p><h1 id="aab5" class="kx jw hi bd jx ky kz la kb lb lc ld kf le lf lg ki lh li lj kl lk ll lm ko ln bi translated">参考</h1><p id="49c3" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">[1]贝洛，I .、费杜斯，w .、杜，x .、库布克，E. D .、斯里尼瓦斯，a .、林，T. Y .、… &amp;佐夫，B. (2021)。改进的训练和扩展策略。<em class="jd"> arXiv预印本arXiv:2103.07579 </em>。</p><p id="a44f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2]n . Srivastava，h . hint on，g . Krizhevsky，a .，Sutskever，I .，&amp; Salakhutdinov，R. (2014年)。辍学:防止神经网络过度拟合的简单方法。<em class="jd">《机器学习研究杂志》</em>，<em class="jd"> 15 </em> (1)，1929–1958。</p><p id="059d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[3]米勒、科恩布利思和辛顿(2019年)。标注平滑何时有帮助？。<em class="jd"> arXiv预印本arXiv:1906.02629 </em>。</p><p id="ff0f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[4]佩雷拉，g .塔克，g .乔洛夫斯基，j .凯泽，，&amp; Hinton，G. (2017)。通过惩罚置信输出分布来调整神经网络。<em class="jd"> arXiv预印本arXiv:1701.06548 </em>。</p><p id="be1f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[5]黄，g，孙，y，刘，z，Sedra，d .，，温伯格，K. Q. (2016年10月)。具有随机深度的深度网络。在<em class="jd">欧洲计算机视觉会议</em>(第646–661页)。斯普林格，查姆。</p><p id="5a4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[6]张，h .，西塞，m .，多芬，Y. N .，&amp;洛佩斯-帕兹，D. (2017)。混淆:超越经验风险最小化。<em class="jd"> arXiv预印本arXiv:1710.09412 </em>。</p></div></div>    
</body>
</html>
<html>
<head>
<title>Ordinary Least Squares Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">普通最小二乘回归</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/ordinary-least-squares-regression-41f40400a58d?source=collection_archive---------6-----------------------#2021-04-12">https://medium.com/geekculture/ordinary-least-squares-regression-41f40400a58d?source=collection_archive---------6-----------------------#2021-04-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/318a2305e855a085bfbe52bf8cc3e6f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*oIz6x7vdP9N8iiYBVz0rxw.png"/></div></figure><blockquote class="im in io"><p id="5cc5" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">本文是<a class="ae jo" href="https://arunaddagatla.medium.com/linear-regression-in-a-nutshell-1714d5665fd2" rel="noopener">线性回归的一部分，概括地说就是</a></p></blockquote><p id="e077" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">普通最小二乘回归(OLS)通常称为线性回归算法，是一种用于估计线性回归模型中未知参数的线性最小二乘法。</p><p id="5c94" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">在具有“n”个解释变量的模型的情况下，OLS回归方程给出为:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es js"><img src="../Images/af32431e6ce7f6a11cfdb73e6157a4e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*NSLwRN6seRo_yRrPAaOXbA.jpeg"/></div></figure><p id="0fc1" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">在哪里，</p><ul class=""><li id="c32a" class="jx jy hi is b it iu ix iy jp jz jq ka jr kb jn kc kd ke kf bi translated">y是因变量</li><li id="49ef" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated">β₀是模型的截距</li><li id="cbf9" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated">xᵢ对应于模型的iᵗʰ解释变量</li><li id="fca5" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated">ε是均值和方差均为零的随机误差σ</li></ul><p id="b20e" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">在OLS，最小平方代表最小平方误差或SSE(误差平方和)。</p><blockquote class="im in io"><p id="9a42" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">模型的误差越小，解释力越强。</p></blockquote><p id="aa04" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">因此，该方法旨在找到使误差平方和最小的线。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kl"><img src="../Images/83d2e40341bf96ee1cb65fd7eb6baaa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YYsA3xMB1nIgwgAUYsJjdA.jpeg"/></div></div></figure><p id="9a61" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">我们可以找到许多符合数据的直线，但OLS会确定误差最小的直线。</p><p id="0fa8" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><em class="ir">从图形上看，它是同时最接近所有点的一个</em></p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kl"><img src="../Images/e2ef14f9d2da67ee63aa3a449f98ab61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o4ev3cj5kCIMSfMpo5h86w.jpeg"/></div></div></figure><p id="ab6a" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">这种系统通常没有精确解，因此目标是找到“最佳”拟合方程的系数β。</p><p id="1b02" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><strong class="is hj"> <em class="ir">简单线性回归</em> </strong></p><p id="417b" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">对于简单的线性回归模型，计算简单。考虑简单线性回归方程:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kq"><img src="../Images/5736d9ab1aeb3aefe8b0a0462f5dd5de.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/0*J9zUj7UwSGrUvfq-"/></div><figcaption class="kr ks et er es kt ku bd b be z dx">Equation of simple linear regression</figcaption></figure><p id="d4f6" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">为了计算α和β的值，OLS使用以下等式来最小化误差项:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kv"><img src="../Images/03a43fb808d96539e07c2eb6015e0677.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/0*twPRUjbzeC66xOr8"/></div></figure><p id="5da2" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><strong class="is hj"> <em class="ir">多元线性回归</em> </strong></p><p id="1838" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">对于多元线性回归，计算变得有点复杂。由于在多元回归中有两个以上的维度，我们用高维超平面来表示它们。</p><p id="50ad" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">这是一个最小化问题，我们将利用微积分和线性代数来确定直线的斜率和截距。</p><p id="9995" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><em class="ir">用于查找最佳拟合线的表达式为:</em></p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kw"><img src="../Images/d470a7939a3bb3cafa1f61cd0b81541b.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/0*We_ZHgfO2lYfGGkU"/></div></figure><p id="2d7b" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><em class="ir">其中，</em></p><ul class=""><li id="3c08" class="jx jy hi is b it iu ix iy jp jz jq ka jr kb jn kc kd ke kf bi translated"><em class="ir"> T表示矩阵转置</em></li><li id="3561" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated"><em class="ir"> X表示与因变量特定值相关的所有自变量的值，Xᵢ = xᵢᵗ </em></li><li id="85f0" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated">y表示因变量</li><li id="44b5" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated">使这个误差平方和最小的b值称为β的OLS估计量。</li></ul><p id="7517" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">假设<em class="ir"> b </em>是<em class="ir"> β </em>的“候选”值。量(<em class="ir">y</em>ᵢ<em class="ir">x</em>ᵢᵗ<em class="ir">b)</em>，称为<strong class="is hj">残差</strong>用于<em class="ir"> i </em> ᵗʰ观测，测量数据点(<em class="ir"> x </em> ᵢ，<em class="ir"> y </em> ᵢ)与超平面<em class="ir">y</em>=<em class="ir">x</em>ᵗ<em class="ir">b</em>之间的垂直距离，以及</p><blockquote class="im in io"><p id="3106" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">观测值的<strong class="is hj">残差</strong>是感兴趣的量的观测值和<em class="hi">估计值</em>之间的差。</p></blockquote><p id="7ca4" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">我们可以尝试在纸上最小化误差的平方和，但对于更大的数据集，这几乎是不可能的。</p><p id="6834" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">如今，回归分析是通过软件和编程语言如SAS、Excel、Python和r。</p><p id="4d57" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">确定回归线还有其他方法。在不同的上下文中，它们通常是首选的。</p><p id="5c55" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><em class="ir">其中有:</em></p><ul class=""><li id="0036" class="jx jy hi is b it iu ix iy jp jz jq ka jr kb jn kc kd ke kf bi translated"><em class="ir">广义最小二乘法</em></li><li id="061b" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated"><em class="ir">最大似然估计</em></li><li id="922a" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated"><em class="ir">贝叶斯回归</em></li><li id="b2cc" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated"><em class="ir">内核回归</em></li><li id="bf61" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated"><em class="ir">高斯过程回归</em></li></ul><p id="a8ee" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">然而，对于大多数线性问题，OLS仍然足够强大。</p><h2 id="9f24" class="kx ky hi bd kz la lb lc ld le lf lg lh jp li lj lk jq ll lm ln jr lo lp lq lr bi translated">OLS假设</h2><p id="81f1" class="pw-post-body-paragraph ip iq hi is b it ls iv iw ix lt iz ja jp lu jd je jq lv jh ji jr lw jl jm jn hb bi translated">在进行回归分析之前，有五个不同的OLS假设需要考虑。</p><ol class=""><li id="a4a6" class="jx jy hi is b it iu ix iy jp jz jq ka jr kb jn lx kd ke kf bi translated"><strong class="is hj">线性度</strong></li><li id="8a5f" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn lx kd ke kf bi translated"><strong class="is hj">无内生性</strong></li><li id="5555" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn lx kd ke kf bi translated">正态性和同方差性</li><li id="e9a1" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn lx kd ke kf bi translated"><strong class="is hj">无自相关</strong></li><li id="21e8" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn lx kd ke kf bi translated"><strong class="is hj">无多重共线性</strong></li></ol><h2 id="c2b5" class="kx ky hi bd kz la lb lc ld le lf lg lh jp li lj lk jq ll lm ln jr lo lp lq lr bi translated">线性</h2><p id="e931" class="pw-post-body-paragraph ip iq hi is b it ls iv iw ix lt iz ja jp lu jd je jq lv jh ji jr lw jl jm jn hb bi translated">线性回归假设线性。每个独立变量乘以一个系数，然后求和来预测值。线性回归是最简单的非平凡关系。因为方程是线性的，所以称之为线性的。</p><blockquote class="im in io"><p id="23bd" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">线性意味着因变量和自变量之间必须有线性关系。</p></blockquote><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/eef5abcff58edde079530aba159e821b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*JVf-tE36-DVn61RsSBAwrw.png"/></div></figure><p id="b3c1" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><strong class="is hj"> <em class="ir">检查线性度</em> </strong></p><p id="d93d" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">一种方法是将自变量与因变量进行散点图。如果数据来自看起来像直线的模式，那么线性回归模型是合适的。</p><p id="c140" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><strong class="is hj"> <em class="ir">修正线性</em> </strong></p><ul class=""><li id="1972" class="jx jy hi is b it iu ix iy jp jz jq ka jr kb jn kc kd ke kf bi translated">运行非线性回归</li><li id="399f" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated">指数变换</li><li id="a439" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated">对数变换</li></ul><h2 id="e5be" class="kx ky hi bd kz la lb lc ld le lf lg lh jp li lj lk jq ll lm ln jr lo lp lq lr bi translated">没有内生性</h2><p id="b632" class="pw-post-body-paragraph ip iq hi is b it ls iv iw ix lt iz ja jp lu jd je jq lv jh ji jr lw jl jm jn hb bi translated">它指的是禁止自变量和误差之间的联系。</p><p id="9147" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><strong class="is hj"> <em class="ir">数学上表示为:</em> </strong></p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/17707520ee5c9bdba0b5065fa6e7bf1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*Ui93xnoJZ365RUOU2aIM0Q.png"/></div></figure><p id="6a64" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">在这种情况下，误差项与观测值和预测值之差的总和与独立变量相关。这个问题被称为<strong class="is hj"> <em class="ir">省略变量偏差</em> </strong>。</p><p id="8b72" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">当相关变量不包括在分析中时，省略变量偏差被引入。</p><blockquote class="im in io"><p id="ec9a" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">基本上，没有被模型解释的一切都进入了错误。</p></blockquote><p id="9e2e" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">所以，</p><ul class=""><li id="8d76" class="jx jy hi is b it iu ix iy jp jz jq ka jr kb jn kc kd ke kf bi translated">不正确地排除一个变量会导致有偏见的和违反直觉的估计，这对回归分析是有害的。</li><li id="50ed" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated">一个变量的不正确包含会导致低效的估计，这种估计不会使回归产生偏差，人们可以丢弃这些变量。</li></ul><p id="b124" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><strong class="is hj"> <em class="ir">修正为内生性</em> </strong></p><p id="f75c" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">遗漏变量偏差因问题而异。它总是偷偷摸摸的，要克服它，你必须有经验和先进的知识。</p><h2 id="3210" class="kx ky hi bd kz la lb lc ld le lf lg lh jp li lj lk jq ll lm ln jr lo lp lq lr bi translated">正态性和同方差性</h2><p id="cb9c" class="pw-post-body-paragraph ip iq hi is b it ls iv iw ix lt iz ja jp lu jd je jq lv jh ji jr lw jl jm jn hb bi translated">1.<strong class="is hj"> <em class="ir">正态- </em> </strong>我们假设误差项是正态分布的。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/1b2f6ef94efdc2b6821ffc4da7aa140c.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/0*4Ql84kkm2RzY82pV"/></div></figure><p id="437b" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><strong class="is hj"> <em class="ir">误差项不是正态分布怎么办？</em> </strong></p><p id="9d48" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">问题的解决方法是中心极限定理。</p><blockquote class="im in io"><p id="a521" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">中心极限定理指出，如果您有一个均值为μ、标准差为σ的总体，并从替换总体中抽取足够大的随机样本，那么样本均值的分布将近似为正态分布。</p></blockquote><p id="4fd1" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">这个定理使得误差项在默认情况下是正常的。</p><p id="4b02" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><strong class="is hj"> 2。<em class="ir">同方差</em></strong><em class="ir">——同方差</em>表示方差相等。误差项应该具有彼此相等的方差。</p><p id="a8f5" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">考虑一个例子，</p><p id="ca49" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">如果一个人很穷，那么他或她会在食物和其他住宿上花费一定数量的钱。但是一个人越富有，他的支出的可变性就越高。因此异方差是存在的。</p><p id="8277" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><em class="ir">同方差</em>指变量的可变性在数值范围内不相等的情况。这主要是由于数据中存在异常值。</p><blockquote class="im in io"><p id="0560" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">异方差中的异常值是指样本中存在的观察值相对于其他观察值而言是小的还是大的。</p></blockquote><p id="fcef" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><strong class="is hj">异方差修正</strong></p><ul class=""><li id="b7d0" class="jx jy hi is b it iu ix iy jp jz jq ka jr kb jn kc kd ke kf bi translated">检查遗漏的可变偏差</li><li id="9294" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated">寻找异常值，并尝试消除它们</li><li id="1905" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated">对解释变量执行对数转换</li></ul><h2 id="f613" class="kx ky hi bd kz la lb lc ld le lf lg lh jp li lj lk jq ll lm ln jr lo lp lq lr bi translated">无自相关</h2><p id="2dd3" class="pw-post-body-paragraph ip iq hi is b it ls iv iw ix lt iz ja jp lu jd je jq lv jh ji jr lw jl jm jn hb bi translated">无自相关也被称为无序列相关。根据假设，误差不应该是不相关的。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/7aa5af290f5aae3c81f538ae31af5118.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/0*_yp-dqax5JKp2ej2"/></div></figure><p id="9df4" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><strong class="is hj"> <em class="ir">检查自相关</em> </strong></p><ul class=""><li id="c1ce" class="jx jy hi is b it iu ix iy jp jz jq ka jr kb jn kc kd ke kf bi translated">在图上画出所有的残差，并检查模式。如果看不到模式，那么就没有自相关。</li><li id="0cfe" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated"><em class="ir">德宾沃森测试</em> -其值在0到4之间。值为2表示没有自相关。低于1和高于3的值表明存在自相关</li></ul><p id="1b1b" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><strong class="is hj"> <em class="ir">修正为自相关</em> </strong></p><p id="e776" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">自相关的唯一解决方案是避免使用线性回归。</p><blockquote class="im in io"><p id="b95e" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">自相关问题的一个例子是时间序列分析。</p></blockquote><h2 id="3b13" class="kx ky hi bd kz la lb lc ld le lf lg lh jp li lj lk jq ll lm ln jr lo lp lq lr bi translated">无多重共线性</h2><p id="32c8" class="pw-post-body-paragraph ip iq hi is b it ls iv iw ix lt iz ja jp lu jd je jq lv jh ji jr lw jl jm jn hb bi translated"><strong class="is hj">多重共线性</strong>是指多元回归模型中两个以上的解释变量高度线性相关的情况。</p><p id="a198" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">当两个或多个变量高度相关时，我们观察到多重共线性。</p><p id="093b" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">考虑一个等式a = 2 + 5 * b的例子。</p><p id="7b34" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">这个等式可以重新排列为b = (a - 2) / 5。</p><p id="f5c9" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">这里，</p><ul class=""><li id="1619" class="jx jy hi is b it iu ix iy jp jz jq ka jr kb jn kc kd ke kf bi translated">“a”和“b”是两个具有精确线性组合的变量</li><li id="6ac4" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated">因为‘b’可以用‘a’来表示，反之亦然。</li></ul><p id="e556" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">包含“a”和“b”作为解释变量的模型将具有完美的多重共线性。这给我们的回归模型带来了一个大问题，因为系数会被错误地估计。</p><p id="5d26" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated">理由是，如果a可以用b来表示，那么两个都用就没有意义了，我们可以只保留其中一个。</p><p id="ca2a" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><strong class="is hj"> <em class="ir">检查多重共线性</em> </strong></p><ul class=""><li id="4884" class="jx jy hi is b it iu ix iy jp jz jq ka jr kb jn kc kd ke kf bi translated">多重共线性是一个大问题，但也是最容易注意到的问题。</li><li id="c8dc" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated">在建立回归之前，找出两对独立变量之间的相关性。</li></ul><p id="ff44" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><strong class="is hj"> <em class="ir">修复多重共线性</em> </strong></p><ul class=""><li id="b383" class="jx jy hi is b it iu ix iy jp jz jq ka jr kb jn kc kd ke kf bi translated">删除两个特征中的一个</li><li id="2d8d" class="jx jy hi is b it kg ix kh jp ki jq kj jr kk jn kc kd ke kf bi translated">将两个特征转换成一个特征</li></ul></div><div class="ab cl mc md gp me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="hb hc hd he hf"><p id="55a7" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja jp jc jd je jq jg jh ji jr jk jl jm jn hb bi translated"><em class="ir">感谢阅读本文！如果你有任何问题，请在下面留言。请务必关注</em><a class="ae jo" href="https://arunaddagatla.medium.com/" rel="noopener"><em class="ir">@ arunadagatla</em></a><em class="ir">，获取关于数据科学和深度学习的最新文章通知。</em></p><blockquote class="im in io"><p id="173d" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="hi">可以在</em><a class="ae jo" href="https://www.linkedin.com/in/arun-addagatla/" rel="noopener ugc nofollow" target="_blank"><strong class="is hj"><em class="hi">LinkedIn</em></strong></a><em class="hi"/><a class="ae jo" href="https://github.com/arun2728" rel="noopener ugc nofollow" target="_blank"><strong class="is hj"><em class="hi">Github</em></strong></a><em class="hi"/><a class="ae jo" href="https://www.kaggle.com/arun2729" rel="noopener ugc nofollow" target="_blank"><strong class="is hj"><em class="hi">Kaggle</em></strong></a><em class="hi">上与我联系，或者通过访问</em><a class="ae jo" href="https://arunaddagatla.medium.com/" rel="noopener"><strong class="is hj"><em class="hi">【Medium.com</em></strong></a><em class="hi">。</em></p></blockquote></div></div>    
</body>
</html>
<html>
<head>
<title>Classification Using Decision Trees:</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用决策树的分类:</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/classification-using-decision-trees-a434a052d82a?source=collection_archive---------14-----------------------#2021-04-13">https://medium.com/geekculture/classification-using-decision-trees-a434a052d82a?source=collection_archive---------14-----------------------#2021-04-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/ad9d5e60a8c0f13d1d8e17d1113bffd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZVftgNFk3zSeChwZf_A8YQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Decision Tree for deciding what to do based on the deadline.</figcaption></figure><h1 id="59ca" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated"><strong class="ak">那么什么是决策树呢？</strong></h1><p id="1190" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">决策树是一种树状决策结构，其中在每个节点评估一个特征值，并根据评估做出进一步的决策。输出值由<em class="kq">叶节点</em>表示。</p><h2 id="5236" class="kr iv hi bd iw ks kt ku ja kv kw kx je kd ky kz ji kh la lb jm kl lc ld jq le bi translated"><strong class="ak">决策树是如何构建的？</strong></h2><p id="eaf5" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">通过说决策树是如何构造的，我们的意思是我们如何决定特征值将被评估的顺序。</p><p id="bea2" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">我们遵循一种贪婪的方法，同时选择评估特征的顺序，其中首先评估最具信息性的特征<em class="kq">。</em></p><h2 id="a745" class="kr iv hi bd iw ks kt ku ja kv kw kx je kd ky kz ji kh la lb jm kl lc ld jq le bi translated"><strong class="ak">我们如何决定一个特征的信息量？</strong></h2><p id="a05e" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">有不同的方法和算法，基于这些方法和算法，我们可以从不同的特征中判断出哪些信息更多，哪些信息更少。</p><h2 id="4886" class="kr iv hi bd iw ks kt ku ja kv kw kx je kd ky kz ji kh la lb jm kl lc ld jq le bi translated"><strong class="ak">使用昆兰的ID3(迭代去优化器版本3)进行分类:</strong></h2><p id="4b82" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">这里我们计算熵，即每个特征的数据的不确定性。它基于熵越多，不确定性越大，预测标签的有用性就越小的想法，因此我们将认为熵越小的特征信息量就越大。</p><p id="22da" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">计算熵的公式:</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/2d444de8561a68a306ee872e09899a10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*F30U8YwY691ayQCrbbHOTw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">log base 2 as we consider everything is encoded using binary digits. Eqn I</figcaption></figure><p id="e908" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">→样本均匀分布时熵最大。</p><p id="7ed0" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">→pi是数据集拥有不同数据样本的概率。</p><p id="9b8d" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><strong class="ju hj">让我们看看如何使用ID3构建如上所示的决策树:</strong></p><p id="2484" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">以下是决策树的数据集:</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lp"><img src="../Images/50ad21e5eeaf85d8621b0d7eee1aae59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5hsMQbCYoM--V8IrG7NkYA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">dataset for the above decision tree.</figcaption></figure><p id="7a73" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><strong class="ju hj">特点</strong>有期限、党、懒。<strong class="ju hj">标签</strong>:活动</p><p id="54be" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">因此，我们将计算所有特征的信息增益，具有最高信息增益的特征将首先被评估。</p><p id="9809" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">任意特征F的信息增益公式:</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lq"><img src="../Images/fb6ce7282bea9207df37845887483f42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tWPnqPlYZm27_qjmvE-n1g.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Information gain of feature F from dataset S Eqn II</figcaption></figure><p id="a259" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><strong class="ju hj">步骤1:熵</strong> →使用等式I计算不同类别标签(聚会、酒吧、学习、电视)的概率。</p><p id="baba" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><code class="du lr ls lt lu b">P(party) = 5/10 = 1/2 , P(pub) = 1/10 , P(study) = 3/10 , P(tv) = 1/10</code></p><p id="4d0f" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><code class="du lr ls lt lu b"><strong class="ju hj">Entropy(S) = -P(party)log2(P(party))-P(pub)log2(P(pub))-P(study)log2(P(study))-P(tv)log2(P(tv)).</strong></code></p><p id="0b15" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><strong class="ju hj">第二步:选择任意一个特征，对该特征F的每个值F计算熵(Sf) </strong>，然后代入以下公式:</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/65ecc8701872af44feadd3aa94eb96fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*Uf0TgQIGzX32ZxYpiAhxbg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Eqn III</figcaption></figure><p id="d19c" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">例如:让我们考虑这个特征的特征截止时间值是(紧急、接近、无)</p><p id="cf25" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><code class="du lr ls lt lu b">→ |S| = 10 , |S urgent| = 3 , |S none| = 3 , |S near| = 4</code></p><p id="bfc5" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">熵(S urgent) →因为urgent具有标签值(研究，团队)，所以计算研究和团队的概率，并按照步骤1进行操作。</p><p id="523d" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">类似地，计算特征截止时间的其他值的熵，并放入等式III</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/c6e14fb3e1d1169d608c44ca62115278.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x2h7nwCzgwyZQJO7vp3I4A.png"/></div></div></figure><p id="a110" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">现在将它们相加并代入方程II:</p><p id="90bc" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><code class="du lr ls lt lu b">InformationGain(Deadline,S) = Entropy(S) -(0.28+0.6+0.28)</code></p><p id="0127" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><strong class="ju hj">步骤3)对所有其他特征重复步骤2 </strong>，计算它们的信息增益，然后选择具有最高信息增益的特征，因为它将成为树的根，并将首先被评估。</p><p id="c7f5" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">在计算时发现<strong class="ju hj">特征方</strong>具有最高的信息增益，那么它将成为树的根。</p><p id="4db4" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><strong class="ju hj">步骤4)为该特性的每个可能值从节点添加一个分支</strong>。聚会特征有两个值<strong class="ju hj">是和否</strong></p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/49f18cd8e85bc15a6c5b5e1a4fa252c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*myNrcRADDc6BpgPU-ixaUA.png"/></div></div></figure><p id="8044" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><strong class="ju hj">步骤5)现在对每个值进行检查</strong>:</p><p id="2e5e" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">如果所有的例子都有相同的标签→返回带有该标签的叶子。(纯节点)</p><p id="d1ee" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">否则，如果没有要测试的特性→返回带有最常见标签的叶子。(这种情况会导致不纯的节点)</p><p id="567e" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">否则再次转到<strong class="ju hj">步骤2 </strong>并计算移除已评估样本后剩余特征的信息增益。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/6a972dbdec631b256f59e5790d45f187.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*6sMC8O0tVhVhwPBSUQyJgA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Again cal information gain for deadline, lazy</figcaption></figure><p id="8d8a" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">最后，它将生成如上所示的树。</p><p id="cd5b" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">优点:可以处理数据集中的噪声。</p><p id="fae4" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">问题:</p><ol class=""><li id="fd40" class="lz ma hi ju b jv lf jz lg kd mb kh mc kl md kp me mf mg mh bi translated">它不能对有缺失值的样本进行分类。</li><li id="c7ee" class="lz ma hi ju b jv mi jz mj kd mk kh ml kl mm kp me mf mg mh bi translated">它不能处理具有连续值的特征。</li></ol><h2 id="bc48" class="kr iv hi bd iw ks kt ku ja kv kw kx je kd ky kz ji kh la lb jm kl lc ld jq le bi translated">使用CART分类(分类和回归树) :</h2><p id="e1d5" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在这里，我们计算基尼系数，因为它相当于计算预期误差率，如果分类是根据类别分布选择的话。</p><p id="d09c" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">计算基尼系数的公式:公式1</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/f3514a66414909316e4c6c90b60903b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*4BFGoQ6yN-kTAivPdwbiqw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">formula for calculating gini impurity</figcaption></figure><p id="8209" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">N(i) →表示属于<br/>I类的数据点分数。</p><p id="c181" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><strong class="ju hj">让我们看看如何使用CART构建如上所示的决策树:</strong></p><p id="7ed8" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">使用的数据集与我们上面使用的相同</p><p id="c7b1" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">因此，与上面相同，我们将计算所有特征的信息增益，具有最高信息增益的特征将首先被评估。</p><p id="3a12" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">任意特征F的信息增益公式:公式II</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/6a172ea07c53378718e6af6962c45a94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Bkr9F9ft7FzpZZX0xymHnQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Information gain using gini impurity</figcaption></figure><p id="783d" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><strong class="ju hj">步骤1: G(S) </strong> →使用等式I计算不同类别标签(派对、酒吧、书房、电视)的N。</p><p id="bdc6" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><code class="du lr ls lt lu b">N(party) = 5/10 = 1/2 , N(pub) = 1/10 , N(study) = 3/10 , N(tv) = 1/10</code></p><p id="2d6d" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><code class="du lr ls lt lu b">G(S) = 1 -(N(party)²+N(pub)²+N(study)²+N(tv)²) = 1-(0.25+0.01+0.09+0.01) = 1–0.36 = 0.64</code></p><p id="ac67" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><strong class="ju hj">第二步:选择任意一个特征，对该特征F的每个值F计算G(Sf) </strong>，然后代入以下公式:</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/5df9ee68a7653733155c365b7b949b63.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*oe-FgUvlDx48KdcEY5c8gA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Eqn III</figcaption></figure><p id="2a66" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">例如:让我们考虑这个特征的特征截止时间值是(紧急、接近、无)</p><p id="939e" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><code class="du lr ls lt lu b">→ |S| = 10 , |S urgent| = 3 , |S none| = 3 , |S near| = 4</code></p><p id="9bd3" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">G(S urgent) →由于urgent具有标签值(病历报告、当事人),因此计算病历报告和当事人的N，并按照步骤1进行操作。</p><p id="0d51" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">类似地，计算特征截止时间的其他值的基尼系数杂质，并放入公式III。</p><p id="ad7a" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">然后使用等式II计算该特征的信息增益。</p><p id="15d2" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><strong class="ju hj">步骤3)对所有其他特征重复步骤2 </strong>并计算它们的信息增益，然后选择具有最高信息增益的特征，因为它将成为树的根并将首先被评估。</p><p id="f433" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">所有其他步骤与ID3算法相同。</p><p id="b9c6" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">文章结尾。</p><p id="839b" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">感谢阅读。</p><p id="f20a" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">希望有帮助:)</p></div></div>    
</body>
</html>
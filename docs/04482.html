<html>
<head>
<title>Introduction to XGBoost — With Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XGBoost简介—使用Python</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/introduction-to-xgboost-with-python-f654b41baf3b?source=collection_archive---------22-----------------------#2021-06-28">https://medium.com/geekculture/introduction-to-xgboost-with-python-f654b41baf3b?source=collection_archive---------22-----------------------#2021-06-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9553" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">XGBoost作为使用最广泛的公有领域boosting软件之一，是数据科学家必备的技能。</p><p id="2513" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">集合模型已经成为预测建模的标准工具。<em class="jd">增强</em>是一种创建模型集合的通用技术[1]。<em class="jd">增压</em>方法几乎与<em class="jd">装袋</em>同时开发。像装袋一样，boosting通常用于决策树。与需要很少运行的装袋相反，增压在应用中需要非常小心。打个比方，装袋就像丰田凯美瑞一样可靠且维护成本低廉，而增压就像波尔琪一样动力强劲但维护成本高昂。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/8ac910c2d484dd24b8861fb7b98af315.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ywnkRVpMg_WwsgkWq0gZUQ.jpeg"/></div></div></figure><p id="c1f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">boosting背后的哲学就像其他集成学习算法一样:利用许多模型，并使用所有输出的平均值作为最终的预测输出，以获得更高的准确性。在线性回归模型中，通常会检查残差以查看是否可以改进拟合度。Boosting进一步发展了这一概念，并适用于一系列模型，其中每个后续模型都试图最小化前一个模型的误差。增强的一些变体有:<em class="jd"> Adaboost、梯度增强、随机梯度增强</em> [1]。</p><h1 id="3765" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">XGBoost</h1><p id="27e9" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">最广泛使用的公共领域boosting软件是XGBoost，这是一种随机梯度boosting的实现，最初由华盛顿大学的陈天琦和Carlos Guestrin开发[1]。</p><p id="2a0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">调节XGBoost涉及许多参数，但最重要的参数是<code class="du kt ku kv kw b">subsample</code>和<code class="du kt ku kv kw b">eta</code>，前者决定每次迭代中用于训练模型的样本分数，后者决定boosting算法中应用于权重的收缩因子。<code class="du kt ku kv kw b">eta</code>越小，模型过度拟合的可能性就越小。</p><h2 id="17d4" class="kx jr hi bd js ky kz la jw lb lc ld ka iq le lf ke iu lg lh ki iy li lj km lk bi translated">个案研究</h2><p id="6432" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">为了演示XGBoost在实践中的应用，我们使用python来实现使用XGBoost的二进制分类。我们将使用来自<code class="du kt ku kv kw b">xgboost</code>库的<code class="du kt ku kv kw b">XGBClassifier </code>。我们将使用<a class="ae ll" href="https://www.kaggle.com/azeembootwala/titanic" rel="noopener ugc nofollow" target="_blank">泰坦尼克号数据集</a>将乘客分类为死亡或幸存。此外，还增加了4栏，从姓名栏重新设计为标题<em class="jd"> 1到标题</em> 4，表示男性&amp;女性，取决于他们是否结婚(先生、夫人、主人、小姐)。一项额外的分析旨在看看已婚人士或换句话说有社会责任感的人是否有更多的生存本能，这一趋势在两性中是相似的。该数据集由15个预测因子组成，如<em class="jd">性别、票价、p_class、家庭规模、…。</em>目标响应是<em class="jd">幸存。</em>请注意，取值有限的因子变量已经通过一键编码进行了转换。这篇文章的完整代码可以在<a class="ae ll" href="https://colab.research.google.com/drive/1GPgmq2h9MNTVyccRhas00_Zgpg23xckG?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="2b9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了形象化的目的，我们将只使用两个预测器来预测乘客的状态:<code class="du kt ku kv kw b">Age </code>和<code class="du kt ku kv kw b">Fare</code>。作为第一步，我们加载dat并应用分类器。下面的代码就是为此而编写的:</p><pre class="jf jg jh ji fd lm kw ln lo aw lp bi"><span id="9248" class="kx jr hi kw b fi lq lr l ls lt">train_df = pd.read_csv('train_data.csv')<br/>predictors = ['Age', 'Fare']</span><span id="7427" class="kx jr hi kw b fi lu lr l ls lt">outcome = 'Survived'</span><span id="b91d" class="kx jr hi kw b fi lu lr l ls lt">X = train_df[predictors]<br/>y = train_df[outcome]</span><span id="ba82" class="kx jr hi kw b fi lu lr l ls lt">xgb = XGBClassifier(objective='binary:logistic', subsample=.63, use_label_encoder=False, eval_metric='error')</span><span id="4204" class="kx jr hi kw b fi lu lr l ls lt">print(xgb.fit(X, y))</span></pre><p id="a77c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出将是</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lv"><img src="../Images/a2b0401d64c04e55acbd4208b71e7113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*2pzqPRJgp-hSzKBdDUrUhA.png"/></div></figure><p id="2dd2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以看出，<code class="du kt ku kv kw b">objective</code>参数指定了学习任务和相应的学习目标或要使用的自定义目标函数(见下面的注释)。这里，由于我们将目标函数设置为<code class="du kt ku kv kw b">binary:logistic. </code>，XGBoost将使用相应的函数进行优化。子样本设置为0.63，这意味着63%的训练数据用于优化。通过设置<code class="du kt ku kv kw b">eval_metric=’error’</code>，我们迫使XGBoost根据二进制分类错误率优化目标函数。对于<code class="du kt ku kv kw b">use_label_encoder=False</code>，使用<em class="jd"> scikit-learn </em>中的标签编码器对标签进行编码。对于新代码，我们建议您将此参数设置为False。</p><p id="42be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了评估预测模型的性能，我们获取训练数据并将其应用于模型。下面的代码根据两个预测值预测乘客的状态。<code class="du kt ku kv kw b">Truth</code>栏反映数据中的地面实况。</p><pre class="jf jg jh ji fd lm kw ln lo aw lp bi"><span id="8f61" class="kx jr hi kw b fi lq lr l ls lt">xgb_df = X.copy()</span><span id="a920" class="kx jr hi kw b fi lu lr l ls lt">xgb_df['prediction'] = ['Survived' if p == 1 else 'Dead' for p in xgb.predict(X)]</span><span id="5976" class="kx jr hi kw b fi lu lr l ls lt">xgb_df['prob_default'] = xgb.predict_proba(X)[:, 0]</span><span id="791e" class="kx jr hi kw b fi lu lr l ls lt">xgb_df['Truth'] = ['Survived' if p == 1 else 'Dead' for p in y]</span><span id="0a5f" class="kx jr hi kw b fi lu lr l ls lt">print(xgb_df.head())</span></pre><p id="7537" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出如下所示:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lw"><img src="../Images/a5b739fd061377d574022d255bebc468.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*l-db8aCH_4Azy8G-PHrymg.png"/></div></figure><p id="cfdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以看出，在前五个记录中，我们有一个错误分类。为了更好地显示乘客状态的<code class="du kt ku kv kw b">Age</code>和<code class="du kt ku kv kw b">Fare</code>范围，我们</p><pre class="jf jg jh ji fd lm kw ln lo aw lp bi"><span id="30af" class="kx jr hi kw b fi lq lr l ls lt">fig, ax = plt.subplots(figsize=(16, 14))</span><span id="87bf" class="kx jr hi kw b fi lu lr l ls lt">xgb_df.loc[xgb_df.prediction=='Dead'].plot(x='Age', y='Fare', style='*', markerfacecolor='none', markeredgecolor='C1', ax=ax)</span><span id="5a2b" class="kx jr hi kw b fi lu lr l ls lt">xgb_df.loc[xgb_df.prediction=='Survived'].plot( x='Age', y='Fare', style='o', markerfacecolor='none', markeredgecolor='C0', ax=ax)<br/>ax.legend(['Dead', 'Survived']);</span><span id="ad9d" class="kx jr hi kw b fi lu lr l ls lt">ax.set_ylim(0, 0.6)<br/>ax.set_xlabel('Age')<br/>ax.set_ylabel('Fare')</span><span id="493e" class="kx jr hi kw b fi lu lr l ls lt">plt.tight_layout()<br/>plt.show()</span></pre><p id="8dd9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出将是</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lx"><img src="../Images/29642a4366d45c50ca08767574a1a71e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3R0cDazsuVvNueu8BHCZYA.png"/></div></div></figure><p id="7188" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">预测值通过蓝色和橙色标记表示。</p><h2 id="e442" class="kx jr hi bd js ky kz la jw lb lc ld ka iq le lf ke iu lg lh ki iy li lj km lk bi translated">正则化:避免过度拟合</h2><p id="874e" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">盲目应用xgboost会导致模型不稳定，因为<em class="jd">过度拟合</em>训练数据。为了避免过度拟合，可以应用正则化来考虑复杂性。</p><p id="8662" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在xgboost中，可以通过添加一个度量模型复杂性的项来修改成本函数。xgboost中有两个参数来正则化模型:alpha和lambda，分别对应于曼哈顿距离(L1正则化)和平方欧几里德距离(L2正则化)[1]。我们将在xgboost中设置<code class="du kt ku kv kw b">reg_lambda </code>,以应用L2规则。</p><pre class="jf jg jh ji fd lm kw ln lo aw lp bi"><span id="306e" class="kx jr hi kw b fi lq lr l ls lt">test_df = pd.read_csv('test_data.csv')</span><span id="e37c" class="kx jr hi kw b fi lu lr l ls lt">predictors = ['Sex', 'Age', 'Fare', 'Pclass_1','Pclass_2', 'Pclass_3', 'Family_size', 'Title_1', 'Title_2', 'Title_3', 'Title_4', 'Emb_1', 'Emb_2', 'Emb_3']</span><span id="664c" class="kx jr hi kw b fi lu lr l ls lt">outcome = 'Survived'</span><span id="b3c1" class="kx jr hi kw b fi lu lr l ls lt">train_X = train_df[predictors]<br/>train_y = train_df[outcome]</span><span id="d0e3" class="kx jr hi kw b fi lu lr l ls lt">valid_X = test_df[predictors]<br/>valid_y = test_df[outcome]</span><span id="d79c" class="kx jr hi kw b fi lu lr l ls lt">xgb_default = XGBClassifier(objective='binary:logistic', n_estimators=250, max_depth=6, reg_lambda=0, learning_rate=0.1, subsample=1, use_label_encoder=False, eval_metric='error')</span><span id="3ec6" class="kx jr hi kw b fi lu lr l ls lt">xgb_default.fit(train_X, train_y)</span><span id="aa0d" class="kx jr hi kw b fi lu lr l ls lt">xgb_penalty = XGBClassifier(objective='binary:logistic', n_estimators=250, max_depth=6, reg_lambda=1000, learning_rate=0.1, subsample=0.63, use_label_encoder=False, eval_metric='error')</span><span id="30ff" class="kx jr hi kw b fi lu lr l ls lt">results = []</span><span id="c703" class="kx jr hi kw b fi lu lr l ls lt">for ntree_limit in range(1, 250):</span><span id="7b2f" class="kx jr hi kw b fi lu lr l ls lt">train_default = xgb_default.predict_proba(train_X, ntree_limit=ntree_limit)[:, 1]</span><span id="991e" class="kx jr hi kw b fi lu lr l ls lt">train_penalty = xgb_penalty.predict_proba(train_X, ntree_limit=ntree_limit)[:, 1]</span><span id="54bf" class="kx jr hi kw b fi lu lr l ls lt">pred_default = xgb_default.predict_proba(valid_X, ntree_limit=ntree_limit)[:, 1]</span><span id="cd7e" class="kx jr hi kw b fi lu lr l ls lt">pred_penalty = xgb_penalty.predict_proba(valid_X, ntree_limit=ntree_limit)[:, 1]</span><span id="6796" class="kx jr hi kw b fi lu lr l ls lt">results.append({</span><span id="db34" class="kx jr hi kw b fi lu lr l ls lt">'iterations': ntree_limit,</span><span id="01e3" class="kx jr hi kw b fi lu lr l ls lt">'default train': np.mean(abs(train_y - train_default) &gt; 0.5),</span><span id="aede" class="kx jr hi kw b fi lu lr l ls lt">'penalty train': np.mean(abs(train_y - train_penalty) &gt; 0.5),</span><span id="917c" class="kx jr hi kw b fi lu lr l ls lt">'default test': np.mean(abs(valid_y - pred_default) &gt; 0.5),</span><span id="7700" class="kx jr hi kw b fi lu lr l ls lt">'penalty test': np.mean(abs(valid_y - pred_penalty) &gt; 0.5),</span><span id="591b" class="kx jr hi kw b fi lu lr l ls lt">})</span><span id="ed3e" class="kx jr hi kw b fi lu lr l ls lt">results = pd.DataFrame(results)</span><span id="ae37" class="kx jr hi kw b fi lu lr l ls lt">ax = results.plot(x='iterations', y='default test')<br/>results.plot(x='iterations', y='penalty test', ax=ax)<br/>results.plot(x='iterations', y='default train', ax=ax)<br/>results.plot(x='iterations', y='penalty train', ax=ax)<br/>plt.show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ly"><img src="../Images/d11d81637db643e3f1b32e849cb80ce5.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*vetiJNnKL_gYlg_9A47t3g.png"/></div></figure><p id="bde8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以看出，随着迭代的增加，训练数据的误差减少，测试增加。这可能是由于正则化参数的错误调整。我们可以通过交叉验证来解决这个问题。</p><h2 id="b19c" class="kx jr hi bd js ky kz la jw lb lc ld ka iq le lf ke iu lg lh ki iy li lj km lk bi translated">超参数和交叉验证</h2><p id="704e" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">我们可以迭代超参数的不同值，并选择具有最小验证误差的值。下面的代码将用于此目的:</p><pre class="jf jg jh ji fd lm kw ln lo aw lp bi"><span id="1c65" class="kx jr hi kw b fi lq lr l ls lt">idx = np.random.choice(range(5), size=len(X), replace=True)</span><span id="716c" class="kx jr hi kw b fi lu lr l ls lt">error = []</span><span id="1049" class="kx jr hi kw b fi lu lr l ls lt">for eta, max_depth in product([0.1, 0.5, 0.9], [3, 6, 9]):</span><span id="b4c0" class="kx jr hi kw b fi lu lr l ls lt">xgb = XGBClassifier(objective='binary:logistic', n_estimators=250, max_depth=max_depth, learning_rate=eta,<br/>use_label_encoder=False, eval_metric='error')<br/>cv_error = []</span><span id="d4fe" class="kx jr hi kw b fi lu lr l ls lt">for k in range(5):<br/>fold_idx = idx == k</span><span id="8c37" class="kx jr hi kw b fi lu lr l ls lt">train_X = X.loc[~fold_idx]; train_y = y[~fold_idx]<br/>valid_X = X.loc[fold_idx]; valid_y = y[fold_idx]</span><span id="11a2" class="kx jr hi kw b fi lu lr l ls lt">xgb.fit(train_X, train_y)</span><span id="97c4" class="kx jr hi kw b fi lu lr l ls lt">pred = xgb.predict_proba(valid_X)[:, 1]</span><span id="f731" class="kx jr hi kw b fi lu lr l ls lt">cv_error.append(np.mean(abs(valid_y - pred) &gt; 0.5))</span><span id="a752" class="kx jr hi kw b fi lu lr l ls lt">error.append({</span><span id="d09e" class="kx jr hi kw b fi lu lr l ls lt">'eta': eta,</span><span id="f8c2" class="kx jr hi kw b fi lu lr l ls lt">'max_depth': max_depth,</span><span id="a541" class="kx jr hi kw b fi lu lr l ls lt">'avg_error': np.mean(cv_error)</span><span id="d995" class="kx jr hi kw b fi lu lr l ls lt">})</span><span id="2203" class="kx jr hi kw b fi lu lr l ls lt">print(error[-1])<br/>errors = pd.DataFrame(error)<br/>print(errors.pivot_table(index='eta', columns='max_depth', values='avg_error') * 100)</span></pre><p id="d6bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不同超参数值的输出如下</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ly"><img src="../Images/162e606772f091d18beb1894f3cd81e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*cvEQecwA82NSu9WiYDA9Bw.png"/></div></figure><p id="c391" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上表可以看出，<code class="du kt ku kv kw b">eta=0.1</code>和<code class="du kt ku kv kw b">max_depth=3</code>的误差最小。</p><h1 id="0b92" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">参考</h1><p id="524b" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">[1]布鲁斯、彼得、安德鲁·布鲁斯和彼得·格德克。<em class="jd">数据科学家实用统计学:使用R和Python的50多个基本概念</em>。奥莱利媒体，2020。</p></div></div>    
</body>
</html>
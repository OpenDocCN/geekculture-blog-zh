<html>
<head>
<title>Simple Chatbot using BERT and Pytorch: Part 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用BERT和Pytorch的简单聊天机器人:第3部分</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/simple-chatbot-using-bert-and-pytorch-part-3-a6832c50b8d1?source=collection_archive---------5-----------------------#2021-06-27">https://medium.com/geekculture/simple-chatbot-using-bert-and-pytorch-part-3-a6832c50b8d1?source=collection_archive---------5-----------------------#2021-06-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="50ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文分为三个部分。</p><p id="e762" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">零件(1/3): <a class="ae jd" rel="noopener" href="/@shrinidhi.rm1990/simple-chatbot-using-bert-and-pytorch-part-1-2735643e0baa">简介及安装</a></p><p id="a1e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">部分(2/3): <a class="ae jd" rel="noopener" href="/@shrinidhi.rm1990/simple-chatbot-using-bert-and-pytorch-part-2-ef48506a4105">数据准备</a></p><p id="07f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第(3/3)部分:<a class="ae jd" rel="noopener" href="/@shrinidhi.rm1990/simple-chatbot-using-bert-and-pytorch-part-3-a6832c50b8d1">模型微调</a></p><p id="aecc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上一篇文章中，我们看到了Transformer和Pytorch概念的简要介绍。我们安装了所有必要的库，并为模型训练准备了数据。现在让我们微调模型，看看结果。</p><h2 id="a540" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">【计算机】优化程序</h2><p id="c3b0" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">使用优化器，我们减少了通过网络反向传播期间的损失。</p><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="ea15" class="je jf hi kj b fi kn ko l kp kq">from transformers import AdamW</span><span id="9c03" class="je jf hi kj b fi kr ko l kp kq"># define the optimizer<br/>optimizer = AdamW(model.parameters(), lr = 1e-3)</span></pre><h2 id="aed9" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">查找类别权重</h2><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="b2dd" class="je jf hi kj b fi kn ko l kp kq">from sklearn.utils.class_weight import compute_class_weight</span><span id="503d" class="je jf hi kj b fi kr ko l kp kq">#compute the class weights<br/>class_wts = compute_class_weight(‘balanced’, np.unique(train_labels), train_labels)</span><span id="561e" class="je jf hi kj b fi kr ko l kp kq">print(class_wts)</span></pre><figure class="ke kf kg kh fd kt er es paragraph-image"><div class="er es ks"><img src="../Images/77ae2903a3366521fdfccc6e83a6ad5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*jP6xcE2o1B18S4rMuZeCkA.png"/></div></figure><h2 id="32da" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">计算误差时平衡重量</h2><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="ccfe" class="je jf hi kj b fi kn ko l kp kq"># convert class weights to tensor<br/>weights= torch.tensor(class_wts,dtype=torch.float)<br/>weights = weights.to(device)</span><span id="447a" class="je jf hi kj b fi kr ko l kp kq"># loss function<br/>cross_entropy = nn.NLLLoss(weight=weights) </span></pre><h2 id="58c2" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">设置纪元</h2><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="e282" class="je jf hi kj b fi kn ko l kp kq"># empty lists to store training and validation loss of each epoch<br/>train_losses=[]</span><span id="20df" class="je jf hi kj b fi kr ko l kp kq"># number of training epochs<br/>epochs = 200</span><span id="c6e6" class="je jf hi kj b fi kr ko l kp kq"># We can also use learning rate scheduler to achieve better results<br/>lr_sch = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)</span></pre><h2 id="ac39" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">微调模型</h2><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="9a42" class="je jf hi kj b fi kn ko l kp kq"># function to train the model<br/>def train():<br/>  <br/>  model.train()</span><span id="644d" class="je jf hi kj b fi kr ko l kp kq">  total_loss = 0<br/>  <br/>  # empty list to save model predictions<br/>  total_preds=[]<br/>  <br/>  # iterate over batches<br/>  for step,batch in enumerate(train_dataloader):<br/>    <br/>    # progress update after every 50 batches.<br/>    if step % 50 == 0 and not step == 0:<br/>      print('  Batch {:&gt;5,}  of  {:&gt;5,}.'.format(step,    len(train_dataloader)))</span><span id="de8b" class="je jf hi kj b fi kr ko l kp kq">    # push the batch to gpu<br/>    batch = [r.to(device) for r in batch] <br/>    sent_id, mask, labels = batch</span><span id="e88d" class="je jf hi kj b fi kr ko l kp kq">    # get model predictions for the current batch<br/>    preds = model(sent_id, mask)</span><span id="b1a6" class="je jf hi kj b fi kr ko l kp kq">    # compute the loss between actual and predicted values<br/>    loss = cross_entropy(preds, labels)</span><span id="7972" class="je jf hi kj b fi kr ko l kp kq">    # add on to the total loss<br/>    total_loss = total_loss + loss.item()</span><span id="4156" class="je jf hi kj b fi kr ko l kp kq">    # backward pass to calculate the gradients<br/>    loss.backward()</span><span id="5c60" class="je jf hi kj b fi kr ko l kp kq">    # clip the the gradients to 1.0. It helps in preventing the    exploding gradient problem<br/>    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)</span><span id="b376" class="je jf hi kj b fi kr ko l kp kq">    # update parameters<br/>    optimizer.step()</span><span id="dfc3" class="je jf hi kj b fi kr ko l kp kq">    # clear calculated gradients<br/>    optimizer.zero_grad()<br/>  <br/>    # We are not using learning rate scheduler as of now<br/>    # lr_sch.step()</span><span id="5cd3" class="je jf hi kj b fi kr ko l kp kq">    # model predictions are stored on GPU. So, push it to CPU<br/>    preds=preds.detach().cpu().numpy()</span><span id="a4d0" class="je jf hi kj b fi kr ko l kp kq">    # append the model predictions<br/>    total_preds.append(preds)</span><span id="bf0d" class="je jf hi kj b fi kr ko l kp kq"># compute the training loss of the epoch<br/>avg_loss = total_loss / len(train_dataloader)<br/>  <br/># predictions are in the form of (no. of batches, size of batch, no. of classes).<br/># reshape the predictions in form of (number of samples, no. of classes)<br/>total_preds  = np.concatenate(total_preds, axis=0)</span><span id="56cb" class="je jf hi kj b fi kr ko l kp kq">#returns the loss and predictions<br/>return avg_loss, total_preds</span></pre><h2 id="762a" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">开始模型训练</h2><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="1cab" class="je jf hi kj b fi kn ko l kp kq">for epoch in range(epochs):<br/>     <br/>    print('\n Epoch {:} / {:}'.format(epoch + 1, epochs))<br/>    <br/>    #train model<br/>    train_loss, _ = train()<br/>    <br/>    # append training and validation loss<br/>    train_losses.append(train_loss)</span><span id="a711" class="je jf hi kj b fi kr ko l kp kq">    # it can make your experiment reproducible, similar to set  random seed to all options where there needs a random seed.</span><span id="0aad" class="je jf hi kj b fi kr ko l kp kq">    torch.backends.cudnn.deterministic = True<br/>    torch.backends.cudnn.benchmark = False</span><span id="4e3c" class="je jf hi kj b fi kr ko l kp kq">print(f'\nTraining Loss: {train_loss:.3f}')</span></pre><figure class="ke kf kg kh fd kt er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es kw"><img src="../Images/4400190ce3719804e608404fb45772b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yVwImmQO9NIV-DEXtsEtdA.png"/></div></div></figure><p id="25e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">梯度损失曲线</strong></p><figure class="ke kf kg kh fd kt er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es lb"><img src="../Images/c7d5e622135c004956abcf6a1464de36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ITljH2CjKgdl_KnDU-XWgQ.png"/></div></div></figure><h2 id="1d78" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">获得测试数据的预测</h2><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="08f7" class="je jf hi kj b fi kn ko l kp kq">def get_prediction(str):<br/> str = re.sub(r’[^a-zA-Z ]+’, ‘’, str)<br/> test_text = [str]<br/> model.eval()<br/> <br/> tokens_test_data = tokenizer(<br/> test_text,<br/> max_length = max_seq_len,<br/> pad_to_max_length=True,<br/> truncation=True,<br/> return_token_type_ids=False<br/> )</span><span id="a0f1" class="je jf hi kj b fi kr ko l kp kq"> test_seq = torch.tensor(tokens_test_data[‘input_ids’])<br/> test_mask = torch.tensor(tokens_test_data[‘attention_mask’])<br/> <br/> preds = None</span><span id="577b" class="je jf hi kj b fi kr ko l kp kq"> with torch.no_grad():<br/>   preds = model(test_seq.to(device), test_mask.to(device))</span><span id="3c82" class="je jf hi kj b fi kr ko l kp kq"> preds = preds.detach().cpu().numpy()<br/> preds = np.argmax(preds, axis = 1)<br/> print(“Intent Identified: “, le.inverse_transform(preds)[0])<br/> return le.inverse_transform(preds)[0]</span><span id="4514" class="je jf hi kj b fi kr ko l kp kq">def get_response(message): <br/>  intent = get_prediction(message)<br/>  for i in data['intents']: <br/>    if i["tag"] == intent:<br/>      result = random.choice(i["responses"])<br/>      break<br/>  print(f"Response : {result}")<br/>  return "Intent: "+ intent + '\n' + "Response: " + result</span></pre><h2 id="adae" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">现在让我们测试模型:</h2><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="bc74" class="je jf hi kj b fi kn ko l kp kq">get_response(“why dont you introduce yourself”)</span></pre><figure class="ke kf kg kh fd kt er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es lc"><img src="../Images/9ff42fcb4fa78794027e0864b04bcd40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EvGwOPyZX0v0XdJLVaRukQ.png"/></div></div></figure></div><div class="ab cl ld le gp lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="hb hc hd he hf"><p id="948e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">出于测试目的，我们使用<strong class="ih hj"> Gradio </strong>部署了模型。以下是结果。</p><figure class="ke kf kg kh fd kt er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es lk"><img src="../Images/0ab199992b4d5d24fae3ddeaf0bd8659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*olevMFSvArqAHkHmPxPW7Q.png"/></div></div></figure><figure class="ke kf kg kh fd kt er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es ll"><img src="../Images/9821b63e2acdcef01e939f190ce3ce13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oAuApDSaAo2-2LS_lAZ8fA.png"/></div></div></figure><figure class="ke kf kg kh fd kt er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es lm"><img src="../Images/7d30eaae366977a9dd20f2ab084a6da8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZE1kfyB6ZKuSqarGpTuF9Q.png"/></div></div></figure></div><div class="ab cl ld le gp lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="hb hc hd he hf"><p id="c3f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">达到更好的效果:</strong> <br/> 1。试验不同型号的变压器<br/> 2。调整参数，如max_seq_len，batch_size <br/> 3。使用学习率计划程序</p></div></div>    
</body>
</html>
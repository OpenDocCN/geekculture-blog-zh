<html>
<head>
<title>Open Table Formats — Delta, Iceberg &amp; Hudi</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">开放式表格格式—德尔塔、冰山和胡迪</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/open-table-formats-delta-iceberg-hudi-732f682ec0bb?source=collection_archive---------0-----------------------#2022-06-02">https://medium.com/geekculture/open-table-formats-delta-iceberg-hudi-732f682ec0bb?source=collection_archive---------0-----------------------#2022-06-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="3713" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于新表格格式的介绍文章</p><h2 id="24b7" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">什么是表格格式？</h2><p id="7ac1" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">表格格式是组织数据文件的一种方式。他们试图给数据湖带来类似数据库的特性。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/b510314308dd85a0244761ac569b9774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z3fn5-UCN0WD-gVrPpqyVQ.png"/></div></div></figure><p id="32b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Apache Hive是最早和最常用的表格式之一。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kp"><img src="../Images/e8c34df660b0878d9b4ca7e5629850d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mx2cNcKfGKzeeG2X6Ot_NA.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx">Hive Table format</figcaption></figure><p id="cae7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">hive表格式存在以下问题:</p><blockquote class="ku kv kw"><p id="4ce5" class="if ig kx ih b ii ij ik il im in io ip ky ir is it kz iv iw ix la iz ja jb jc hb bi translated">陈旧表统计数据(分区生命周期)<br/>强迫用户了解物理数据布局<br/>缺乏沿袭/历史<br/>缺乏模式进化</p></blockquote><p id="2ec5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Hive是前云时代写的。它不是为对象存储而写的(hive的数据组织结构是一种对象存储的反模式)。因此，当新的使用模式出现在云中时，它的性能开始受到影响。Hive中的元数据表增长迅速，降低了其性能。</p><p id="a18b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随着data domain的增长，新的使用情形出现了，要求从当前的数据湖中获得一组新的功能。新系统主要要求数据湖具有以下特性:</p><blockquote class="ku kv kw"><p id="444b" class="if ig kx ih b ii ij ik il im in io ip ky ir is it kz iv iw ix la iz ja jb jc hb bi translated">事务(ACID) <br/>统一批处理&amp;流式传输<br/>数据突变(针对延迟数据到达的合并/纠正选项)<br/>模式实施、进化&amp;版本控制<br/>元数据缩放</p></blockquote><p id="f5fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除此之外，还需要支持新的用例:</p><blockquote class="ku kv kw"><p id="04ee" class="if ig kx ih b ii ij ik il im in io ip ky ir is it kz iv iw ix la iz ja jb jc hb bi translated">时间旅行<br/>并发读取&amp;写入<br/>独立消耗来自存储<br/>数据质量<br/>可插拔存储</p></blockquote><p id="d041" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了解决这些问题，社区开始创建新的开放表格格式。在这篇博客中，我们将讨论以下三点:</p><blockquote class="ku kv kw"><p id="24ff" class="if ig kx ih b ii ij ik il im in io ip ky ir is it kz iv iw ix la iz ja jb jc hb bi translated">三角洲表—数据块<br/>冰山—网飞<br/>胡迪—优步</p></blockquote><h2 id="8362" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">增量表</h2><p id="43cb" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">增量表是带有事务日志的拼花表。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es lb"><img src="../Images/248b9a87740806ceec6b847c88962b8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VPk3Dh9h3817S2JWdgIpkA.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx">Delta table format representation</figcaption></figure><p id="d906" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">增量日志— </strong>它是<strong class="ih hj"> </strong>对增量表上执行的所有操作的变更日志。这些动作就是1。添加文件2。删除文件3。更新元数据4。设置交易5。改变协议和6。提交信息。增量日志包含JSO文件，这些文件存储反映增量表在任何给定时间的状态。</p><p id="8d4f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据文件— </strong>保存实际数据的拼花文件。这些在本质上是不可改变的。如果在提交中，文件的行将受到影响，那么当前文件中的所有行都将被加载到内存中，应用更新，并作为新文件写出。在此之后，当旧的拼花文件从状态中移除并且新的拼花文件被添加到状态中时，提交被记录在增量日志中。</p><p id="674b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您投资了Databrick的产品，这种表格格式是一个很好的选择。</p><pre class="ke kf kg kh fd lc ld le bn lf lg bi"><span id="fe7b" class="lh je hi ld b be li lj l lk ll">from pyspark.sql import SparkSession<br/>from delta import DeltaTable<br/><br/>spark = SparkSession.builder \<br/>    .appName("Delta with PySpark") \<br/>    .config('spark.jars.packages', 'io.delta:delta-core_2.12:2.1.1') \<br/>    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \<br/>    .config(<br/>        "spark.sql.catalog.spark_catalog",<br/>        "org.apache.spark.sql.delta.catalog.DeltaCatalog",<br/>    ) \<br/>    .config("spark.sql.warehouse.dir", "spark-warehouse") \<br/>    .master("local[*]") \<br/>    .enableHiveSupport() \<br/>    .getOrCreate()<br/><br/>df_sales = spark.read.parquet("dataset/sales.parquet/*parquet")<br/>df_sales.write \<br/>    .format("delta") \<br/>    .mode("overwrite") \<br/>    .option("mergeSchema", True) \<br/>    .saveAsTable("sales_delta_managed_delta")<br/><br/><br/>dt = DeltaTable.forName(spark, "sales_delta_managed")<br/>dt.history().select("version", "timestamp").show(truncate=False)<br/><br/>%%sparksql<br/>select * from default.sales_delta_managed limit 5;<br/>update default.sales_delta_managed set amount = 428 where trx_id = 123<br/><br/>dt.history().select("version", "timestamp").show(truncate=False)</span></pre><h2 id="0e0e" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">冰山</h2><blockquote class="lm"><p id="3f17" class="ln lo hi bd lp lq lr ls lt lu lv jc dx translated">在Iceberg中，元数据在三个文件中被跟踪:1 .元数据文件2。清单3。清单文件。</p></blockquote><figure class="lx ly lz ma mb ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es lw"><img src="../Images/c26ea688c49b2fdc8391e4ac3a75c462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ej1VLvrLLtnZB6Z_rwoURw.png"/></div></div></figure><h2 id="a420" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">元数据. json</h2><p id="0970" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">此文件包含有关表的元数据的信息。它包含以下小节:</p><ul class=""><li id="a2df" class="mc md hi ih b ii ij im in iq me iu mf iy mg jc mh mi mj mk bi translated"><strong class="ih hj">快照</strong> —快照表中所有文件的完整列表。它包含关于表模式、分区规格T24清单位置的信息。</li><li id="47b1" class="mc md hi ih b ii ml im mm iq mn iu mo iy mp jc mh mi mj mk bi translated"><strong class="ih hj">模式</strong> —它跟踪表模式。所有表模式更改都在schemas数组中进行跟踪。</li><li id="6b34" class="mc md hi ih b ii ml im mm iq mn iu mo iy mp jc mh mi mj mk bi translated"><strong class="ih hj">分区规格</strong> —跟踪分区信息。</li><li id="9cea" class="mc md hi ih b ii ml im mm iq mn iu mo iy mp jc mh mi mj mk bi translated"><strong class="ih hj">分拣订单</strong></li></ul><p id="56f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">清单列表</strong> —包含所有清单及其指标(清单文件为分区列跨越的值范围)的文件。充当清单&amp;快照之间的链接。</p><p id="c4c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">清单</strong> —数据文件、其格式、位置和指标的列表(文件的每列界限或文件级别，如行数)</p><p id="c989" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据文件</strong> —实际的物理文件。</p><p id="0dd0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Iceberg将有助于解决与S3对象列表或Hive Metastore分区枚举相关的性能问题。</p><p id="ad3c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意:有一个插件叫做<strong class="ih hj"> <em class="kx"> Hiveberg </em> </strong>可以让我们从Hive metastore中读取冰山表。</p><pre class="ke kf kg kh fd lc ld le bn lf lg bi"><span id="5f10" class="lh je hi ld b be li lj l lk ll">from pyspark.sql import SparkSession<br/><br/>spark = SparkSession.builder \<br/>    .appName("Icerberg with PySpark") \<br/>    .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.2_2.12:1.0.0") \<br/>    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \<br/>    .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog",) \<br/>    .config("spark.sql.warehouse.dir", "spark-warehouse") \<br/>    .config("spark.sql.catalog.spark_catalog.type", "hive") \<br/>    .config("spark.sql.catalog.demo", "org.apache.iceberg.spark.SparkCatalog") \<br/>    .config("spark.sql.catalog.demo.warehouse", "path/to/warehouse")<br/>    .config("spark.sql.catalog.iceberg.type", "hadoop") \<br/>    .config("spark.sql.defaultCatalog", "demo")<br/>    .master("local[*]") \<br/>    .enableHiveSupport() \<br/>    .getOrCreate()<br/><br/>df_sales = spark.read.parquet("dataset/sales.parquet/*parquet")<br/>df_sales.write \<br/>    .format("iceberg") \<br/>    .mode("overwrite") \<br/>    .saveAsTable("sales_delta_managed_iceberg")</span></pre><p id="2abb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用于Nessie的配置</p><pre class="ke kf kg kh fd lc ld le bn lf lg bi"><span id="76f2" class="lh je hi ld b be li lj l lk ll">org.projectnessie:nessie-spark-extensions-3.3_2.12:0.44.0 # Library for working with Nessie-based catalogs like Dremio Arctic<br/>org.projectnessie.spark.extensions.NessieSparkSessionExtensions<br/>spark.sql.extensions="org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions" \<br/>spark.sql.catalog.nessie.uri=$ARCTIC_URI \<br/>spark.sql.catalog.nessie.ref=main \<br/>spark.sql.catalog.nessie.authentication.type=BEARER \<br/>spark.sql.catalog.nessie.authentication.token=$TOKEN \<br/>spark.sql.catalog.nessie.catalog-impl=org.apache.iceberg.nessie.NessieCatalog \<br/>spark.sql.catalog.nessie.warehouse=$WAREHOUSE \<br/>spark.sql.catalog.nessie=org.apache.iceberg.spark.SparkCatalog \<br/>spark.sql.catalog.nessie.io-impl=org.apache.iceberg.aws.s3.S3FileIO</span></pre><h2 id="f86b" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">Iceberg中的文件格式</h2><blockquote class="ku kv kw"><p id="3af1" class="if ig kx ih b ii ij ik il im in io ip ky ir is it kz iv iw ix la iz ja jb jc hb bi translated">数据— parquet <br/>元数据— JSON <br/>清单列表— avro <br/>清单— avro</p></blockquote><p id="3219" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">冰山酸支持:</strong></p><p id="b6c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Iceberg中，编写器启动更新，创建元数据文件，并尝试通过将元数据文件指针从当前版本交换到新版本来提交更新。但是，如果编写器发现更新所基于的快照不再是最新的，则编写器必须基于新版本重试更新。</p><blockquote class="ku kv kw"><p id="25a4" class="if ig kx ih b ii ij ik il im in io ip ky ir is it kz iv iw ix la iz ja jb jc hb bi translated">一个表元数据文件与另一个表元数据文件的原子交换为可序列化隔离提供了基础。</p></blockquote><p id="ee01" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">读取与并发写入相隔离，并且总是使用表数据的已提交快照。</p><h2 id="fb94" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">胡迪</h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mq"><img src="../Images/5861f525c0d5986688b1242930fed2e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yEA3DgqtWLlnoTyVOOuasA.png"/></div></div></figure><p id="9b56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">索引</strong> —记录关键字和文件组/文件id之间的映射</p><p id="3873" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">时间线</strong> —不同时刻在桌子上执行的所有动作的事件顺序。</p><p id="09d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据文件</strong>——拼花格式的实际数据文件。</p><p id="8e8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您有各种各样的数据湖文件消费工具，并且需要管理不断变化的数据集，这似乎是一个不错的选择。</p><pre class="ke kf kg kh fd lc ld le bn lf lg bi"><span id="86ac" class="lh je hi ld b be li lj l lk ll">from pyspark.sql import SparkSession<br/><br/>spark = SparkSession.builder \<br/>    .appName("Hudiwith PySpark") \<br/>    .config("spark.jars.packages", "org.apache.hudi:hudi-spark3.2-bundle_2.12:0.11.1") \<br/>    .config("spark.sql.extensions", "org.apache.spark.sql.hudi.HoodieSparkSessionExtension") \<br/>    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.hudi.catalog.HoodieCatalog",) \<br/>    .config("spark.sql.warehouse.dir", "spark-warehouse") \<br/>    .config("spark.sql.catalog.spark_catalog.type", "hive") \<br/>    .master("local[*]") \<br/>    .enableHiveSupport() \<br/>    .getOrCreate()<br/><br/>df_sales = spark.read.parquet("dataset/sales.parquet/*parquet")<br/>df_sales.write \<br/>    .format("hudi") \<br/>    .mode("overwrite") \<br/>    .saveAsTable("sales_delta_managed_hudi")</span></pre><h2 id="4069" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">摘要</h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mr"><img src="../Images/d018f2d182f8fc31c7f21b9f900de323.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VD5bC9_XSFckNbT6uyVVAQ.png"/></div></div></figure><p id="7a1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">快乐阅读！！</p></div></div>    
</body>
</html>
# 这篇文章是由一个人工智能写的

> 原文：<https://medium.com/geekculture/this-article-was-written-by-an-ai-8b3c961fd390?source=collection_archive---------2----------------------->

让我们来谈谈 GPT 3 号，并解释它是如何工作的

![](img/ac26169ab9102ea3e9d739c51f397453.png)

Photo by [Markus Winkler](https://www.pexels.com/@markus-winkler-1430818?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels) from [Pexels](https://www.pexels.com/photo/industry-internet-writing-technology-4604607/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)

如果你还没有听说过 GPT 3 号，那么向你介绍它的最好方式就是向你展示程序员用这项新技术创造出的所有很酷的东西。

Taking a simple prompt and expanding it by paragraphs

Turns an English prompt into a full-blown design mockup

Automatically generate music from a user-provided song title and artist

Translate English into SQL code

是的，你没看错。GPT-3 可以做任何事情，从简单的文本生成到音乐生成到文字编码。你可以给它一个类似“用 python 写代码，打印从 1 到 1000 的所有偶数”的提示，它会回:

x=0
而 x < 1000:
x=x+2
打印(x)

这实际上是准确的。这显然是一个简单的例子，但这种可能性让每个人都大吃一惊。那么什么是 GPT 3 号呢？它是如何工作的？让我们用简单的英语解释一下。

# 什么是 GPT-3？

GPT-3 是由 Elon Musk 支持的人工智能研究实验室 OpenAI 创建的神经网络驱动的语言模型。语言模型根据出现在它之前的单词预测某个字符序列下一个出现的可能性。它本质上试图模仿我们人类理解语言的方式。例如，如果你看到“纽约是一个非常大的城市”，你可能会猜测“城市”可能是下一个单词，因为纽约就是一个大城市。

这并不新鲜。第一个语言模型创建于 20 世纪 50 年代末。从那以后，类似的模型已经被用于拼写检查和生成基于单词的视频字幕。是什么让 GPT-3 与众不同？真的很大。我是说非常大。它接受了普通爬行的训练，普通爬行是人类生成的网络文本的真实世界数据集。依靠志愿者驱动的互联网档案，共同爬行是一个从博客，论坛和百科全书中提取的 12 亿字的语料库，是有史以来向公众发布的最大的公共网络数据资源之一。同样，它有 1750 亿个参数，是有史以来最大的语言模型(GPT 2 号只有 1.5 个参数！).模型和训练集的庞大规模是 GPT-3 如此令人印象深刻的主要原因。然而，GPT 3 号并不真正理解语言。它比任何人都有更好的记忆力。这几乎就像你在智能手机上使用自动完成功能，不断按程序生成新文本，以创建各种有趣的句子。除了，在这种情况下，GPT-3 有更多的信息，根据它们各自的概率，在句子中接下来会是什么。

# GPT-3 是如何训练的？

在第一代 GPT 之前，大多数自然语言处理模型都被训练来完成非常具体的任务，如为静止图像生成字幕，生成文本以支持语音识别，或使用监督学习生成网页内容。训练这些模型需要大量的时间、数据和计算能力，并且它们通常不能很好地推广到新数据，因为它们学习该数据的结构，而不是学习通用模型。

另一方面，GPT 模型使用未标记的数据进行预训练(这是生成式预训练转换器中 P 的来源),并通过提供小的标记数据集进行微调。这意味着您可以使用 GPT 模型完成各种任务，包括文本翻译、编码和使用最少量的计算能力生成文本，因为它们已经在类似的任务上进行了预先训练。

让我们把它分成几个步骤:

1.  一个巨大的无标签数据集包含了从博客帖子到脸书评论的所有内容，全部被输入到一个神经网络中。我们假设数据集中存在以下短语:“狗是一种动物”。模型会将短语拆分为前四个单词和最后一个单词，将前四个单词视为自变量，将最后一个单词视为因变量。与任何神经网络一样，该模型将尝试猜测从“狗是动物”逻辑上得出的单词。第一次尝试将遥遥无期。然而，随着更多的尝试，神经网络从错误中学习，并重新调整模型中的权重和偏差，以试图接近正确的单词。这个过程被称为反向传播，这是 GPT 模型中使用的方法。有了足够多的句子，该模型最终会学习人类语言的一般结构，并最终可以接受任何一串单词，并对接下来会发生什么提供良好的猜测。
2.  虽然这个预先训练的模型擅长猜测句子中的下一句话，但如果你想让它帮助将法语翻译成英语呢？这是微调的切入点。你可以用 10 个法语段落的例子和它们对应的英语段落，重新调整模型参数。模型参数现在将被调整，这样，如果一个法语段落被输入到模型中，它将吐出相同的英语段落。

要更详细地分析这一切是如何工作的，我建议你看看下面的视频:

# 为什么 GPT-3 如此具有开创性

通过在具有如此多参数的如此大的数据集上对 GPT-3 进行预训练，GPT-3 可以做任何其他模型都不能(很好地)做的事情:执行*特定*任务，而无需太多特殊调整。你可以让 GPT-3 成为一名翻译、程序员、诗人或著名作家，它可以用不到 10 个训练例子来做到这一点。该死的。

大多数其他模型(如 BERT)需要一个精心的微调步骤，你收集成千上万的(比如)法英句子对的例子，教它如何翻译。有了 GPT-3，你不需要做微调的步骤。这是它的核心。这就是让人们对《GPT 3:没有训练数据的定制语言任务》感到兴奋的原因。

公司已经通过创建基于 GPT-3 的易用应用程序获得了数百万美元的资金。例如， [Copy AI](https://www.copy.ai/) 开发了一个做文案工作的应用程序(写广告导语的人)。他们使用 GPT-3 制作不同风格和色调的广告。用户可以输入广告的目标受众，复制风格偏好和主题，并在几秒钟内获得 10 个不同复制风格的示例。到目前为止，他们已经为他们的项目争取到了超过 290 万美元的资金[。](https://techcrunch.com/2021/03/17/gpt-3-powered-copy-ai-raises-2-9m-in-a-round-led-by-craft-ventures/)

# 让我们解决房间里的大象。

如果你已经到了这一步，你要么直接跳到大揭露(在这种情况下，回去至少浏览一下其余部分)，要么你实际上已经阅读了这篇文章，并且你可能怀疑像这样的整篇文章可以由人工智能编写。再想想。这篇文章中写的几乎所有内容都是由一个经过训练可以写文章的 AI 生成的。我所要做的就是提供一个关于所写内容的简短说明，然后它就消失了。在这种情况下，我写道:“写一篇关于 GPT-3 的文章，解释它是什么，它是如何工作的，以及它将如何改变世界，”提供了几个小标题来引导程序，让它写下去。通过在人工智能生成的文本(每次吐出几段)之间添加副标题，我可以指导人工智能写我想要的文章，但即使这一步也不是必要的。如果我喜欢它写的东西，我会把它留在里面。如果没有，我告诉它再试一次。由于人工智能不是确定性的，因此有一点随机性，它每次都会生成全新的段落。我知道这很疯狂。也很可怕，因为我们才刚刚开始。由于我不擅长结束文章，我会让人工智能自己来做:

# 结论

一天下来，我已经就一个我所知甚少的话题写了 4000 字。但不仅仅是我，实际上是我的 AI 写了这个专栏，这是一件相当大的事情。一旦这篇文章被交给人工智能，它的下一个工作就是为我写一个结论。人工智能说，结束一篇文章的最好方式是，总结你学到的东西，然后留给读者一些收获。我不太喜欢这种结论，我觉得这让我听起来像是推销员之类的，但它确实有效。尽管如此，如果你第一次阅读一篇人工智能的文章(如果你正在阅读这篇文章，那么你可能正在阅读)，不要惊讶你的答案是“一定要看看我们的其他文章！”

*好吧，又是杰西。是的。GPT 三号确实自己写了整个结论。超级 meta，我知道*
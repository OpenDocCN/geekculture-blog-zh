<html>
<head>
<title>Transformers in Cheminformatics Part-1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">化学信息学中的变压器第一部分</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/transformers-in-cheminformatics-part-1-6ebda8f2781c?source=collection_archive---------44-----------------------#2021-06-15">https://medium.com/geekculture/transformers-in-cheminformatics-part-1-6ebda8f2781c?source=collection_archive---------44-----------------------#2021-06-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/5dd1e0f5f10148cb0d7d899d7efeb1a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*YSrw3oRTcE4N3xbtrUk7LQ.gif"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Bird’s eye view of the central piece of a Transformer Network i.e. “A Transformer Block” comprising of <strong class="bd iu">Multiheaded Self-Attention</strong> and <strong class="bd iu">Position wise feed-forward network</strong></figcaption></figure><p id="6187" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">更新:该系列的第二部分在这里发布<a class="ae jt" rel="noopener" href="/geekculture/transformers-in-cheminformatics-f569ad302033"/>。</p><p id="560b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">自从神经机器翻译出现以来，转换网络(“你所需要的只是注意力”)已经变得非常流行。在自然语言处理中，许多下游任务的最新性能是通过转换器及其变体实现的。化学信息学和药物发现也没有幸免于这次起义。从预测模型到预测分子特性，再到学习连续的分子表示和生成药物设计，变形金刚取得了显著的成功。</p><p id="0443" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">虽然变压器功能强大，但理解起来也很复杂。由于它们的流行，人们试图用图表和动画来解释复杂的架构。一个流行的解释是由<a class="ae jt" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">杰伊·阿拉玛</a>提出的。在这3个部分的系列中，我在<em class="ju">中分解了各种移动的部分</em>并与杰伦的解释保持一致。我进一步编写与解释相对应的易理解的代码。最后，我通过展示Transformers如何能够学习药物样化合物的鲁棒和有意义的分子表示来结束本系列，并且学习的表示在直接从分子结构预测水溶性方面优于Morgan指纹表示。</p></div><div class="ab cl jv jw gp jx" role="separator"><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka"/></div><div class="hb hc hd he hf"><p id="d810" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这一部分中，我们将介绍变形金刚的核心部分，即<strong class="ix hj">自我关注</strong>和<strong class="ix hj">多头关注。</strong>变压器试图学习用于输入令牌的上下文感知的丰富嵌入。</p><blockquote class="kc"><p id="9572" class="kd ke hi bd kf kg kh ki kj kk kl js dx translated"><em class="km">我游到了对岸</em> <strong class="ak"> <em class="km">岸边</em> </strong> <em class="km">。</em></p></blockquote><p id="58ce" class="pw-post-body-paragraph iv iw hi ix b iy kn ja jb jc ko je jf jg kp ji jj jk kq jm jn jo kr jq jr js hb bi translated">像Word2vec这样的上下文无关的嵌入方案向银行(金融机构)和银行(沿着水体的陆地)分配相同的矢量表示。然而，Transformer在句子中的所有2个标记的集合之间学习加权的成对分数，以生成丰富的嵌入。因此，银行(金融机构)和银行(水体沿岸的土地)被投影到表示空间中的不同位置。Jay通过一个词性标注的例子证明了这一点。为了理解单词“it”与句子中其他标记的对应关系，Transformer计算O(n)相关性(注意力)分数。变压器的这种O(n)时间和空间复杂度是该架构的显著限制。Tay等人对各种改进的X-former模型进行了调查。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/daff0c225f32769a8bce5b5c12f28bd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*X0QT-9SubakMl-vMx5Jq2g.png"/></div></figure><blockquote class="kx ky kz"><p id="983a" class="iv iw ju ix b iy iz ja jb jc jd je jf la jh ji jj lb jl jm jn lc jp jq jr js hb bi translated">作为一个迂回，我想知道“变形金刚”这个名字的起源和意义。我能想到的两个理由是。它<em class="hi">“转换”一个简单的编码为丰富的编码。<br/> 2。</em>跟上最近为来自漫画/电视世界(ELMO、伯特等)的自然语言处理架构提供命名的实践。)</p></blockquote><h1 id="9956" class="ld le hi bd iu lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">单头点量表自我注意</h1><p id="efbe" class="pw-post-body-paragraph iv iw hi ix b iy ma ja jb jc mb je jf jg mc ji jj jk md jm jn jo me jq jr js hb bi translated">最初的Transformers论文将输入嵌入拆分为“nheads”个更小的嵌入，并将它们抽象为“key”、“query”和“value”向量。作为本帖中的一个运行示例，我们将考虑一个简短的2令牌序列“<em class="ju">思维机器”。</em>在化学信息学的上下文中，这可能是一些带有2个字符的假想微笑字符串。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/0785fc14768baa31dcf18f8c156ca03f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*MGbiUz7vM_5LpAuj2yp8ZA.gif"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Figure 1: Creating key, value, query abstract vectors from input embedding. q,k and v share the same copy of a section of input X</figcaption></figure><p id="52ac" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">自我关注机制通过获取查询向量与我们正在评分的相应单词的关键向量的点积来计算关注分数，从而计算关注加权丰富嵌入(Z)。因此，如果我们在#1(X₁(思考)位置处理对单词的自我关注，第一个分数将是q1和k1的点积。第二个分数是q1和k2的点积。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/158337e8b30c477bfa06f9e5d12ae9f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EZoi_zDfOSa_k4yNvcv9Gw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx"><strong class="bd iu">Left</strong> (a) Self attention steps from “Attention is all you need” (b) Vector operations performed to get enriched vectors Zᵢ <strong class="bd iu">Right </strong>Stacking vectors in left (b) to perform the same self-attention calculation in matrix form</figcaption></figure><p id="a2a4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">比例因子√dₖ有助于稳定梯度。托马斯·库尔比尔对此有一个很好的解释。计算自我注意的步骤可以通过将向量(qᵢ's、vᵢ's、kᵢ's)堆积成矩阵并转向矩阵运算来实现。矩阵运算高度并行化，适合GPU执行。自我关注可以在Pytorch中实现，如下所示</p><figure class="kt ku kv kw fd ij"><div class="bz dy l di"><div class="mj mk l"/></div><figcaption class="iq ir et er es is it bd b be z dx">Self-attention in pytorch</figcaption></figure></div><div class="ab cl jv jw gp jx" role="separator"><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka"/></div><div class="hb hc hd he hf"><h1 id="41e7" class="ld le hi bd iu lf ml lh li lj mm ll lm ln mn lp lq lr mo lt lu lv mp lx ly lz bi translated">多头注意力</h1><p id="b167" class="pw-post-body-paragraph iv iw hi ix b iy ma ja jb jc mb je jf jg mc ji jj jk md jm jn jo me jq jr js hb bi translated">多头注意力通过以下步骤进行。</p><ol class=""><li id="4ce4" class="mq mr hi ix b iy iz jc jd jg ms jk mt jo mu js mv mw mx my bi translated">变换器将输入嵌入分成“nheads”个较小的相同大小的嵌入。这如图1所示。这些更小的代表子空间中的每一个都让注意力机制聚焦于序列的不同部分。上面的动画显示了将输入嵌入到更小的查询、键和值中的分解。</li></ol><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nd"><img src="../Images/175510e987d7115ef7a7c74a29f37728.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JvcpK7R5mvhU00TiwyZxww@2x.png"/></div></div></figure><p id="dfe1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.为来自每个头部的每个<q>三元组计算注意力富集Zᵢ。这使用了上面显示的self_attention函数。</q></p><p id="6c6a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.相应的Zᵢ's都连接在一起。</p><p id="186a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">4.通过线性层投影级联的Zᵢ's矩阵，生成与输入嵌入维数相同的最终富集嵌入Z_enriched。</p><p id="1ee6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看看pytorch代码来实现这一点</p><figure class="kt ku kv kw fd ij"><div class="bz dy l di"><div class="mj mk l"/></div><figcaption class="iq ir et er es is it bd b be z dx">Multiheaded attention in pytorch</figcaption></figure></div><div class="ab cl jv jw gp jx" role="separator"><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka"/></div><div class="hb hc hd he hf"><h1 id="f453" class="ld le hi bd iu lf ml lh li lj mm ll lm ln mn lp lq lr mo lt lu lv mp lx ly lz bi translated">位置式前馈网络</h1><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/fc584fe5d7ce3e1cc690c4dfe67e9615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*fRCFwG1JJ5L3jkSxAWW1Zg.gif"/></div></div></figure><p id="381e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对应于序列中每个标记的富集Z，由多头张力模块返回，通过单独的前馈神经网络馈送。由于每个令牌都有自己的前馈神经网络，因此本文称之为位置式前馈网络。他们将其定义为</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ne"><img src="../Images/974b384bccd18db1ad603421fc7c36e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/1*5opJMkAxFOf-LbVjQS3QdQ.gif"/></div></figure><p id="d3b0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这对于编码来说很简单</p><figure class="kt ku kv kw fd ij"><div class="bz dy l di"><div class="mj mk l"/></div></figure></div><div class="ab cl jv jw gp jx" role="separator"><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka"/></div><div class="hb hc hd he hf"><h1 id="631c" class="ld le hi bd iu lf ml lh li lj mm ll lm ln mn lp lq lr mo lt lu lv mp lx ly lz bi translated">结论</h1><p id="e44f" class="pw-post-body-paragraph iv iw hi ix b iy ma ja jb jc mb je jf jg mc ji jj jk md jm jn jo me jq jr js hb bi translated">在这一部分，我们研究了变压器网络的关键，即构成变压器模块的多头自我关注网络。在下一部分，我们将看看其他辅助模块。特别是，我们将查看位置嵌入和图层规范化。我们将进一步将所有这些结合到一个用于语言模型预训练任务的功能编码器-解码器中。</p><p id="e517" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">本系列的文章以及所有代码、动画和数据集将很快在我的<a class="ae jt" href="http://mohitkpandey.github.io" rel="noopener ugc nofollow" target="_blank">网站</a>上提供下载。</p><p id="2127" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">回头见！</p></div><div class="ab cl jv jw gp jx" role="separator"><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka"/></div><div class="hb hc hd he hf"><h1 id="e58c" class="ld le hi bd iu lf ml lh li lj mm ll lm ln mn lp lq lr mo lt lu lv mp lx ly lz bi translated">参考</h1><ol class=""><li id="1814" class="mq mr hi ix b iy ma jc mb jg nf jk ng jo nh js mv mw mx my bi translated"><a class="ae jt" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">你需要的只是关注</a></li><li id="df9a" class="mq mr hi ix b iy ni jc nj jg nk jk nl jo nm js mv mw mx my bi translated"><a class="ae jt" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">图解变压器</a></li><li id="57c0" class="mq mr hi ix b iy ni jc nj jg nk jk nl jo nm js mv mw mx my bi translated"><a class="ae jt" href="https://arxiv.org/abs/2009.06732" rel="noopener ugc nofollow" target="_blank">高效变压器</a></li></ol></div></div>    
</body>
</html>
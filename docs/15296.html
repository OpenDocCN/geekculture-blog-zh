<html>
<head>
<title>Deploying GPU-based Models on SageMaker using ‘Multi-Model’ Endpoint (Part 2 - Final)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用“多模型”端点在SageMaker上部署基于GPU的模型(第2部分-最终版)</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/deploying-gpu-based-models-on-sagemaker-using-multi-model-endpoint-part-2-final-6e05cf10142f?source=collection_archive---------9-----------------------#2022-10-24">https://medium.com/geekculture/deploying-gpu-based-models-on-sagemaker-using-multi-model-endpoint-part-2-final-6e05cf10142f?source=collection_archive---------9-----------------------#2022-10-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/8ea0a40c0197d7ec768a0c1f04321f27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4oMxmoB7ptyLyQlx"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Photo by <a class="ae iu" href="https://unsplash.com/@ourselp?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Philippe Oursel</a> on <a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="iv iw ix"><p id="ceaa" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">本文是上一篇文章(第1部分)的延续。如果您碰巧看到这篇文章，强烈建议您先阅读下面的第1部分文章。</p><p id="32ea" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jb hj">2022年17月11日更新:</strong>我从<a class="ae iu" href="https://www.linkedin.com/feed/update/urn:li:activity:6990336296409858048?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A6990336296409858048%2C6998715078984302592%29&amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%286998715078984302592%2Curn%3Ali%3Aactivity%3A6990336296409858048%29" rel="noopener ugc nofollow" target="_blank">一位AWS SageMaker员工</a>那里得到消息，终于AWS在SageMaker上发布了多模型端点GPU。酷！你可以在这里 阅读文章<a class="ae iu" href="https://aws.amazon.com/about-aws/whats-new/2022/10/amazon-sagemaker-cost-effectively-host-1000s-gpu-multi-model-endpoint/" rel="noopener ugc nofollow" target="_blank">。</a></p></blockquote><div class="jx jy ez fb jz ka"><a href="https://utomorezadwi.medium.com/deploying-gpu-based-models-on-sagemaker-using-multi-model-endpoint-part-1-da68cbbf3d04" rel="noopener follow" target="_blank"><div class="kb ab dw"><div class="kc ab kd cl cj ke"><h2 class="bd hj fi z dy kf ea eb kg ed ef hh bi translated">使用“多模型”端点在SageMaker上部署基于GPU的模型(第1部分)</h2><div class="kh l"><h3 class="bd b fi z dy kf ea eb kg ed ef dx translated">本文是第一部分。如果您可以在SageMaker上部署基于GPU的多模型端点会怎么样？</h3></div><div class="ki l"><p class="bd b fp z dy kf ea eb kg ed ef dx translated">utomorezadwi.medium.com</p></div></div><div class="kj l"><div class="kk l kl km kn kj ko io ka"/></div></div></a></div><blockquote class="iv iw ix"><p id="3e08" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jb hj">2022年26月10日更新:</strong>我刚刚发现<a class="ae iu" href="https://developer.nvidia.com/blog/run-multiple-ai-models-on-same-gpu-with-sagemaker-mme-powered-by-triton/" rel="noopener ugc nofollow" target="_blank"> <strong class="jb hj">一篇来自NVIDIA </strong> </a>的文章，提到了使用NVIDIA Triton推理服务器在Amazon SageMaker的多模型端点上运行基于GPU的模型。我自己还没有检查这个功能，但如果这是真的，这是一个游戏改变者。</p></blockquote><h2 id="1663" class="kp kq hi bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">前一篇文章的一些总结</h2><ul class=""><li id="9154" class="ln lo hi jb b jc lp jg lq la lr le ls li lt jw lu lv lw lx bi translated">我们已经讨论了优化SageMaker推理成本的可用选项。</li><li id="7345" class="ln lo hi jb b jc ly jg lz la ma le mb li mc jw lu lv lw lx bi translated">我们已经为拥有这种“多模式”端点的所有需求做好了准备。</li><li id="018f" class="ln lo hi jb b jc ly jg lz la ma le mb li mc jw lu lv lw lx bi translated">我们训练了两个样本模型(一个PyTorch模型和一个TensorFlow模型)并测试了它们的预测。</li><li id="ea72" class="ln lo hi jb b jc ly jg lz la ma le mb li mc jw lu lv lw lx bi translated">我们已经讨论了SageMaker推理端点所需的目录结构，以及为我们的案例建议的目录结构。</li><li id="ea3f" class="ln lo hi jb b jc ly jg lz la ma le mb li mc jw lu lv lw lx bi translated">我们已经讨论了<code class="du md me mf mg b">inference.py</code>文件中的四个主要函数。</li></ul><h1 id="edc5" class="mh kq hi bd kr mi mj mk kv ml mm mn kz mo mp mq ld mr ms mt lh mu mv mw ll mx bi translated">目录</h1><p id="6cdb" class="pw-post-body-paragraph iy iz hi jb b jc lp je jf jg lq ji jj la my jm jn le mz jq jr li na ju jv jw hb bi translated">第2部分文章的安排如下:</p><ul class=""><li id="055a" class="ln lo hi jb b jc jd jg jh la nb le nc li nd jw lu lv lw lx bi translated">部署<br/> -调试推理. py脚本<br/> -上传model.tar.gz文件到S3桶<br/> -使用SageMaker部署</li><li id="3e46" class="ln lo hi jb b jc ly jg lz la ma le mb li mc jw lu lv lw lx bi translated">调用端点<br/> -使用预测器类<br/> -使用。invoke_endpoint()方法<br/> -删除端点</li><li id="37d2" class="ln lo hi jb b jc ly jg lz la ma le mb li mc jw lu lv lw lx bi translated">限制</li><li id="0084" class="ln lo hi jb b jc ly jg lz la ma le mb li mc jw lu lv lw lx bi translated">结论</li></ul><h1 id="bbda" class="mh kq hi bd kr mi mj mk kv ml mm mn kz mo mp mq ld mr ms mt lh mu mv mw ll mx bi translated">部署</h1><h2 id="efc3" class="kp kq hi bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">调试<code class="du md me mf mg b">inference.py</code>脚本</h2><p id="fec3" class="pw-post-body-paragraph iy iz hi jb b jc lp je jf jg lq ji jj la my jm jn le mz jq jr li na ju jv jw hb bi translated">现在，我们已经编写了<code class="du md me mf mg b">inference.py</code>文件和<code class="du md me mf mg b">requirements.txt</code>文件，并将两个模型所需的文件和文件夹放入主目录中。在将主目录压缩到一个<code class="du md me mf mg b">model.tar.gz</code>文件之前，建议提前调试<code class="du md me mf mg b">inference.py</code>文件，以便在模型部署后将任何不必要的错误和副作用降到最低。</p><p id="a4b5" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">为了简化这个过程，我们将把<code class="du md me mf mg b">inference.py</code>复制到两个文件中:<code class="du md me mf mg b">debug_pt.py</code>和<code class="du md me mf mg b">debug_tf.py</code>。对于每一个，我们将保留其模型类型所需的所有代码行，并删除其模型类型不必要的(或只是注释)代码行。例如，如果我们想调试PyTorch模型，在<code class="du md me mf mg b">debug_pt.py</code>文件中，我们只保留处理PyTorch模型所需的所有代码，反之亦然。关于这两个文件的更多细节，你可以分别在这里<a class="ae iu" href="https://github.com/utomoreza/MultimodelEndpoint_hacked/blob/main/debug_pt.py" rel="noopener ugc nofollow" target="_blank">和</a><a class="ae iu" href="https://github.com/utomoreza/MultimodelEndpoint_hacked/blob/main/debug_tf.py" rel="noopener ugc nofollow" target="_blank">这里</a>看到我的文件。</p><ul class=""><li id="cfa8" class="ln lo hi jb b jc jd jg jh la nb le nc li nd jw lu lv lw lx bi translated">调试PyTorch模型</li></ul><p id="8e3d" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">我们要做的基本上类似于<strong class="jb hj">模型预测</strong>部分(在第1部分中)。因此，我们将测试前面(第1部分)定义的所有四个函数。正如我前面解释的，我们使用一个名为<code class="du md me mf mg b">debug_pt.py</code>的<code class="du md me mf mg b">inference.py</code>文件的副本，我们只是简单地导入所有四个函数。</p><p id="fc2b" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">之后，我们准备用于调试测试的数据。然后，数据被包装在JSON结构中，以模拟推理服务器收到的请求结构。</p><p id="a3f0" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">随后，JSON输入可以用作<code class="du md me mf mg b">input_fn()</code>函数的第一个参数。在这种情况下，我将<code class="du md me mf mg b">“model”</code>输入到<code class="du md me mf mg b">model_fn()</code>函数中，因为我将主目录(这将是<code class="du md me mf mg b">model.tar.gz</code>压缩文件)命名为<code class="du md me mf mg b">model/</code>。对于<code class="du md me mf mg b">predict_fn()</code>功能，我们可以输入(1)来自<code class="du md me mf mg b">input_fn()</code>的输出和(2)来自<code class="du md me mf mg b">model_fn()</code>的输出。最后，<code class="du md me mf mg b">predict_fn()</code>的输出被输入到<code class="du md me mf mg b">output_fn()</code>功能。</p><figure class="ne nf ng nh fd ij"><div class="bz dy l di"><div class="ni nj l"/></div></figure><p id="e044" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">对于每个函数，我们可以打印它的值来检查它是否按预期工作。下面是打印结果。我们可以看到，所有四个功能都按照我们的预期工作。</p><figure class="ne nf ng nh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nk"><img src="../Images/7addbec5bf48baf2a3d44ca032ef5978.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BZ35qDJQuehuN3EaecPotQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Results from debugging PyTorch model</figcaption></figure><ul class=""><li id="6437" class="ln lo hi jb b jc jd jg jh la nb le nc li nd jw lu lv lw lx bi translated">调试张量流模型</li></ul><p id="2633" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">为了调试TensorFlow模型，我们使用了<code class="du md me mf mg b">debug_tf.py</code>文件。基本上，这个过程类似于PyTorch模型，只是对于JSON输入，我们必须将其设置为文本。随后，我们在每个函数中应用与PyTorch模型相同的方法。</p><figure class="ne nf ng nh fd ij"><div class="bz dy l di"><div class="ni nj l"/></div></figure><p id="21aa" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">之后，打印结果如下所示。所有四个功能都如我们预期的那样工作。</p><figure class="ne nf ng nh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nl"><img src="../Images/3b7a4fd897ca93692cb8b5e39d6b6cd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NgcTdUq1CQXcoKti0kzg6A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Results from debugging TensorFlow model</figcaption></figure><h2 id="6c40" class="kp kq hi bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">把model.tar.gz的文件上传到S3桶</h2><ul class=""><li id="5429" class="ln lo hi jb b jc lp jg lq la lr le ls li lt jw lu lv lw lx bi translated">压缩主目录</li></ul><p id="18b8" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">调试清楚后，下一步就是将主目录压缩成<code class="du md me mf mg b">model.tar.gz</code>文件。我们可以使用<code class="du md me mf mg b"><a class="ae iu" href="https://linuxhint.com/linux-tar-command/" rel="noopener ugc nofollow" target="_blank">tar</a></code>的<a class="ae iu" href="https://linuxhint.com/linux-tar-command/" rel="noopener ugc nofollow" target="_blank"> Linux命令</a>如下</p><pre class="ne nf ng nh fd nm mg nn no aw np bi"><span id="6bd6" class="kp kq hi mg b fi nq nr l ns nt"># if using tar command</span><span id="c8eb" class="kp kq hi mg b fi nu nr l ns nt">artifact="model.tar.gz"<br/>main_dir_path="to/your/main/dir/"<br/>cd $main_dir_path<br/>tar -zcvf $artifact .</span></pre><p id="9b4e" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">或者使用<a class="ae iu" href="https://stackoverflow.com/a/17081026" rel="noopener ugc nofollow" target="_blank"> Python tar代码</a>如下</p><pre class="ne nf ng nh fd nm mg nn no aw np bi"><span id="50d0" class="kp kq hi mg b fi nq nr l ns nt"># if using Python code</span><span id="6b0f" class="kp kq hi mg b fi nu nr l ns nt">import tarfile<br/>import os</span><span id="c0af" class="kp kq hi mg b fi nu nr l ns nt">def make_tarfile(output_filename, source_dir):<br/>    with tarfile.open(output_filename, "w:gz") as tar:<br/>        tar.add(source_dir, arcname=os.path.basename(source_dir))</span><span id="fa53" class="kp kq hi mg b fi nu nr l ns nt">main_dir_path = "/your/main/dir/path"<br/>os.chdir(main_dir_path)</span><span id="cfa3" class="kp kq hi mg b fi nu nr l ns nt">artifact = "model.tar.gz"<br/>make_tarfile(artifact, ".")</span></pre><p id="0828" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">压缩过程完成后，最好重新检查一下你的压缩文件是否包含了所有需要的文件和文件夹。您可以使用下面的命令来检查压缩文件内部的结构。</p><pre class="ne nf ng nh fd nm mg nn no aw np bi"><span id="429c" class="kp kq hi mg b fi nq nr l ns nt">tar -tvf $artifact</span></pre><ul class=""><li id="a462" class="ln lo hi jb b jc jd jg jh la nb le nc li nd jw lu lv lw lx bi translated">上传到S3桶</li></ul><blockquote class="iv iw ix"><p id="760a" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jb hj">注意:</strong>假设您已经创建了自己的S3存储桶来存储<code class="du md me mf mg b"><em class="hi">model.tar.gz</em></code>文件，并设置了所有必要的IAM策略。</p></blockquote><p id="c13e" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">要将压缩文件上传到您的S3 bucket，只需使用如下的<code class="du md me mf mg b"><a class="ae iu" href="https://docs.aws.amazon.com/cli/latest/reference/s3/cp.html#examples" rel="noopener ugc nofollow" target="_blank">cp </a></code> <a class="ae iu" href="https://docs.aws.amazon.com/cli/latest/reference/s3/cp.html#examples" rel="noopener ugc nofollow" target="_blank"> AWS CLI </a>。请提前在您的系统中安装AWS CLI。</p><pre class="ne nf ng nh fd nm mg nn no aw np bi"><span id="df76" class="kp kq hi mg b fi nq nr l ns nt">bucket_name="your-bucket-name"<br/>key="model.tar.gz"<br/>s3_uri="s3://$bucket_name/$key"<br/>aws s3 cp $artifact $s3_uri</span></pre><p id="8b82" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">或者您可以使用来自<a class="ae iu" href="https://stackoverflow.com/a/37017853" rel="noopener ugc nofollow" target="_blank"/><code class="du md me mf mg b"><a class="ae iu" href="https://stackoverflow.com/a/37017853" rel="noopener ugc nofollow" target="_blank">boto3</a></code><a class="ae iu" href="https://stackoverflow.com/a/37017853" rel="noopener ugc nofollow" target="_blank">库</a>的代码，如下所示。请提前在您的环境中安装<a class="ae iu" href="https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html#installation" rel="noopener ugc nofollow" target="_blank">库</a><code class="du md me mf mg b"><a class="ae iu" href="https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html#installation" rel="noopener ugc nofollow" target="_blank">boto3</a></code><a class="ae iu" href="https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html#installation" rel="noopener ugc nofollow" target="_blank"/>。</p><pre class="ne nf ng nh fd nm mg nn no aw np bi"><span id="b5bd" class="kp kq hi mg b fi nq nr l ns nt">import boto3<br/>s3 = boto3.resource('s3')<br/>s3.meta.client.upload_file('/path/to/model.tar.gz/in/local', 'your-bucket-name', 'model.tar.gz')</span></pre><h2 id="1a18" class="kp kq hi bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">使用SageMaker部署</h2><p id="6e90" class="pw-post-body-paragraph iy iz hi jb b jc lp je jf jg lq ji jj la my jm jn le mz jq jr li na ju jv jw hb bi translated">到目前为止我们做得很好。只剩下最后一步了。现在，我们将使用SageMaker Python SDK。不要忘记在您的环境中安装它。请使用最新版本的SageMaker Python SDK。写这篇文章的时候，我用的是<code class="du md me mf mg b">2.108.0</code>这个版本。</p><p id="05a1" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">默认情况下，SageMaker还不支持GPU多模型端点。因此，为了智胜SageMaker SDK，以便我们可以拥有自己的GPU“多模型”端点，我们必须做到以下几点。当我们准备使用SageMaker SDK部署训练好的模型时，我们必须选择将使用什么预构建的容器映像，因为这里列出了许多映像<a class="ae iu" href="https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html" rel="noopener ugc nofollow" target="_blank"/>。在本例中，我选择了PyTorch容器映像，其中PyTorch版本为<code class="du md me mf mg b">1.10.2</code>，Python版本为<code class="du md me mf mg b">3.8</code>。我选择它的原因是，我已经测试了图像，它与我的模型版本。这个映像已经包含了安装<code class="du md me mf mg b">torch</code>库的需求，所以你不需要自己安装。然而，镜像将安装我们在前面的<code class="du md me mf mg b">requirements.txt</code>文件中列出的<code class="du md me mf mg b">tensorflow</code>库。</p><blockquote class="iv iw ix"><p id="b5f0" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jb hj">注意:</strong>您可以将PyTorch切换到TensorFlow，其中您的预构建容器映像是TensorFlow(具有特定版本)，然后您在您的<code class="du md me mf mg b"><em class="hi">requirements.txt</em></code>文件中列出<code class="du md me mf mg b"><em class="hi">torch</em></code>库以便SageMaker SDK安装它。但是，一定要确保您要做的事情能按预期工作，没有任何版本/环境冲突。</p></blockquote><p id="189a" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">接下来，我们只是实例化必要的变量，例如<code class="du md me mf mg b">model_s3_uri</code>(上传的<code class="du md me mf mg b">model.tar.gz</code>文件的S3·URI)、<code class="du md me mf mg b">role</code>(执行部署所需的IAM角色)和<code class="du md me mf mg b">sagemaker_session</code>(执行部署所需的SageMaker会话)。因为我们使用PyTorch容器图像作为基础，所以我们必须使用<code class="du md me mf mg b">PyTorchModel</code>类并设置必要的参数。最后，我们可以使用<code class="du md me mf mg b">.deploy()</code>方法和必要的参数来部署<code class="du md me mf mg b">PyTorchModel</code>对象。有关更多详细信息，请参见下面的代码片段。</p><figure class="ne nf ng nh fd ij"><div class="bz dy l di"><div class="ni nj l"/></div></figure><p id="1f90" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">当我们运行上面的代码时，如果没有出现错误，我们将看到几个连字符连续打印，直到部署过程完成。</p><h1 id="f908" class="mh kq hi bd kr mi mj mk kv ml mm mn kz mo mp mq ld mr ms mt lh mu mv mw ll mx bi translated">调用端点</h1><p id="e720" class="pw-post-body-paragraph iy iz hi jb b jc lp je jf jg lq ji jj la my jm jn le mz jq jr li na ju jv jw hb bi translated">在我们从AWS网络环境外部调用端点之前，直接从SageMaker笔记本(与您刚才部署模型的笔记本相同)访问端点总是一个好主意。因此，我们仍将使用在部署过程中定义的<code class="du md me mf mg b">predictor</code>变量。</p><figure class="ne nf ng nh fd ij"><div class="bz dy l di"><div class="ni nj l"/></div></figure><p id="1291" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">下面是结果。我们可以看到部署的模型如预期的那样工作。</p><figure class="ne nf ng nh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nv"><img src="../Images/d589fffefe88d9f67c9df118b5c4b07a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xTwzJLGceSdhRXyA33_Lbg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Results from PyTorchModel.deploy().predict()</figcaption></figure><p id="808c" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">至少，我们可以使用两种方法从AWS网络环境外部调用推理端点:使用<code class="du md me mf mg b">Predictor</code>类和使用<code class="du md me mf mg b">.invoke_endpoint()</code>方法。</p><h2 id="5e3f" class="kp kq hi bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">使用<code class="du md me mf mg b">Predictor</code>类</h2><p id="0b21" class="pw-post-body-paragraph iy iz hi jb b jc lp je jf jg lq ji jj la my jm jn le mz jq jr li na ju jv jw hb bi translated"><code class="du md me mf mg b">Predictor</code>类是来自SageMaker SDK 的<a class="ae iu" href="https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html" rel="noopener ugc nofollow" target="_blank">类。它用于向Amazon SageMaker端点发出预测请求。</a></p><p id="edf5" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">首先，我们必须在发出任何请求之前定义凭证。我们可以使用许多技术来定义凭证。您可以按照AWS网站上的说明进行操作。</p><div class="jx jy ez fb jz ka"><a href="https://boto3.amazonaws.com/v1/documentation/api/1.9.42/guide/configuration.html" rel="noopener  ugc nofollow" target="_blank"><div class="kb ab dw"><div class="kc ab kd cl cj ke"><h2 class="bd hj fi z dy kf ea eb kg ed ef hh bi translated">凭证— Boto 3文档1.9.42文档</h2><div class="kh l"><h3 class="bd b fi z dy kf ea eb kg ed ef dx translated">Boto可以通过多种方式进行配置。无论您选择哪个或哪些源，您都必须拥有AWS…</h3></div><div class="ki l"><p class="bd b fp z dy kf ea eb kg ed ef dx translated">boto3.amazonaws.com</p></div></div></div></a></div><p id="36d1" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">下面的代码片段展示了如何使用<code class="du md me mf mg b">Predictor</code>类来调用我们之前部署的端点。在代码片段中，我将凭证定义为<code class="du md me mf mg b">boto3.Session()</code>的输入。这种方式是您可以用来定义凭证的技术之一。</p><blockquote class="iv iw ix"><p id="c6b2" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jb hj">重要提示:</strong>不建议<strong class="jb hj"/>将您的凭证硬编码为脚本中的字符串。相反，您可以创建一个单独的隐藏文件，其中包含您需要的凭证，然后让您的Python脚本从这样的文件中加载凭证。</p></blockquote><p id="f53e" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">我们还需要端点名称。如果我们在使用<code class="du md me mf mg b">PyTorchModel</code>对象的<code class="du md me mf mg b">.deploy()</code>方法时定义了名称(就像我们之前做的那样)，我们可以使用这样的名称。否则，SageMaker SDK将设置一个默认名称，即<code class="du md me mf mg b">&lt;framework-name&gt;-&lt;timestamp&gt;</code>。</p><p id="9cb1" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">随后，我们使用之前定义的凭证实例化<code class="du md me mf mg b">boto_session</code>。通过使用<code class="du md me mf mg b">boto_session</code>，我们然后实例化<code class="du md me mf mg b">sagemaker.Session()</code>。接下来，我们实例化一个<code class="du md me mf mg b">Predictor</code>对象，然后向其中输入所有必要的参数。</p><p id="845c" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">最后一部分只是准备要测试的数据(与我们之前在调试或模型预测(第1部分)中所做的一样)。我们准备了<code class="du md me mf mg b">payload</code>，即JSON输入，它将通过使用<code class="du md me mf mg b">.predict()</code>方法到达端点。</p><figure class="ne nf ng nh fd ij"><div class="bz dy l di"><div class="ni nj l"/></div></figure><p id="e081" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">下面是结果。我们希望结果应该与我们使用<code class="du md me mf mg b">PyTorchModel.deploy().predict()</code>时的结果相同。</p><figure class="ne nf ng nh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nv"><img src="../Images/d589fffefe88d9f67c9df118b5c4b07a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xTwzJLGceSdhRXyA33_Lbg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Results from Predictor.predict()</figcaption></figure><h2 id="1a57" class="kp kq hi bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">使用<code class="du md me mf mg b">.invoke_endpoint()</code>方法</h2><p id="8366" class="pw-post-body-paragraph iy iz hi jb b jc lp je jf jg lq ji jj la my jm jn le mz jq jr li na ju jv jw hb bi translated"><code class="du md me mf mg b">.invoke_endpoint()</code>方法<a class="ae iu" href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint" rel="noopener ugc nofollow" target="_blank">来源于</a> <code class="du md me mf mg b"><a class="ae iu" href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html#SageMakerRuntime.Client.invoke_endpoint" rel="noopener ugc nofollow" target="_blank">boto3.client('sagemaker-runtime')</a></code>。您的客户端应用程序使用这个API(方法)从托管在指定端点的模型中获取推论。</p><p id="8532" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">使用这种调用端点的方式，我们还必须像前面在<code class="du md me mf mg b">Predictor</code>类中一样定义凭证，并且我们还需要端点名称。在这里，我们不需要使用<code class="du md me mf mg b">sagemaker</code>库。我们只需要实例化<code class="du md me mf mg b">boto3.client()</code>并输入所有需要的参数。</p><p id="3897" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">我们向端点请求预测的方式是使用来自<code class="du md me mf mg b">boto3.client()</code>的<code class="du md me mf mg b">.invoke_endpoint()</code>方法。给定的响应是一个复杂的JSON，但是我们只需要包含预测结果的<code class="du md me mf mg b">"Body"</code>字段；然后我们<code class="du md me mf mg b">.read()</code>和<code class="du md me mf mg b">.decode()</code>结果得到干净的预测。有关更多详细信息，请参见下面的代码片段。</p><figure class="ne nf ng nh fd ij"><div class="bz dy l di"><div class="ni nj l"/></div></figure><p id="fb11" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">下面是结果。我们希望结果应该与我们使用<code class="du md me mf mg b">PyTorchModel.deploy().predict()</code>和<code class="du md me mf mg b">Predictor.predict()</code>时的结果相同。</p><figure class="ne nf ng nh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nv"><img src="../Images/d589fffefe88d9f67c9df118b5c4b07a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xTwzJLGceSdhRXyA33_Lbg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Results from boto3.client(‘sagemaker-runtime’).invoke_endpoint()</figcaption></figure><h2 id="ec01" class="kp kq hi bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">删除端点</h2><p id="6edb" class="pw-post-body-paragraph iy iz hi jb b jc lp je jf jg lq ji jj la my jm jn le mz jq jr li na ju jv jw hb bi translated">这一部分只是一个附加部分，但在使用SageMaker时，让你满足预算约束真的很重要。</p><blockquote class="nw"><p id="5e21" class="nx ny hi bd nz oa ob oc od oe of jw dx translated">如果您不再需要它，不要忘记删除您的SageMaker端点。你不想在月底时在你的信用卡上收取任何不必要的费用，是吗？</p></blockquote><p id="3da5" class="pw-post-body-paragraph iy iz hi jb b jc og je jf jg oh ji jj la oi jm jn le oj jq jr li ok ju jv jw hb bi translated">简单地说，您可以按照下面的说明删除所有未使用的SageMaker端点、配置或模型。</p><div class="jx jy ez fb jz ka"><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-delete-resources.html" rel="noopener  ugc nofollow" target="_blank"><div class="kb ab dw"><div class="kc ab kd cl cj ke"><h2 class="bd hj fi z dy kf ea eb kg ed ef hh bi translated">删除端点和资源</h2><div class="kh l"><h3 class="bd b fi z dy kf ea eb kg ed ef dx translated">删除端点以停止产生费用。使用AWS SDK for Python (Boto3)以编程方式删除端点…</h3></div><div class="ki l"><p class="bd b fp z dy kf ea eb kg ed ef dx translated">docs.aws.amazon.com</p></div></div></div></a></div><h1 id="5f7d" class="mh kq hi bd kr mi mj mk kv ml mm mn kz mo mp mq ld mr ms mt lh mu mv mw ll mx bi translated">限制</h1><p id="7885" class="pw-post-body-paragraph iy iz hi jb b jc lp je jf jg lq ji jj la my jm jn le mz jq jr li na ju jv jw hb bi translated">如果你达到这一点，没有任何持续的错误，为你的不可思议而自豪吧！破解SageMaker来创建自己的GPU多模型’端点并不是那么容易。</p><p id="2485" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">尽管如此，不幸的是，我不得不告诉你一些事情:</p><p id="f82e" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">我使用PyTorch模型和TensorFlow模型完成了部署这种“多模型”端点所需的所有工作。此时，端点成功工作，可以从AWS网络环境外部调用。我应该感到感激，不是吗？</p><p id="c8e0" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">基于这个工作流程，我试图模仿我所做的来创建GPU的“多伯特模型”端点。BERT模型的规模显然比我们在这两篇文章中讨论的PyTorch/TensorFlow模型大得多。如果你有不止一个BERT模型会怎样？当然，这将是一个巨大的规模和许多资源的需要。</p><p id="71be" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj la jl jm jn le jp jq jr li jt ju jv jw hb bi translated">所有步骤都做得很清楚，没有任何重大问题。多个BERT模型被安全地部署在SageMaker端点上。但是，当我试图使用<code class="du md me mf mg b">HuggingFaceModel.deploy().predict()</code>时，出现了一个错误。上面说什么GPU内存不足。随即，我也尝试在部署时增加GPU实例的数量。但是，这样的错误一直存在。该错误很可能与专用于GPU硬件的内存有关，而不是常规内存。如果是这样，那么这意味着我需要一个更强大的GPU，从而导致更多的成本。<strong class="jb hj">因此，这意味着这种黑客攻击SageMaker以获得GPU“多模型”端点的技术不适用于繁重的深度学习模型。</strong></p><h1 id="a1e9" class="mh kq hi bd kr mi mj mk kv ml mm mn kz mo mp mq ld mr ms mt lh mu mv mw ll mx bi translated">结论</h1><p id="fb4f" class="pw-post-body-paragraph iy iz hi jb b jc lp je jf jg lq ji jj la my jm jn le mz jq jr li na ju jv jw hb bi translated">太棒了，你已经完成了所有两篇文章。让我们回顾一下到目前为止我们所学的内容。</p><ul class=""><li id="698c" class="ln lo hi jb b jc jd jg jh la nb le nc li nd jw lu lv lw lx bi translated">SageMaker上有许多部署类型，我们可以用来优化成本。其中之一是使用多模型端点。</li><li id="025f" class="ln lo hi jb b jc ly jg lz la ma le mb li mc jw lu lv lw lx bi translated">为了实现这种“多模型”端点，我们必须准备一些东西，例如AWS帐户、IAM策略、S3桶、Python环境等。</li><li id="5b3c" class="ln lo hi jb b jc ly jg lz la ma le mb li mc jw lu lv lw lx bi translated">我们用来实现这种“多模型”端点的模型样本是在MNIST数据集上训练的PyTorch模型和在IMDB数据集上训练的TensorFlow模型。在这两个模型被训练之后，我们使用它们来预测简单的测试数据。</li><li id="e5ec" class="ln lo hi jb b jc ly jg lz la ma le mb li mc jw lu lv lw lx bi translated">为了黑掉SageMaker，我们必须准备很多东西，以便能够实现这种“多模型”端点:<br/> -定义正确的目录结构<br/> -编写<code class="du md me mf mg b">inference.py</code>脚本<br/> -调试脚本<br/> -压缩主目录并将其上传到S3桶，以及<br/> -使用SageMaker SDK进行部署。</li><li id="2f20" class="ln lo hi jb b jc ly jg lz la ma le mb li mc jw lu lv lw lx bi translated">我们还尝试使用两种方式从AWS网络环境外部调用部署的端点，即使用<code class="du md me mf mg b">Predictor</code>类和使用<code class="du md me mf mg b">invoke_endpoint()</code>方法。</li><li id="f49b" class="ln lo hi jb b jc ly jg lz la ma le mb li mc jw lu lv lw lx bi translated">最后，我们讨论了这种“多模型”端点的限制，其中它不适合于繁重的深度学习模型。</li></ul><blockquote class="iv iw ix"><p id="08e3" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">最后，一如既往，你可以在我的GitHub账户<a class="ae iu" href="https://github.com/utomoreza/MultimodelEndpoint_hacked" rel="noopener ugc nofollow" target="_blank">这里</a>找到我的回购中需要的所有文件和文件夹。</p></blockquote></div><div class="ab cl ol om gp on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="hb hc hd he hf"><h1 id="0f8a" class="mh kq hi bd kr mi os mk kv ml ot mn kz mo ou mq ld mr ov mt lh mu ow mw ll mx bi translated">关于作者</h1><p id="3ac1" class="pw-post-body-paragraph iy iz hi jb b jc lp je jf jg lq ji jj la my jm jn le mz jq jr li na ju jv jw hb bi translated"><a class="ae iu" href="https://www.linkedin.com/in/utomorezadwi/" rel="noopener ugc nofollow" target="_blank"> Reza </a>是一名专门从事数据驱动分析的工程师。他目前在Tokopedia担任高级数据科学家。在业余时间，他喜欢在<a class="ae iu" rel="noopener" href="/@utomorezadwi">媒体</a>上写文章，学习新东西，或者<a class="ae iu" href="https://github.com/utomoreza" rel="noopener ugc nofollow" target="_blank">创建副业</a>。</p></div></div>    
</body>
</html>
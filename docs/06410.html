<html>
<head>
<title>SVM Classification with sklearn.svm.SVC: How To Plot A Decision Boundary With Margins in 2D Space</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用sklearn.svm.SVC进行SVM分类:如何在2D空间中绘制带有边距的决策边界</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/svm-classification-with-sklearn-svm-svc-how-to-plot-a-decision-boundary-with-margins-in-2d-space-7232cb3962c0?source=collection_archive---------2-----------------------#2021-08-19">https://medium.com/geekculture/svm-classification-with-sklearn-svm-svc-how-to-plot-a-decision-boundary-with-margins-in-2d-space-7232cb3962c0?source=collection_archive---------2-----------------------#2021-08-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/60b6dc54072049083fcaf77e1b2844cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dl-Nk6flF_mX5S1kh96T8Q.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Photo by <a class="ae iu" href="https://www.pexels.com/@cottonbro?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">cottonbro</a> from <a class="ae iu" href="https://www.pexels.com/photo/healthy-wood-playing-sport-5740517/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><p id="fc68" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，如果你厌倦了挖掘大量无用的链接，希望找到如何用支持向量机算法绘制决策边界和边缘线的详细解释，那么恭喜你——你终于找到了！还有最后一步——阅读我关于这个主题的简单指南。开始吧！</p><blockquote class="jt ju jv"><p id="c536" class="iv iw jw ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">灵感来源:<a class="ae iu" href="https://scikit-learn.org/stable/auto_examples/svm/plot_svm_margin.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/SVM/plot _ SVM _ margin . html</a></p></blockquote><p id="517f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jw">拜托，不要只走过上面的链接。也许，它会帮助你更好地理解这个话题。</em></p></div><div class="ab cl ka kb gp kc" role="separator"><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf"/></div><div class="hb hc hd he hf"><h1 id="584f" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">0.让模特学会！</h1><p id="2784" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">我相信你已经熟悉这一步了。这里我们创建一个数据集，然后按训练和测试样本进行拆分，最后用<code class="du lk ll lm ln b">sklearn.svm.SVC(kernel='linear')</code>训练一个模型。注意，我们确切地使用了<strong class="ix hj">线性</strong>内核类型(一些例子中的<a class="ae iu" href="https://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html" rel="noopener ugc nofollow" target="_blank">链接</a>)。仔细阅读评论:</p><pre class="lo lp lq lr fd ls ln lt lu aw lv bi"><span id="e078" class="lw ki hi ln b fi lx ly l lz ma"><strong class="ln hj">import numpy as np<br/>from sklearn.svm import SVC</strong></span><span id="c845" class="lw ki hi ln b fi mb ly l lz ma"># Creating a random dataset of 2,000 samples and only 2 features<br/># (for 2–dimensional space). And yeah, it's a binary classification<br/># here (`y` contains two classes: 0 and 1).<br/><strong class="ln hj">X, y = make_classification(n_samples=2000, n_features=2,<br/>                           n_informative=2, n_redundant=0,<br/>                           n_classes=2,<br/>                           random_state=32)</strong></span><span id="e1b6" class="lw ki hi ln b fi mb ly l lz ma"># Splitting our dataset by train and test parts.<br/># `stratify` is here to make our splitting balanced<br/># in terms of classes.<br/><strong class="ln hj">X_train, X_test, y_train, y_test = train_test_split(X, y,<br/>                                   test_size=0.3, stratify=y,<br/>                                   random_state=32)</strong></span><span id="7c1d" class="lw ki hi ln b fi mb ly l lz ma"># And here we train our model. IMPORTANT: we use kernel='linear'.<br/><strong class="ln hj">svc_model = SVC(kernel='linear', random_state=32)<br/>svc_model.fit(X_train, y_train)</strong></span></pre><p id="eb61" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">很好！模型已经训练好了，现在你想绘制一个决策边界(超平面)，但是等等……你不知道从哪里开始。别担心，我会一步步指导你。</p><h1 id="78ed" class="kh ki hi bd kj kk mc km kn ko md kq kr ks me ku kv kw mf ky kz la mg lc ld le bi translated">1.我们需要什么来绘制决策边界？</h1><p id="a40f" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">我们需要一个通用的等式:</p><figure class="lo lp lq lr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/54a8cc7647bb48d80b893b52768a38f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i0IVm-YFUdbk6lVuyoXqCw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">General equation of a decision boundary in a vector form</figcaption></figure><blockquote class="jt ju jv"><p id="263d" class="iv iw jw ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated"><strong class="ix hj">注:</strong>由于我们在二维空间中操作，决策边界将由2D直线表示，因此<code class="du lk ll lm ln b">w</code>和<code class="du lk ll lm ln b">x</code>都由两个元素组成。</p></blockquote><p id="7a6a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">诀窍就在这里。我们转换矢量<code class="du lk ll lm ln b">w</code>和<code class="du lk ll lm ln b">x</code>的点积，然后求解<code class="du lk ll lm ln b">x</code>的第二个元素的方程:</p><figure class="lo lp lq lr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/4fe692a868b962961f4b795f3b0a6597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OrKFPKs6wDi3zIJININ3Fg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">From general equation to a 2D-line equation</figcaption></figure><p id="0e7d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">结果，我们得到了斜率截距形式的经典线性方程，在我们的例子中，它描述了一条2D直线。好的，我们知道了，但是你可能还有一些问题，比如:</p><ol class=""><li id="aefa" class="mi mj hi ix b iy iz jc jd jg mk jk ml jo mm js mn mo mp mq bi translated"><code class="du lk ll lm ln b">w_1</code>和<code class="du lk ll lm ln b">w_2</code>到底存储在我们的<code class="du lk ll lm ln b">sklearn</code>模型对象的什么地方？</li><li id="922a" class="mi mj hi ix b iy mr jc ms jg mt jk mu jo mv js mn mo mp mq bi translated">还有什么是<code class="du lk ll lm ln b">x_1</code>和<code class="du lk ll lm ln b">x_2</code>？</li></ol><p id="11cf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">看图:</p><figure class="lo lp lq lr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/4d125356ddcff5d025162be5cd502977.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JXRQiHgNPlbALL1jVIHk4A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Revealing the parts of a 2D-line equation</figcaption></figure><p id="38f7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du lk ll lm ln b"><strong class="ix hj">w</strong></code>包含在我们的模型(<code class="du lk ll lm ln b"><strong class="ix hj">svc_model.coef_</strong></code>)的属性<code class="du lk ll lm ln b"><strong class="ix hj">coef_</strong></code>中，这些是我们的决策边界的法向量的坐标(该向量与超平面正交)。</p><p id="507c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du lk ll lm ln b"><strong class="ix hj">b</strong></code>是我们模型(<code class="du lk ll lm ln b"><strong class="ix hj">svc_model.intercept_</strong></code>)的一个属性<code class="du lk ll lm ln b"><strong class="ix hj">intercept_</strong></code>。</p><p id="c25c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du lk ll lm ln b"><strong class="ix hj">x_1</strong></code>是一个坐标平面的<strong class="ix hj"> x点</strong>的数组。</p><p id="a783" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du lk ll lm ln b"><strong class="ix hj">x_2</strong></code>是一个坐标平面的<strong class="ix hj"> y点</strong>的<em class="jw">结果</em>数组。</p><blockquote class="jt ju jv"><p id="06bc" class="iv iw jw ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">正如你所注意到的，我们只有2个元素用于<code class="du lk ll lm ln b">w</code>，同样的元素用于<code class="du lk ll lm ln b">x</code>。这是因为我们正在探索二维空间中的SVM分类，只是为了让绘制SVM决策边界和边际的想法更加清晰。</p></blockquote><h1 id="f8c7" class="kh ki hi bd kj kk mc km kn ko md kq kr ks me ku kv kw mf ky kz la mg lc ld le bi translated">2.把所有的东西放在一起</h1><p id="0b4c" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">只要对2D线公式做一些替换…</p><figure class="lo lp lq lr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/c023a4788a175c9dbecfeaedafa33ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FLY-N1jpEjLdvRimDuRGDw.png"/></div></div></figure><p id="c3bd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">…在代码中，我们得到:</p><pre class="lo lp lq lr fd ls ln lt lu aw lv bi"><span id="f3a4" class="lw ki hi ln b fi lx ly l lz ma"><strong class="ln hj">import matplotlib.pyplot as plt<br/>import seaborn as sns</strong></span><span id="f538" class="lw ki hi ln b fi mb ly l lz ma"><strong class="ln hj">plt.figure(figsize=(10, 8))</strong></span><span id="c2f8" class="lw ki hi ln b fi mb ly l lz ma"># Plotting our two-features-space<br/><strong class="ln hj">sns.scatterplot(x=X_train[:, 0], <br/>                y=X_train[:, 1], <br/>                hue=y_train, <br/>                s=8);</strong></span><span id="6b25" class="lw ki hi ln b fi mb ly l lz ma"># Constructing a hyperplane using a formula.<br/><strong class="ln hj">w = </strong><strong class="ln hj">svc_model</strong><strong class="ln hj">.coef_[0]           </strong># w consists of 2 elements<strong class="ln hj"><br/>b = </strong><strong class="ln hj">svc_model</strong><strong class="ln hj">.intercept_[0]      </strong># b consists of 1 element<strong class="ln hj"><br/>x_points = np.linspace(-1, 1)    </strong># generating x-points from -1 to 1<strong class="ln hj"><br/>y_points = -(w[0] / w[1]) * x_points - b / w[1]  </strong># getting corresponding y-points</span><span id="a785" class="lw ki hi ln b fi mb ly l lz ma"># Plotting a red hyperplane<br/><strong class="ln hj">plt.plot(x_points, y_points, c='r');</strong></span></pre><p id="5198" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">它的输出是:</p><figure class="lo lp lq lr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/b86fa26b1934a7f928bef583232fcf29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sKV7okWSKdlkajw3mo7Pvw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Two features (that’s why we have exactly 2 axis), two classes (blue and yellow) and a red decision boundary (hyperplane) in a form of 2D-line</figcaption></figure><p id="7e32" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">太好了！我们现在已经绘制了决策边界！因此，是时候转向SVM利润率了。</p><h1 id="ee2a" class="kh ki hi bd kj kk mc km kn ko md kq kr ks me ku kv kw mf ky kz la mg lc ld le bi translated">4.绘制SVM边距</h1><p id="5879" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">在这里，我创建了一个决策边界和利润的图片。灰色虚线是“边界”,显示了它们的基础，以及如何计算它们的大小。仔细看各方面:</p><figure class="lo lp lq lr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/852ea955d8f1b7f435113a9ab189c44a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bQz9W852S0Sh_rzpqWyIXg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">SVM classification illustrated. Decision boundary, margins, and support vectors.</figcaption></figure><p id="8c05" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，虚线只是沿着向量<code class="du lk ll lm ln b">w</code>的方向平移了等于<code class="du lk ll lm ln b">margin</code>的距离的判定边界线。我们可以用这个算法来实现:</p><pre class="lo lp lq lr fd ls ln lt lu aw lv bi"><span id="6b1a" class="lw ki hi ln b fi lx ly l lz ma"><strong class="ln hj">Step 1.</strong> Find a normal vector to the decision boundary;</span><span id="cf59" class="lw ki hi ln b fi mb ly l lz ma"><strong class="ln hj">Step 2.</strong> Calculate a unit vector of that normal vector -- let's call it `w_hat`;</span><span id="c0d3" class="lw ki hi ln b fi mb ly l lz ma"><strong class="ln hj">Step 3.</strong> Get a distance between the lines (margin);</span><span id="c781" class="lw ki hi ln b fi mb ly l lz ma"><strong class="ln hj">Step 4.</strong> Translate all points of the decision boundary to a new location by this formula:</span><span id="0232" class="lw ki hi ln b fi mb ly l lz ma"># for a line above<br/>new_points_above = hyperplane_points <strong class="ln hj">+</strong> w<!-- -->_hat * margin</span><span id="0648" class="lw ki hi ln b fi mb ly l lz ma"># for a line below<br/>new_points_below = hyperplane_points <strong class="ln hj">-</strong> w<!-- -->_hat * margin</span></pre><p id="3c1e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">第一步:</strong>我们已经知道，<code class="du lk ll lm ln b">w</code>是我们需要的法向量——它总是正交于超平面(顺便说一句，这就是为什么——<a class="ae iu" href="https://datascience.stackexchange.com/questions/6054/in-svm-algorithm-why-vector-w-is-orthogonal-to-the-separating-hyperplane" rel="noopener ugc nofollow" target="_blank">stack overflow link</a>)。而且我们还知道它包含在我们模型的属性<code class="du lk ll lm ln b"><strong class="ix hj">coef_</strong></code>(<code class="du lk ll lm ln b"><strong class="ix hj">svc_model.coef_</strong></code>)中。</p><p id="d73e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">第二步:</strong>我们来计算单位矢量:</p><figure class="lo lp lq lr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/6ba626daebfa60c96fbd1e457cf6e35c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dA10OjcsPbjvQHPhdC0faQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Unit-vector formula</figcaption></figure><pre class="lo lp lq lr fd ls ln lt lu aw lv bi"><span id="d5fa" class="lw ki hi ln b fi lx ly l lz ma"># Calculating the unit vector of w<br/><strong class="ln hj">w_hat = </strong><strong class="ln hj">svc_model</strong><strong class="ln hj">.coef_[0] / (np.sqrt(np.sum(</strong><strong class="ln hj">svc_model</strong><strong class="ln hj">.coef_[0] ** 2)))</strong></span></pre><p id="daf4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">第三步:</strong>现在我们计算保证金:</p><figure class="lo lp lq lr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/d3f00dc2dfb04cb61e32210949acdc81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*isurD3vZa0NCLP2alUc4JQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Margin formula</figcaption></figure><pre class="lo lp lq lr fd ls ln lt lu aw lv bi"><span id="1332" class="lw ki hi ln b fi lx ly l lz ma"># Calculating margin<br/><strong class="ln hj">margin = 1 / np.sqrt(np.sum(svc_model.coef_[0] ** 2))</strong></span></pre><p id="b3df" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第四步:最后，我们计算新线条的点数:</p><pre class="lo lp lq lr fd ls ln lt lu aw lv bi"><span id="5bf1" class="lw ki hi ln b fi lx ly l lz ma"># Calculating margin lines<br/><strong class="ln hj">new_points_up   = </strong><strong class="ln hj">hyperplane_points</strong><strong class="ln hj"> + w_hat * margin<br/>new_points_down = </strong><strong class="ln hj">hyperplane_points</strong><strong class="ln hj"> - w_hat * margin</strong></span></pre><p id="6383" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">还有一件事…是的，让我们把它们放在一起，然后画出结果:</p><pre class="lo lp lq lr fd ls ln lt lu aw lv bi"><span id="5bbb" class="lw ki hi ln b fi lx ly l lz ma"><strong class="ln hj">plt.figure(figsize=(10, 8))</strong></span><span id="5ceb" class="lw ki hi ln b fi mb ly l lz ma"># Plotting our two-features-space<br/><strong class="ln hj">sns.scatterplot(x=X_train[:, 0], <br/>                y=X_train[:, 1], <br/>                hue=y_train, <br/>                s=8);</strong></span><span id="8018" class="lw ki hi ln b fi mb ly l lz ma"># Constructing a hyperplane using a formula.<br/><strong class="ln hj">w = </strong><strong class="ln hj">svc_model</strong><strong class="ln hj">.coef_[0]           </strong># w consists of 2 elements<strong class="ln hj"><br/>b = </strong><strong class="ln hj">svc_model</strong><strong class="ln hj">.intercept_[0]      </strong># b consists of 1 element<strong class="ln hj"><br/>x_points = np.linspace(-1, 1)    </strong># generating x-points from -1 to 1<strong class="ln hj"><br/>y_points = -(w[0] / w[1]) * x_points - b / w[1]  </strong># getting corresponding y-points</span><span id="4c31" class="lw ki hi ln b fi mb ly l lz ma"># Plotting a red hyperplane<br/><strong class="ln hj">plt.plot(x_points, y_points, c='r');</strong></span><span id="b949" class="lw ki hi ln b fi mb ly l lz ma"># Encircle support vectors<br/><strong class="ln hj">plt.scatter(</strong><strong class="ln hj">svc_model</strong><strong class="ln hj">.support_vectors_[:, 0],<br/>            </strong><strong class="ln hj">svc_model</strong><strong class="ln hj">.support_vectors_[:, 1], <br/>            s=50, <br/>            facecolors='none', <br/>            edgecolors='k', <br/>            alpha=.5);</strong></span><span id="807b" class="lw ki hi ln b fi mb ly l lz ma"># Step 2 (unit-vector):<br/><strong class="ln hj">w_hat = </strong><strong class="ln hj">svc_model</strong><strong class="ln hj">.coef_[0] / (np.sqrt(np.sum(</strong><strong class="ln hj">svc_model</strong><strong class="ln hj">.coef_[0] ** 2)))</strong></span><span id="7f26" class="lw ki hi ln b fi mb ly l lz ma"># Step 3 (margin):<br/><strong class="ln hj">margin = 1 / np.sqrt(np.sum(</strong><strong class="ln hj">svc_model</strong><strong class="ln hj">.coef_[0] ** 2))</strong></span><span id="6975" class="lw ki hi ln b fi mb ly l lz ma"># Step 4 (calculate points of the margin lines):<br/><strong class="ln hj">decision_boundary_points = np.array(list(zip(x_points, y_points)))<br/>points_of_line_above = decision_boundary_points + w_hat * margin<br/>points_of_line_below = decision_boundary_points - w_hat * margin</strong></span><span id="acab" class="lw ki hi ln b fi mb ly l lz ma"># Plot margin lines</span><span id="ad4f" class="lw ki hi ln b fi mb ly l lz ma"># Blue margin line above<strong class="ln hj"><br/>plt.plot(points_of_line_above[:, 0], <br/>         points_of_line_above[:, 1], <br/>         'b--', <br/>         linewidth=2)</strong></span><span id="2542" class="lw ki hi ln b fi mb ly l lz ma"># Green margin line below<strong class="ln hj"><br/>plt.plot(points_of_line_below[:, 0], <br/>         points_of_line_below[:, 1], <br/>         'g--',<br/>         linewidth=2)</strong></span></pre><figure class="lo lp lq lr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/84a999eab84e2f0ddde3db031ead5a86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ovOnPaQ7tKLtGCOMmGDhw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Final result: decision boundary (red), support vector (encircled dots), and margin lines (dashed blue and green lines)</figcaption></figure><p id="480c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们完了。希望对你有帮助。如果是，按下<code class="du lk ll lm ln b">thumbs up 👍</code>。</p></div><div class="ab cl ka kb gp kc" role="separator"><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf"/></div><div class="hb hc hd he hf"><p id="9d15" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">感谢您的宝贵时间！🎉<br/> <strong class="ix hj">关注我</strong>更多有趣话题😉👍</p></div></div>    
</body>
</html>
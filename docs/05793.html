<html>
<head>
<title>Is Batch Normalization harmful? Improving Normalizer-Free ResNets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">批量归一化有害吗？改进无规格化器的结果</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/is-batch-normalization-harmful-improving-normalizer-free-resnets-cf44f2fc0b2e?source=collection_archive---------4-----------------------#2021-07-30">https://medium.com/geekculture/is-batch-normalization-harmful-improving-normalizer-free-resnets-cf44f2fc0b2e?source=collection_archive---------4-----------------------#2021-07-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/3ef3b08f590d47d08346eae047de97bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SBErFmc7KDd38j-2"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">Photo by <a class="ae iu" href="https://unsplash.com/@rojekilian?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Sarah Kilian</a> on <a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="c089" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最近几乎所有的深度学习架构都普遍使用批处理规范化来提高收敛速度和改善性能。但实际上并没有多少作品关注BN的弊端，而是将它们视为对模型有益的某种魔法。在这篇文章中，我们将试图理解BN的真实动态，理解它的挑战和局限性，并讨论一种训练深度网络的替代方法。我们还将看看最近的一项技术，它通过消除批处理规范化实现了一流的分类性能。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jt"><img src="../Images/7f89bd9dc23be0f0bf89ed58e9dd80c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OOgNQxks9CUBRvDSaLNIsA.png"/></div></div></figure><p id="0555" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该论文可通过以下链接获得。</p><p id="946b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">批量归一化:<a class="ae iu" href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener ugc nofollow" target="_blank">批量归一化:通过减少内部协变量偏移来加速深度网络训练</a></p><p id="13b0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">NF-resnet:<a class="ae iu" href="https://arxiv.org/pdf/2101.08692.pdf" rel="noopener ugc nofollow" target="_blank">特征化信号传播，以弥补非标准化resnet中的性能差距</a></p><p id="2277" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">背景+自适应梯度裁剪:<a class="ae iu" href="https://arxiv.org/pdf/2102.06171v1.pdf" rel="noopener ugc nofollow" target="_blank">无需归一化的高性能大规模图像识别</a></p><h2 id="2c35" class="jy jz hi bd ka kb kc kd ke kf kg kh ki jg kj kk kl jk km kn ko jo kp kq kr ks bi translated">批量标准化和收益<a class="ae iu" href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener ugc nofollow" target="_blank">【论文】</a></h2><p id="a082" class="pw-post-body-paragraph iv iw hi ix b iy kt ja jb jc ku je jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">批量规范化是做什么的？简而言之，批量标准化旨在通过减去和除以小批量统计数据来标准化输入。准确地说，该过程被描述为下面的函数。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ky"><img src="../Images/2695e2cda86cc0a3bbb17dd9b0af177d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a38Ek_REldjBl5PnE3ZC3w.png"/></div></div></figure><p id="3d97" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">根据经验，使用批量标准化的好处是收敛速度更快，准确度更高。如果我们更深入地研究这些改进的动力，批处理规范化</p><ul class=""><li id="cc70" class="kz la hi ix b iy iz jc jd jg lb jk lc jo ld js le lf lg lh bi translated">缩减剩余分支:作为剩余网络的特殊协同，BN约束了剩余分支的规模，稳定了早期训练。</li><li id="7928" class="kz la hi ix b iy li jc lj jg lk jk ll jo lm js le lf lg lh bi translated">消除了均值漂移:ReLU的输出是[0，inf)，自然有非零均值。这在深度神经网络中会变得复杂并成为问题，因为它会导致网络为所有数据预测相同的标签。</li><li id="ed25" class="kz la hi ix b iy li jc lj jg lk jk ll jo lm js le lf lg lh bi translated">具有正则化效果:因为批量统计是在训练数据的另一个子集上计算的，所以BN本质上充当了加强网络泛化的噪声。</li><li id="108c" class="kz la hi ix b iy li jc lj jg lk jk ll jo lm js le lf lg lh bi translated">允许有效的大批量训练:BN增加了最大稳定学习率，这是大批量训练的关键组成部分。</li></ul><p id="f944" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面列出的益处是其他研究的结论，在原始论文中有进一步的阐述。</p><h2 id="dd95" class="jy jz hi bd ka kb kc kd ke kf kg kh ki jg kj kk kl jk km kn ko jo kp kq kr ks bi translated">BN的问题</h2><p id="3d1f" class="pw-post-body-paragraph iv iw hi ix b iy kt ja jb jc ku je jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">哇，批处理规范化似乎确实为任何深度神经网络提供了很大的好处。也许不是，因为批处理规范化是</p><ul class=""><li id="57e8" class="kz la hi ix b iy iz jc jd jg lb jk lc jo ld js le lf lg lh bi translated">惊人的计算开销:BN是一个非常占用内存的任务，因为所有的批次统计数据都必须存储在层中。计算成本也很昂贵(大约20%，取决于网络)</li><li id="397b" class="kz la hi ix b iy li jc lj jg lk jk ll jo lm js le lf lg lh bi translated">训练和at推断之间的差异:推断的结果取决于批量大小？此外，引入必须调整的超参数。</li><li id="4454" class="kz la hi ix b iy li jc lj jg lk jk ll jo lm js le lf lg lh bi translated">打破了小批量中训练样本之间的独立性:这是一个明显的问题，因为每个样本都是使用小批量统计数据进行标准化的。后果是由于担心<em class="ln">作弊</em>而在一些任务中受到限制，如序列建模、对比学习。</li><li id="ae2b" class="kz la hi ix b iy li jc lj jg lk jk ll jo lm js le lf lg lh bi translated">性能对批量大小很敏感:当批量太小时，BN的性能通常很差。</li><li id="9c79" class="kz la hi ix b iy li jc lj jg lk jk ll jo lm js le lf lg lh bi translated">归一化会限制模型的能力:本文中没有讨论，但StyleGAN2、ESRGAN等论文在使用归一化时会在生成的图像中发现伪像。因为归一化会强制零均值，所以该算法会在图像中产生尖峰，从而改变图像其余部分的幅度。</li></ul><p id="4683" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">已经提出了批处理规范化的一些替代规范化来解决所提到的问题。一些例子是实例规范化和层规范化。其他工作修改初始化过程和网络架构，以逐个获得BN的好处。一些作品甚至能够成功地超越使用BN的ResNets(使用额外的正则化)。</p><p id="c0b2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，当批量非常小时，它们优于它们的批量归一化对应物，但是对于大批量，它们的性能比批量归一化网络差。大多数方法也有BN的一些局限性或有其独特的问题。这个结果也不能与美国国家运输署网络的结果相提并论。</p><p id="8e26" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://arxiv.org/pdf/2101.08692.pdf" rel="noopener ugc nofollow" target="_blank">无规格化器的ResNets </a> (NF-ResNets)是一个没有规格化的网络架构，可以用ResNets训练到有竞争力的精度。NF-resnet的剩余块按以下函数进行缩放。我们将在另一篇文章中深入讨论这篇文章，但是这篇文章中提出的技术通常适用于任何其他网络架构。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/c1721d501fc27dc9e6b45557d88577b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*Tf6qouuHWLMUM-OQRZ2LkA.png"/></div></figure><h2 id="381b" class="jy jz hi bd ka kb kc kd ke kf kg kh ki jg kj kk kl jk km kn ko jo kp kq kr ks bi translated">自适应渐变裁剪<a class="ae iu" href="https://arxiv.org/pdf/2102.06171v1.pdf" rel="noopener ugc nofollow" target="_blank">【论文】</a></h2><p id="e555" class="pw-post-body-paragraph iv iw hi ix b iy kt ja jb jc ku je jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">本文提出自适应梯度裁剪(AGC)技术来改善无归一化网络。梯度裁剪通过按照下面的公式约束梯度来稳定训练。然而，梯度削波被发现对削波阈值λ敏感。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/4ff0c7755057b5c67aa749e584a0e2ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*VWOhSeuSmVr0WMF-YT6m6w.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Gradient clipping</figcaption></figure><p id="3286" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">假设W和G是第<em class="ln"> l </em>层的权重矩阵和梯度。权重的Frobenius范数||W ||ᶠ被计算为权重平方之和的平方根。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/72a9a2487b2a81658874aa121ba7beef.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*IcAeRGp1e3XSzcL4frPD-g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">Frobenius norm</figcaption></figure><p id="f3a6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">权重和梯度||G ||ᶠ/||W ||ᶠ的Frobenius范数之间的比率提供了梯度步长有多大的更好的<em class="ln">相对</em>度量。为了解决对限幅阈值λ的依赖性，AGC限幅梯度基于梯度范数与参数范数的单位比，如下式所示。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/f19d1bdf1610a9e7a655751409083015.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*kB-jRQn5VVbjt49xdMB6VQ.png"/></div></figure><p id="252b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作者认为</p><blockquote class="ls lt lu"><p id="9f7c" class="iv iw ln ix b iy iz ja jb jc jd je jf lv jh ji jj lw jl jm jn lx jp jq jr js hb bi translated">AGC是一些归一化优化器的放宽，其基于参数范数施加最大更新大小，但不同时施加更新大小的下限或忽略梯度幅度。</p></blockquote><h2 id="3afe" class="jy jz hi bd ka kb kc kd ke kf kg kh ki jg kj kk kl jk km kn ko jo kp kq kr ks bi translated">实验</h2><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ly"><img src="../Images/2946705cf94f747275c90182e6b7a1b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ABSEoBrLU3a3v4KOxlusNQ.png"/></div></div></figure><p id="d07f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">左图比较了带批处理归一化的resnet、NF-resnet和带AGC的NF-resnet。AGC似乎有助于将NF-resnet扩展到更大的批量，同时保持与批量归一化网络类似/更好的性能。对较大批量的训练更不稳定，因为学习率也随着批量的增加而线性增加。比如批量4096训练时，学习率是压倒性的1.6。右图比较了不同限幅阈值和批量的结果。需要较小的限幅阈值来约束具有大批量的训练。</p><p id="5d0f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">另一个实验通过将AGC应用于网络中的某些层来研究AGC是否对所有层都有益。结果表明，不剪切最终的线性层总是更好的，并且在不剪切初始卷积的情况下稳定地训练是可能的，但是当以批量大小4096以默认学习速率1.6训练时，必须剪切所有四个阶段。AGC应用于除最终线性层之外的每一层。</p><h2 id="ca06" class="jy jz hi bd ka kb kc kd ke kf kg kh ki jg kj kk kl jk km kn ko jo kp kq kr ks bi translated">寻求最先进的</h2><p id="8b33" class="pw-post-body-paragraph iv iw hi ix b iy kt ja jb jc ku je jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">本文应用各种规则和技术来提高性能并与EfficientNet竞争。通过修改，ResNeXt-D模型大大超过了EfficientNet。取消批量标准化的效果似乎令人失望，因为NF-ResNet和AGC的修改并没有显示出下表所述的精度提高。然而，它们确实在延迟方面提供了显著的收益，这可以通过模型缩放转化为更高的准确性。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/90e32d7e23ad7b26254251842f23629f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c1ief3-JVrRhH6LYeyPAUw.png"/></div></div></figure><h2 id="8bc7" class="jy jz hi bd ka kb kc kd ke kf kg kh ki jg kj kk kl jk km kn ko jo kp kq kr ks bi translated">结论</h2><p id="e3ee" class="pw-post-body-paragraph iv iw hi ix b iy kt ja jb jc ku je jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">已经有许多关于分析标准化层的局限性的工作，包括批量标准化和提出替代方案。大多数BN替代品在大批量训练时表现不佳。本文提出了一种自适应梯度裁剪策略，使神经网络能够在大批量和大学习速率下训练。</p><p id="4310" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，本文中使用的NF-ResNet主干网似乎有许多局限性。首先，这些修改是特定于基于ResNet的具有跳过连接的体系结构的。这将不适用于深度学习的许多其他应用。接下来，虽然提出了许多与性能相关的BN问题，但除了延迟之外，该解决方案与批量标准化网络相比并未显示出显著的改进。这可能表明，该解决方案实际上并没有解决BN的问题，而只是更有效地实现了BN。</p><p id="bdd7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">尽管如此，这篇论文提供的结果和见解给我留下了深刻的印象。对我来说，在稳定训练的同时去除批处理规范化似乎是深度学习要实现的一个重要里程碑。</p></div></div>    
</body>
</html>
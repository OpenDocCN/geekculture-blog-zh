<html>
<head>
<title>Hugging Face DistilBert &amp; Tensorflow for Custom Text Classification.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于自定义文本分类的拥抱脸蒸馏和张量流。</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/hugging-face-distilbert-tensorflow-for-custom-text-classification-1ad4a49e26a7?source=collection_archive---------0-----------------------#2021-02-18">https://medium.com/geekculture/hugging-face-distilbert-tensorflow-for-custom-text-classification-1ad4a49e26a7?source=collection_archive---------0-----------------------#2021-02-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="3811" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">逐步指南</h2><div class=""/><div class=""><h2 id="6f6f" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">如何通过TensorFlow的拥抱脸API微调文本二进制分类的DistilBERT？</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/78526cf363bb1a9d856e205070aa96c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*drciYNtmeyyeF4WNNXMlcg.jpeg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Photo by <a class="ae jw" href="https://unsplash.com/@ninjason?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Jason Leung</a> on <a class="ae jw" href="https://unsplash.com/s/photos/bookshelf?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="c60b" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">介绍。</h1><p id="343f" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">在本教程中，您将看到一个使用<a class="ae jw" href="https://machinelearningmastery.com/transfer-learning-for-deep-learning/#:~:text=Transfer%20learning%20is%20a%20machine,model%20on%20a%20second%20task." rel="noopener ugc nofollow" target="_blank">迁移学习</a>技术的二进制文本分类实现。为此，我们将使用<em class="ll"> DistilBert，一个来自拥抱脸<em class="ll">变形金刚</em>库的</em>预训练模型及其用于<em class="ll"> Tensorflow </em>的API。</p><h1 id="1abd" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">为什么蒸馏。</h1><blockquote class="lm ln lo"><p id="d518" class="kp kq ll kr b ks lp is ku kv lq iv kx lr ls la lb lt lu le lf lv lw li lj lk hb bi translated"><a class="ae jw" rel="noopener" href="/huggingface/distilbert-8cf3380435b5">更小、更快、更便宜、更轻:推出蒸馏版DistilBERT。</a></p></blockquote><p id="8ca9" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">Medium上的<em class="ll">拥抱脸</em>的评论文章标题给出了为什么我们应该在我们的任务中使用这个模型的完整解释。我们有一个小的数据集，这个模型可能是我们尝试的一个不错的首选。另外，<a class="ae jw" href="https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8" rel="noopener" target="_blank">另一篇关于Medium </a>的文章建议使用<em class="ll"> DistilBERT </em>作为快速基线模型。<em class="ll"> DistilBERT </em>可以在<em class="ll"> BERT </em>的表现上达到一个合理的下限，具有训练更快的优势。这个API我们在一个写得非常好的<a class="ae jw" href="https://huggingface.co/transformers/master/model_doc/distilbert.html" rel="noopener ugc nofollow" target="_blank"> <em class="ll">抱抱脸</em>文档</a>和<a class="ae jw" href="https://blog.tensorflow.org/2019/11/hugging-face-state-of-art-natural.html" rel="noopener ugc nofollow" target="_blank"> <em class="ll"> Tensorflow </em>博客</a>。你会发现应用它是多么简单和直观。</p><blockquote class="lm ln lo"><p id="024f" class="kp kq ll kr b ks lp is ku kv lq iv kx lr ls la lb lt lu le lf lv lw li lj lk hb bi translated">一个更小的通用语言表示模型，称为DistilBERT，它可以像它的更大对应物一样在广泛的任务上有良好的性能。</p></blockquote><p id="c640" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">如果你还在犹豫从<em class="ll">抱脸</em>库中选择哪个模型，你可以用他们的<a class="ae jw" href="https://huggingface.co/models?filter=en,tf&amp;pipeline_tag=text-classification" rel="noopener ugc nofollow" target="_blank">过滤器</a>按任务、库、语言等选择一个模型。<em class="ll"> DistilBERT </em>是文本分类任务<em class="ll">列表中的第一个(</em>是<a class="ae jw" href="https://huggingface.co/distilbert-base-uncased" rel="noopener ugc nofollow" target="_blank">distil Bert-base-uncased</a>的一个微调检查点，在SST-2上微调)。所以我们选择了它——太好了！</p><h1 id="34e9" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">对数据的回顾。</h1><p id="75b1" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">代码示例的数据来自我之前的项目。我从一个菜谱网站上收集的，拆分成训练集和测试集(拆分比例0.2)。数据集包含一列源文本(你可以在这里阅读数据集<a class="ae jw" rel="noopener" href="/codestory/nlp-text-pre-processing-and-feature-engineering-python-69338fa0372e">或查看这个</a><a class="ae jw" href="https://colab.research.google.com/drive/1YOZ60sdOjSbIiB3IiJVUYprQhJDEVxXB?usp=sharing" rel="noopener ugc nofollow" target="_blank">笔记本</a>)。还有一个带标签的列<em class="ll">。</em><strong class="kr hs">业务目标</strong>是确定每段的标签是<em class="ll">“配料”</em>还是<em class="ll">“配方说明”。</em></p><p id="7086" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">让我们安装、导入库，并为模型的超参数定义常数:</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="ea73" class="mc jy hi ly b fi md me l mf mg">!pip install transformers</span><span id="14ac" class="mc jy hi ly b fi mh me l mf mg">import pandas as pd<br/>import tensorflow as tf<br/>import transformers<br/>from transformers import DistilBertTokenizer<br/>from transformers import TFDistilBertForSequenceClassification</span><span id="18e9" class="mc jy hi ly b fi mh me l mf mg">pd.set_option('display.max_colwidth', None)<br/>MODEL_NAME = 'distilbert-base-uncased-finetuned-sst-2-english'<br/>BATCH_SIZE = 16<br/>N_EPOCHS = 3</span></pre><p id="cc70" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">目前，我们只对“段落”和“标签”列感兴趣。看下图(图1):“段落”中的文本是一个源文本，而且是用字节表示的。在X_train集合中，我们有3898行，X_test集合有973行。在这些集合中没有NaNs或空字符串。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mi"><img src="../Images/f089ab863523e25be6321871817014b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1hON-1TM81e1EWVDPK4n1Q.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Pic.1 Load Train and Test data sets, a sample from X_train, shape check.</figcaption></figure><p id="8892" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">如果段落是“配方成分”，目标变量是“1”，如果是“说明”，目标变量是“0”。标签的比例大约是20% 1和80% 0。现在，让我们进入下一步。</p><h1 id="c95c" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">准备数据作为模型输入。</h1><p id="cda2" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">在文本成为模型输入之前，首先，我们应该对它进行标记化。<a class="ae jw" href="https://huggingface.co/transformers/model_doc/distilbert.html#distilberttokenizer" rel="noopener ugc nofollow" target="_blank"><em class="ll">distilbertokenizer</em></a>接受“str”(单个示例)、“List[str]”(批量或单个预标记示例)或“List[List[str]]”(批量预标记示例)类型的文本。因此，我们需要将字节表示转换成字符串。Lambda函数是一个很好的解决方案。</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="af9f" class="mc jy hi ly b fi md me l mf mg">X_train = X_train.apply(lambda x: str(x[0], 'utf-8'))<br/>X_test = X_test.apply(lambda x:  str(x[0], 'utf-8'))</span></pre><p id="826d" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">DistilBERT中最大支持的标记化句子长度是512个单词。</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="33e3" class="mc jy hi ly b fi md me l mf mg"><em class="ll">#define a tokenizer object<br/></em>tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)</span><span id="36ee" class="mc jy hi ly b fi mh me l mf mg">#tokenize the text<br/>train_encodings = tokenizer(list(X_train.values),<br/>                            truncation=True, <br/>                            padding=True)</span><span id="2dfd" class="mc jy hi ly b fi mh me l mf mg">test_encodings = tokenizer(list(X_test.values),<br/>                           truncation=True, <br/>                           padding=True)</span></pre><p id="ab1d" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">我们传递给记号赋予器的参数hold:我们的set in "list[str]"表示，truncation=True，padding=True。如果标记化句子长度小于最大模型输入长度，标记化器将其截断到标记化句子最大长度。如果标记化的句子长度小于最大标记化的句子长度，标记化器用零填充，直到最大标记化的句子长度。在下图中，您可以看到结果示例:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mj"><img src="../Images/a1e04486ccd83c615591d495e5bb2de5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sU-GnyDLPcpEOBiyyHGY1g.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx">Pic2. An example of tokenized sentece by DistilBertTokenizer.</figcaption></figure><p id="4c5f" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">DistilBertTokenizer引用超类<code class="du mk ml mm ly b"><a class="ae jw" href="https://huggingface.co/transformers/model_doc/bert.html#transformers.BertTokenizer" rel="noopener ugc nofollow" target="_blank"><strong class="kr hs">BertTokenizer</strong></a><strong class="kr hs">.</strong></code>，它返回给我们一组输入<a class="ae jw" href="https://huggingface.co/transformers/glossary.html#input-ids" rel="noopener ugc nofollow" target="_blank">索引</a>和<a class="ae jw" href="https://huggingface.co/transformers/glossary.html#attention-mask" rel="noopener ugc nofollow" target="_blank">注意屏蔽</a>。现在，我们只需要将标签和编码转换成Tensorflow数据集对象:</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="1846" class="mc jy hi ly b fi md me l mf mg">train_dataset = <br/>tf.data.Dataset.from_tensor_slices((dict(train_encodings),<br/>                                    list(y_train.values)))<br/>test_dataset = <br/>tf.data.Dataset.from_tensor_slices((dict(test_encodings),<br/>                                    list(y_test.values)))</span></pre><h1 id="7c3a" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">使用native TensorFlow进行微调。</h1><p id="f607" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">在下一步中，我们采用TFDistilBertForSequenceClassification，并将模型的名称作为参数。设置学习率并定义损失函数。编译模型并运行model.fit()方法进行训练。</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="820f" class="mc jy hi ly b fi md me l mf mg">model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)</span><span id="4c36" class="mc jy hi ly b fi mh me l mf mg"><em class="ll">#chose the optimizer<br/></em>optimizerr = tf.keras.optimizers.Adam(learning_rate=5e-5)</span><span id="5b65" class="mc jy hi ly b fi mh me l mf mg"><em class="ll">#define the loss function <br/></em>losss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)</span><span id="0f10" class="mc jy hi ly b fi mh me l mf mg">#build the model<br/>model.compile(optimizer=optimizerr,<br/>              loss=losss,<br/>              metrics=['accuracy'])</span><span id="b811" class="mc jy hi ly b fi mh me l mf mg"># train the model <br/>model.fit(train_dataset.shuffle(len(X_train)).batch(BATCH_SIZE),<br/>          epochs=N_EPOCHS,<br/>          batch_size=BATCH_SIZE)</span></pre><p id="1db4" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">下面你可以看到我们的模型有多精确。在第二个时期，我们已经获得了100%的准确度:</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="7a21" class="mc jy hi ly b fi md me l mf mg">&gt;&gt;&gt; Epoch 1/3<br/>&gt;&gt;&gt; 244/244 [==============================] - 131s 374ms/step - <br/>&gt;&gt;&gt; loss: 0.1468 - accuracy: 0.9568<br/>&gt;&gt;&gt; Epoch 2/3<br/>&gt;&gt;&gt; 244/244 [==============================] - 95s 388ms/step - <br/>&gt;&gt;&gt; loss: 3.1370e-04 - accuracy: 1.0000<br/>&gt;&gt;&gt; Epoch 3/3<br/>&gt;&gt;&gt; 244/244 [==============================] - 97s 396ms/step -<br/>&gt;&gt;&gt; loss: 5.7763e-05 - accuracy: 1.0000</span></pre><h1 id="559f" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">模型评估。</h1><p id="61de" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">Tensorflow的拥抱脸API对任何数据科学家方法都具有直观性。让我们在测试集上评估模型，并在新数据出现之前进行评估:</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="42c1" class="mc jy hi ly b fi md me l mf mg"><em class="ll"># model evaluation on the test set<br/></em>model.evaluate(test_dataset.shuffle(len(X_test)).batch(BATCH_SIZE), <br/>               return_dict=True, <br/>               batch_size=BATCH_SIZE)</span><span id="c088" class="mc jy hi ly b fi mh me l mf mg"><br/>&gt;&gt;&gt; 61/61 [==============================] - 10s 147ms/step - <br/>&gt;&gt;&gt; loss: 1.7124e-05 - accuracy: 1.0000<br/>&gt;&gt;&gt; {'accuracy': 1.0, 'loss': 1.7123966244980693e-05}</span></pre><p id="1505" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">我们得到了相当好的结果！现在，对于其他文本段落的模型估计，我们创建一个函数来查看每个类别的预测概率(以查看我们的模型在预测中有多确定):</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="5906" class="mc jy hi ly b fi md me l mf mg">def predict_proba(text_list, model, tokenizer):  </span><span id="0bea" class="mc jy hi ly b fi mh me l mf mg"><em class="ll">    #tokenize the text<br/>    </em>encodings = tokenizer(text_list, <br/>                          max_length=MAX_LEN, <br/>                          truncation=True, <br/>                          padding=True)<br/><em class="ll">    #transform to tf.Dataset<br/></em>    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings)))</span><span id="e936" class="mc jy hi ly b fi mh me l mf mg">    #predict<br/>    preds = model.predict(dataset.batch(1)).logits  <br/>    <br/>    #transform to array with probabilities<br/>    res = tf.nn.softmax(preds, axis=1).numpy()      <br/>    <br/>    return res</span></pre><p id="5488" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">我们这里取一个. txt文件<a class="ae jw" href="https://github.com/Galina-Blokh/ai_assignment_aidock/blob/refator/data/test_links.txt" rel="noopener ugc nofollow" target="_blank"/>。这个文件包含10个食谱页面的10个URL。我们的模型还没有看到来自它们的文本数据。假设您从第一个<a class="ae jw" href="https://www.loveandlemons.com/green-bean-salad-recipe/" rel="noopener ugc nofollow" target="_blank"> URL </a>获取数据。您输入到预测模型中的字符串列表将类似于下面的单元格。(*第一串是配料，后面三串是说明):</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="60d9" class="mc jy hi ly b fi md me l mf mg">strings_list =["""<br/>                  1 pound green beans, trimmed<br/>                  ½ head radicchio, sliced into strips<br/>                  Scant ¼ cup thinly sliced red onion<br/>                  Honey Mustard Dressing, for drizzling<br/>                  2 ounces goat cheese<br/>                  2 tablespoons chopped walnuts<br/>                  2 tablespoons sliced almonds<br/>                  ¼ cup tarragon<br/>                  Flaky sea salt""",</span><span id="b8d9" class="mc jy hi ly b fi mh me l mf mg">               """Bring a large pot of salted water to a boil and                  set a bowl of ice water nearby. Drop the green beans into the boiling water and blanch for 2 minutes. Remove the beans and immediately immerse in the ice water long enough to cool completely, about 15 seconds. Drain and place on paper towels to dry. """,</span><span id="063e" class="mc jy hi ly b fi mh me l mf mg">               """Transfer the beans to a bowl and toss with the radicchio, onion, and a few spoonfuls of the dressing.""",</span><span id="b8b7" class="mc jy hi ly b fi mh me l mf mg">               """Arrange on a platter and top with small dollops of goat cheese, the walnuts, almonds, and tarragon. Drizzle with more dressing, season to taste with flaky salt, and serve."""]</span></pre><p id="d9ef" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">当您为新数据调用predict_proba()函数时，结果将是一个形状为(4，2)的NumPy数组。四个n数组(每个段落一个)，具有两个概率值(对于类0和类1):</p><pre class="jh ji jj jk fd lx ly lz ma aw mb bi"><span id="f137" class="mc jy hi ly b fi md me l mf mg">predict_proba(string1, model, tokenizer)</span><span id="f631" class="mc jy hi ly b fi mh me l mf mg">&gt;&gt;&gt; array([<br/>&gt;&gt;&gt; [1.63417135e-05, 9.99983668e-01],<br/>&gt;&gt;&gt; [9.99986053e-01, 1.39580325e-05],<br/>&gt;&gt;&gt; [9.99986053e-01, 1.39833473e-05],        <br/>&gt;&gt;&gt; [9.99988914e-01, 1.11078716e-05]], dtype=float32)</span></pre><p id="3fc7" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated">该模型的预测非常准确。</p><h1 id="f4de" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">结论。</h1><p id="65e9" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">在本文中，您了解了如何在带有自定义小文本数据的二进制分类任务中，对DistilBert(来自Hugging Face Transformers库的预训练模型)及其tensor flow API进行微调。</p><p id="289e" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated"><em class="ll">本文的</em><a class="ae jw" href="https://colab.research.google.com/drive/14PtqRbrGa70M5NJnLfSllyJYyvl1nYnK?usp=sharing" rel="noopener ugc nofollow" target="_blank"><em class="ll">Google collab</em></a><em class="ll">笔记本。</em></p><p id="9a1c" class="pw-post-body-paragraph kp kq hi kr b ks lp is ku kv lq iv kx ky ls la lb lc lu le lf lg lw li lj lk hb bi translated"><em class="ll">一个</em> <a class="ae jw" href="https://huggingface.co/transformers/master/community.html#community-notebooks" rel="noopener ugc nofollow" target="_blank"> <em class="ll">抱脸文档页面</em> </a> <em class="ll">，其中重新组合了各地的资源🤗社区开发的变形金刚</em>。</p></div></div>    
</body>
</html>
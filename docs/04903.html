<html>
<head>
<title>AdaBoosting, Ensemble Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集成学习</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/adaboosting-640e8cc0c0a8?source=collection_archive---------54-----------------------#2021-07-05">https://medium.com/geekculture/adaboosting-640e8cc0c0a8?source=collection_archive---------54-----------------------#2021-07-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5d45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这只是我很久以前学到的另一个故事。</p><p id="e0a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Adaboost(自适应增强)是集成机器学习的一个例子。简而言之，我们可以说集成学习是一种机器学习方法，它从不同算法或相同算法创建的各种学习器中学习，以构建一个可以在数据上表现良好的强学习器。通过使用该算法的学习过程，可以基于不同的选择变量(即，数据特征)、参数、数据集来构造这些学习器。Boosting是一种集成算法，其中弱学习器按顺序创建，后继弱后继学习器从前任弱学习器所犯的错误中学习。</p><p id="40a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">工作在AdaBoost(自适应Boosting)后面:</em> </strong></p><p id="09ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Adaboost通常可以通过决策树算法来演示。该算法构造了许多称为Stumps(具有一层节点的决策树)的弱学习器。树桩通常具有数据的一个特征及其叶节点。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/9c030e228f485a96e8ae27471b7daaa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BSsAgTWjvPh60kQ_1v0QbA.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Fig 1: Sample dataset</figcaption></figure><p id="eaec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果前一个树桩的性能会影响下一个树桩的施工，则按顺序设计树桩。所有的树桩都有助于建立一个更强大或更准确的学习者或模型。</p><p id="5bd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让上表展示申请人的贷款批准数据，包括申请人的年龄、性别和收入范围(年龄是数字数据，性别和收入是分类数据)。这三个变量都有助于预测贷款批准状态的结果。</p><p id="61b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，各种薄弱的学习者(树桩)为每一个功能寻找根源。根的特征可以使用像基尼系数、熵、信息增益这样的标准来决定。在这种情况下，如果我们考虑基尼系数杂质，将考虑分配具有最低基尼系数杂质的贷款批准的特征。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ju"><img src="../Images/a2b2991880ab04fb3b14eb51d9eedfe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*lfwGCvQ8P2N5FHUNtuKROw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx">Fig 2: A stump</figcaption></figure><p id="d71a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">node Male的基尼杂质=(1-(1/3)*(1/3)-(2/3 * 2/3))= 0.444</p><p id="63ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">节点Female的Gini杂质=(1-(1*1)-0)=0(该节点中没有杂质，因为所有女性都获得了贷款)</p><p id="0cfc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">加权基尼杂质= (3/5)*0.44+(2/5)*0=0.266</p><p id="7caa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了简单起见，我们假设性别在所有特征中具有最低的基尼不纯指数。</p><p id="c563" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该数据集被赋予一个样本权重，该权重在适当的时候随着模型的性能而变化。假设每个样品的初始重量为1/5(1/存在的样品总数)。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jv"><img src="../Images/c8523b2a2ece8d8804b2cb48c62be716.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WlWzlNSfVEjiZQgKVzxCsA.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Fig 3: Sample weights associated</figcaption></figure><p id="bf26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用弱学习器对数据集进行预测。如果存在不正确的预测，则不正确预测的样本权重将增加一个因子，该因子由其在最终分类中的发言权/显著性的量(对最终分类的影响程度)决定。同样，正确预测的权重应该使用相同的显著性来降低。弱学习器中的这种变化能够帮助纠正要创建的下一个弱学习器中的不正确预测/分类的错误。这就是为什么说在boosting算法中，新创建的弱学习者依赖于他们的前辈所犯的错误或者从他们的前辈所犯的错误中学习。</p><p id="c695" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该公式证明了不正确预测的误差的显著性/重要性，</p><p id="0409" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数量/重要性= 1/2 log((1-总误差)/总误差)</p><p id="6178" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的总误差是被错误分类的样本重量的总和。</p><p id="2248" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设样本3和5在上述数据中被错误地分类，那么总误差将是(1/5+1/5)=2/5</p><p id="e7c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，重要性/数量=1/2log(1.5)=0.17/2= 0.085</p><p id="161f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算显著性值后，调整样本的权重。</p><p id="e1ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">为不正确的分类新增权重</em> </strong></p><p id="047e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">=原始样品重量* e^significance</p><p id="46ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">= 1/5*e^(0.085)=x(新重量)</p><p id="53fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">为正确分类新增权重</em> </strong></p><p id="35ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">=原始样品重量*e^(-significance)</p><p id="89a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">= 1/5*e^(-0.085)=y(新重量)</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jw"><img src="../Images/4c04f053316ed3ea126828f89ebdb012.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OdAdVBdd9K9bMJRKb5QxwA.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Fig 4: Adjusted weights attached</figcaption></figure><p id="a7f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">新分配的权重被调整(归一化)为总和1；现在，下一个stump将通过计算调整后的样本权重来知道哪些预测是不正确的，并在定义了一个bootstrap数据集(随机准备一个等长的数据集，从原始数据集中选择样本)后纠正错误。如此循环，直到学习者能够正确地对样本进行分类。</p><p id="4f77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">sklearn库有一个包含AdaBoostClassifier和AdaBoostRegressor的集成包，用于分类和回归应用。</p><p id="679f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总的来说，AdaBoost是遵循“从错误中学习”这句话的增强算法之一</p></div></div>    
</body>
</html>
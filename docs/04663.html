<html>
<head>
<title>Applying 7 Classification Algorithms on the Titanic Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Titanic数据集上应用7种分类算法</h1>
<blockquote>原文：<a href="https://medium.com/geekculture/applying-7-classification-algorithms-on-the-titanic-dataset-278ef222b53c?source=collection_archive---------3-----------------------#2021-07-01">https://medium.com/geekculture/applying-7-classification-algorithms-on-the-titanic-dataset-278ef222b53c?source=collection_archive---------3-----------------------#2021-07-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="d069" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">寻找最精确的算法！</em></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/cbc3047572eac47be530564b2c6738d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZQihVhO7g3mhG5oAJSccQ.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Photo by <a class="ae ju" href="https://unsplash.com/@oobiora22?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Olisa Obiora</a> on <a class="ae ju" href="https://unsplash.com/s/photos/titanic?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="dd4b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你刚开始接触数据科学，Kaggle上的<a class="ae ju" href="https://www.kaggle.com/c/titanic" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> Titanic:机器从灾难中学习</strong> </a>项目是学习分类算法的最好方法之一！在这篇文章中，我讲述了如何在这个数据集上应用不同的分类算法，并找出哪种算法能给我们最好的准确度。</p><p id="cdd9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我甚至将这些模型提交给了Kaggle竞赛，因此我们可以确切地了解每个模型在未知数据上的表现！</p><p id="26f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，如果你要使用这些模型中的任何一个，你必须清理你的数据。在这篇 文章中，我已经通过各种方式来清理和可视化泰坦尼克号数据集。因此，如果您想知道如何处理那些丢失的值，并想知道每个特性如何与<strong class="ih hj">幸存的</strong>列相关联，那么请先查看那篇文章。</p><div class="jv jw ez fb jx jy"><a href="https://eshitagoel.medium.com/eda-on-titanic-machine-learning-from-disaster-6b518bb97e17" rel="noopener follow" target="_blank"><div class="jz ab dw"><div class="ka ab kb cl cj kc"><h2 class="bd hj fi z dy kd ea eb ke ed ef hh bi translated">泰坦尼克号上的EDA:从灾难中学习机器</h2><div class="kf l"><h3 class="bd b fi z dy kd ea eb ke ed ef dx translated">学习机器学习中的基本EDA技术</h3></div><div class="kg l"><p class="bd b fp z dy kd ea eb ke ed ef dx translated">eshitagoel.medium.com</p></div></div><div class="kh l"><div class="ki l kj kk kl kh km jo jy"/></div></div></a></div><p id="0a76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看制作模型的先决条件:</p><h2 id="56ba" class="kn ko hi bd kp kq kr ks kt ku kv kw kx iq ky kz la iu lb lc ld iy le lf lg lh bi translated">将我们的数据存储在变量中</h2><p id="568c" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">Kaggle的数据可在。csv文件，我们将数据存储到变量中。</p><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="0f0c" class="kn ko hi lo b fi ls lt l lu lv">X = train[["Pclass","Sex","Age","Fare","Cabin","Prefix","Q","S","Family"]]<br/>Y = train["Survived"]<br/>X_TEST = test[["Pclass","Sex","Age","Fare","Cabin","Prefix","Q","S","Family"]]</span></pre><h2 id="d3e3" class="kn ko hi bd kp kq kr ks kt ku kv kw kx iq ky kz la iu lb lc ld iy le lf lg lh bi translated">数据的标准化</h2><p id="ae1b" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">尽管并非所有算法都需要标准化，但逻辑回归和K近邻等算法需要标准化，因为它们使用欧几里德距离或曼哈顿距离。因此，我们将数据标准化。</p><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="eba5" class="kn ko hi lo b fi ls lt l lu lv"><strong class="lo hj">from</strong> <strong class="lo hj">sklearn.preprocessing</strong> <strong class="lo hj">import</strong> StandardScaler<br/>sc = StandardScaler()<br/>X = sc.fit_transform(X)<br/>X_TEST =  sc.transform(X_TEST)</span></pre><p id="76bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意，我们仅使用训练数据来训练标准标量。这防止了训练和测试数据之间的任何信息泄漏。</p><h2 id="8d1e" class="kn ko hi bd kp kq kr ks kt ku kv kw kx iq ky kz la iu lb lc ld iy le lf lg lh bi translated">训练-测试-拆分</h2><p id="791e" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">我们对训练数据执行训练-测试-分割</p><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="9ad6" class="kn ko hi lo b fi ls lt l lu lv"><strong class="lo hj">from</strong> <strong class="lo hj">sklearn.model_selection</strong> <strong class="lo hj">import</strong> train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.2, random_state=1)</span></pre><h1 id="09b5" class="lw ko hi bd kp lx ly lz kt ma mb mc kx md me mf la mg mh mi ld mj mk ml lg mm bi translated">制作模型</h1><h2 id="5ff4" class="kn ko hi bd kp kq kr ks kt ku kv kw kx iq ky kz la iu lb lc ld iy le lf lg lh bi translated">1.k-最近邻算法</h2><p id="3125" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">如果选择了正确的K值，K-最近邻算法对于分类很有效。我们可以使用一个小的for循环来选择正确的k值，该循环测试1到20之间的每个k值的精度。</p><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="8df4" class="kn ko hi lo b fi ls lt l lu lv"><strong class="lo hj">from</strong> <strong class="lo hj">sklearn.neighbors</strong> <strong class="lo hj">import</strong> KNeighborsClassifier<br/><strong class="lo hj">from</strong> <strong class="lo hj">sklearn.metrics</strong> <strong class="lo hj">import</strong> accuracy_score<br/><br/>acc = []<br/><br/><strong class="lo hj">for</strong> i <strong class="lo hj">in</strong> range(1,20):<br/>    knn = KNeighborsClassifier(n_neighbors = i)<br/>    knn.fit(X_train,y_train)<br/>    yhat = knn.predict(X_test)<br/>    acc.append(accuracy_score(y_test,yhat))<br/>    print("For k = ",i," : ",accuracy_score(y_test,yhat))</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mn"><img src="../Images/b4efbd1e2a889955ba0237d2e2adb1ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FeAiYIFwl6Mjnf_abx-LHQ.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Image by Author</figcaption></figure><p id="c34d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了直观显示和比较这些值，我们绘制了一个线图，以找出哪个k值给出了最佳精度:</p><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="7c12" class="kn ko hi lo b fi ls lt l lu lv">plt.figure(figsize=(8,6))<br/>plt.plot(range(1,20),acc, marker = "o")<br/>plt.xlabel("Value of k")<br/>plt.ylabel("Accuracy Score")<br/>plt.title("Finding the right k")<br/>plt.xticks(range(1,20))<br/>plt.show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mo"><img src="../Images/639a967134a38c841337472acd7c1308.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Wkp7x3Ga3G5Oc0p-5e-NA.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Image by Author</figcaption></figure><p id="1857" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">给出最高精度的k的优选值是<strong class="ih hj"> k = 9。<br/> </strong>我们现在可以使用这个k值来制作我们的模型:</p><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="3c30" class="kn ko hi lo b fi ls lt l lu lv">KNN = KNeighborsClassifier(n_neighbors = 9)<br/>KNN.fit(X,Y)<br/>y_pred = KNN.predict(X_TEST)</span><span id="a0b5" class="kn ko hi lo b fi mp lt l lu lv">df_KNN = pd.DataFrame()<br/>df_KNN["PassengerId"] = test2["PassengerId"]<br/>df_KNN["Survived"] = y_pred</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mq"><img src="../Images/dd0495429252acb3998f1af420807153.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*j-R8xEvEaQPsIC_ybUE72A.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx">KNN Output</figcaption></figure><p id="7359" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们将这个模型提交给Kaggle竞赛来看看我们的模型表现如何时，我们得到的准确率分数是<strong class="ih hj"> 77.27% </strong></p><h2 id="d19c" class="kn ko hi bd kp kq kr ks kt ku kv kw kx iq ky kz la iu lb lc ld iy le lf lg lh bi translated">2.决策树算法</h2><p id="73f2" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">我们尝试了决策树算法来解决这个分类问题。我们需要找到决策树分割数据的正确深度，因为没有指定最大深度，模型很容易过度拟合。</p><p id="cc55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这可以使用交叉验证来完成，但是对于初学者来说，一个简单的for循环也可以帮助你比较选择哪个深度。</p><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="68fe" class="kn ko hi lo b fi ls lt l lu lv"><strong class="lo hj">from</strong> <strong class="lo hj">sklearn.tree</strong> <strong class="lo hj">import</strong> DecisionTreeClassifier<br/><br/>depth = [];<br/><br/><strong class="lo hj">for</strong> i <strong class="lo hj">in</strong> range(1,8):<br/>    clf_tree = DecisionTreeClassifier(criterion="entropy", random_state = 100, max_depth = i)<br/>    clf_tree.fit(X_train,y_train)<br/>    yhat = clf_tree.predict(X_test)<br/>    depth.append(accuracy_score(y_test,yhat))<br/>    print("For max depth = ",i, " : ",accuracy_score(y_test,yhat))</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mr"><img src="../Images/08c4e837b2f8158c2b58667b4e413519.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZgYevRww17qp-2D1haobNw.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Image by Author</figcaption></figure><p id="55ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我们也可以绘图，看看哪个深度给我们最准确的预测:</p><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="ce0f" class="kn ko hi lo b fi ls lt l lu lv">plt.figure(figsize=(8,6))<br/>plt.plot(range(1,8),depth,color="red", marker = "o")<br/>plt.xlabel("Depth of Tree")<br/>plt.ylabel("Accuracy Score")<br/>plt.title("Finding the right depth with highest accuracy")<br/>plt.xticks(range(1,8))<br/>plt.show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ms"><img src="../Images/775e18138a7ad32a95b9721eed498a60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fnUE1NJIhmdoxoOhz5IzPg.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Image by Author</figcaption></figure><p id="9f1d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度= 3时精度最高，我们现在用这个深度进行训练和预测。</p><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="9acf" class="kn ko hi lo b fi ls lt l lu lv">clf_tr = DecisionTreeClassifier(criterion="entropy", random_state = 100, max_depth = 3)<br/>clf_tr.fit(X,Y)<br/>pred_tree = clf_tr.predict(X_TEST)</span><span id="632b" class="kn ko hi lo b fi mp lt l lu lv">df_TREE = pd.DataFrame()<br/>df_TREE["PassengerId"] = test2["PassengerId"]<br/>df_TREE["Survived"] = pred_tree<br/>df_TREE.head()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mt"><img src="../Images/8bc98184ee00d9473e027acb91ccafc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*6MSiKR2UATrbPYe43-rZtg.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx">Decision Tree Output</figcaption></figure><p id="6179" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们将这个模型提交给Kaggle竞赛来看看我们的模型表现如何时，我们得到的准确率分数是<strong class="ih hj"> 78.46% </strong></p><h2 id="9c97" class="kn ko hi bd kp kq kr ks kt ku kv kw kx iq ky kz la iu lb lc ld iy le lf lg lh bi translated">3.随机森林算法</h2><p id="379f" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">随机森林是使用多个决策树的集成技术之一。我们可以看到它在我们的数据上的表现:</p><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="f1d6" class="kn ko hi lo b fi ls lt l lu lv"><strong class="lo hj">from</strong> <strong class="lo hj">sklearn.ensemble</strong> <strong class="lo hj">import</strong> RandomForestClassifier<br/><br/>clf_forest = RandomForestClassifier(random_state=0)<br/>clf_forest.fit(X_train,y_train)<br/>yhat = clf_forest.predict(X_test)<br/>print("Accuracy for training data : ",accuracy_score(y_test,yhat))</span><span id="e553" class="kn ko hi lo b fi mp lt l lu lv"><strong class="lo hj">Accuracy for training data :  0.776536312849162</strong></span></pre><p id="7a39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在保存它并提交给Kaggle</p><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="0bcb" class="kn ko hi lo b fi ls lt l lu lv">clf_for = RandomForestClassifier(random_state=0)<br/>clf_for.fit(X,Y)<br/>y_forest = clf_for.predict(X_TEST)</span><span id="200c" class="kn ko hi lo b fi mp lt l lu lv">df_FOREST = pd.DataFrame()<br/>df_FOREST["PassengerId"] = test2["PassengerId"]<br/>df_FOREST["Survived"] = y_forest<br/>df_FOREST.head()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mq"><img src="../Images/8134e733c4f18b31f8b60b1b630bfced.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*1ySGgHKMzVxVAdAFtrnEdg.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx">Random Forest Output</figcaption></figure><p id="1ff9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在kaggle上提交我们对这个模型的预测，参加<strong class="ih hj">泰坦尼克号:从灾难中学习机器</strong> Kaggle竞赛，并检查我们的准确性。</p><p id="03cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的准确率是<strong class="ih hj"> 77.27% </strong></p><h2 id="1e9b" class="kn ko hi bd kp kq kr ks kt ku kv kw kx iq ky kz la iu lb lc ld iy le lf lg lh bi translated">4.支持向量机</h2><p id="be98" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">我们尝试了支持向量机算法来解决这个分类问题。</p><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="21a7" class="kn ko hi lo b fi ls lt l lu lv"><strong class="lo hj">from</strong> <strong class="lo hj">sklearn.svm</strong> <strong class="lo hj">import</strong> SVC<br/>clf_svm = SVC(gamma='auto')<br/>clf_svm.fit(X_train,y_train)<br/>yhat = clf_svm.predict(X_test)</span><span id="98b7" class="kn ko hi lo b fi mp lt l lu lv">clf_SVM = SVC(gamma='auto')<br/>clf_SVM.fit(X,Y)<br/>pred_svm = clf_SVM.predict(X_TEST)</span><span id="0651" class="kn ko hi lo b fi mp lt l lu lv">df_SVM = pd.DataFrame()<br/>df_SVM["PassengerId"] = test2["PassengerId"]<br/>df_SVM["Survived"] = pred_svm<br/>df_SVM.head()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mu"><img src="../Images/b08e38900fa1e78cfaf76eb43bdeac9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*aIqgZJ1CM6ckVv1dud0nKQ.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx">SVM Output</figcaption></figure><p id="c52a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在Kaggle上提交我们对这个模型的预测，参加<strong class="ih hj">泰坦尼克号:从灾难中学习机器</strong> Kaggle竞赛，并检查我们的准确性</p><p id="2817" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的准确率是<strong class="ih hj"> 77.51% </strong></p><h2 id="bd59" class="kn ko hi bd kp kq kr ks kt ku kv kw kx iq ky kz la iu lb lc ld iy le lf lg lh bi translated">5.朴素贝叶斯算法</h2><p id="2dbb" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">我们尝试了朴素贝叶斯算法来解决这个分类问题。</p><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="3bc8" class="kn ko hi lo b fi ls lt l lu lv"><strong class="lo hj">from</strong> <strong class="lo hj">sklearn.naive_bayes</strong> <strong class="lo hj">import</strong> GaussianNB<br/>clf_NB = GaussianNB()<br/>clf_NB.fit(X_train,y_train)<br/>y_hat = clf_NB.predict(X_test)<br/>print("Accuracy for training data : ",accuracy_score(y_test,y_hat))</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mv"><img src="../Images/fe774b60529752ca1530b05d9dbcff6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_r255Ui7QZ-asqIfBlVfwQ.png"/></div></div></figure><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="9965" class="kn ko hi lo b fi ls lt l lu lv">clf_NB = GaussianNB()<br/>clf_NB.fit(X,Y)<br/>pred_NB = clf_NB.predict(X_TEST)</span><span id="a76b" class="kn ko hi lo b fi mp lt l lu lv">df_NB = pd.DataFrame()<br/>df_NB["PassengerId"] = test2["PassengerId"]<br/>df_NB["Survived"] = pred_NB<br/>df_NB.head()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mw"><img src="../Images/779d7f55f9f5b146299cf1a8196d39b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*igPbkJsUKM2y2tw7ZSK8dw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx">Naive Bayes Output</figcaption></figure><p id="c1cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在kaggle上提交了我们对这个模型的预测，用于<strong class="ih hj">泰坦尼克号:从灾难中学习机器</strong> Kaggle竞赛，并检查我们的准确性</p><p id="f7f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的准确率是<strong class="ih hj"> 72.72% </strong></p><h2 id="da50" class="kn ko hi bd kp kq kr ks kt ku kv kw kx iq ky kz la iu lb lc ld iy le lf lg lh bi translated">6.逻辑回归算法</h2><p id="5104" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">我们尝试使用逻辑回归算法来解决这个分类问题。</p><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="ff18" class="kn ko hi lo b fi ls lt l lu lv"><strong class="lo hj">from</strong> <strong class="lo hj">sklearn.linear_model</strong> <strong class="lo hj">import</strong> LogisticRegression<br/>regr = LogisticRegression(solver='liblinear', random_state=1)<br/>regr.fit(X_train,y_train)<br/>yhat = regr.predict(X_test)<br/>print("Accuracy for training data : ",accuracy_score(y_test,y_hat))</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mx"><img src="../Images/7701bd83e7d5d24e0f0a611ffd19e2fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ysfm3-IQ8ydC5LBzi9bWZw.png"/></div></div></figure><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="fc4a" class="kn ko hi lo b fi ls lt l lu lv">reg = LogisticRegression(solver='liblinear', random_state=1)<br/>reg.fit(X,Y)<br/>y_LR = reg.predict(X_TEST)</span><span id="b34d" class="kn ko hi lo b fi mp lt l lu lv">df_LR = pd.DataFrame()<br/>df_LR["PassengerId"] = test2["PassengerId"]<br/>df_LR["Survived"] = y_LR<br/>df_LR.head()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mt"><img src="../Images/0fc0e57ceb790c12916005b84fc17d95.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*Rki5L5WjOlYoy-yNzoTR7Q.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx">Logistic Regression Output</figcaption></figure><p id="7af7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在kaggle上提交了我们对这个模型的预测，用于<strong class="ih hj">泰坦尼克号:从灾难中学习机器</strong> Kaggle竞赛，并检查我们的准确性</p><p id="a537" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的准确率是<strong class="ih hj"> 76.55% </strong></p><h2 id="b076" class="kn ko hi bd kp kq kr ks kt ku kv kw kx iq ky kz la iu lb lc ld iy le lf lg lh bi translated">7.随机梯度下降分类器</h2><p id="924f" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">我们尝试使用随机梯度下降分类器来解决这个分类问题。</p><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="d3c3" class="kn ko hi lo b fi ls lt l lu lv"><strong class="lo hj">from</strong> <strong class="lo hj">sklearn.linear_model</strong> <strong class="lo hj">import</strong> SGDClassifier<br/><br/>clf_SGD = SGDClassifier(loss="squared_loss", penalty="l2", max_iter=4500,tol=-1000, random_state=1)<br/>clf_SGD.fit(X_train,y_train)<br/>yhat = clf_SGD.predict(X_test)<br/>print(accuracy_score(y_test,yhat))</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es my"><img src="../Images/e9c78749e197231c2091e9a9936b33d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*ariIDflZULQujDyXPENiPA.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx">Training Accuracy</figcaption></figure><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="f8c7" class="kn ko hi lo b fi ls lt l lu lv">clf_SGD = SGDClassifier(loss="squared_loss", penalty="l2", max_iter=4500, tol=-1000, random_state=1)<br/>clf_SGD.fit(X,Y)<br/>y_SGD = clf_SGD.predict(X_TEST)</span><span id="9e1d" class="kn ko hi lo b fi mp lt l lu lv">df_SGD = pd.DataFrame()<br/>df_SGD["PassengerId"] = test2["PassengerId"]<br/>df_SGD["Survived"] = y_SGD<br/>df_SGD.head()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mt"><img src="../Images/e4f2c5adfbc550736e5c4f4caa4bf9b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*jNq9udLIKkj66_RSGzNzvA.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx">SGD Classifier Output</figcaption></figure><p id="b57e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在kaggle上提交了我们对这个模型的预测，用于<strong class="ih hj">泰坦尼克号:从灾难中学习机器</strong> Kaggle竞赛，并检查我们的准确性</p><p id="d8d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的准确率是76.79% </p><h1 id="2e47" class="lw ko hi bd kp lx ly lz kt ma mb mc kx md me mf la mg mh mi ld mj mk ml lg mm bi translated">决赛成绩</h1><p id="cfe0" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">让我们试着画出目前为止我们得到的各种精度，看看哪个模型对我们来说表现最好:</p><pre class="jf jg jh ji fd ln lo lp lq aw lr bi"><span id="19d5" class="kn ko hi lo b fi ls lt l lu lv">plt.figure(figsize=(8,6))<br/>plt.plot(range(1,8),[KNN_accuracy,TREE_accuracy,FOREST_accuracy,SVM_accuracy,NB_accuracy,LR_accuracy,SGD_accuracy],marker='o')<br/>plt.xticks(range(1,8),['KNN','Decision Tree','Random Forest','SVM','Naive Bayes','Log Regression','SGD'],rotation=25)<br/>plt.title('Accuracy of Various Models')<br/>plt.xlabel('Model Names')<br/>plt.ylabel("Accuracy Score")<br/>plt.show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mz"><img src="../Images/73bc654896fc162a70b0e5c044ec1cf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E37YVx4BnpS4tGrGatHuSw.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx">Image by Author</figcaption></figure><p id="ff05" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">显然，<strong class="ih hj">决策树</strong>似乎给了我们最好的准确性。我们需要注意的是，这是我们在没有任何超参数调整或交叉验证的情况下直接应用模型时的情况。当我们采取额外的步骤时，我们的准确度一定会提高。但是这个简单的应用程序是学习将模型应用于分类问题的好方法！</p><p id="9b9d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这标志着我们的<strong class="ih hj">泰坦尼克号:机器从灾难中学习</strong>项目的结束！</p></div><div class="ab cl na nb gp nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="hb hc hd he hf"><p id="d533" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">你可以在我的GitHub上查看所有步骤的完整代码——清理数据、预处理、标准化和应用模型:</em></p><div class="jv jw ez fb jx jy"><a href="https://github.com/eshitagoel/Titanic_Survival/blob/master/Titanic%20Survival.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="jz ab dw"><div class="ka ab kb cl cj kc"><h2 class="bd hj fi z dy kd ea eb ke ed ef hh bi translated">eshitagoel/泰坦尼克号_生存</h2><div class="kf l"><h3 class="bd b fi z dy kd ea eb ke ed ef dx translated">预测泰坦尼克号上乘客的生还几率</h3></div><div class="kg l"><p class="bd b fp z dy kd ea eb ke ed ef dx translated">github.com</p></div></div><div class="kh l"><div class="nh l kj kk kl kh km jo jy"/></div></div></a></div></div></div>    
</body>
</html>